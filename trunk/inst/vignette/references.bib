% This file was created with JabRef 2.7.2.
% Encoding: UTF8

@ARTICLE{vanulden2006,
  author = {{van Ulden}, A. and {van Oldenborgh}, G.},
  title = {{Large-scale atmospheric circulation biases and changes in global
	climate model simulations and their importance for climate change
	in Central Europe}},
  journal = {Atmospheric Chemistry and Physics},
  year = {2006},
  volume = {6},
  pages = {863--881},
  number = {4},
  abstract = {The quality of global sea level pressure patterns has been assessed
	for simulations by 23 coupled climate models. Most models showed
	high pattern correlations. With respect to the explained spatial
	variance, many models showed serious large-scale deficiencies, especially
	at mid-latitudes. Five models performed well at all latitudes and
	for each month of the year. Three models had a reasonable skill.
	We selected the five models with the best pressure patterns for a
	more detailed assessment of their simulations of the climate in Central
	Europe. We analysed observations and simulations of monthly mean
	geostrophic flow indices and of monthly mean temperature and precipitation.
	We used three geostrophic flow indices: the west component and south
	component of the geostrophic wind at the surface and the geostrophic
	vorticity. We found that circulation biases were important, and affected
	precipitation in particular. Apart from these circulation biases,
	the models showed other biases in temperature and precipitation,
	which were for some models larger than the circulation induced biases.
	For the 21st century the five models simulated quite different changes
	in circulation, precipitation and temperature. Precipitation changes
	appear to be primarily caused by circulation changes. Since the models
	show widely different circulation changes, especially in late summer,
	precipitation changes vary widely between the models as well. Some
	models simulate severe drying in late summer, while one model simulates
	significant precipitation increases in late summer. With respect
	to the mean temperature the circulation changes were important, but
	not dominant. However, changes in the distribution of monthly mean
	temperatures, do show large indirect influences of circulation changes.
	Especially in late summer, two models simulate very strong warming
	of warm months, which can be attributed to severe summer drying in
	the simulations by these models. The models differ also significantly
	in the simulated warming of cold winter months. Finally, the models
	simulate rather different changes in North Atlantic sea surface temperature,
	which is likely to impact on changes in temperature and precipitation.
	These results imply that several important aspects of climate change
	in Central Europe are highly uncertain. Other aspects of the simulated
	climate change appear to be more robust. All models simulate significant
	warming all year round and an increase in precipitation in the winter
	half-year.},
  doi = {10.5194/acp-6-863-2006},
  owner = {rojasro},
  timestamp = {2011.02.07}
}

@ARTICLE{telinde+al2010,
  author = {{te Linde}, A. and Aerts, J. and Bakker, A. and Kwadijk, J.},
  title = {Simulating low--probability peak discharges for the {R}hine basin
	using resampled climate modeling data},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W03512},
  number = {3},
  abstract = {Climate change will increase winter precipitation, and in combination
	with earlier snowmelt it will cause a shift in peak discharge in
	the Rhine basin from spring to winter. This will probably lead to
	an increase in the frequency and magnitude of extreme floods. In
	this paper we aim to enhance the simulation of future low-probability
	flood peak events in the Rhine basin using different climate change
	scenarios, and downscaling methods. We use the output of a regional
	climate model (RCM) and a weather generator to create long, resampled
	time series (1000 years) of climate change scenarios as input for
	hydrological (daily) and hydrodynamic (hourly) modeling. We applied
	this approach to three parallel modeling chains, where the transformation
	method from different resampled RCM outputs to the hydrological model
	varied (delta change approach, direct output, and bias-corrected
	output). On the basis of numerous 1000 year model simulations, the
	results indicate a basin-wide increase in peak discharge in 2050
	of 8%--17% for probabilities between 1/10 and 1/1250 years. Furthermore,
	the results show that increasing the length of the climate data series
	using a weather generator reduced the statistical uncertainty when
	estimating low-probability flood peak events from 13% to 3%. We further
	conclude that bias-corrected direct RCM output is to be preferred
	over the delta change approach because it provides insight into geographical
	differences in discharge projections under climate change. Also,
	bias-corrected RCM output can simulate changes in the variance of
	temperature and rainfall and in the number of precipitation days,
	as changes in temporal structure are expected under climate change.
	These added values are of major importance when identifying future
	problem areas due to climate change and when planning potential adaptation
	measures},
  doi = {10.1029/2009WR007707},
  file = {:Simulating low-probability peak discharges for the Rhine basinf using resampled climate modeling data (te Linde et al. 2010).pdf:PDF},
  tags = {Impacts}
}

@ARTICLE{vangriensvenbauwens2003,
  author = {{van Griensven}, A. and Bauwens, W.},
  title = {Multiobjective autocalibration for semidistributed water quality
	models},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1348},
  number = {12},
  abstract = {SWAT is a simulator that integrates catchment and river water quantity
	and quality processes. The integration leads to a high number of
	model parameters, which complicates model calibration. As the model
	is semidistributed, the water quality and quantity variables at different
	observation sites inside the catchment can, and should, be used during
	this process, in order to use all the available information. A simultaneous
	use of all the different observed series and a high number of free
	parameters, however, creates a complex mathematical problem. Existing
	methods such as Pareto-optimization are practically very difficult,
	if not impossible, to implement. We present therefore a new methodology
	that reduces the many objective functions to a single global criterion
	in an objective way, excluding the weighting problem. The global
	criterion then is minimized using a global search algorithm, i.e.,
	the shuffled complex evolution method. The methodology is applied
	on the Dender River basin (Belgium), a heavily modified river basin
	with irregular flows.},
  doi = {10.1029/2003WR002284},
  keywords = {autocalibration, catchment, modeling, river, water quality, AUTOMATIC
	CALIBRATION, PARAMETER-ESTIMATION, GLOBAL OPTIMIZATION, MULTIPLE
	OBJECTIVES, HYDROLOGIC-MODELS, BIRKENES MODEL, METHODOLOGY},
  tags = {SWAT, Calibration}
}

@ARTICLE{vanulden+al2007,
  author = {{van Ulden}, A. and Lenderink, G. and {van den Hurk}, B. and {van
	Meijgaard}, E.},
  title = {{Circulation statistics and climate change in Central Europe: PRUDENCE
	simulations and observations}},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {179--192},
  number = {1},
  abstract = {PRUDENCE simulations of the climate in Central Europe are analysed
	with respect to mean temperature, mean precipitation and three monthly
	mean geostrophic circulation indices. The three global models show
	important circulation biases in the control climate, in particular
	in the strength of the west-circulations in winter and summer. The
	nine regional models inherit much of the circulation biases from
	their host model, especially in winter. In summer, the regional models
	show a larger spread in circulation statistics, depending on nesting
	procedures and other model characteristics. Simulated circulation
	biases appear to have a significant inluence on simulated temperature
	and precipitation. The PRUDENCE ensemble appears to be biased towards
	warmer and wetter than observed circulations in winter, and towards
	warmer and dryer circulations in summer. A2-scenario simulations
	show important circulation changes, which have a significant impact
	on changes in the distributions of monthly mean temperature and precipitation.
	It is likely that interactions between land–surface processes and
	atmospheric circulation play an important role in the simulated changes
	in the summer climate in Central Europe.},
  doi = {10.1007/s10584-006-9212-5},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@ARTICLE{vangriensvenmeixner2007,
  author = {{van Griensven}, A. and Meixner, T.},
  title = {A global and efficient multi-objective auto-calibration and uncertainty
	estimation method for water quality catchment models},
  journal = {Journal of Hydroinformatics},
  year = {2007},
  volume = {9},
  pages = {277--291},
  number = {4},
  note = {Description of the PARASOL algorithm, which provides parameter uncertainty
	(ONLY) without being based on assumptions on prior parameter distributions
	for the sampling strategy. They suggest to use Chi2 statistics, insted
	of Bayesian ones, because they account for the number of freee parameters,
	giving more realistic results and a NARROWER confidence region.},
  abstract = {Catchment water quality models have many parameters, several output
	variables and a complex structure leading to multiple minima in the
	objective function. General uncertainty/optimization methods based
	on random sampling (e.g. GLUE) or local methods (e.g. PEST) are often
	not applicable for theoretical or practical reasons. This paper presents
	"ParaSol", a method that performs optimization and uncertainty analysis
	for complex models such as distributed water quality models. Optimization
	is done by adapting the Shuffled Complex Evolution algorithm (SCE-UA)
	to account for multi-objective problems and for large numbers of
	parameters. The simulations performed by the SCE-UA are used further
	for uncertainty analysis and thereby focus the uncertainty analysis
	on solutions near the optimum/optima. Two methods have been developed
	that select "good" results out of these simulations based on an objective
	threshold. The first method is based on chi(2) statistics to delineate
	the confidence regions around the optimum/optima and the second method
	uses Bayesian statistics to define high probability regions. The
	ParaSol method was applied to a simple bucket model and to a Soil
	and Water Assessment Tool (SWAT) model Of Honey Creek, OH, USA. The
	bucket model case showed the success of the method in finding the
	minimum and the applicability of the statistics under importance
	sampling. Both cases showed that the confidence regions are very
	small when the chi(2) statistics are used and even smaller when using
	the Bayesian statistics. By comparing the ParaSol uncertainty results
	to those derived from 500,000 Monte Carlo simulations it was shown
	that the SCE-UA sampling used for ParaSol was more effective and
	efficient, as none of the Monte Carlo samples were close to the minimum
	or even within the confidence region defined by ParaSol.},
  doi = {doi:10.2166/hydro.2007.104},
  keywords = {auto-calibration, model, river basin, water quality, PARASOL, SWAT},
  tags = {Calibration, Uncertainty, SWAT},
  url = {http://www.iwaponline.com/jh/009/jh0090277.htm}
}

@ARTICLE{vangriensven+al2006,
  author = {{van Griensven}, A. and Meixner, T. and Grunwald, S. and Bishop,
	T. and Diluzio, M. and Srinivasan, R.},
  title = {A global sensitivity analysis tool for the parameters of multi-variable
	catchment models},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {324},
  pages = {10--23},
  number = {1--4},
  abstract = {Over-parameterisation is a well-known and often described problem
	in hydrological models, especially for distributed models. Therefore,
	methods to reduce the number of parameters via sensitivity analysis
	are important for the efficient use of these models. This paper describes
	a novel sampling strategy that is a combination of latin-hypercube
	and one-factor-at-a-time sampling that allows a global sensitivity
	analysis for a long list of parameters with only a limited number
	of model runs. The method is illustrated with an application of the
	water flow and water quality parameters of the distributed water
	quality program SWAT, considering flow, suspended sediment, total
	nitrogen, total phosphorus, nitrate and ammonia outputs at several
	locations in the Upper North Bosque River catchment in Texas and
	the Sandusky River catchment in Ohio. The application indicates that
	the methodology works successfully. The results also show that hydrologic
	parameters are dominant in controlling water quality predictions.
	Finally, the sensitivity results are not transferable between basins
	and thus the analysis needs to be conducted separately for each study
	catchment.},
  doi = {10.1016/j.jhydrol.2005.09.008},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{vangriensven+al2008,
  author = {{van Griensven},A. and Meixner, T. and Srinivasan, R. and Grunwald,
	S.},
  title = {Fit-for-purpose analysis of uncertainty using split-sampling evaluations},
  journal = {Hydrological Sciences Journal},
  year = {2008},
  volume = {53},
  pages = {1090--1103},
  abstract = {Support for this work was provided by the National Science Foundation
	through a CAREER award to T. Meixner (EAR-0094312). The research
	on the Sandusky River catchment was supported by the Florida Agricultural
	Experiment Station and approved for publication.},
  doi = {10.1029/2003WR002540,.},
  keywords = {uncertainty, modelling, fit-for-purpose, catchment, RAINFALL-RUNOFF
	MODELS, IMPROVED PARAMETER INFERENCE, CATCHMENT MODELS, AUTOMATIC
	CALIBRATION, SENSITIVITY-ANALYSIS, VARIABILITY, VALIDATION, MULTIPLE},
  tags = {Uncertainty, SWAT}
}

@ARTICLE{deRoo2001,
  author = {{de Roo}, A. and Odijk, M. and Shcmuck, G. and Koster, E. and Lucieer,
	A.},
  title = {Assessing the effects of land use changes on floods in the {M}euse
	and {O}der catchments},
  journal = {Physics and Chemistry of the Earth, Part B: Hydrology, Oceans and
	Atmosphere},
  year = {2001},
  volume = {26},
  pages = {593--599},
  number = {7--8},
  abstract = {Recently, dramatic flooding occurred in several regions of the world.
	To investigate the causes of the flooding and the influence of land
	use, soil characteristics and antecedent catchment moisture conditions,
	the distributed catchment model LISFLOOD has been developed. LISFLOOD
	simulates runoff in large river basins. Two transnational European
	river basins are used to test and validate the model: the Meuse catchment
	(France, Belgium, Germany and The Netherlands) and the Oder basin
	(The Czech Republic, Poland and Germany). In the Meuse and Oder catchment,
	land use change information over the past 200 years is processed
	at the moment. The LISFLOOD simulation model is used to simulate
	the effects of these land use changes on floods. Some influences
	of land use and vegetation on the water balance are clear, such as
	the changing vegetation cover (leaf area index) which will influence
	evapotranspiration. However, not so much is known about the influences
	of vegetation on soil properties, which influence infiltration, soil
	water redistribution, throughflow and groundwater recharge.},
  doi = {10.1016/S1464-1909(01)00054-5},
  owner = {rojasro},
  timestamp = {2010.08.10}
}

@ARTICLE{deRoo2000,
  author = {{de Roo}, A. and Wesseling, C. and {van Deuren}, W.},
  title = {Physically based river basin modelling within a GIS: the {LISFLOOD}
	model},
  journal = {Hydrological Processes},
  year = {2000},
  volume = {14},
  pages = {1981--1992},
  number = {11--12},
  abstract = {Although many geographical information systems (GISs) are very advanced
	in data processing and display, current GIS are not capable of physically
	based modelling. Especially, simulating transport of water and pollutants
	through landscapes is a problem in a GIS environment. A number of
	specific routing methods are needed in a GIS for hydrologic modelling,
	amongst these are the numerical solutions of the Saint-Venant equations,
	such as the kinematic wave approximation for transport of surface
	water in a landscape. The PCRaster Spatial Modelling language is
	a GIS capable of dynamic modelling. It has been extended recently
	with a kinematic wave approximation simulation tool to allow for
	physically based water flow modelling. The LISFLOOD model is an example
	of a physically based model written using the PCRaster GIS environment.
	The LISFLOOD model simulates river discharge in a drainage basin
	as a function of spatial data on topography, soils and land cover.
	Although hydrological modelling capabilities have largely increased,
	there is still a need for development of other routing methods, such
	as a diffusion wave.},
  doi = {10.1002/1099-1085(20000815/30)14:11/12<1981::AID-HYP49>3.0.CO;2-F},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@TECHREPORT{vanmeijgaard+al2008,
  author = {{van Meijgaard}, E. and {van Ulft}, L. and {van de Berg}, W. and
	Bosveld, F. and {van den Hurk}, B. and Lenderink, G. and Siebesma,
	A.},
  title = {{The KNMI regional atmospheric climate model RACMO, version 2.1}},
  institution = {KNMI},
  year = {2008},
  number = {TR-302},
  address = {{De Bilt, The Netherlands}},
  owner = {rojasro},
  timestamp = {2011.01.12}
}

@ARTICLE{vandenberghengelbrecht2006,
  author = {{van den Bergh}, F. and Engelbrecht, A.},
  title = {A study of particle swarm optimization particle trajectories},
  journal = {Information Sciences},
  year = {2006},
  volume = {176},
  pages = {937--971},
  number = {8},
  abstract = {Particle swarm optimization (PSO) has shown to be an efficient, robust
	and simple optimization algorithm. Most of the PSO studies are empirical,
	with only a few theoretical analyses that concentrate on understanding
	particle trajectories. These theoretical studies concentrate mainly
	on simplified PSO systems. This paper overviews current theoretical
	studies, and extend these studies to investigate particle trajectories
	for general swarms to include the influence of the inertia term.
	The paper also provides a formal proof that each particle converges
	to a stable point. An empirical analysis of multi-dimensional stochastic
	particles is also presented. Experimental results are provided to
	support the conclusions drawn from the theoretical findings},
  doi = {10.1016/j.ins.2005.02.003},
  tags = {PSO, Calibration}
}

@BOOK{demarsily1986,
  title = {Quantitative hydrogeology -- {G}roundwater hydrology for engineers},
  publisher = {Academic Press},
  year = {1986},
  author = {{de Marsily}, G.},
  pages = {440},
  address = {San Diego},
  edition = {First},
  abstract = {This book attempts to combine two separate themes: a description of
	one of the links in the chain of the water cycle inside the earth's
	crust i.e., the subsurface flow; and the quantification of the various
	types of this flow, obtained by applying the principles of fluid
	mechanics in porous media. The first part is the more descriptive,
	and geological of the two. It deals with the concept of water resources,
	which then leads us on to other links in the cycle: rainfall, infiltration,
	evaporation. runoff, and surface water resources. The second part
	is necessary in order to quantify ground water resources. It points
	the way to other applications, such as solutions to civil engineering
	problems including drainage and compaction; and transport problems
	in porous media, including aquifer pollution by miscible fluids,
	multiphase flow of immiscible fluids, and heat transfer in porous
	media, i.e., geothermal problems. However, the qualitative and the
	quantitative aspects are not treated separately but combined and
	blended together, just as geology and hydrology are woven together
	in hydrogeology.},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{demarsily2005,
  author = {{de Marsily}, G. and Delay, F. and Gon\ccalv\'es, J. and Renard,
	P. and Teles, V. and Violette, S.},
  title = {Dealing with spatial heterogeneity},
  journal = {Hydrogeology Journal},
  year = {2005},
  volume = {13},
  pages = {161--183},
  number = {1},
  abstract = {Heterogeneity can be dealt with by defining homogeneous equivalent
	properties, known as averaging, or by trying to describe the spatial
	variability of the rock properties from geologic observations and
	local measurements. The techniques available for these descriptions
	are mostly continuous Geostatistical models, or discontinuous facies
	models such as the Boolean, Indicator or Gaussian-Threshold models
	and the Markov chain model. These facies models are better suited
	to treating issues of rock strata connectivity, e.g. buried high
	permeability channels or low permeability barriers, which greatly
	affect flow and, above all, transport in aquifers. Genetic models
	provide new ways to incorporate more geology into the facies description,
	an approach that has been well developed in the oil industry, but
	not enough in hydrogeology. The conclusion is that future work should
	be focused on improving the facies models, comparing them, and designing
	new in situ testing procedures (including geophysics) that would
	help identify the facies geometry and properties. A world-wide catalog
	of aquifer facies geometry and properties, which could combine site
	genesis and description with methods used to assess the system, would
	be of great value for practical applications.},
  doi = {10.1007/s10040-004-0432-3},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@INPROCEEDINGS{demarsily2000,
  author = {{de Marsily}, G., Delhomme, J. and {Coudrain-Ribstein}, A. and, Lavenue,
	A.},
  title = {Four decades of inverse problems in hydrogeology},
  booktitle = {Theory, modelling and field investigation in hydrogeology: {A} special
	volume in honour of Shlomo P. Neuman's 60th birthday},
  year = {2000},
  editor = {Zhang, D. and Winter, C.},
  pages = {1--17},
  address = {Boulder, Colorado},
  publisher = {Geological Society of America Special paper 348},
  note = {demarsily2000},
  abstract = {We review the main stages of the evolution of ideas and methods for
	solving the inverse problem in hydrogeology; i.e., the identification
	of the transmissivity field in single-phase flow from piezometric
	data, in mainly steady-state and, occasionally, transient flow conditions.
	We first define the data needed to solve an inverse problem in hydrogeology,
	then describe the numerous approaches that have been developed over
	the past 40 years to solve it, emphasizing the major contributions
	made by Shlomo P. Neuman. Finally, we briefly discuss fitting processes
	that start by defining the unknown field as geological images (generated
	by Boolean or geostatistical methods). The early attempts at solving
	the inverse problem were direct, i.e., the transmissivity field was
	directly determined by using stream lines of the flow and inverting
	the flow equation along these lines. Faced with the poor results
	obtained in this manner, hydrogeologists have tried many different
	ways of minimizing the balance error representing an integral of
	the mass-balance error for each mesh for a given transmissivity field.
	These attempts were accompanied by constraints imposed on the transmissivity
	field in order to avoid instabilities. The idea then emerged that
	the unknown field should reproduce the local observations of the
	pressure at the measurement points instead of minimizing a balance
	error. Second, it should also satisfy a condition of plausibility,
	which means that the transmissivity field obtained through the inverse
	solution should not deviate too far from an a priori estimate of
	the real transmissivity field. This a priori notion led to the inclusion
	of a Bayesian approach resulting in the search for an optimal solution
	by maximum likelihood, as expounded later. Simultaneously, the existence
	of locally measured values in the transmissivity field (obtained
	by pumping tests) allowed geostatistical methods to be used in the
	formulation of the problem; the result of this innovation was that
	three major approaches came into being: (1) the definition of the
	a priori transmissivity field by kriging; (2) the method of cokriging;
	(3) the pilot point method. Furthermore, geostatistics made it possible
	to pose the inverse problem in a stochastic framework and to solve
	an ensemble of possible and equally probable fields, each of them
	equally acceptable as a solution.},
  doi = {10.1130/0-8137-2348-5.1},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@TECHREPORT{lisvap2008,
  author = {{van der knijff}, J.},
  title = {{LISVAP, Evaporation pre-processor for the LISFLOOD water balance
	and flood simulation model. Revised user manual}},
  institution = {Joint Research Centre, European Commission},
  year = {2008},
  number = {EUR22639 EN/2},
  owner = {rojasro},
  timestamp = {2011.06.22}
}

@TECHREPORT{lisflood2008,
  author = {{van der Knijff}, J. and {de Roo}, A.},
  title = {{LISFLOOD: Distributed water balance and flood simulation model.
	Revised user manual}},
  institution = {Institute for Environment and Sustainability. Joint Reserach Centre.
	European Commission, 108 pp},
  year = {2008},
  type = {JRC Scientific and Technical Reports.},
  number = {EUR 22166 EN/2},
  owner = {rojasro},
  timestamp = {2012.01.03}
}

@ARTICLE{lisflood2010,
  author = {{van der Knijff}, J. and Younis, J. and {de Roo}, A.},
  title = {{LISFLOOD: a GIS-based distributed model for river basin scale water
	balance and flood simulation}},
  journal = {International Journal of Geographical Information Science},
  year = {2010},
  volume = {24},
  pages = {189--212},
  number = {2},
  abstract = {In this paper we describe the spatially distributed LISFLOOD model,
	which is a hydrological model specifically developed for the simulation
	of hydrological processes in large European river basins. The model
	was designed to make the best possible use of existing data sets
	on soils, land cover, topography and meteorology. We give a detailed
	description of the simulation of hydrological processes in LISFLOOD,
	and discuss how the model is parameterized. We also describe how
	the model was implemented technically using a combination of the
	PCRaster GIS system and the Python programming language, and discuss
	the management of in- and output data. Finally, we review some recent
	applications of LISFLOOD, and we present a case study for the Elbe
	river.},
  doi = {10.1080/13658810802549154},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{vanroosmalen+al2009,
  author = {{van Roosmalen}, L. and Sonnenborg, T. and Jensen, K.},
  title = {Impact of climate and land use change on the hydrology of a large-scale
	agricultural catchment},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = { W00A15},
  abstract = {The authors wish to thank the Danish Water and Wastewater Association
	and Copenhagen Energy for their financial support to this study.},
  doi = {10.1029/2007WR006760},
  keywords = {REGIONAL CLIMATE, RIVER-BASIN, WATER-RESOURCES, GROUNDWATER RECHARGE,
	BRITISH-COLUMBIA, HIGH-RESOLUTION, DANISH RIVERS, MODEL, SIMULATIONS,
	CALIBRATION},
  tags = {Applications}
}

@ARTICLE{dewit+al2007,
  author = {{de Wit}, M. and {van den Hurk}, B. and Warmerdam, P. and Torfs,
	P. and Roulin, E. and {van Deursen, W.}},
  title = {Impact of climate change on low--flows in the river {M}euse},
  journal = {Climatic Change},
  year = {2007},
  volume = {82},
  pages = {351--372},
  number = {3--4},
  abstract = {In this study observed precipitation, temperature, and discharge records
	from the Meuse basin for the period 1911–2003 are analysed. The primary
	aim is to establish which meteorological conditions generate (critical)
	low-flows of the Meuse. This is achieved by examining the relationships
	between observed seasonal precipitation and temperature anomalies,
	and low-flow indices. Secondly, the possible impact of climate change
	on the (joint) occurrence of these low-flow generating meteorological
	conditions is addressed. This is based on the outcomes of recently
	reported RCM climate simulations for Europe given a scenario with
	increased atmospheric greenhouse-gas concentrations. The observed
	record (1911–2003) hints at the importance of multi-seasonal droughts
	in the generation of critical low-flows of the river Meuse. The RCM
	simulations point to a future with wetter winters and drier summers
	in Northwest Europe. No increase in the likelihood of multi-seasonal
	droughts is simulated. However, the RCM scenario runs produce multi-seasonal
	precipitation and temperature anomalies that are out of the range
	of the observed record for the period 1911–2003. The impact of climate
	change on low-flows has also been simulated with a hydrological model.
	This simulation indicates that climate change will lead to a decrease
	in the average discharge of the Meuse during the low-flow season.
	However, the model has difficulties to simulate critical low-flow
	conditions of the Meuse.},
  doi = {10.1007/s10584-006-9195-2},
  owner = {rojasro},
  timestamp = {2010.08.03}
}

@ARTICLE{vanliew+al2005,
  author = {{van Liew}, M.W. and Arnold, J.G. and Bosch, D.D},
  title = {Problems and Potential of Autocalibrating a Hydrologic Model},
  journal = {Transactions of the ASAE },
  year = {2005},
  volume = {48},
  pages = {1025-1040},
  number = {3},
  abstract = {An investigation was conducted to evaluate strengths and limitations
	of auto and manual calibration in the watershed scale model referred
	to as the Soil and Water Assessment Tool (SWAT). Performance of the
	model was tested on the Little River Experimental Watershed (LREW)
	in Georgia and the Little Washita River Experimental Watershed (LWREW)
	in Oklahoma, two USDA-ARS watersheds. A long record of multi-gage
	streamflow data on each of the watersheds was used for model calibration
	and validation. Model performance of the streamflow response in SWAT
	was assessed using a 6 parameter manual calibration based on mass
	balance and visual inspection of hydrographs and duration of daily
	flow curves, a 6 parameter autocalibration method based on the sum
	of squares of the residuals after ranking objective function (autoSSQR6),
	a 6 parameter method based on the sum of squares of residuals (SSQauto6),
	and an 11 parameter method based on the sum of square of residuals
	(SSQauto11). Results show that for both watersheds, the manual calibration
	generally outperformed the autocalibration methods based upon percent
	bias (PBIAS) and simulation of the range in magnitude of daily flows.
	For the calibration period on LREW subwatershed F, PBIAS was 0.0%,
	-24.0%, -21.5%, and +29.0% for the manual, SSQRauto6, SSQauto6, and
	SSQauto11 approaches, respectively. Based on the coefficient of efficiency,
	the SSQauto6 and SSQauto11 methods gave substantially better results
	than did the manual calibration on the LREW. On the LWREW, however,
	the manual approach did a better job estimating the coefficient of
	efficiency statistic. Results of this study suggest that for the
	practitioner who is faced with substantial time restrictions, the
	autocalibration option in SWAT provides a labor saving tool that
	shows promising results. However, manual adjustments following autocalibration
	may be necessary to maintain mass balance and adequately represent
	the range in magnitude of output variables. Caution should also be
	exercised in utilizing the autocalibration tool so that calibrated
	values in the model are representative of watershed conditions. },
  tags = {Calibration, SWAT}
}

@ARTICLE{vanliew+al2007,
  author = {{van Liew}, Michael W. and Veith, Tamie L. and Bosch, David D. and
	Arnold, Jeffrey G.},
  title = {Suitability of {SWAT} for the {C}onservation {E}ffects {A}ssessment
	{P}roject: Comparison on {USDA} {A}gricultural {R}esearch {S}ervice
	Watersheds},
  journal = {Journal of Hydrologic Engineering},
  year = {2007},
  volume = {12},
  pages = {173-189},
  number = {2},
  abstract = {Recent interest in tracking environmental benefits of conservation
	practices on agricultural watersheds throughout the United States
	has led to the development of the U.S. Department of Agriculture’s
	(USDA) Conservation Effects Assessment Project (CEAP). The purpose
	of CEAP is to assess environmental benefits derived from implementing
	various USDA conservation programs for cultivated, range, and irrigated
	lands. Watershed scale, hydrologic simulation models such as the
	Soil and Water Assessment Tool (SWAT) will be used to relate principal
	source areas of contaminants to transport paths and processes under
	a range in climatic, soils, topographic, and land use conditions
	on agricultural watersheds. To better understand SWAT’s strengths
	and weaknesses in simulating streamflow for anticipated applications
	related to CEAP, we conducted a study to evaluate the model’s performance
	under a range of climatic, topographic, soils, and land use conditions.
	Hydrologic responses were simulated on five USDA Agricultural Research
	Service watersheds that included Mahantango Creek Experimental Watershed
	in Pennsylvania and Reynolds Creek Experimental Watershed in Idaho
	in the northern part of the United States, and Little River Experimental
	Watershed in Georgia, Little Washita River Experimental Watershed
	in Oklahoma, and Walnut Gulch Experimental Watershed in Arizona in
	the south. Model simulations were performed on a total of 30 calibration
	and validation data sets that were obtained from a long record of
	multigauge climatic and streamflow data on each of the watersheds.
	A newly developed autocalibration tool for the SWAT model was employe
	d to calibrate eleven parameters that govern surface and subsurface
	response for the three southern watersheds, and an additional five
	parameters that govern the accumulation of snow and snowmelt runoff
	processes for the two northern watersheds. Based on a comparison
	of measured versus simulated average annual streamflow, SWAT exhibits
	an element of robustness in estimating hydrologic responses across
	a range in topographic, soils, and land use conditions. Differences
	in model performance, however, are noticeable on a climatic basis
	in that SWAT will generally perform better on watersheds in more
	humid climates than in desert or semidesert climates. The model may
	therefore be better suited for CEAP investigations in wetter regions
	of the eastern part of the United States that are predominantly cultivated
	than the dryer regions of the West that are more characteristically
	rangeland.},
  doi = {10.1061/(ASCE)1084-0699(2007)12:2(173)},
  tags = {SWAT, Applications, Calibration}
}

@TECHREPORT{ensembles2009,
  author = {{van der Linden}, P. and Mitchell, J.},
  title = {{ENSEMBLES}: {C}limate change and its impacts: {S}ummary of research
	and results from the {ENSEMBLES} project},
  institution = {Met Office Hadley Centre},
  year = {2009},
  owner = {rojasro},
  timestamp = {2010.06.21},
  url = {http://ensembles-eu.metoffice.com/docs/Ensembles_final_report_Nov09.pdf}
}

@ARTICLE{venderlinden2003,
  author = {{van der Linden}, S. and Christensen, J.},
  title = {Improved hydrological modeling for remote regions using a combinationof
	observed and simulated precipitation data},
  journal = {Journal of Geophysical Research},
  year = {2003},
  volume = {108},
  pages = {4072},
  number = {D2},
  abstract = {Precipitation, as simulated by climate models, can be used as input
	in hydrological models, despite possible biases both in the total
	annual amount simulated as well as the seasonal variation. Here we
	elaborated on a new technique, which adjusted precipitation data
	generated by a high-resolution regional climate model (HIRHAM4) with
	a mean-field bias correction using observed precipitation. A hydrological
	model (USAFLOW) was applied to simulate runoff using observed precipitation
	and a combination of observed and simulated precipitation as input.
	The method was illustrated for the remote Usa basin, situated in
	the European part of Arctic Russia, close to the Ural Mountains.
	It was shown that runoff simulations agree better with observations
	when the combined precipitation data set was used than when only
	observed precipitation was used. This appeared to be because the
	HIRHAM4 model data compensated for the absence of observed data from
	mountainous areas where precipitation is orographically enhanced.
	In both cases, the runoff simulated by USAFLOW was superior to the
	runoff simulated within the HIRHAM4 model itself. This was attributed
	to the rather simplistic description of the water balance in the
	HIRHAM4 model compared to a more complete representation in USAFLOW.},
  doi = {10.1029/2001JD001420},
  owner = {rojasro},
  timestamp = {2010.06.23}
}

@ARTICLE{vanpelt+al2009,
  author = {{van Pelt}, S. and Kabat, P. and {ter Maat}, H. and {van den Hurk},
	B. and Weerts, A.},
  title = {Discharge simulations performed with a hydrological model using bias
	corrected regional climate model input},
  journal = {Hydrology and Earth System Sciences},
  year = {2009},
  volume = {13},
  pages = {2387--2397},
  number = {12},
  abstract = {Studies have demonstrated that precipitation on Northern Hemisphere
	mid-latitudes has increased in the last decades and that it is likely
	that this trend will continue. This will have an influence on discharge
	of the river Meuse. The use of bias correction methods is important
	when the effect of precipitation change on river discharge is studied.
	The objective of this paper is to investigate the effect of using
	two different bias correction methods on output from a Regional Climate
	Model (RCM) simulation. In this study a Regional Atmospheric Climate
	Model (RACMO2) run is used, forced by ECHAM5/MPIOM under the condition
	of the SRES-A1B emission scenario, with a 25 km horizontal resolution.
	The RACMO2 runs contain a systematic precipitation bias on which
	two bias correction methods are applied. The first method corrects
	for the wet day fraction and wet day average (WD bias correction)
	and the second method corrects for the mean and coefficient of variance
	(MV bias correction). The WD bias correction initially corrects well
	for the average, but it appears that too many successive precipitation
	days were removed with this correction. The second method performed
	less well on average bias correction, but the temporal precipitation
	pattern was better. Subsequently, the discharge was calculated by
	using RACMO2 output as forcing to the HBV-96 hydrological model.
	A large difference was found between the simulated discharge of the
	uncorrected RACMO2 run, the WD bias corrected run and the MV bias
	corrected run. These results show the importance of an appropriate
	bias correction.},
  doi = {10.5194/hess-13-2387-2009},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{abbaspour+al2009,
  author = {Abbaspour, K. and Faramarzi, M. and Ghasemi, S. and Yang, H.},
  title = {Assessing the impact of climate change on water resources in {I}ran},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W10434},
  number = {10},
  abstract = {As water resources become further stressed due to increasing levels
	of societal demand, understanding the effect of climate change on
	various components of the water cycle is of strategic importance
	in management of this essential resource. In this study, we used
	a hydrologic model of Iran to study the impact of future climate
	on the country's water resources. The hydrologic model was created
	using the Soil and Water Assessment Tool (SWAT) model and calibrated
	for the period from 1980 to 2002 using daily river discharges and
	annual wheat yield data at a subbasin level. Future climate scenarios
	for periods of 2010--2040 and 2070--2100 were generated from the
	Canadian Global Coupled Model (CGCM 3.1) for scenarios A1B, B1, and
	A2, which were downscaled for 37 climate stations across the country.
	The hydrologic model was then applied to these periods to analyze
	the effect of future climate on precipitation, blue water, green
	water, and yield of wheat across the country. For future scenarios
	we found that in general, wet regions of the country will receive
	more rainfall while dry regions will receive less. Analysis of daily
	rainfall intensities indicated more frequent and larger-intensity
	floods in the wet regions and more prolonged droughts in the dry
	regions. When aggregated to provincial levels, the differences in
	the predictions due to the three future scenarios were smaller than
	the uncertainty in the hydrologic model. However, at the subbasin
	level the three climate scenarios produced quite different results
	in the dry regions of the country, although the results in the wet
	regions were more or less similar. },
  doi = {10.1029/2008WR007615},
  keywords = {SWAT, CLimate Change impacts, SUFI-2, uncertainty},
  tags = {SWAT, Thesis, Climate Change, application, Uncertainty}
}

@ARTICLE{abbaspour+al2007,
  author = {Abbaspour, K. and Yang, J. and Maximov, I. and Siber, R. and Bogner,
	K. and Mieleitner, J. and Zobrist, J. and Srinivasan, R.},
  title = {Modelling hydrology and water quality in the pre-alpine/alpine {T}hur
	watershed using {SWAT}},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {333},
  pages = {413--430},
  number = {2-4},
  abstract = {In a national effort, since 1972, the Swiss Government started the
	{``}National Long-term Monitoring of Swiss Rivers{''} (NADUF) program
	aimed at evaluating the chemical and physical states of major rivers
	leaving Swiss political boundaries. The established monitoring network
	of 19 sampling stations included locations on all major rivers of
	Switzerland. This study complements the monitoring program and aims
	to model one of the program{'}s catchments {--} Thur River basin
	(area 1700 km2), which is located in the north-east of Switzerland
	and is a direct tributary to the Rhine. The program SWAT (Soil and
	Water Assessment Tool) was used to simulate all related processes
	affecting water quantity, sediment, and nutrient loads in the catchment.
	The main objectives were to test the performance of SWAT and the
	feasibility of using this model as a simulator of flow and transport
	processes at a watershed scale. Model calibration and uncertainty
	analysis were performed with SUFI-2 (Sequential Uncertainty FItting
	Ver. 2), which was interfaced with SWAT using the generic iSWAT program.
	Two measures were used to assess the goodness of calibration: (1)
	the percentage of data bracketed by the 95\% prediction uncertainty
	calculated at the 2.5 and 97.5 percentiles of the cumulative distribution
	of the simulated variables, and (2) the d-factor, which is the ratio
	of the average distance between the above percentiles and the standard
	deviation of the corresponding measured variable. These statistics
	showed excellent results for discharge and nitrate and quite good
	results for sediment and total phosphorous. We concluded that: in
	watersheds similar to Thur {--} with good data quality and availability
	and relatively small model uncertainty {--} it is feasible to use
	SWAT as a flow and transport simulator. This is a precursor for watershed
	management studies.},
  doi = {10.1016/j.jhydrol.2006.09.014},
  keywords = {Watershed modelling, Water quality modelling, Calibration, Uncertainty
	analysis, SWAT, SUFI-2},
  tags = {SWAT, Uncertainty}
}

@INPROCEEDINGS{abbaspour+al2008,
  author = {Abbaspour, K. and Yang, J. and Vejdani, M. and Haghighat, S.},
  title = {{SWAT-CUP}: calibration and uncertainty programs for {SWAT}},
  booktitle = {4th Int. {SWAT} Conf. Proc.},
  year = {2008},
  note = {in press.},
  keywords = {SWAT-CUP reference},
  tags = {SWAT, Uncertainty}
}

@MANUAL{abbaspour2007,
  title = {{SWAT-CUP2}: {SWAT} calibration and uncertainty analysis programs
	- A User Manual. 95pp},
  author = {Karim C. Abbaspour},
  address = {Eawag: {S}wiss {F}ed. {I}nst. of {A}quat. {S}ci. and {T}echnol. D{\"u}bendorf,
	Switzerland},
  year = {2008},
  note = {{A}vailable at \url{http://www.eawag.ch/organisation/abteilungen/siam/software/swat/index_EN}},
  tags = {SWAT, Uncertainty}
}

@ARTICLE{abebe+al2010,
  author = {Abebe, N. and Ogden, F. and Pradhan, N.},
  title = {{Sensitivity and uncertainty analysis of the conceptual HBV rainfall-runoff
	model: Implications for parameter estimation}},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {389},
  pages = {301--310},
  number = {3-4},
  abstract = {HBV is a conceptual hydrological model extensively used in operational
	hydrological forecasting and water balance studies. In this paper,
	we apply the HBV model on the 1924 km2 semi-humid Leaf River catchment
	near Collins, Mississippi. We analyze individual sensitivity of the
	parameters by calibrating the model using the Multi-Objective Shuffled
	Complex Evolution (MOSCEM) algorithm, perform Monte-Carlo based identifiability
	analysis and investigate the dynamic behavior of the parameters using
	the Dynamic Identifiability Analysis (DYNIA) approach in reference
	to the hydrological process in the catchment. The sensitivity analysis
	using two objective measures showed that there are distinct groups
	of parameters that control total runoff volume errors and errors
	from the high-flow series. The DYNIA analysis revealed that parameters
	have specific periods where they show higher identifiability and
	play a crucial role in representing the predicted stream flow. Temporal
	changes of parameter optima were observed due either to inadequacies
	in the model structure or possible time-varying catchment response
	subject to unsteady hydrodynamic and hydroclimatic conditions.},
  doi = {10.1016/j.jhydrol.2010.06.007},
  keywords = {HBV, Sensitivity analysis, Identifiability analysis, DYNIA, Model
	structure},
  tags = {Sensitivity Analysis, Calibration, Uncertainty}
}

@ARTICLE{abrahart2002,
  author = {Abrahart, R. and See, L.},
  title = {Multi--model data fusion for river flow forecasting: {A}n evaluation
	of six alternative methods based on two contrasting catchments},
  journal = {Hydrology and Earth System Sciences},
  year = {2002},
  volume = {6},
  pages = {655-670},
  number = {4},
  abstract = {This paper evaluates six published data fusion strategies for hydrological
	forecasting based on two contrasting catchments: the River Ouse and
	the Upper River Wye. The input level and discharge estimates for
	each river comprised a mixed set of single model forecasts. Data
	fusion was performed using: arithmetic-averaging, a probabilistic
	method in which the best model from the last time step is used to
	generate the current forecast, two different neural network operations
	and two different soft computing methodologies. The results from
	this investigation are compared and contrasted using statistical
	and graphical evaluation. Each location demonstrated several options
	and potential advantages for using data fusion tools to construct
	superior estimates of hydrological forecast. Fusion operations were
	better in overall terms in comparison to their individual modelling
	counterparts and two clear winners emerged. Indeed, the six different
	mechanisms on test revealed unequal aptitudes for fixing different
	categories of problematic catchment behaviour and, in such cases,
	the best method(s) were a good deal better than their closest rival(s).
	Neural network fusion of differenced data provided the best solution
	for a stable regime (with neural network fusion of original data
	being somewhat similar) — whereas a fuzzified probabilistic mechanism
	produced a superior output in a more volatile environment. The need
	for a data fusion research agenda within the hydrological sciences
	is discussed and some initial suggestions are presented.},
  doi = {10.5194/hess-6-655-2002},
  owner = {Rodrigo},
  timestamp = {2008.04.23}
}

@ARTICLE{ajami2005,
  author = {Ajami, N. and Duan, Q. and Gao, X. and Sorooshian, S.},
  title = {Multimodel combination techniques for hydrologic forecasting: {A}pplication
	to distributed model intercomparison project results},
  journal = {Journal of Hydrometeorology},
  year = {2005},
  volume = {7},
  pages = {755-768},
  number = {4},
  abstract = {This paper examines several multimodel combination techniques that
	are used for streamflow forecasting: the simple model average (SMA),
	the multimodel superensemble (MMSE), modified multimodel superensemble
	(M3SE), and the weighted average method (WAM). These model combination
	techniques were evaluated using the results from the Distributed
	Model Intercomparison Project (DMIP), an international project sponsored
	by the National Weather Service (NWS) Office of Hydrologic Development
	(OHD). All of the multimodel combination results were obtained using
	uncalibrated DMIP model simulations and were compared against the
	best-uncalibrated as well as the best-calibrated individual model
	results. The purpose of this study is to understand how different
	combination techniques affect the accuracy levels of the multimodel
	simulations. This study revealed that the multimodel simulations
	obtained from uncalibrated single-model simulations are generally
	better than any single-member model simulations, even the best-calibrated
	single-model simulations. Furthermore, more sophisticated multimodel
	combination techniques that incorporated bias correction step work
	better than simple multimodel average simulations or multimodel simulations
	without bias correction.},
  doi = {10.1175/JHM519.1},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{ajami2007,
  author = {Ajami, N. and Duan, Q. and Sorooshian, S.},
  title = {An integrated hydrologic {B}ayesian multimodel combination framework:
	{C}onfronting input, parameter, and model structural uncertainty
	in hydrologic prediction},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W01403},
  abstract = {The conventional treatment of uncertainty in rainfall-runoff modeling
	primarily attributes uncertainty in the input-output representation
	of the model to uncertainty in the model parameters without explicitly
	addressing the input, output, and model structural uncertainties.
	This paper presents a new framework, the Integrated Bayesian Uncertainty
	Estimator (IBUNE), to account for the major uncertainties of hydrologic
	rainfall-runoff predictions explicitly. IBUNE distinguishes between
	the various sources of uncertainty including parameter, input, and
	model structural uncertainty. An input error model in the form of
	a Gaussian multiplier has been introduced within IBUNE. These multipliers
	are assumed to be drawn from an identical distribution with an unknown
	mean and variance which were estimated along with other hydrological
	model parameters by a Monte Carlo Markov Chain (MCMC) scheme. IBUNE
	also includes the Bayesian model averaging (BMA) scheme which is
	employed to further improve the prediction skill and address model
	structural uncertainty using multiple model outputs. A series of
	case studies using three rainfall-runoff models to predict the streamflow
	in the Leaf River basin, Mississippi, are used to examine the necessity
	and usefulness of this technique. The results suggest that ignoring
	either input forcings error or model structural uncertainty will
	lead to unrealistic model simulations and incorrect uncertainty bounds.},
  doi = {10.1029/2005WR004745},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{ajami+al2004,
  author = {Ajami, N. and Gupta, H. and Wagener, T. and Sorooshian, S.},
  title = {Calibration of a semi-distributed hydrologic model for streamflow
	estimation along a river system},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {298},
  pages = {112--135},
  number = {1-4},
  abstract = {An important goal of spatially distributed hydrologic modeling is
	to provide estimates of streamflow (and river levels) at any point
	along the river system. To encourage collaborative research into
	appropriate levels of model complexity, the value of spatially distributed
	data, and methods suitable for model development and calibration,
	the US National Weather Service Hydrology Laboratory (NWSHL) is promoting
	the distributed modeling intercomparison project (DMIP). In particular,
	the project is interested in how spatially distributed estimates
	of precipitation provided by the next generation radar (NEXRAD) network,
	high resolution digital elevation models (DEM), soil, land-use and
	vegetation data can be integrated into an improved system for distributed
	hydrologic modeling that provides more accurate and informative flood
	forecasts.},
  doi = {10.1016/j.hydrol.2004.03.033},
  keywords = {NEXRAD, distributed hydrologic modeling, calibration, flow forecasting,
	DISTRIBUTED CATCHMENT MODEL, AUTOMATIC CALIBRATION, SPATIAL VARIABILITY,
	RUNOFF, RAINFALL, PRECIPITATION, UNCERTAINTY, INFORMATION, SENSITIVITY,
	PREDICTION},
  tags = {Calibration}
}

@ARTICLE{ajami2008,
  author = {Ajami, N. and Hornberger, G. and Sunding, D.},
  title = {Sustainable water resource management under hydrological uncertainty},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W11406},
  abstract = {A proper understanding of the sources and effects of uncertainty is
	needed to achieve the goals of reliability and sustainability in
	water resource management and planning. Many studies have focused
	on uncertainties relating to climate inputs (e.g., precipitation
	and temperature), as well as those related to supply and demand relationships.
	In the end-to-end projection of the hydrological impacts of climate
	variability, however, hydrological uncertainties have often been
	ignored or addressed indirectly. In this paper, we demonstrate the
	importance of hydrological uncertainties for reliable water resources
	management. We assess the uncertainties associated with hydrological
	inputs, parameters, and model structural uncertainties using an integrated
	Bayesian uncertainty estimator framework. Subsequently, these uncertainties
	are propagated through a simple reservoir management model in order
	to evaluate how various operational rules impact the characteristics
	of the downstream uncertainties, such as the width of the uncertainty
	bounds. By considering different operational rules, we examine how
	hydrological uncertainties impact reliability, resilience, and vulnerability
	of the management system. The results of this study suggest that
	a combination of operational rules (i.e., an adaptive operational
	approach) is the most reliable and sustainable overall management
	strategy.},
  doi = {10.1029/2007WR006736},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{akaike1974,
  author = {Akaike, H.},
  title = {A new look at the statistical model identification},
  journal = {IEEE Transactions on Automatic Control},
  year = {1974},
  volume = {19},
  pages = {716--723},
  number = {6},
  abstract = {The history of the development of statistical hypothesis testing in
	time series analysis is reviewed briefly and it is pointed out that
	the hypothesis testing procedure is not adequately defined as the
	procedure for statistical model identilication. The classical maximum
	likelihood estimation procedure is reviewed and a new estimate minimum
	information theoretical criterion (AIC) estimate (MAICE) which is
	designed for the purpose of statistical identification is introduced.
	When there are several competing models the MAICE is defined by the
	model and the maximum likelihood estimates of the parameters which
	give the minimum of AIC defined by AIC = (-2)log(maximum likelihood)
	+ 2(number of independently adjusted parameters within the model).
	MAICE provides a versatile procedure for statistical model identification
	which isf ree from the ambiguities inherienn tth e application of
	conventional hypothesis testing procedure. The practical utility
	of MAICE in time series analysis is demonstratwedit h some numerical
	examples.},
  file = {:E\:\\rojasro\\My Documents\\articles\\A new look at the statistical model identification (Akaike, H. 1974).pdf:PDF},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1100705&userType=inst}
}

@ARTICLE{alcamo2003,
  author = {Alcamo, J. and D\"oll, P. and Henrichs, T. and Kaspar, F. and Lehner,
	B. and R\"osch, T. and Siebert, S.},
  title = {Development and testing of the {WaterGAP 2} global model of water
	use and availability},
  journal = {Hydrological Sciences Journal},
  year = {2003},
  volume = {48},
  pages = {317--337},
  number = {3},
  month = {June},
  abstract = {Growing interest in global environmental issues has led to the need
	for global and regional assessment of water resources. A global water
	assessment model called “WaterGAP 2” is described, which consists
	of two main components'a Global Water Use model and a Global Hydrology
	model. These components are used to compute water use and availability
	on the river basin level. The Global Water Use model consists of
	(a) domestic and industry sectors which take into account the effect
	of structural and technological changes on water use, and (b) an
	agriculture sector which accounts especially for the effect of climate
	on irrigation water requirements. The Global Hydrology model calculates
	surface runoff and groundwater recharge based on the computation
	of daily water balances of the soil and canopy. A water balance is
	also performed for surface waters, and river flow is routed via a
	global flow routing scheme. The Global Hydrology model provides a
	testable method for taking into account the effects of climate and
	land cover on runoff. The components of the model have been calibrated
	and tested against data on water use and runoff from river basins
	throughout the world. Although its performance can and needs to be
	improved, the WaterGAP 2 model already provides a consistent method
	to fill in many of the existing gaps in water resources data in many
	parts of the world. It also provides a coherent approach for generating
	scenarios of changes in water resources. Hence, it is especially
	useful as a tool for globally comparing the water situation in river
	basins.},
  doi = {10.1623/hysj.48.3.317.45290},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@INCOLLECTION{alcamo+al2007,
  author = {Alcamo, J. and Moreno, J. and Nov\'aky, B. and Bondi, M. and Corobov,
	R. and Devoy, R. and Giannakopoulos, C. and Martin, E. and Olesen,
	J. and Shividenko, A.},
  title = {Europe},
  booktitle = {Climate change 2007: impacts, adaptation and vulnerability. Contribution
	of working group II to the fourth assessment report of the intergovernmental
	panel on climate change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {M. L. Parry and O. F. Canziani and J. P. Palutikof and P. J. {van
	der Linden} and C. E. Hanson},
  pages = {541--580},
  address = {UK, Cambridge},
  tags = {IPCC}
}

@ARTICLE{alcolea2006,
  author = {Alcolea, A. and Carrera, J. and Medina, A.},
  title = {Pilot points method incorporating prior information for solving the
	groundwater flow inverse problem},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {1678--1689},
  number = {11},
  abstract = {The pilot points method is often used in nonlinear geostatistical
	calibration. The method consists of estimating the values of the
	hydraulic properties at a set of arbitrary (pilot) points so as to
	best fit the aquifer response as measured by available indirect observations
	(i.e., heads or drawdowns). Though this method remains general and
	appealing, no prior information of the hydraulic properties is usually
	included in the optimization process, which constrains the number
	of pilot points to ensure stability. In this paper, we present a
	modification of the pilot points method, including prior information
	in the optimization process by adding a plausibility term to the
	objective function to be minimized. This results from formulating
	the inverse problem in a maximum likelihood framework. The performance
	of the method is tested on a synthetic example. Results show that
	including the plausibility term improves the identification of heterogeneity.
	Furthermore, this term makes the inverse problem more stable and
	allows the use of larger number of pilot points, thus improving the
	identification of the heterogeneity as well. Therefore, the use of
	the plausibility term is recommended.},
  doi = {10.1016/j.advwatres.2005.12.009},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{aliKaelo2008,
  author = {Ali, M. and Kaelo, P.},
  title = {Improved particle swarm algorithms for global optimization},
  journal = {Applied Mathematics and Computation},
  year = {2008},
  volume = {196},
  pages = {578--593 },
  number = {2},
  abstract = {Particle swarm optimization algorithm has recently gained much attention
	in the global optimization research community. As a result, a few
	variants of the algorithm have been suggested. In this paper, we
	study the efficiency and robustness of a number of particle swarm
	optimization algorithms and identify the cause for their slow convergence.
	We then propose some modifications in the position update rule of
	particle swarm optimization algorithm in order to make the convergence
	faster. These modifications result in two new versions of the particle
	swarm optimization algorithm. A numerical study is carried out using
	a set of 54 test problems some of which are inspired by practical
	applications. Results show that the new algorithms are much more
	robust and efficient than some existing particle swarm optimization
	algorithms. A comparison of the new algorithms with the differential
	evolution algorithm is also made.},
  doi = {10.1016/j.amc.2007.06.020},
  keywords = {Particle swarm, Global optimization, Population set, Differential
	evolution},
  tags = {PSO, Calibration}
}

@ARTICLE{allenIngram2002,
  author = {Allen, M. and Ingram, W.},
  title = {Constraints on future changes in climate and the hydrologic cycle},
  journal = {Nature},
  year = {2002},
  volume = {419},
  pages = {224--232},
  number = {6903},
  abstract = {What can we say about changes in the hydrologic cycle on 50-year timescales
	when we cannot predict rainfall next week? Eventually, perhaps, a
	great deal: the overall climate response to increasing atmospheric
	concentrations of greenhouse gases may prove much simpler and more
	predictable than the chaos of short-term weather. Quantifying the
	diversity of possible responses is essential for any objective, probability-based
	climate forecast, and this task will require a new generation of
	climate modelling experiments, systematically exploring the range
	of model behaviour that is consistent with observations. It will
	be substantially harder to quantify the range of possible changes
	in the hydrologic cycle than in global-mean temperature, both because
	the observations are less complete and because the physical constraints
	are weaker.},
  doi = {10.1038/nature01092},
  keywords = {ATLANTIC THERMOHALINE CIRCULATION, NORTH-ATLANTIC, GLOBAL PRECIPITATION,
	SOUTHERN OSCILLATIONS, WATER-VAPOR, ATMOSPHERE, MODEL, VARIABILITY,
	TRENDS, FREQUENCY}
}

@ARTICLE{allen+al2001,
  author = {Allen, M. and Raper, S. and Mitchell, J.},
  title = {Climate change. {U}ncertainty in the {IPCC}'s Third Assessment Report},
  journal = {Science},
  year = {2001},
  volume = {293},
  pages = {430--433},
  number = {5529},
  abstract = {Reilly et al. (1) (page 430) raise several important points regarding
	the explanation and presentation of the climate change issue. They
	criticize the treatment of uncertainty in the Third Assessment Report
	(TAR) of the Intergovernmental Panel on Climate Change (IPCC), citing
	in particular the lack of an estimate of the probability that human-induced
	warming over the 1990-2100 period will lie either above or below
	the projected range of 1.4º to 5.8ºC. Wigley and Raper (2) provide
	such an estimate based on an exhaustive perturbation analysis of
	a simple climate model. Here, we address why the authors of the TAR
	were not in a position to provide a probabilistic forecast of 2100
	temperatures, although a consensus statement could be made about
	the likelihood of different warming rates on shorter (50-year) time
	scales.},
  doi = {10.1126/science.1062823},
  pmid = {11463898},
  tags = {Uncertainty}
}

@ARTICLE{allen+al2000,
  author = {Allen, M. and Stott, P. and Mitchell, J. and Schnur, R. and Delworth,
	T.},
  title = {Quantifying the uncertainty in forecasts of anthropogenic climate
	change},
  journal = {Nature},
  year = {2000},
  volume = {407},
  pages = {617--620},
  number = {6804},
  abstract = {Forecasts of climate change are inevitably uncertain. It is therefore
	essential to quantify the risk of significant departures from the
	predicted response to a given emission scenario. Previous analyses
	of this risk have been based either on expert opinion, perturbation
	analysis of simplified climate models or the comparison of predictions
	from general circulation models6. Recent observed changes that appear
	to be attributable to human influence provide a powerful constraint
	on the uncertainties in multi-decadal forecasts. Here we assess the
	range of warming rates over the coming 50 years that are consistent
	with the observed near-surface temperature record as well as with
	the overall patterns of response predicted by several general circulation
	models. We expect global mean temperatures in the decade 2036–46
	to be 1–2.5 K warmer than in pre-industrial times under a 'business
	as usual' emission scenario. This range is relatively robust to errors
	in the models' climate sensitivity, rate of oceanic heat uptake or
	global response to sulphate aerosols as long as these errors are
	persistent over time. Substantial changes in the current balance
	of greenhouse warming and sulphate aerosol cooling would, however,
	increase the uncertainty. Unlike 50-year warming rates, the final
	equilibrium warming after the atmospheric composition stabilizes
	remains very uncertain, despite the evidence provided by the emerging
	signal.},
  doi = {10.1038/35036559},
  pmid = {11034207},
  tags = {Uncertainty, Thesis}
}

@BOOK{FAO56,
  title = {Crop evapotranspiration - Guidelines for computing crop water requirements},
  publisher = {FAO},
  year = {1998},
  author = {Allen, R. and Pereira, L. and Raes, D. and Smith, M.},
  volume = {FAO irrigation and drainage paper 56},
  owner = {rojasro},
  timestamp = {2010.08.19}
}

@ARTICLE{anctil+al2006,
  author = {Anctil, F. and Lauzon, N. and Andreassian, V. and Oudin, L. and Perrin,
	C.},
  title = {Improvement of rainfall-runoff forecasts through mean areal rainfall
	optimization},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {328},
  pages = {717--725},
  abstract = {Rainfall information is a dominant element in the development of Lumped
	neural network rainfall-runoff forecasting models. In this study,
	forecasting improvement is sought through the optimization of the
	mean daily areal rainfall time series. The experimental protocol
	is structured in two phases. First, the rain gage network is randomly
	sampled to produce subsets of specific number of rain gages, in order
	to assess the impact of reduced rainfall knowledge on streamflow
	forecasting performance. Then, genetic algorithm is used to orient
	the rain gage combinatorial problem toward improved forecasting performance.
	The analysis consists of one-day ahead forecast for a mountainous
	watershed (3234 km(2)) known for its heterogeneous rainfall.},
  doi = {10.1016/j.jhydrol.2006.01.016},
  keywords = {rainfall-runoff, neural networks, genetic algorithm, rain gage network,
	streamflow forecasting, model performance, mean areal rainfall, ARTIFICIAL
	NEURAL-NETWORKS, GENETIC ALGORITHM, COMBINATORIAL OPTIMIZATION, FEEDFORWARD
	NETWORKS, MODEL CALIBRATION, FLASH-FLOOD, PREDICTION, DESIGN, ANN,
	VALIDATION},
  tags = {Applications, Rainfall}
}

@ARTICLE{anderman1996,
  author = {Anderman, E. and Holl, M. and Poeter, E.},
  title = {Two--dimensional advective transport in ground-water flow parameter
	estimation},
  journal = {Ground Water},
  year = {1996},
  volume = {34},
  pages = {1001--1009},
  number = {6},
  abstract = {Nonlinear regression is useful in ground-water flow parameter estimation,
	but problems of parameter insensitivity and correlation often exist
	given commonly available hydraulic-head and head-dependent flow (for
	example, stream and lake gain or loss) observations. To address this
	problem, advective-transport observations are added to the ground-water
	flow, parameter-estimation model MODFLOWP using particle-tracking
	methods. The resulting model is used to investigate the importance
	of advective-transport observations relative to head-dependent flow
	observations when either or both are used in conjunction with hydraulic-head
	observations in a simulation of the sewage-discharge plume at Otis
	Air Force Base, Cape Cod, Massachusetts, USA. The analysis procedure
	for evaluating the probable effect of new observations on the regression
	results consists of two steps: (1) parameter sensitivities and correlations
	calculated at initial parameter values are used to assess the model
	parameterization and expected relative contributions of different
	types of observations to the regression; and (2) optimal parameter
	values are estimated by nonlinear regression and evaluated. In the
	Cape Cod parameter-estimation model, advective-transport observations
	did not significantly increase the overall parameter sensitivity;
	however: (1) inclusion of advective-transport observations decreased
	parameter correlation enough for more unique parameter values to
	be estimated by the regression; (2) realistic uncertainties in advective-transport
	observations had a small effect on parameter estimates relative to
	the precision with which the parameters were estimated; and (3) the
	regression results and sensitivity analysis provided insight into
	the dynamics of the ground-water flow system, especially the importance
	of accurate boundary conditions. In this work, advective-transport
	observations improved the calibration of the model and the estimation
	of ground-water flow parameters, and use of regression and related
	techniques produced significant insight into the physical system.},
  doi = {10.1111/j.1745-6584.1996.tb02165.x},
  owner = {rojasro},
  timestamp = {2010.02.16}
}

@ARTICLE{anderson+al2000,
  author = {Anderson, M. and Kavvas, L. and Mierzwa, M.},
  title = {Assessing hydrologic drought risk using simplified climate model},
  journal = {Journal of Hydrologic Engineering},
  year = {2000},
  volume = {5},
  pages = {393--401},
  number = {4},
  abstract = {Water resources systems operation requires drought risk estimates
	to mitigate possible drought-related damages. Drought risk assessment
	is complicated by the nonlinear interaction of the atmospheric hydrologic
	and oceanic systems where highly varied hydrologic system responses
	to similar drought-forcing phenomena can occur. A methodology capable
	of assessing drought risk associated with hydroclimatic events by
	using a simplified climate model is presented. Ensemble mean and
	standard deviations of hydrologic water storage represent the expected
	hydrologic system response to the hydroclimatic event. Relative frequency
	histograms and cumulative distribution functions characterize the
	range of hydrologic system responses that can occur and are used
	to obtain the spatially and temporally evolving drought risks. The
	methodology is presented in a framework suitable for application
	to resources management. An outline of the approach, description
	of the simplified climate model used in this study, and an illustrative
	example using a La Niña type event as the drought-forcing mechanism
	are given. Simulation results, the methodology, and future directions
	are discussed.},
  doi = {10.1061/(ASCE)1084-0699(2000)5:4(393)},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@BOOK{anderson1992,
  title = {Applied groundwater modelling--{S}imulation of flow and advective
	transport},
  publisher = {Academic Press},
  year = {1992},
  author = {Anderson, M. and Woessner, W.},
  pages = {381},
  address = {San Diego Califormia},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{andreasson+al2004,
  author = {Andr\'easson, J. and Bergstr\"om, S. and Carlsson, B. and Graham,
	L. and Lindstr\"om, G.},
  title = {Hydrological Change. {C}limate change impact simulations for {S}weeden},
  journal = {Ambio},
  year = {2004},
  volume = {33},
  pages = {228--234},
  number = {4--5},
  abstract = {Climate change resulting from the enhanced greenhouse effect is expected
	to give rise to changes in hydrological systems. This hydrological
	change, as with the change in climate variables, will vary regionally
	around the globe. Impact studies at local and regional scales are
	needed to assess how different regions will be affected. This study
	focuses on assessment of hydrological impacts of climate change over
	a wide range of Swedish basins. Different methods of transferring
	the signal of climate change from climate models to hydrological
	models were used. Several hydrological model simulations using regional
	climate model scenarios from Swedish Regional Climate Modelling Programme
	(SWECLIM) are presented. A principal conclusion is that subregional
	impacts to river flow vary considerably according to whether a basin
	is in northern or southern Sweden. Furthermore, projected hydrological
	change is just as dependent on the choice of the global climate model
	used for regional climate model boundary conditions as the choice
	of anthropogenic emissions scenario.},
  doi = {10.1579/0044-7447-33.4.228},
  file = {:E\:\\rojasro\\My Documents\\articles\\Hydrological change - climate change impact simualtionsfor Sweden (Andreason et al. 2004).pdf:PDF},
  tags = {Climate Change}
}

@ARTICLE{andrews+al2011,
  author = {Andrews, F. and Croke, B. and Jakeman, A.},
  title = {An open software environment for hydrological model assessment and
	development},
  journal = {Environmental Modelling \& Software},
  year = {2011},
  volume = {26},
  pages = {1171--1185},
  number = {10},
  abstract = {The hydromad (Hydrological Model Assessment and Development) package
	provides a set of functions which work together to construct, manipulate,
	analyse and compare hydrological models. The class of hydrological
	models considered are dynamic, spatially-aggregated conceptual or
	statistical models. The package functions are designed to fit seamlessly
	into the R system, and builds on its powerful data manipulation and
	analysis capabilities. The framework used in the package encourages
	a separation of model components based on Unit Hydrograph theory;
	many published models are consistent with this and implementations
	of several are included. For comparative assessment, model performance
	can be analysed over time and with respect to covariates to reveal
	systematic biases. Support has been built in for event-based analysis
	of data and assessment of model performance. Fit statistics can be
	defined by choices of (1) temporal scale and aggregation function;
	(2) weighting and transformation; and (3) reference model. One can
	define new Soil Moisture Accounting models, routing models, calibration
	methods, objective functions, and evaluation statistics, while retaining
	as much of the default framework as is useful. And as the package
	code is available under a free software licence, one always has the
	freedom to adapt it as required. Use of the software is demonstrated
	in a case study of the Queanbeyan River catchment in South-East Australia.},
  doi = {10.1016/j.envsoft.2011.04.006},
  issn = {1364-8152},
  keywords = {Model evaluation}
}

@ARTICLE{andreassian+al2010,
  author = {Andr{\'e}assian, V. and Perrin, C. and Parent, E. and Bardossy, A.},
  title = {{The court of miracles of hydrology: Can failure stories contribute
	to hydrological science?}},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {849--856},
  number = {6},
  abstract = {In this article, we suggest that giving greater prominence to the
	analysis of failures and errors would more fruitfully advance the
	hydrological sciences. As widely recognised by philosophers of science,
	we can all learn from our mistakes, and errors can lead to discovery
	if they are properly diagnosed. However, failure stories are very
	seldom communicated and published, even though they represent the
	bulk of the results obtained by researchers and modellers. This article
	is the result of passionate discussions held in a workshop called
	the Court of Miracles of Hydrology held in Paris in June 2008. The
	participants had been invited to present their unpublished experience
	with what could be called monsters, anomalies, outliers and failures
	in their everyday practice of hydrology. The review of these studies
	clearly shows that in-depth analysis of these observations and results
	that deviate from the expected norm blazes a trail that can only
	lead to progress. },
  doi = {10.1080/02626667.2010.506050},
  tags = {Calibration, Philosophical}
}

@ARTICLE{apel+al2009,
  author = {Apel, H. and Aronica, G. and Kreibich, H. and Thieken, A.},
  title = {Flood risk analyses—how detailed do we need to be?},
  journal = {Natural Hazards},
  year = {2009},
  volume = {49},
  pages = {79--98},
  note = {10.1007/s11069-008-9277-8},
  abstract = {Applied flood risk analyses, especially in urban areas, very often
	pose the question how detailed the analysis needs to be in order
	to give a realistic figure of the expected risk. The methods used
	in research and practical applications range from very basic approaches
	with numerous simplifying assumptions up to very sophisticated, data
	and calculation time demanding applications both on the hazard and
	on the vulnerability part of the risk. In order to shed some light
	on the question of required model complexity in flood risk analyses
	and outputs sufficiently fulfilling the task at hand, a number of
	combinations of models of different complexity both on the hazard
	and on the vulnerability side were tested in a case study. The different
	models can be organized in a model matrix of different complexity
	levels: On the hazard side, the approaches/models selected were (A)
	linear interpolation of gauge water levels and intersection with
	a digital elevation model (DEM), (B) a mixed 1D/2D hydraulic model
	with simplifying assumptions (LISFLOOD-FP) and (C) a Saint-Venant
	2D zero-inertia hyperbolic hydraulic model considering the built
	environment and infrastructure. On the vulnerability side, the models
	used for the estimation of direct damage to residential buildings
	are in order of increasing complexity: (I) meso-scale stage-damage
	functions applied to CORINE land cover data, (II) the rule-based
	meso-scale model FLEMOps+ using census data on the municipal building
	stock and CORINE land cover data and (III) a rule-based micro-scale
	model applied to a detailed building inventory. Besides the inundation
	depths, the latter two models consider different building types and
	qualities as well as the level of private precaution and contamination
	of the floodwater. The models were applied in a municipality in east
	Germany, Eilenburg. It suffered extraordinary damage during the flood
	of August 2002, which was well documented as were the inundation
	extent and depths. These data provide an almost unique data set for
	the validation of flood risk analyses. The analysis shows that the
	combination of the 1D/2D model and the meso-scale damage model FLEMOps+
	performed best and provide the best compromise between data requirements,
	simulation effort, and an acceptable accuracy of the results. The
	more detailed approaches suffered from complex model set-up, high
	data requirements, and long computation times.},
  affiliation = {Deutsches GeoForschungsZentrum GFZ Section 5.4: Engineering Hydrology,
	Telegrafenberg 14473 Potsdam Germany},
  issn = {0921-030X},
  issue = {1},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Netherlands}
}

@BOOK{applebaum1996,
  title = {Probability and information: {A}n integrated approach},
  publisher = {Cambridge University Press},
  year = {1996},
  author = {Applebaum, D.},
  address = {Cambridge},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{apsite+al2010,
  author = {Apsite, E. and Bakute, A. and Kurpniece, L. and Pallo, I.},
  title = {{Changes in river runoff in Latvia at the ens of the 21st century}},
  journal = {FENNIA - International Journal of Geoggraphy},
  year = {2010},
  volume = {188},
  pages = {50--60},
  number = {1},
  abstract = {This study deals with future climate change impacts on the runoff
	of five Latvian river basins at the end of this century. Climate
	data series have been provided by the Faculty of Physics and Mathematics
	of the University of Latvia where the
	
	regional climate model Rossby Centre Atmosphere Ocean was selected
	for further statistical downscaling. Changes in hydrometeorological
	data have been analysed based upon one control run (period 1961?1990)
	and two future scenario
	
	A2 and B2 runs (2071?2100). The conceptual rainfall-runoff model,
	the latest version of METQ2007BDOPT, was used for simulation of hydrological
	processes in particular river basins. Simulation results revealed
	that in comparison
	
	to the control period, major differences in hydrometeorological parameters
	in future were observed according to A2 scenario, where long-term
	mean air temperature will grow by 4 degrees and precipitation by
	12%, while mean annual
	
	river flow will decrease by 19%. Both scenarios demonstrate changes
	in seasonal runoff patterns where the major part of river runoff
	will be generated in winter, followed by spring, autumn and summer.
	The river hydrograph is going
	
	to take a different shape, where the maximum river discharges will
	occur in winter instead of spring.},
  owner = {rojasro},
  timestamp = {2011.07.05},
  url = {http://ojs.tsv.fi/index.php/fennia/article/view/2844/3456}
}

@ARTICLE{aravena1995,
  author = {Aravena, R.},
  title = {Isotope hydrology and geochemistry of northern chile groundwaters},
  journal = {Bulletin de l'Institut francais d'\`etudes andines},
  year = {1995},
  volume = {24},
  pages = {495--503},
  number = {3},
  abstract = {This paper reviews studies that applied idotope techniwues in aquifers
	located in the Pampa del tamarugal and the Salar de Atacama Basins
	in northern Chile. The main aims studies were to obtain information
	about the origin and residence time of groundwater, groundwater quality,
	evaporation rates from Salares, and the relationship between flooding
	and aquifer recharge. The main conclusions of these studies, that
	have implications for water resources management in this region are:
	a) most of the groundwtaer is of good quality, with the exception
	of areas close to the Salares; b) a multiaquifer system was identified
	in the Pampa del Tamarugal basin, associated with recharge areas
	located at different altitudes and; c) a significan t portion of
	groundwaters in the Pampa aquifers should be treated as a non renewable
	water resource.},
  owner = {RRojas},
  timestamp = {2009.02.19},
  url = {http://www.ifeanet.org/publicaciones/boletines/24%283%29/495.pdf}
}

@CONFERENCE{aravena1989,
  author = {Aravena, R. and Pe\~na, H. and Grilli, A. and Suzuki, O. and Mordeckai,
	M.},
  title = {Evoluci\'on isot\'opica de las lluvias y origen de las masas de aire
	en el altiplano chileno},
  booktitle = {Isotope hydrology investigations in Latin America},
  year = {1989},
  address = {Vienna, Austria},
  organization = {IAEA},
  publisher = {IAEA},
  owner = {RRojas},
  timestamp = {2009.04.08}
}

@ARTICLE{aravena1999,
  author = {Aravena, R. and Suzuki, O. and Pe\~na, H. and Pollastri, A. and Fuenzalida,
	H. and Grilli, A.},
  title = {Isotopic composition and origin of the precipitation in {N}orthern
	{Chile}},
  journal = {Applied Geochemistry},
  year = {1999},
  volume = {14},
  pages = {411--422},
  number = {4},
  abstract = {A 3 a data set of isotopes in precipitation from northern Chile show
	a very distinct pattern, with ?18O values ranging between ?18 and
	?15‰ at high altitude stations, compared to ?18O values between ?10
	and ?6‰ at the lower altitude areas. The 18O-depleted values observed
	in the high altitude area, the Altiplano, are related to processes
	that affect the air masses that originated over the Atlantic, cross
	the Amazon Basin (continental effect), ascend the Andes (altitude
	effect) and precipitated (convective effect) in the Altiplano. It
	is postulated that a second source of moisture, associated with air
	masses from the Pacific, may contribute to the 18O-enriched values
	observed in the lower altitude areas. Similar isotopic patterns are
	documented in springs and groundwater indicating that the data presented
	in this paper are an accurate representation of the long term behavior
	isotopic composition of rain in northern Chile.},
  doi = {10.1016/S0883-2927(98)00067-5},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{argent2004,
  author = {Argent, R.},
  title = {An overview of model integration for environmental applications-components,
	frameworks and semantics},
  journal = {Environmental Modelling \& Software},
  year = {2004},
  volume = {19},
  pages = {219--234},
  number = {3},
  abstract = {In recent years, pressure has increased on environmental scientist/modellers
	to both undertake good science in an efficient and timely manner,
	under increasing resource constraints, and also to ensure that the
	science being performed is immediately relevant to a particular environmental
	management context. At the same time, environmental management is
	changing, with increasing requirements for multi-scale and multi-objective
	assessment and decision making that considers economic and social
	systems, as well as the ecosystem. Integration of management activities,
	and also of the modelling undertaken to support management, has become
	a high priority. To solve the problems of application and integration,
	knowledge encapsulation in models is being undertaken in a way that
	both meets the needs for good science, and also provides the conceptual
	and technical structures required for broader and more integrated
	application of that knowledge by managers. To support this modelling,
	tools and technologies from computer science and software engineering
	are being transferred to applied environmental science fields, and
	a range of new modelling and software development approaches are
	being pursued. The papers in this Special Issue provide examples
	of the integrated modelling concepts and applications that have been,
	or are being, developed. These include the use of object-oriented
	concepts, component-based modelling techniques and modelling frameworks,
	as well as the emerging use of integrated modelling platforms and
	metadata support for modelling semantics. This paper provides an
	overview of the science and management imperatives underlying recent
	developments, discusses the technological and conceptual developments
	that have taken place, and highlights some of the semantic, operational
	and process requirements that need to be addressed now that the technological
	aspects of integrated modelling are well advanced.},
  doi = {10.1016/S1364-8152(03)00150-6},
  owner = {rojasro},
  timestamp = {2012.03.22}
}

@ARTICLE{arheimer+al2005,
  author = {Arheimer, B. and Andr\'easson, J. and Fogelberg, S. and Johnsson,
	H. and Pers, C. and Persson, K.},
  title = {Climate change impact on water quality: {M}odel results from southern
	{S}weden},
  journal = {Ambio},
  year = {2005},
  volume = {34},
  pages = {559--566},
  number = {7},
  abstract = {Starting from six regional climate change scenarios, nitrogen leaching
	from arable-soil, water discharge, and nitrogen retention was modeled
	in the R{\"o}nne{\aa} catchment. Additionally, biological response
	was modeled in the eutrophic Lake Ringsj{\"o}n. The results are compared
	with similar studies on other catchments. All scenarios gave similar
	impact on water quality but varied in quantities. However, one scenario
	resulted in a different transport pattern due to less-pronounced
	seasonal variations in the hydrology. On average, the study shows
	that, in a future climate, we might expect: i) increased concentrations
	of nitrogen in the arable root zone (+50%) and in the river (+13%);
	ii) increased annual load of nitrogen from land to sea (+22%) due
	to more pronounced winter high flow; moreover, remote areas in the
	catchment may start to contribute to the outlet load; iii) radical
	changes in lake biochemistry with increased concentrations of total
	phosphorus (+50%), total nitrogen (+20%), and planktonic algae such
	as cyanobacteria (+80%).},
  doi = {10.1579/0044-7447-34.7.559},
  tags = {Impacts}
}

@ARTICLE{arnell2005,
  author = {Arnell, N.},
  title = {Implications of climate change for freshwater inflows to the {A}rctic
	{O}cean},
  journal = {Journal of Geophysical Research},
  year = {2005},
  volume = {110},
  pages = {1--9},
  number = {D07105},
  abstract = {Observational evidence suggests that river inflows to the Arctic Ocean
	have increased over the last 30 years. Continued increases have the
	potential to alter the freshwater balance in the Arctic and North
	Atlantic Oceans and hence the thermohaline circulation. Simulations
	with a macroscale hydrological model and climate change scenarios
	derived from six climate models and two emissions scenarios suggest
	increases of up to 31% in river inflows to the Arctic by the 2080s
	under high emissions and up to 24% under lower emissions, although
	there are large differences between climate models. Uncertainty analysis
	suggests low sensitivity to model form and parameterization but higher
	sensitivity to the input data used to drive the model. The addition
	of up to 0.048 sverdrup (Sv, 10(6) m(3) s(-1)) is a large proportion
	of the 0.06-0.15 Sv of additional freshwater that may trigger thermohaline
	collapse. Changes in the spatial distribution of inflows to the Arctic
	Ocean may influence circulation patterns within the ocean.},
  doi = {10.1029/2004JD005348},
  keywords = {RIVER RUNOFF, BALANCE MODEL, GLOBAL RIVERS, LAND-COVER, DISCHARGE,
	ICE, CIRCULATION, BASIN, VARIABILITY, SENSITIVITY},
  tags = {Impacts}
}

@ARTICLE{arnell2004,
  author = {Arnell, N.},
  title = {Climate change and global water resources: {SRES} emissions and socio--economic
	scenarios},
  journal = {Global Environmental Change},
  year = {2004},
  volume = {14},
  pages = {31--52},
  number = {1},
  abstract = {In 1995, nearly 1400 million people lived in water-stressed watersheds
	(runoff less than 1000 m3/capita/year), mostly in south west Asia,
	the Middle East and around the Mediterranean. This paper describes
	an assessment of the relative effect of climate change and population
	growth on future global and regional water resources stresses, using
	SRES socio-economic scenarios and climate projections made using
	six climate models driven by SRES emissions scenarios. River runoff
	was simulated at a spatial resolution of 0.5Ã—0.5Â° under current
	and future climates using a macro-scale hydrological model, and aggregated
	to the watershed scale to estimate current and future water resource
	availability for 1300 watersheds and small islands under the SRES
	population projections. The A2 storyline has the largest population,
	followed by B2, then A1 and B1 (which have the same population).
	In the absence of climate change, the future population in water-stressed
	watersheds depends on population scenario and by 2025 ranges from
	2.9 to 3.3 billion people (36--40% of the world's population). By
	2055 5.6 billion people would live in water-stressed watersheds under
	the A2 population future, and {\grqq}onlyâ€? 3.4 billion under A1/B1.
	Climate change increases water resources stresses in some parts of
	the world where runoff decreases, including around the Mediterranean,
	in parts of Europe, central and southern America, and southern Africa.
	In other water-stressed parts of the worldâ€”particularly in southern
	and eastern Asiaâ€”climate change increases runoff, but this may
	not be very beneficial in practice because the increases tend to
	come during the wet season and the extra water may not be available
	during the dry season. The broad geographic pattern of change is
	consistent between the six climate models, although there are differences
	of magnitude and direction of change in southern Asia. By the 2020s
	there is little clear difference in the magnitude of impact between
	population or emissions scenarios, but a large difference between
	different climate models: between 374 and 1661 million people are
	projected to experience an increase in water stress. By the 2050s
	there is still little difference between the emissions scenarios,
	but the different population assumptions have a clear effect. Under
	the A2 population between 1092 and 2761 million people have an increase
	in stress; under the B2 population the range is 670--1538 million,
	respectively. The range in estimates is due to the slightly different
	patterns of change projected by the different climate models. Sensitivity
	analysis showed that a 10% variation in the population totals under
	a storyline could lead to variations in the numbers of people with
	an increase or decrease in stress of between 15% and 20%. The impact
	of these changes on actual water stresses will depend on how water
	resources are managed in the future},
  doi = {10.1016/j.gloenvcha.2003.10.006},
  keywords = {Climate change impacts, Global water resources, Water resources stresses,
	SRES emissions scenarios, Macro-scale hydrological model, Multi-decadal
	variability},
  tags = {Scenarios}
}

@ARTICLE{arnell2003a,
  author = {Arnell, N.},
  title = {Effects of {IPCC SRES} emissions scenarios on river runoff: {A} global
	perspective},
  journal = {Hydrology and Earth System Sciences},
  year = {2003},
  volume = {7},
  pages = {619--641},
  number = {5},
  abstract = {This paper describes an assessment of the implications of future climate
	change for river runoff across the entire world, using six climate
	models which have been driven by the SRES emissions scenarios. Streamflow
	is simulated at a spatial resolution of 0.5oÃ—0.5o using a macro-scale
	hydrological model, and summed to produce total runoff for almost
	1200 catchments. The effects of climate change have been compared
	with the effects of natural multi-decadal climatic variability, as
	determined from a long unforced climate simulation using HadCM3.
	By the 2020s, change in runoff due to climate change in approximately
	a third of the catchments is less than that due to natural variability
	but, by the 2080s, this falls to between 10 and 30%. The climate
	models produce broadly similar changes in runoff, with increases
	in high latitudes, east Africa and south and east Asia, and decreases
	in southern and eastern Europe, western Russia, north Africa and
	the Middle East, central and southern Africa, much of North America,
	most of South America, and south and east Asia. The pattern of change
	in runoff is largely determined by simulated change in precipitation,
	offset by a general increase in evaporation. There is little difference
	in the pattern of change between different emissions scenarios (for
	a given model), and only by the 2080s is there evidence that the
	magnitudes of change in runoff vary, with emissions scenario A1FI
	producing the greatest change and B1 the smallest. The inter-annual
	variability in runoff increases in most catchments due to climate
	change -- even though the inter-annual variability in precipitation
	is not changed -- and the frequency of flow below the current 10-year
	return period minimum annual runoff increases by a factor of three
	in Europe and southern Africa and of two across North America. Across
	most of the world climate change does not alter the timing of flows
	through the year but, in the marginal zone between cool and mild
	climates, higher temperatures mean that peak streamflow moves from
	spring to winter as less winter precipitation falls as snow. The
	spatial pattern of changes in the 10-year return period maximum monthly
	runoff follows changes in annual runoff.},
  doi = {10.5194/hess-7-619-2003},
  keywords = {SRES emissions scenarios, climate change impacts on runoff, multi-decadal
	variability, macro-scale hydrological model, drought frequency, flood
	frequency},
  tags = {Impacts}
}

@ARTICLE{arnell2003b,
  author = {Arnell, N.},
  title = {Relative effects of multi--decadal climatic variability and changes
	in the mean and variability of climate due to global warming: {F}uture
	streamflows in {B}ritain},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {270},
  pages = {195--213},
  number = {3--4},
  abstract = {Climate change impact assessments conventionally assess just the implications
	of a change in mean climate due to global warming. This paper compares
	such effects of such changes with those due to natural multi-decadal
	variability, and also explores the effects of changing the year-to-year
	variability in climate as well as the mean. It estimates changes
	in mean monthly flows and a measure of low flow (the flow exceeded
	95% of,the time) in six catchments in Britain, Using the UKCIP98
	climate change scenarios and a calibrated hydrological model. Human-induced
	climate change has a different seasonal effect on flows than natural
	multi-decadal variability (an increase in winter and decrease in
	summer), and by the 2050s the climate change signal is apparent in
	winter and, in lowland Britain, in summer. Superimposing natural
	multidecadal variability onto the human-induced climate change increases
	substantially the range in possible future streamflows (in some instances
	counteracting the climate change signal), with important implications
	for the development of adaptation strategies. Increased year-to-year
	variability in climate leads to slight increases in mean monthly
	flows (relative to changes due just to changes in mean climate),
	and slightly greater decreases in low flows. The greatest effect
	on low flows occurs in upland catchments. (C) 2002 Elsevier Science
	B.V. All rights reserved.},
  doi = {S0022-1694(02)00288-3},
  keywords = {climate change impacts on streamflow, Britain, multi-decadal climatic
	variability, year-to-year variability, low flows, CHANGE SCENARIOS,
	CHANGE IMPACTS, RUNOFF, MODELS},
  tags = {Impacts}
}

@ARTICLE{arnell1999a,
  author = {Arnell, N.},
  title = {The effect of climate change on hydrological regimes in {E}urope:
	{A} continental perspective},
  journal = {Global Environmental Change},
  year = {1999},
  volume = {9},
  pages = {5--23},
  number = {1},
  abstract = {This paper outlines the effects of climate change by the 2050s on
	hydrological regimes at the continental scale in Europe, at a spatial
	resolution of 0.5 Ã— 0.5Â°. Hydrological regimes are simulated using
	a macro-scale hydrological model, operating at a daily time step,
	and four climate change scenarios are used. There are differences
	between the four scenarios, but each indicates a general reduction
	in annual runoff in southern Europe (south of around 50Â°N), and
	an increase in the north. In maritime areas there is little difference
	in the timing of flows, but the range through the year tends to increase
	with lower flows during summer. The most significant changes in flow
	regime, however, occur where snowfall becomes less important due
	to higher temperatures, and therefore both winter runoff increases
	and spring flow decreases: these changes occur across a large part
	of eastern Europe. In western maritime Europe low flows reduce, but
	further east minimum flows will increase as flows during the present
	low flow season - winter - rise. {\grqq}Droughtâ€? was indexed as
	the maximum total deficit volume below the flow exceeded 95% of the
	time: this was found to increase in intensity across most of western
	Europe, but decrease in the east and north. The study attempted to
	quantify several sources of uncertainty, and showed that the effects
	of model uncertainty on the estimated change in runoff were generally
	small compared to the differences between scenarios and the assumed
	change in global temperature by 2050},
  doi = {10.1016/S0959-3780(98)00015-6},
  keywords = {Climate change, River runoff, Europe},
  mzbnote = {Used in Ph.D thesis},
  tags = {Scenarios}
}

@ARTICLE{arnell1999b,
  author = {Arnell, N.},
  title = {Climate change and global water resources},
  journal = {Global Environmental Change},
  year = {1999},
  volume = {9},
  pages = {S31--S49},
  number = {1},
  abstract = {By 2025, it is estimated that around 5 billion people, out of a total
	population of around 8 billion, will be living in countries experiencing
	water stress (using more than 20% of their available resources).
	Climate change has the potential to impose additional pressures in
	some regions. This paper describes an assessment of the implications
	of climate change for global hydrological regimes and water resources.
	It uses climate change scenarios developed from Hadley Centre climate
	simulations (HadCM2 and HadCM3), and simulates global river flows
	at a spatial resolution of 0.5Ã—0.5Â° using a macro-scale hydrological
	model. Changes in national water resources are calculated, including
	both internally generated runoff and upstream imports, and compared
	with national water use estimates developed for the United Nations
	Comprehensive Assessment of the Freshwater Resources of the World.
	Although there is variation between scenarios, the results suggest
	that average annual runoff will increase in high latitudes, in equatorial
	Africa and Asia, and southeast Asia, and will decrease in mid-latitudes
	and most subtropical regions. The HadCM3 scenario produces changes
	in runoff which are often similar to those from the HadCM2 scenarios
	â€” but there are important regional differences. The rise in temperature
	associated with climate change leads to a general reduction in the
	proportion of precipitation falling as snow, and a consequent reduction
	in many areas in the duration of snow cover. This has implications
	for the timing of streamflow in such regions, with a shift from spring
	snow melt to winter runoff. Under the HadCM2 ensemble mean scenario,
	the number of people living in countries with water stress would
	increase by 53 million by 2025 (relative to those who would be affected
	in the absence of climate change). Under the HadCM3 scenario, the
	number of people living in countries with water stress would rise
	by 113 million. However, by 2050 there would be a net reduction in
	populations in stressed countries under HadCM2 (of around 69 million),
	but an increase of 56 million under HadCM3. The study also showed
	that different indications of the impact of climate change on water
	resource stresses could be obtained using different projections of
	future water use. The paper emphasises the large range between estimates
	of {\grqq}impactâ€?, and also discusses the problems associated with
	the scale of analysis and the definition of indices of water resource
	impac},
  doi = {10.1016/S0959-3780(99)00017-5},
  keywords = {Climate change, Global water resources, Global runoff, Hydrological
	impacts of climate change},
  mzbnote = {NOT USED in Ph.D thesis},
  tags = {Impacts}
}

@ARTICLE{arnell1992,
  author = {Arnell, N.},
  title = {Impacts of climatic change on river flow regimes in {UK}},
  journal = {Water and Environment Journal},
  year = {1992},
  volume = {6},
  pages = {432--442},
  number = {5},
  abstract = {Much has been written in recent years about the potential threats
	posed by increasing greenhouse gas concentrations. This paper summarizes
	the implications of global warming for hydrological processes in
	general and river flow characteristics in the UK in particular, emphasizing
	the present high degree of uncertainty. Current climate change scenarios
	for the UK imply that rainfall between autumn and spring will increase,
	and this may have beneficial implications for UK water resources.
	However, the effect of this increase may be outweighed by higher
	evapotranspiration. Average annual runoff in a catchment in southern
	UK may be reduced by around 5% by the middle of the next century,
	but this estimate is very uncertain: runoff may reduce by 30% or
	increase by 30%. Runoff in northern and western UK is likely to show
	a slight increase (but with similarly large confidence intervals).
	It is probable that river flows in the UK will be much more concentrated
	in winter than at present. The effect of a given climate change scenario
	on monthly flow regimes depends on the current summer water balance
	and on catchment geological conditions.},
  doi = {10.1111/j.1747-6593.1992.tb00772.x},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{arnell+al2003,
  author = {Arnell, N. and Hudson, D. and Jones, R.},
  title = {Climate change scenarios from a regional climate model: {E}stimating
	change in runoff in southern {A}frica},
  journal = {Journal of Geophysical Research},
  year = {2003},
  volume = {108},
  pages = {4519},
  number = {D16},
  abstract = {This paper describes an analysis of different ways of constructing
	climate change scenarios using output from three climate models.
	It focuses on using the HadRM3H regional climate model applied across
	southern Africa and a macroscale runoff model operating at a scale
	of 0.5 Ã— 0.5Â° to simulate river runoff. HadRM3H has a spatial resolution
	of 0.44 Ã— 0.44Â° and is driven by boundary conditions from HadAM3H,
	a global atmosphere general circulation model with a spatial resolution
	of 1.875 Ã— 1.25Â°. This, in turn, used sea-surface boundary conditions
	from HadCM3, a coupled global ocean-atmosphere general circulation
	model that operates at a spatial resolution of 3.75 Ã— 2.5Â°. Sixteen
	climate scenarios were constructed from the three models, representing
	different combinations of model scale, whether the climate model
	simulations were used directly or changes were applied to an observed
	baseline, and whether observed or simulated variations from year-to-year
	were used. The different ways of deriving climate scenarios from
	a single initial climate model experiment result in a range in change
	in average annual runoff at a location of at least 10%, and often
	more than 20%. There is a clear difference in the large-scale spatial
	pattern of change in runoff from HadCM3 to HadRM3H. Many of the climate
	features in HadRM3H are already present in HadAM3H simulations, as
	would be expected from the experimental design. This suggests that
	for studies over a large geographic domain, an intermediate-resolution
	global climate model can produce useful scenarios for impact assessments.
	HadRM3H overestimates rainfall across much of southern Africa and
	so results in too much runoff: This leads to smaller estimates of
	future change in runoff than arise when changes in climate are applied
	to an observed climate baseline. It is concluded that under these
	circumstances it is preferable to apply modeled changes in climate
	to observed data to construct climate scenarios rather than derive
	these directly from the regional climate model simulations. Incorporating
	increases in interannual variability as simulated by HadRM3H leads
	to little change in simulated annual mean runoff. However, it has
	a larger impact on the frequency distributions of runoff, with extreme
	flows predicted to increase more than mean flows and even to increase
	in areas where the mean flow decreases. This demonstrates the importance
	of considering not only changes in mean climate but also climate
	variability},
  doi = {10.1029/2002JD002782},
  keywords = {regional climate model, climate change scenarios, climate change impacts,
	dynamical downscaling, runoff, southern Africa},
  tags = {Scenarios, RCMs}
}

@ARTICLE{arnell+al2004,
  author = {Arnell, N. and Livermore, M. and Kovats, S. and Levy, P. and Nicholls,
	R. and Parry, M. and Gaffin, S.},
  title = {Climate and socio--economic scenarios for global-scale climate change
	impacts assessments: {C}haracterising the {SRES} storylines},
  journal = {Global Environmental Change},
  year = {2004},
  volume = {14},
  pages = {3--20},
  number = {1},
  abstract = {This paper describes the way in which the socio-economic projections
	in the SRES scenarios were applied in a global-scale assessment of
	the impacts of climate change on food security, water stresses, coastal
	flood risk and wetland loss, exposure to malaria risk and terrestrial
	ecosystems. There are two key issues: (i) downscaling from the world-region
	level of the original scenarios to the scale of analysis (0.5Â°Ã—0.5Â°),
	and (ii) elaborating the SRES narrative storylines to quantify other
	indicators affecting the impact of climate change. National estimates
	of population and GDP were derived by assuming that each country
	changed at the regional rate, and population was downscaled to the
	0.5Â°Ã—0.5Â° scale assuming that everywhere in a country changed
	at the same rate. SRES scenarios for future cropland extent were
	applied to current baseline data, assuming everywhere within a region
	changed at the same rate. The narrative storylines were used to construct
	scenarios of future adaptation to the coastal flood risk and malaria
	risk. The paper compares the SRES scenarios with other global-scale
	scenarios, and identifies sources of uncertainty. It concludes by
	recommending three refinements to the use of the SRES scenarios in
	global and regional-scale impact assessment: (i) improved disaggregation
	to finer spatial resolutions, using both {\grqq}downscaled narrative
	storylinesâ€? and new technical procedures, (ii) explicit consideration
	of uncertainty in the population, GDP and land cover characterisations
	of each storyline, and (iii) use of a wider range of future socio-economic
	scenarios than provided by SRES if the aim of an impact assessment
	is to estimate the range of possible future impacts},
  doi = {10.1016/j.gloenvcha.2003.10.004},
  keywords = {Socio-economic scenarios, SRES, Climate change impact assessment,
	Population, Land use change},
  tags = {Uncertainty, Scenarios}
}

@ARTICLE{arnell1996,
  author = {Arnell, N. and Reynard, N.},
  title = {The effects of climate change due to global warming on river flows
	in {Great Britain}},
  journal = {Journal of Hydrology},
  year = {1996},
  volume = {183},
  pages = {397--424},
  number = {3--4},
  abstract = {Global warming due to an increasing concentration of greenhouse gases
	in the atmosphere will affect temperature and rainfall, and hence
	river flows and water resources. This paper presents results from
	an investigation into potential changes in river flows in 21 catchments
	in Great Britain, using a daily rainfall-runoff model and both equilibrium
	and transient climate change scenarios. Annual runoff was simulated
	to increase by 2050 by over 20% in the wettest scenarios and decline
	by over 20% in the driest scenarios — and different catchments respond
	differently to the same change scenario. Monthly flows change by
	a greater percentage than annual flows, and under all the scenarios
	considered there would be a greater concentration of flow in winter.
	Snowfall, and hence snowmelt, would be almost entirely eliminated.
	Progressive changes in river flows over the next few decades would
	be small compared with year-to-year variability, but would be noticeable
	on a decade-to-decade basis.},
  doi = {10.1016/0022-1694(95)02950-8},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{arnoldfohrer2005,
  author = {Arnold, J. and Fohrer, N.},
  title = {{SWAT2000}: current capabilities and research opportunities in applied
	watershed modelling},
  journal = {Hydrological Processes},
  year = {2005},
  volume = {19},
  pages = {563-572},
  number = {3},
  abstract = {SWAT (Soil and Water Assessment Tool) is a conceptual, continuous
	time model that was developed in the early 1990s to assist water
	resource managers in assessing the impact of management and climate
	on water supplies and non-point source pollution in watersheds and
	large river basins. SWAT is the continuation of over 30 years of
	model development within the US Department of Agriculture's Agricultural
	Research Service and was developed to 'scale up' past field-scale
	models to large river basins. Model components include weather, hydrology,
	erosion/sedimentation, plant growth, nutrients, pesticides, agricultural
	management, stream routing and pond/reservoir routing. The latest
	version, SWAT2000, has several significant enhancements that include:
	bacteria transport routines; urban routines; Green and Ampt infiltration
	equation; improved weather generator; ability to read in daily solar
	radiation, relative humidity, wind speed and potential ET; Muskingum
	channel routing; and modified dormancy calculations for tropical
	areas. A complete set of model documentation for equations and algorithms,
	a user manual describing model inputs and outputs, and an ArcView
	interface manual are now complete for SWAT2000. The model has been
	recoded into Fortran 90 with a complete data dictionary, dynamic
	allocation of arrays and modular subroutines. Current research is
	focusing on bacteria, riparian zones, pothole topography, forest
	growth, channel downcutting and widening, and input uncertainty analysis.},
  doi = {10.1002/hyp.5611},
  keywords = {hydrologic modelling, water quality modelling, environmental policy,
	SYSTEME HYDROLOGIQUE EUROPEEN, LAND-USE CHANGES, CLIMATE-CHANGE,
	RUNOFF, SIMULATION, CATCHMENTS, SCALE, BASIN, RIVER, SHE},
  tags = {SWAT}
}

@ARTICLE{arnold+al2000,
  author = {Arnold, J. and Muttiah, R. and Srinivasan, R. and Allen, P.},
  title = {Regional estimation of baseflow and groundwater recharge in the {U}pper
	{M}ississippi {R}iver {B}asin},
  journal = {Journal of Hydrology},
  year = {2000},
  volume = {227},
  pages = {21--40},
  number = {1-4},
  abstract = {Groundwater recharge and discharge (base flow) estimates from two
	methods were compared in the Upper Mississippi River basin (USGS
	hydrologic cataloging unit 07). The Upper Mississippi basin drains
	491,700 km2 in Illinois, Iowa, Missouri, Minnesota, and Wisconsin
	and outlets in the Mississippi River north of Cairo, Illinois. The
	first method uses the water balance components from the soil and
	water assessment tool model (SWAT). The model was used to simulate
	the daily water balance of approximately 16 soil/land use hydrologic
	response units (HRU) within each of the 131 USGS 8-digit watersheds.
	The water balance of each HRU is simulated with four storages: snow,
	soil (up to ten layers), shallow aquifer, and deep aquifer. Groundwater
	recharge is defined as water that percolates past the bottom soil
	layer into the shallow aquifer. Recharge is lagged to become base
	flow and can also be lost to ET. The second method consists of two
	procedures to estimate base flow and recharge from daily stream flow:
	(1) a digital recursive filter to separate base flow from daily flow
	and (2) a modified hydrograph recession curve displacement technique
	to estimate groundwater recharge. These procedures were applied to
	283 USGS stations ranging in area from 50 to 1200 km2. A smoothed
	surface was obtained using a thin plate spline technique and estimates
	were averaged for each 8-digit basin. Simulated flow was calibrated
	against average annual flow for each 8-digit. Without further calibration,
	simulated monthly stream flow was compared against measured flow
	at Alton, Illinois (445,000 km2) from 1961{\^a}**1980. To validate
	the model, measured and simulated monthly stream flow at Alton from
	1981{\^a}**1985 were compared with an R2 of 0.65. No attempt was
	made to calibrate base flow and recharge independent of total stream
	flow. Base flow and recharge from both methods were shown to be in
	general agreement. The filter and recession methods have the potential
	to provide realistic estimates of base flow and recharge for input
	into regional groundwater models and as a check for surface hydrologic
	models},
  bibkey = {Base flow and groundwater recharge; Soil and water assessment tool;
	SWAT; Filter and recession methods},
  doi = {10.1016/S0022-1694(99)00139-0},
  tags = {SWAT}
}

@ARTICLE{arnold+al1999,
  author = {Arnold, J. and Srinivasan, R. and Muttiah, R. and Allen, P.},
  title = {Continental scale simulation of the hydrologic balance},
  journal = {Journal of the American Water Resources Association},
  year = {1999},
  volume = {35},
  pages = {1037--1051},
  number = {5},
  abstract = {This paper describes the application of a continuous daily water balance
	model called SWAT (Soil and Water Assessment Tool) for the conterminous
	U.S. The local water balance is represented by four control volumes;
	(1) snow, (2) soil profile, (3) shallow aquifer, and (4) deep aquifer.
	The components of the water balance are simulated using storage models
	and readily available input parameters. All the required databases
	(soils, landuse, and topography) were assembled for the conterminous
	U.S. at 1:250,000 scale. A GIS interface was utilized to automate
	the assembly of the model input files from map layers and relational
	databases. The hydrologic balance for each soil association polygon
	(78,863 nationwide) was simulated without calibration for 20 years
	using dominant soil and land use properties. The model was validated
	by comparing simulated average annual runoff with long term average
	annual runoff from USGS stream gage records. Results indicate over
	45 percent of the modeled U.S. are within 50 mm of measured, and
	18 percent are within 10 mm without calibration. The model tended
	to underpredict runoff in mountain areas due to lack of climate stations
	at high elevations. Given the limitations of the study, (i.e., spatial
	resolution of the data bases and model simplicity), the results show
	that the large scale hydrologic balance can be realistically simulated
	using a continuous water balance model.},
  doi = {10.1111/j.1752-1688.1999.tb04192.x},
  keywords = {SWAT (Soil and Water Assessment Tool), Surface water hydrology, Hydrologic
	budget, Distributed parameter models, Geographic information systems
	(GIS), Long term trends, Model validation, Runoff, Evapotranspiration,
	HUMUS (Hydrologic Unit Modeling of the United States) project},
  tags = {SWAT, Large Scale}
}

@ARTICLE{arnold+al1998,
  author = {Arnold, J. and Srinivasan, R. and Muttiah, R. and Williams, J.},
  title = {Large Area Hydrologic Modeling and Assessment Part {I}: Model Development},
  journal = {Journal of the American Water Resources Association},
  year = {1998},
  volume = {34},
  pages = {73-89},
  number = {1},
  abstract = {A conceptual, continuous time model called SWAT (Soil and Water Assessment
	Tool) was developed to assist water resource managers in assessing
	the impact of management on water supplies and nonpoint source pollution
	in watersheds and large river basins. The model is currently being
	utilized in several large area projects by EPA, NOAA, NRCS and others
	to estimate the off-site impacts of climate and management on water
	use, non-point source loadings, and pesticide contamination. Model
	development, operation, limitations, and assumptions are discussed
	and components of the model are described. In Part II, a GIS input/output
	interface is presented along with model validation on three basins
	within the Upper Trinity basin in Texas. },
  keywords = {SWAT},
  tags = {SWAT, conceptual model}
}

@ARTICLE{aronica2002,
  author = {Aronica, G. and Bates, P. and Horritt, M.},
  title = {Assessing the uncertainty in distributed model predictions using
	oberved binary pattern information within {GLUE}},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {2001--2002},
  number = {10},
  abstract = {In this paper we extend the generalized likelihood uncertainty estimation
	(GLUE) technique to estimate spatially distributed uncertainty in
	models conditioned against binary pattern data contained in flood
	inundation maps. Untransformed binary pattern data already have been
	used within GLUE to estimate domain-averaged (zero-dimensional) likelihoods,
	yet the pattern information embedded within such sources has not
	been used to estimate distributed uncertainty. Where pattern information
	has been used to map distributed uncertainty it has been transformed
	into a continuous function prior to use, which may introduce additional
	errors. To solve this problem we use here raw binary pattern data
	to define a zero-dimensional global performance measure for each
	simulation in a Monte Carlo ensemble. Thereafter, for each pixel
	of the distributed model we evaluate the probability that this pixel
	was inundated. This probability is then weighted by the measure of
	global model performance, thus taking into account how well a given
	parameter set performs overall. The result is a distributed uncertainty
	measure mapped over real space. The advantage of the approach is
	that it both captures distributed uncertainty and contains information
	on global likelihood that can be used to condition predictions of
	further events for which observed data are not available. The technique
	is applied to the problem of flood inundation prediction at two test
	sites representing different hydrodynamic conditions. In both cases,
	the method reveals the spatial structure in simulation uncertainty
	and simultaneously enables mapping of flood probability predicted
	by the model. Spatially distributed uncertainty analysis is shown
	to contain information over and above that available from global
	performance measures. Overall, the paper highlights the different
	types of information that may be obtained from mappings of model
	uncertainty over real and n-dimensional parameter spaces.},
  doi = {10.1002/hyp.398},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{aronica1998,
  author = {Aronica, G. and Hankin, B. and Beven, K.},
  title = {Uncertainty and equifinality in calibrating distributed roughness
	coefficients in a flood propagation model with limited data},
  journal = {Advances in Water Resources},
  year = {1998},
  volume = {22},
  pages = {349--365},
  number = {4},
  abstract = {Monte-Carlo simulations of a two-dimensional finite element model
	of a flood in the southern part of Sicily were used to explore the
	parameter space of distributed bed-roughness coefficients. For many
	real-world events specific data are extremely limited so that there
	is not only fuzziness in the information available to calibrate the
	model, but fuzziness in the degree of acceptability of model predictions
	based upon the different parameter values, owing to model structural
	errors. Here the GLUE procedure is used to compare model predictions
	and observations for a certain event, coupled with both a fuzzy-rule-based
	calibration, and a calibration technique based upon normal and heteroscedastic
	distributions of the predicted residuals. The fuzzy-rule-based calibration
	is suited to an event of this kind, where the information about the
	flood is highly uncertain and arises from several different types
	of observation. The likelihood (relative possibility) distributions
	predicted by the two calibration techniques are similar, although
	the fuzzy approach enabled us to constrain the parameter distributions
	more usefully, to lie within a range which was consistent with the
	modellers' a priori knowledge of the system.},
  doi = {10.1016/S0309-1708(98)00017-7},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{arpe+al2005,
  author = {Arpe, K. and Hagemann, S. and Jacob, D. and Roeckner, E.},
  title = {The realism of the {ECHAM5} models to simulate the hydrological cycle
	in the {A}rctic and {N}orth {E}uropean area},
  journal = {Nordic Hydrology},
  year = {2005},
  volume = {36},
  pages = {349--367},
  number = {4--5},
  abstract = {A new version of the ECHAM model is investigated in respect of the
	hydrological cycle in the Arctic and North European area. Several
	horizontal and two vertical resolution versions are studied. The
	higher-resolution ECHAM5 models are, in many respects, superior to
	the lower-resolution versions of the same model family and the older
	ECHAM4 model. The vertical resolution has a decisive impact but also
	increased horizontal resolution leads mostly to improvements. Here
	T106 (about 110?km) often gives the best results. The summer maxima
	of precipitation, surface temperature and latent heat flux are simulated
	too early by about a month for several river catchment areas. This
	shift is strongest in the T106 and T159 models. Another problem with
	the annual cycle of precipitation is a relative minimum in August
	to October, especially in the low-resolution ECHAM5 models. The precipitation
	of the ECHAM5 simulations over the Arctic region exceeds all observational
	estimates by 5–15?mm/month, strongest in May–June. The latent heat
	flux over the river catchments has a clear trend towards increased
	fluxes with higher horizontal and vertical resolution, which seems
	to reach a maximum with T106. In the comparison of annual mean P-E
	(precipitation minus evaporation) with observed river discharge only
	the horizontal resolution seems to be important, again giving best
	results for the high-resolution models. The year-by-year variability
	of the simulations is too high, which is more pronounced for the
	higher-resolution versions. Especially strong impacts are found from
	the vertical resolution. The interannual variability of the latent
	heat flux is much smaller than that of precipitation and therefore
	the results shown for precipitation apply also for the simulated
	river discharge. Some forcing of ocean temperature anomalies on the
	precipitation over the Rhine, Kolyma and Indigirka catchment areas
	have been found, from the northeastern Atlantic and from the Pacific
	with developing El Niños. Despite the increased random variability
	in the higher-resolution models, the signal could be detected in
	almost all simulations. On the whole the higher-resolution (horizontal
	and vertical) ECHAM5 model simulations are quite improved compared
	to the low-resolution version of the same model and an older T42
	model version. Increasing the vertical resolution from 19 to 31 levels
	is decisive for this better performance.},
  owner = {rojasro},
  timestamp = {2010.08.04},
  url = {http://www.iwaponline.com/nh/036/nh0360349.htm}
}

@ARTICLE{arpe1999,
  author = {Arpe, K. and Roeckner, E.},
  title = {Simulation of the hydrological cycle over {E}urope: {M}odel validation
	and impacts of increased greenhouse gases},
  journal = {Advances in Water Resources},
  year = {1999},
  volume = {23},
  pages = {105--119},
  number = {2},
  abstract = {Different methods of estimating precipitation area means, based on
	observations, are compared with each other to investigate their usefulness
	for model validation. For the applications relevant to this study
	the ECMWF reanalyses provide a good and comprehensive data set for
	validation. The uncertainties of precipitation analyses, based on
	observed precipitation or from numerical weather forecasting schemes,
	are generally in the range of 20% but regionally much larger. The
	MPI atmospheric general circulation model is able to reproduce long
	term means of the main features of the hydrological cycle within
	the range of uncertainty of observational data, even for relatively
	small areas such as the Rhine river basin. Simulations with the MPI
	coupled general circulation model, assuming a further increase of
	anthropogenic greenhouse gases, show clear trends in temperature
	and precipitation for the next century which would have significant
	implications for human activity, e.g. a further increase of the sea
	level of the Caspian Sea and less water in the Rhine and the Danube.
	We have gained confidence in these results because trends in the
	temperature and precipitation in the coupled model simulations up
	to the present are partly confirmed by an atmospheric model simulation
	forced with observed SSTs and by observational data. We gained further
	confidence because the simulations with the same coupled model but
	using constant greenhouse gases do not show such trends. However,
	doubts arise from the fact that these trends are strong where the
	systematic errors of the model are large.},
  doi = {10.1016/S0309-1708(99)00015-9},
  owner = {rojasro},
  timestamp = {2010.08.04}
}

@ARTICLE{arumugamrao2008,
  author = {Arumugam, M. and Rao, M.},
  title = {On the improved performances of the particle swarm optimization algorithms
	with adaptive parameters, cross-over operators and root mean square
	({RMS}) variants for computing optimal control of a class of hybrid
	systems},
  journal = {Applied Soft Computing},
  year = {2008},
  volume = {8},
  pages = {324--336},
  number = {1},
  abstract = {This paper deals with the concept of including the popular genetic
	algorithm operator, cross-over and root mean square (RMS) variants
	into particle swarm optimization (PSO) algorithm to make the convergence
	faster. Two different PSO algorithms are considered in this paper:
	the first one is the conventional PSO (cPSO) and the second is the
	global-local best values based PSO (GLbest-PSO). The GLbest-PSO includes
	global-local best inertia weight (GLbestIW) with global-local best
	acceleration coefficient (GLbestAC), whereas the cPSO has a time
	varying inertia weight (TVIW) and either time varying acceleration
	coefficient (TVAC) or fixed AC (FAC). The effectiveness of the cross-over
	operator with both PSO algorithms is tested through a constrained
	optimal control problem of a class of hybrid systems. The experimental
	results illustrate the advantage of PSO with cross-over operator,
	which sharpens the convergence and tunes to the best solution. In
	order to compare and verify the validity and effectiveness of the
	new approaches for PSO, several statistical analyses are carried
	out. The results clearly demonstrate that the GLbest-PSO with the
	cross-over operator is a very promising optimization technique. Similar
	conclusions can be made for the GLbest-PSO with RMS variants also},
  doi = {10.1016/j.asoc.2007.01.010},
  highlights = {TVc1, TVc2, TVw},
  keywords = {Particle swarm optimization (PSO), Inertia weight, Acceleration coefficient,
	Cross-over operators, RMS variants, Optimal control},
  tags = {PSO, Calibration}
}

@BOOK{ayyub2006,
  title = {Uncertainty modeling and analysis in engineering and sciences},
  publisher = {Chapman \& Hall/CRC},
  year = {2006},
  author = {Ayyub, B. and Klir, G.},
  pages = {368},
  address = {Florida},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{burger+al2007,
  author = {B\"urger, C. and Kolditz, O. and Fowler, H. and Blenkinsop, S.},
  title = {Future climate scenarios and rainfall-runoff modelling in the {U}pper
	{G}allego catchment ({S}pain)},
  journal = {Environmental Pollution},
  year = {2007},
  volume = {148},
  pages = {842--854},
  number = {3},
  note = {AquaTerra: Pollutant behavior in the soil, sediment, ground, and
	surface water system},
  abstract = {Global climate change may have large impacts on water supplies, drought
	or flood frequencies and magnitudes in local and regional hydrologic
	systems. Water authorities therefore rely on computer models for
	quantitative impact prediction. In this study we present kernel-based
	learning machine river flow models for the Upper Gallego catchment
	of the Ebro basin. Different learning machines were calibrated using
	daily gauge data. The models posed two major challenges: (1) estimation
	of the rainfall--runoff transfer function from the available time
	series is complicated by anthropogenic regulation and mountainous
	terrain and (2) the river flow model is weak when only climate data
	are used, but additional antecedent flow data seemed to lead to delayed
	peak flow estimation. These types of models, together with the presented
	downscaled climate scenarios, can be used for climate change impact
	assessment in the Gallego, which is important for the future management
	of the system. Future climate change and data-based rainfall--runoff
	predictions are presented for the Upper Gallego.},
  bibkey = {climate change, rainfall-runoff, hydrology, kernel-methods, weather
	scenarios},
  doi = {10.1016/j.envpol.2007.02.002},
  issn = {0269-749},
  tags = {Impacts}
}

@ARTICLE{burgerchen2005,
  author = {B\"urger, G. and Chen, Y.},
  title = {Regression--based downscaling of spatial variability for hydrologic
	applications},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {311},
  pages = {299--317},
  number = {1--4},
  abstract = {There is an obvious imbalance between, on the one hand, the importance
	of spatio-temporal variability of precipitation for river flows and,
	on the other, their representation in current empirical downscaling
	models that are applied for climate scenarios. The imperfect variability
	results from incomplete forcing of the large scales. The last IPCC
	report mentioned three regression-based methods that try to overcome
	the imperfection of point-wise variability: randomization, inflation,
	and expanded downscaling, Here, we analyze and compare these methods
	with respect to their spatial variability and how that relates to
	river runoff. Using the downscaled temperature and precipitation
	for observed and simulated large-scale forcings (climate scenarios),
	we applied the hydrologic model HBV for two river basins in Germany.
	We discuss the obvious and hidden model imperfections regarding present
	and future precipitation climate, along with their relevance for
	runoff. The overall picture is quite diverse, and it appears that
	temporal characteristics, i.e. time-lagged effects, are at least
	as important as spatial characteristics. We conclude that, although
	the models agree in a number of essential projections for river flow,
	a more consistent picture requires the full spatio-temporal variability
	as it depends on the large scale atmosphere},
  doi = {10.1016/j.jhydrol.2005.01.025},
  keywords = {Downscaling, Randomization, Inflation, Expanded downscaling, Hydrological
	scenarios, Spatial correlations},
  tags = {Downscaling}
}

@TECHREPORT{bader+al2008,
  author = {Bader, D. and Covey, C. and {Gutowski Jr.}, W. and Held, I. and Kunkel,
	K. and Miller, R. and Tokmakian, R. and Zhang, M.},
  title = {Climate models: {A}n assessment of strengths and limitations},
  institution = {U.S. Climate Change Science Program},
  year = {2008},
  month = {July},
  note = {[Online; last accessed April-2010]},
  tags = {Climate Models},
  url = {http://www.climatescience.gov/Library/sap/sap3-1/final-report/sap3-1-final-all.pdf}
}

@ARTICLE{baguis+al2010,
  author = {Baguis, P. and Roulin, E. and Willems, P. and Ntegeka, V.},
  title = {Climate change and hydrological extremes in {B}elgian catchments},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2010},
  volume = {7},
  pages = {5033--5078},
  number = {4},
  abstract = {In this study we focus our attention on the climate change impacts
	on the hydrological balance in Belgium. There are two main rivers
	in the country, the Scheldt and the Meuse, supplied with water almost
	exclusively by precipitation. With the climate change projected by
	climate models for the end of the current century, one would expect
	that the hydrological regime of the rivers may be affected mainly
	through the changes in precipitation patterns and the increased potential
	evapotranspiration (PET) due to increased temperature throughout
	the year. We examine the hydrology of two important tributaries of
	the rivers Scheldt and Meuse, the Gete and the Ourthe, respectively.
	Our analysis is based on simulations with the SCHEME hydrological
	model and on climate change data from the European PRUDENCE project.
	Two emission scenarios are considered, the SRES A2 and B2 scenarios,
	and the perturbation (or delta) method is used in order to assess
	the climate change signal at monthly time scale and provide appropriate
	input time series for the hydrological simulations. The ensemble
	of climate change scenarios used allows us to estimate the combined
	model and scenario uncertainty in the streamflow calculations, inherent
	to this kind of analysis. In this context, we also analyze extreme
	river flows using two probability distribution families, allowing
	us to quantify the shift of the extremes under climate change conditions.},
  doi = {10.5194/hessd-7-5033-2010},
  owner = {rojasro},
  timestamp = {2010.08.03}
}

@ARTICLE{baigorria2007,
  author = {Baigorria, G. and Jones, J. and Shin, {D.-W.} and Mishra, A. and
	{O'Brien}, J.},
  title = {Assessing uncertainties in crop model simulations using daily bias-corrected
	regional circulation model outputs},
  journal = {Climate Research},
  year = {2007},
  volume = {34},
  pages = {211--222},
  number = {3},
  abstract = {Outputs from the Florida State University/Center for Ocean-Atmospheric
	Prediction Studies (FSU/COAPS) regional spectral model were linked
	to the CERES-Maize dynamic crop model, and the sources of uncertainty
	in yield prediction at 3 sites in the southeastern USA were examined.
	Daily incoming solar radiation, Tmax and Tmin, and rainfall output
	data were obtained from 1987 to 2004 of retrospective forecasts (hindcasts)
	that contained 20 ensemble members. These raw hindcasts were bias-corrected
	on their cumulative probability functions by using the historical
	daily weather records prior to the 18 yr hindcasted period. Six combinations
	of the 4 meteorological variables from raw and bias-corrected hindcasts
	and climatological values were used as sets of weather inputs into
	the CERES-Maize crop model. Uncertainties related to these combinations
	of sets of weather inputs were analyzed. The bias-correction method
	improved values of monthly statistics of the ensemble compared to
	the raw hindcasts in relation to the observed data. The number and
	length of dry spells were also made more accurate with this correction.
	The main source of uncertainty in linking the FSU/COAPS climate model
	to the CERES-Maize crop model was the specific timing of the occurrence
	of dry spells during the cropping seasons. Plant growth stress caused
	by soil water deficit during crucial phenological states largely
	affects simulated yields. Operationally, the inability of FSU/COAPS
	to accurately predict the timing of the occurrence of dry spells
	makes its climate forecasts less useful for farmers wishing to optimize
	planting dates and crop varieties for crops with short crucial phenological
	phases, such as maize.},
  doi = {10.3354/cr00703},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{baltarfontane2008,
  author = {Baltar, Alexandre M. and Fontane, Darrell G.},
  title = {Use of Multiobjective Particle Swarm Optimization in Water Resources
	Management},
  journal = {Journal of Water Resources Planning and Management},
  year = {2008},
  volume = {134},
  pages = {257},
  number = {3},
  abstract = {Water resources management presents a large variety of multiobjective
	problems that require powerful optimization tools in order to fully
	characterize the existing trade-offs. Different optimization methods,
	based on mathematical programming at first and on evolutionary computation
	more recently, have been applied with various degrees of success.
	This paper explores the use of a relatively recent heuristic technique
	called particle swarm optimization (PSO), which has been found to
	perform very well in a wide spectrum of optimization problems. Many
	extensions of the single-objective PSO to handle multiple objectives
	have been proposed in the evolutionary computation literature. This
	paper presents an implementation of multiobjective particle swarm
	optimization (MOPSO) that evaluates alternative solutions based on
	Pareto dominance, using an external repository to store nondominated
	solutions, a fitness sharing approach to promote diversity, and a
	mutation operator to improve global search. The MOPSO solver is used
	on three applications: (1) test function for comparison with results
	of other MOPSO and other evolutionary algorithms reported in the
	literature; (2) multipurpose reservoir operation problem with up
	to four objectives; and (3) problem of selective withdrawal from
	a thermally stratified reservoir with three objectives. In the test
	function application, standard performance metrics were used to measure
	closeness to the true Pareto front and evenness of coverage of the
	nondominated set. Results for the other two applications are compared
	to Pareto solutions obtained using the ε-constraint method with nonlinear
	optimization (ε-NLP). MOPSO performed very well when compared with
	other evolutionary algorithms for the test function and also provided
	encouraging results on the water management applications with the
	advantage of being much simpler than the ε-NLP approach.},
  doi = {10.1061/(ASCE)0733-9496(2008)134:3(257)},
  tags = {PSO}
}

@ARTICLE{basu+al2010,
  author = {Basu, N. and Rao, P. and Winzeler, H. and Kumar, S. and Owens, P.
	and Merwade, V.},
  title = {Parsimonious modeling of hydrologic responses in engineered watersheds:
	Structural heterogeneity versus functional homogeneity},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W04501},
  number = {4},
  abstract = {The central premise of this paper is that extensive modifications
	of land use and hydrology, coupled with intensive management of watersheds
	in the Midwestern United States over the past century, have increased
	the predictability of hydrologic responses, allowing for the use
	of simpler, minimum-calibration models. In these engineered watersheds,
	extensive tile-and-ditch networks have increased the effective drainage
	density and have created bypass flow hydrologic systems that generate
	{``}flashy{''} and {``}predictable{''} hydrographs. We propose a
	simple, threshold-based model, the Threshold-Exceedance-Lagrangian
	Model (TELM), for predicting event hydrographs. TELM was evaluated
	by comparing predicted hydrographs with those measured over a 4 year
	period at the outlet of a mesoscale watershed (Cedar Creek, {$\sim$}700
	km2) in northeastern Indiana. Application of the Soil-Land Inference
	Model (SoLIM) indicated that, despite structural heterogeneities
	(e.g., spatial variability in soil taxonomic mapping units), about
	80\% of the area of the watershed could be assigned a single value
	of available soil water storage, which was the primary soil parameter
	that defined hydrograph response. Hydrograph recession curves for
	multiple events were described well using an exponential function,
	with the mean arrival time (tr) estimated on the basis of the contributing
	drainage area (A) and the mean occurrence time (th) of the event
	hyetograph. Also, functional responses (event hydrographs) at the
	subwatershed scale could be grouped into just two categories on the
	basis of only spatial variability in rainfall patterns. TELM, with
	no parameter calibration, matched the observed hydrographs as well
	as the widely used SWAT model predictions with calibration. Advantages
	and limitations of the proposed modeling approach were identified,
	and needed improvements were discussed},
  doi = {10.1029/2009WR007803},
  tags = {Disturbed Catchments}
}

@INPROCEEDINGS{batelaan2001,
  author = {Batelaan, O. and {De Smedt}, F.},
  title = {Wet{S}pass: {A} flexible {GIS} based, distributed recharge methodology
	for regional groundwater modelling},
  booktitle = {Impact of Human Activity on Groundwater Dynamics},
  year = {2001},
  editor = {Gehers, H. and Peters, J. and Hoehn, E. and Jensen, K. and Leibundgut,
	C. and Griffioen, J. and Webb, B. and Zaadnoordijk, W.},
  pages = {11--17},
  address = {Wallingford},
  publisher = {IAHS},
  owner = {RRojas},
  refid = {BATELAAN2001},
  timestamp = {2008.11.04}
}

@ARTICLE{batelaan2007a,
  author = {Batelaan, O. and {De Smedt}, F.},
  title = {G{IS}--based recharge estimation by coupling surface--subsurface
	water balances},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {337},
  pages = {337--355},
  number = {3--4},
  abstract = {A spatially distributed water balance model is developed to simulate
	long-term average recharge depending on land cover, soil texture,
	topography and hydrometeorological parameters. The model simulates
	recharge iteratively connected to a groundwater model, such that
	the recharge estimate is also influenced by the groundwater depth
	and vice versa. Parameter estimation for the model is performed on
	the basis of literature values of water balance fluxes from mainly
	Belgium and The Netherlands. By graphical and non-linear baseflow
	separation for 17 catchments it is shown that recharge spatially
	varies considerably. The water balance model coupled to a regional
	groundwater model is applied and successfully tested on the 17 catchments.
	The application shows that the resulting recharge has a spatial complex
	pattern, depending to a large extend on the soil texture and land
	cover. Moreover, shallow groundwater levels in valleys cause negative
	recharge conditions as a result of evapotranspiration by abundant
	phreatophytic vegetation. GIS analysis shows how recharge strongly
	varies for different combinations of land cover and soil texture
	classes. The performed analysis provides a better insight into the
	sustenance and management of groundwater resources.},
  doi = {10.1016/j.jhydrol.2007.02.001},
  owner = {RRojas},
  refid = {BATELAAN2007},
  timestamp = {2008.11.04}
}

@ARTICLE{batelaan2004,
  author = {Batelaan, O. and {De Smedt}, F.},
  title = {S{EEPAGE}, a new {MODFLOW DRAIN} {P}ackage},
  journal = {Ground Water},
  year = {2004},
  volume = {42},
  pages = {576--588},
  number = {4},
  abstract = {The prediction of the location of ground water discharge areas is
	a key aspect for the protection and (re)development of ground water-dependent
	wetlands. Ground water discharge areas can be simulated with MODFLOW
	using the DRAIN package by setting the drain level equal to the topography,
	while the conductance is mostly set to an arbitrary high value. However,
	conceptual and practical problems arise in the calculation of the
	ground water discharge by the DRAIN package as calculated water tables
	above the land surface, difficult parameterization of the conductance,
	and large water balance errors. To overcome these problems, a new
	SEEPAGE package for MODFLOW is proposed. The basic idea of this package
	is an adaptable constant head cell. It has a variable head, unless
	the ground water rises above the seepage level, in which case it
	has a constant head cell. The estimation of the ground water discharge
	location along a homogeneous, isotropic, linear sloping profile is
	used to verify the model and to compare it to the DRAIN solution.
	In an application to three basins in Belgium, it is shown that the
	SEEPAGE package can be used in combination with the DRAIN package
	in situations where an upper boundary for a free water table and
	additional resistance for drainage is required. It is clearly demonstrated
	that the identification and delineation of regional ground water
	discharge areas is more accurate using the SEEPAGE package.},
  doi = {10.1111/j.1745-6584.2004.tb02626.x},
  owner = {RRojas},
  refid = {BATELAAN2004},
  timestamp = {2008.11.04}
}

@INCOLLECTION{batelaan1998,
  author = {Batelaan, O. and {De Smedt}, F. and {De Becker}, P. and Huybrechts,
	W.},
  title = {Characterization of a regional groundwater discharge area by combined
	analysis of hydrochemistry, remote sensing and groundwater modelling.},
  booktitle = {Shallow {G}roundwater {S}ystems. {I}nternational contributions to
	hydrogeology 18},
  publisher = {A.A. Balkema},
  year = {1998},
  editor = {Dillon, P. and Simmers, I.},
  pages = {75--86},
  address = {Rotterdam},
  owner = {RRojas},
  refid = {BATELAAN1998},
  timestamp = {2008.11.04}
}

@INPROCEEDINGS{batelaan1993,
  author = {Batelaan, O. and {De Smedt}, F. and {Otero Valle}, M. and Huybrechts,
	W.},
  title = {Development and application of a groundwater model integrated in
	the {GIS GRASS}},
  booktitle = {Application of geographic information systems in hydrology and water
	resources management},
  year = {1993},
  editor = {Kovar, K. and Nachtebel, H.},
  pages = {581--590},
  address = {Vienna, Austria},
  publisher = {IAHS Publ. No 211},
  owner = {RRojas},
  refid = {BATELAAN1993},
  timestamp = {2008.11.04}
}

@ARTICLE{batelaan2007b,
  author = {Batelaan, O. and Meyus, Y. and {De Smedt}, F.},
  title = {De grondwatervoeding van {V}laanderen},
  journal = {Water},
  year = {2007},
  volume = {28},
  pages = {64--71},
  owner = {RRojas},
  refid = {BATELAAN2007A},
  timestamp = {2008.11.04},
  url = {http://www.tijdschriftwater.be/water28-15HI.pdf}
}

@BOOK{batesanderson2001,
  title = {Model Validation - Perspectives In Hydrological Science},
  publisher = {John Wiley \& Sons},
  year = {2001},
  editor = {Bates, P. and Anderson, M.},
  author = {Bates, P. and Anderson, M.},
  note = {ISBN: 0471985724 ISBN-13: 9780471985723},
  abstract = {Description Eighteen chapters discuss various issues surrounding the
	assessment of models currently used in hydrology and environmental
	science for prediction and process understanding. Two common threads
	running throughout the book are the immaturity of the field and the
	lack of data. Topics include calibration and equifinality, models
	in the courtroom, data-based mechanistic modeling and validation
	of rainfall-flow processes, the use of remote sensing to validate
	hydrological models, snow models, ice-sheet models, modeling water
	quality processes in riverine systems, and modeling sediment entrainment
	into transport and deposition in rivers. Annotation c. Book News,
	Inc., Portland, OR (booknews.com) },
  altauthor = {Anderson},
  tags = {Calibration}
}

@ARTICLE{bathurst+al2004,
  author = {Bathurst, J. and Ewen, J. and Parkin, G. and {O'Connel}, P. and Cooper,
	J.},
  title = {Validation of catchment models for predicting land--use and climate
	change impacts. 3. {B}lind validation for internal and outlet responses},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {287},
  pages = {74},
  number = {1--4},
  abstract = {The capability of the physically based, distributed SHETRAN catchment
	modelling system for predictive modelling of hypothetical future
	catchments is validated for the 0.94 km2 Slapton Wood catchment in
	southwest England. A ‘blind’ procedure (without sight of measured
	response data) is used which accounts also for uncertainty in model
	parameter evaluation. Internal catchment conditions as well as the
	outlet discharge are considered, making the test perhaps the severest
	to which a model can be subjected. Data collection formed an integral
	part of the validation procedure and was designed specifically to
	satisfy the needs of the modelling component. The extensive dataset
	which was collected included rainfall, evapotranspiration, soil property
	data, channel geometry, phreatic surface elevation, soil water potential
	and stream discharge. Following a prescribed method, blind predictions
	were made of ten features of the phreatic surface, soil water potential
	and surface runoff responses. Output uncertainty bounds were determined
	as a function of uncertainty in the model parameter values. Subsequent
	comparison of the bounds with the measured data showed that eight
	of the ten predictions passed the specified success criteria, constituting
	a successful validation. Within reasonable uncertainty bounds, and
	on a spatially distributed basis, SHETRAN is shown able to represent
	the annual catchment water balance as well as important features
	of the event-scale response. The results are an encouraging demonstration
	of the fitness of such models for predictive modelling.},
  doi = {10.1016/j.jhydrol.2003.09.021},
  tags = {Uncertainty}
}

@ARTICLE{bavay2009,
  author = {Bavay, M. and Lehning M. and Jonas, T. and Lowe, H.},
  title = {Simulaions of future snow cover and discharge in {Alpine} headwater
	catchments},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {23},
  pages = {95--108},
  number = {1},
  month = {December},
  abstract = {The snow cover in the Alps is heavily affected by climate change.
	Recent data show that at altitudes below 1200 m a.s.l. a time-continuous
	winter snow cover is becoming an exception rather than the rule.
	This would also change the timing and characteristics of river discharge
	in Alpine catchments. We present an assessment of future snow and
	runoff in two Alpine catchments, the larger Inn catchment (1945 km2)
	and the smaller Dischma catchment (43 km2), based on two common climate
	change scenario (IPCC A2 and B2 (IPCC, 2007)). [etc]. The changes
	in snow cover and discharge are predicted using Alpine3D, a model
	for the high-resolution simulation of Alpine surface processes, in
	particular snow, soil and vegetation processes. The predicted changes
	in snow and discharge are extreme. While the current climate still
	supports permanent snow and ice on the highest peaks at altitudes
	above 3000 m a.s.l., this zone would disappear under the future climate
	scenarios. The changes in snow cover could be summarized by approximately
	shifting the elevation zones down by 900 m. The corresponding changes
	in discharge are also severe: while the current climate scenario
	shows a significant contribution from snow melt until middle to late
	summer, the future climate scenarios would feature a much narrower
	snow melt discharge peak in spring. A further observation is that
	heavy precipitation events in the fall would change from mainly snow
	to mainly rain and would have a higher probability of producing flooding.
	Future work is needed to quantify the effect of model uncertainties
	on such predictions.},
  doi = {10.1002/hyp.7195},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{bayer+al2010,
  author = {Bayer, P. and {de Paly}, M. and B\"urger, C.},
  title = {Optimization of high-reliability-based hydrological design problems
	by robust automatic sampling of critical model realizations},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W05504},
  number = {5},
  abstract = {This study demonstrates the high efficiency of the so-called stack-ordering
	technique for optimizing a groundwater management problem under uncertain
	conditions. The uncertainty is expressed by multiple equally probable
	model representations, such as realizations of hydraulic conductivity.
	During optimization of a well-layout problem for contaminant control,
	a ranking mechanism is applied that extracts those realizations that
	appear most critical for the optimization problem. It is shown that
	this procedure works well for evolutionary optimization algorithms,
	which are to some extent robust against noisy objective functions.
	More precisely, differential evolution (DE) and the Covariance Matrix
	Adaptation Evolution Strategy (CMA-ES) are applied. Stack ordering
	is comprehensively investigated for a plume management problem at
	a hypothetical template site based on parameter values measured at
	and on a geostatistical model developed for the Lauswiesen study
	site near T{\"u}bingen, Germany. The straightforward procedure yields
	computational savings above 90\% in comparison to always evaluating
	the full set of realizations. This is confirmed by cross testing
	with four additional validation cases. The results show that both
	evolutionary algorithms obtain highly reliable near-optimal solutions.
	DE appears to be the better choice for cases with significant noise
	caused by small stack sizes. On the other hand, there seems to be
	a problem-specific threshold for the evaluation stack size above
	which the CMA-ES achieves solutions with both better fitness and
	higher reliability.},
  doi = {10.1029/2009WR008081},
  tags = {Calibration}
}

@ARTICLE{beck1987,
  author = {Beck, M.},
  title = {Water quality modeling: a review of the analysis of uncertainty},
  journal = {Water Resources Research},
  year = {1987},
  volume = {23},
  pages = {1393--1442},
  number = {8},
  abstract = {This paper reviews the role of uncertainty in the identification of
	mathematical models of water quality and in the application of these
	models to problems of prediction. More specifically, four problem
	areas are examined in detail: uncertainty about model structure,
	uncertainty in the estimated model parameter values, the propagation
	of prediction errors, and the design of experiments in order to reduce
	the critical uncertainties associated with a model. The review is
	rather lengthy, and it has therefore been prepared in effect as two
	papers. There is a shorter, largely nontechnical version, which gives
	a quick impression of the current and future issues in the analysis
	of uncertainty in water quality modeling. Enclosed by this shorter
	discussion is the main body of the review dealing in turn with (1)
	identifiability and experimental design, (2) the generation of preliminary
	model hypotheses under conditions of sparse, grossly uncertain field
	data, (3) the selection and evaluation of model structure, (4) parameter
	estimation (model calibration), (5) checks and balances on the identified
	model, i.e., model {``}verification{''} and model discrimination,
	and (6) prediction error propagation. Much time is spent in discussing
	the algorithms of system identification, in particular, the methods
	of recursive estimation, and in relating these algorithms and the
	subject of identification to the problems of prediction uncertainty
	and first-order error analysis. There are two obvious omissions from
	the review. It is not concerned primarily with either the development
	and solution of stochastic differential equations or the issue of
	decision making under uncertainty, although clearly some reference
	must be made to these topics. In brief, the review concludes (not
	surprisingly) that much work has been done on the analysis of uncertainty
	in the development of mathematical models of water quality, and much
	remains to be done. A lack of model identifiability has been an outstanding
	difficulty in the interpretation and explanation of past observed
	system behavior, and there is ample evidence to show that the {``}larger,{''}
	more {``}comprehensive{''} models are easily capable of generating
	highly uncertain predictions of future behavior. For the future of
	the subject, it is speculated that there is the possibility of progress
	in the development of novel algorithms for model structure identification,
	a need for new questions to be posed in the problem of prediction,
	and a distinct challenge to the conventional views of this review
	in the new forms of knowledge representation and manipulation now
	emerging from the field of artificial intelligence},
  doi = {10.1029/WR023i008p01393},
  tags = {Uncertainty, Water Quality}
}

@BOOK{beirlant+al2004,
  title = {Statistics of extremes - Theory and applications},
  publisher = {Wiley},
  year = {2004},
  author = {Beirlant, J. and Goegebeur, Y. and Segers, J. and Teugels, J. and
	{De Waal}, D. and Ferro, C.},
  pages = {522},
  series = {Wiley series in probability and statistics},
  address = {Chichester},
  edition = {First},
  owner = {rojasro},
  timestamp = {2010.08.23}
}

@ARTICLE{bekelenicklow2007,
  author = {Bekele, E. and Nicklow, J.},
  title = {Multi-objective automatic calibration of {SWAT} using {NSGA-II}},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {341},
  pages = {165--176},
  abstract = {This paper presents a diagnostic study on muttiobjective, automatic
	calibration of a physically based, semi-distributed watershed model
	known as Soil and Water Assessment Too[ (SWAT). Unlike lumped models,
	distributed models involve large number of calibration parameters,
	representing the spatial heterogeneity of inputs and various physical
	processes within a watershed. An automatic calibration routine is
	developed using the Non-dominated Sorting Genetic Algorithm II (NSGA-II)
	that has been proved to be an effective and efficient muttiobjective
	search technique in various applications. The automatic routine is
	capable of incorporating multiple objectives into the calibration
	process and also employs parameterization to help reduce the number
	of calibration parameters. In this study, SWAT is calibrated for
	daily streamftow and sediment concentration. Two calibration scenarios
	have been considered; the first scenario uses specific objective
	functions to fit different portions of the time series whereas in
	the second scenario, the calibration is performed using data from
	multiple gauging stations, simultaneously. In addition, two cases
	of parameter distribution have been considered in the second scenario
	during parameterization. The application results show that the approach
	is consistent and effective in estimating parameters of the model.
	The use of multiple objectives during the calibration process resulted
	in improved model performance and the second scenario, in particular,
	provided better results partly due to the respective location of
	the gauging stations within the watershed. Further distribution of
	parameters during parameterization also resulted in better sediment
	simulation. (c) 2007 Elsevier B.V. AR rights reserved.},
  doi = {10.1016/j.jhydrol.2007.05.014},
  keywords = {distributed hydrologic models, automatic calibration, multi-objective
	evolutionary algorithms, multiple calibration objectives, RUNOFF
	MODEL, OPTIMIZATION, SENSITIVITY, VALIDATION, OBJECTIVES, MULTIPLE},
  tags = {SWAT, Calibration}
}

@ARTICLE{beldring+al2008,
  author = {Beldring, S. and {Engen-Skaugen}, T. and F{\o}rland, E. and Roald,
	L.},
  title = {Climate change impacts on hydrological processes in {Norway} based
	on two methods for transferring regional climate model results to
	meteorological station sites},
  journal = {Tellus {A}},
  year = {2008},
  volume = {60},
  pages = {439--450},
  number = {3},
  abstract = {Climate change impacts on hydrological processes in Norway have been
	estimated through combination of results from the IPCC SRES A2 and
	B2 emission scenarios, global climate models from the Hadley Centre
	and the Max-Planck Institute, and dynamical downscaling using the
	RegClim HIRHAM regional climate model. Temperature and precipitation
	simulations from the regional climate model were transferred to meteorological
	station sites using two different approaches, the delta change or
	perturbation method and an empirical adjustment procedure that reproduces
	observed monthly means and standard deviations for the control period.
	These climate scenarios were used for driving a spatially distributed
	version of the HBV hydrological model, yielding a set of simulations
	for the baseline period 1961–1990 and projections of climate change
	impacts on hydrological processes for the period 2071–2100. A comparison
	between the two methods used for transferring regional climate model
	results to meteorological station sites is provided by comparing
	the results from the hydrological model for basins located in different
	parts of Norway. Projected changes in runoff are linked to changes
	in the snow regime. Snow cover will be more unstable and the snowmelt
	flood will occur earlier in the year. Increased rainfall leads to
	higher runoff in the autumn and winter.},
  doi = {10.1111/j.1600-0870.2008.00306.x},
  owner = {rojasro},
  timestamp = {2010.08.03}
}

@ARTICLE{bell2007,
  author = {Bell, V. and Kay, A. and Jones, R. and Moore, R.},
  title = {Use of a grid-based hydrological model and regional climate model
	outputs to assess changing flood risk},
  journal = {International Journal of Climatology},
  year = {2007},
  volume = {27},
  pages = {1657--1674},
  number = {12},
  abstract = {A grid-based flow routing and runoff-production model, configured
	to employ regional climate model (RCM) precipitation estimates as
	input, is used to assess the effects of climate change on river flows
	in catchments across the UK. This model, the Grid-to-Grid model or
	G2G, has previously been calibrated and assessed with respect to
	observed river flows under current climate conditions. Here, the
	G2G distributed model together with a lumped catchment model, the
	parameter-generalized PDM, are applied to simulate river flow and
	derive flood frequency curves. Two sets of RCM precipitation time-series,
	on a 25-km grid and at hourly intervals, are used: (1) Current (1961–1990)
	and (2) Future (2071–2100). The effect of one extreme rainfall event
	in the current precipitation series is to raise the estimated peak
	flows for some catchments for high return periods under present day
	rainfall conditions. The future flow series does not contain a comparable
	flow peak. This significantly affects comparison of the flood frequency
	curves derived from the flow simulations obtained using the current
	and future precipitation estimates. Such variability in the results
	and the dependence on one or two extreme rainfall events emphasizes
	the need to examine more than one set of current/future precipitation
	scenarios for flood impact studies.
	
	In the absence of a formal ensemble of climate predictions, a resampling
	method is used to investigate the robustness of the modelled changes
	in flood frequency. Changes in flood frequency at higher return periods
	are, not surprisingly, found to be generally less robust than at
	lower return periods. This is particularly the case for catchments
	in the south and east of England, which were especially affected
	by the extreme rainfall event in the current precipitation series.},
  doi = {10.1002/joc.1539},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@TECHREPORT{psoptim2012,
  author = {Bendtsen, C.},
  title = {{Particle Swarm Optimization (PSO) Package 1.0.3}},
  institution = {{R Foundation for Statistical Computing}},
  year = {2012},
  abstract = {The package provides an implementation of PSO consistent with the
	standard PSO 2007 by Maurice Clerc et al. Additionally a number of
	ancillary routines are provided for easy testing and graphics.},
  owner = {rojasro},
  timestamp = {2011.11.28},
  url = {http://cran.r-project.org/}
}

@ARTICLE{beniston+al2007,
  author = {Beniston, Martin and Stephenson, David and Christensen, Ole and Ferro,
	Christopher and Frei, Christoph and Goyette, Stéphane and Halsnaes,
	Kirsten and Holt, Tom and Jylhä, Kirsti and Koffi, Brigitte and Palutikof,
	Jean and Schöll, Regina and Semmler, Tido and Woth, Katja},
  title = {Future extreme events in European climate: an exploration of regional
	climate model projections},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {71-95},
  abstract = {This paper presents an overview of changes in the extreme events that
	are most likely to affect Europe in forthcoming decades. A variety
	of diagnostic methods are used to determine how heat waves, heavy
	precipitation, drought, wind storms, and storm surges change between
	present (1961–90) and future (2071–2100) climate on the basis of
	regional climate model simulations produced by the PRUDENCE project.
	A summary of the main results follows. Heat waves – Regional surface
	warming causes the frequency, intensity and duration of heat waves
	to increase over Europe. By the end of the twenty first century,
	countries in central Europe will experience the same number of hot
	days as are currently experienced in southern Europe. The intensity
	of extreme temperatures increases more rapidly than the intensity
	of more moderate temperatures over the continental interior due to
	increases in temperature variability. Precipitation – Heavy winter
	precipitation increases in central and northern Europe and decreases
	in the south; heavy summer precipitation increases in north-eastern
	Europe and decreases in the south. Mediterranean droughts start earlier
	in the year and last longer. Winter storms – Extreme wind speeds
	increase between 45°N and 55°N, except over and south of the Alps,
	and become more north-westerly than cuurently. These changes are
	associated with reductions in mean sea-level pressure, leading to
	more North Sea storms and a corresponding increase in storm surges
	along coastal regions of Holland, Germany and Denmark, in particular.
	These results are found to depend to different degrees on model formulation.
	While the responses of heat waves are robust to model formulation,
	the magnitudes of changes in precipitation and wind speed are sensitive
	to the choice of regional model, and the detailed patterns of these
	changes are sensitive to the choice of the driving global model.
	In the case of precipitation, variation between models can exceed
	both internal variability and variability between different emissions
	scenarios.},
  affiliation = {University of Geneva Climate Research Geneva Switzerland},
  doi = {10.1007/s10584-006-9226-z},
  issn = {0165-0009},
  issue = {0},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{bergstrom+al2001,
  author = {Bergstr\"om, S. and Carlsson, B. and Gardelin, M. and Lindstr\"om,
	G. and Pettersson, A. and Rummukainen, M.},
  title = {Climate change impacts on runoff in {S}weeden--{A}ssessments by global
	climate models, dynamical downscaling and hydrological modelling},
  journal = {Climate Research},
  year = {2001},
  volume = {16},
  pages = {101--112},
  number = {2},
  abstract = {The Swedish regional climate modelling programme, SWECLIM, started
	in 1997 with the main goal being to produce regional climate change
	scenarios over the Nordic area on a time scale of 50 to 100 yr. An
	additional goal is to produce water resources scenarios with a focus
	on hydropower production, dam safety, water supply and environmental
	aspects of water resources. The scenarios are produced by a combination
	of global climate models (GCMs), regional climate models and hydrological
	runoff models. The GCM simulations used thus far are 10 yr time slices
	from 2 different GCMs, UKMO HadCM2 from the Hadley Centre and the
	ECHAM4/OPYC3 of the Max Planck Institute for Meteorology. The regional
	climate model is a modified version of the international HIRLAM forecast
	model and the hydrological model is the HBV model developed at the
	Swedish Meteorological and Hydrological Institute. Scenarios of river
	runoff have been simulated for 6 selected basins covering the major
	climate regions in Sweden. Changes in runoff totals, runoff regimes
	and extreme values have been analysed with a focus on the uncertainties
	introduced by the choice of GCM and routines for estimation of evapotranspiration
	in the hydrological model. It is further shown how these choices
	affect the statistical return periods of future extremes in a design
	situation.},
  doi = {10.3354/cr016101},
  file = {:E\:\\rojasro\\My Documents\\articles\\Climate change impacts on runoff in Sweeden-Assessments by global climate models, dynamical downscaling and hydrological modelling (Bergstrom et al. 2001).pdf:PDF},
  tags = {Uncertainty, Climate Change}
}

@BOOK{bernardosmith2000,
  title = {Bayesian theory},
  publisher = {John Wiley \& Sons Inc.},
  year = {2000},
  author = {Bernardo, J. and Smith, A.},
  pages = {608},
  address = {Chichester},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{berthet+al2010a,
  author = {Berthet, L. and Andreassian, V. and Perrin, C. and Loumagne, C.},
  title = {{How significant are quadratic criteria? Part 1. How many years are
	necessary to ensure the data-independence of a quadratic criterion
	value?}},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {1051--1062},
  number = {6},
  abstract = {Quadratic criteria are widely used to assess the performance of hydrological
	models. However, the statistical nature of the errors makes the interpretation
	of these criteria tricky. In this paper, root mean square error (RMSE)
	values were computed for a hydrological model over a set of 178 varied
	catchments on two 5-year data series: the computed RMSE values can
	actually reflect the content of the data series with which they are
	calculated as much as the intrinsic skills of the model. The error
	model proposed by Yang et al. (2007) is used to assess a lower bound
	of the RMSE confidence interval width, depending on the length of
	the data series used for the assessment. Our analysis indicates that
	the data series would have to be longer than several decades to ensure
	that computed RMSEs are close to their statistical expectation. The
	practical consequences of this result are raised and discussed. },
  doi = {10.1080/02626667.2010.505890},
  keywords = {model assessment, skill scores, quadratic criterion },
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{berthet+al2010b,
  author = {Berthet, L. and Andreassian, V. and Perrin, C. and Loumagne, C.},
  title = {{How significant are quadratic criteria? Part 2. On the relative
	contribution of large flood events to the value of a quadratic criterion}},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {1063--1073},
  number = {6},
  abstract = {Quadratic criteria (i.e. based on squared residuals) are widely used
	to assess the performance of hydrological models. However, the largest
	errors have a relatively strong influence on the final criterion
	values, which may be considered a drawback for a complete assessment.
	This paper studies the case of updated models used for real-time
	forecasting. It is shown that the fraction of the data series actually
	impacting the final criterion value is small on many catchments and
	corresponds to the time steps characterised by the greatest runoff
	variations. In fact, model updating makes the error distribution
	more peak-shaped, giving even more relative importance to the time
	steps with the largest errors. Therefore, assessing the performance
	of an updated model with a quadratic criterion emphasises that these
	criteria focus more on the most difficult time steps to model (and
	the most interesting ones in the case of short-term flood forecasting).},
  doi = {10.1080/02626667.2010.505891},
  keywords = {model assessment, skill scores, quadratic criteria, flood forecasting,
	persistence index, heteroscedasticity},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{beven2009,
  author = {Beven, K.},
  title = {Comment on ``{Equifinality of formal (DREAM) and informal (GLUE)
	Bayesian approaches in hydrologic modeling?" by Vrugt, J. and {ter
	Braak}, C. and Gupta, H. and Robinson, B.}},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2009},
  volume = {In press},
  pages = {1059--1060},
  number = {7},
  doi = {10.1007/s00477-008-0283-x},
  owner = {rojasro},
  timestamp = {2009.09.14}
}

@ARTICLE{beven2007,
  author = {Beven, K.},
  title = {Towards integrated environmental models of everywhere: uncertainty,
	data and modelling as a learning process},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {460--467},
  abstract = {Developing integrated environmental models of everywhere such as are
	demanded by the requirements of, for example, implementing the Water
	Framework Directive in Europe, is constrained by the limitations
	of current understanding and data availability. The possibility of
	such models raises questions about system design requirements to
	allow modelling as a learning and data assimilation process in the
	representation of places, which might well be treated as active objects
	in such a system. Uncertainty in model predictions not only poses
	issues about the value of different types of data in characterising
	places and constraining predictive uncertainty but also about how
	best to present the pedigree of such uncertain predictions to users
	and decision-makers.},
  doi = {10.5194/hess-11-460-2007},
  keywords = {hydrological models, hydroecological models, characterising places,
	prediction uncertainty, SYSTEME HYDROLOGIQUE EUROPEEN, RAINFALL-RUNOFF
	MODEL, DISTRIBUTED MODEL, SUBSURFACE FLOW, METHODOLOGY, CALIBRATION,
	VALIDATION, CATCHMENT, PREDICTIONS, EQUIFINALITY},
  tags = {Philosophical}
}

@ARTICLE{beven2006a,
  author = {Beven, K.},
  title = {A manifesto for the equifinality thesis},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {320},
  pages = {18--36},
  number = {1--2},
  abstract = {This essay discusses some of the issues involved in the identification
	and predictions of hydrological models given some calibration data.
	The reasons for the incompleteness of traditional calibration methods
	are discussed. The argument is made that the potential for multiple
	acceptable models as representations of hydrological and other environmental
	systems (the equifinality thesis) should be given more serious consideration
	than hitherto. It proposes some techniques for an extended GLUE methodology
	to make it more rigorous and outlines some of the research issues
	still to be resolved.},
  doi = {10.1016/j.jhydrol.2005.07.007},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven2006b,
  author = {Beven, K.},
  title = {On undermining the science?},
  journal = {Hydrological Processes},
  year = {2006},
  volume = {20},
  pages = {3141--3146},
  number = {14},
  doi = {10.1002/hyp.6396},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven2005,
  author = {Beven, K.},
  title = {On the concept of model structural error},
  journal = {Water Science \& Technology},
  year = {2005},
  volume = {52},
  pages = {167--175},
  number = {6},
  abstract = {A consideration of model structural error leads to some particularly
	interesting tensions in the model calibration/conditioning process.
	In applying models we can usually only assess the total error on
	some output variable for which we have observations. This total error
	may arise due to input and boundary condition errors, model structural
	errors and error on the output observation itself (not only measurement
	error but also as a result of differences in meaning between what
	is modelled and what is measured). Statistical approaches to model
	uncertainty generally assume that the errors can be treated as an
	additive term on the (possibly transformed) model output. This allows
	for compensation of all the sources of error, as if the model predictions
	are correct and the total error can be treated as "measurement error."
	Model structural error is not easily evaluated within this framework.
	An alternative approach to put more emphasis on model evaluation
	and rejection is suggested. It is recognised that model success or
	failure within this framework will depend heavily on an assessment
	of both input data errors (the "perfect" model will not produce acceptable
	results if driven with poor input data) and effective observation
	error (including a consideration of the meaning of observed variables
	relative to those predicted by a model).},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.iwaponline.com/wst/05206/wst052060167.htm}
}

@ARTICLE{beven2001a,
  author = {Beven, K.},
  title = {How far can we go in distributed hydrological modelling?},
  journal = {Hydrology and Earth System Sciences},
  year = {2001},
  volume = {5},
  pages = {1--12},
  number = {1},
  abstract = {This paper considers distributed hydrological models in hydrology
	as an expression of a pragmatic realism. Some of the problems of
	distributed modelling are discussed including the problem of nonlinearity,
	the problem of scale, the problem of equifinality, the problem of
	uniqueness and the problem of uncertainty. A structure for the application
	of distributed modelling is suggested based on an uncertain or fuzzy
	landscape space to model space mapping. This is suggested as the
	basis for an Alternative Blueprint for distributed modelling in the
	form of an application methodology. This Alternative Blueprint is
	scientific in that it allows for the formulation of testable hypotheses.
	It focuses attention on the prior evaluation of models in terms of
	physical realism and on the value of data in model rejection. Finally,
	some unresolved questions that distributed modelling must address
	in the future are outlined, together with a vision for distributed
	modelling as a means of learning about places.},
  doi = {10.5194/hess-5-1-2001},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven2001b,
  author = {Beven, K.},
  title = {On stochastic models and the single realization},
  journal = {Hydrological Processes},
  year = {2001},
  volume = {15},
  pages = {895--896},
  number = {5},
  doi = {10.1002/hyp.438},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven2000a,
  author = {Beven, K.},
  title = {On model uncertainty, risk and decision making},
  journal = {Hydrological Processes},
  year = {2000},
  volume = {14},
  pages = {2605--2606},
  number = {14},
  doi = {10.1002/1099-1085(20001015)14:14<2605::AID-HYP400>3.0.CO;2-W},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven2000b,
  author = {Beven, K.},
  title = {Uniqueness of place and process representations in hydrological modelling},
  journal = {Hydrology and Earth System Sciences},
  year = {2000},
  volume = {4},
  pages = {203--213},
  number = {2},
  abstract = {This paper addresses the problem of uniqueness of catchment areas
	in relation to model representations of flow processes. The uniqueness
	of field measurements as a limitation on model representations is
	discussed. The treatment of uniqueness as a residual from a modelled
	relationship may conceal information about the uniqueness of catchments,
	while the treatment of uniqueness as a set of parameter values within
	a particular model structure is problematic due to the equifinality
	of model structures and parameter sets. The analysis suggests that
	a fully reductionist approach to describe the uniqueness of individual
	catchment areas by the aggregation of descriptions of small scale
	behaviour will be impossible given current measurement technologies.
	A suggested strategy for the representation of uniqueness of place
	as a fuzzy mapping of the landscape into a model space is suggested.
	This will lead to a quantification of the uncertainty in predictions
	of any particular location in a way that allows a conditioning of
	the mapping on the basis of the available data. This process can
	incorporate a hypothesis testing approach to model evaluation but
	the problem of multiple behavioural models may provide an ultimate
	limitation on the realism of process representations: not on the
	principle of realism but on the possibility of unambiguous process
	representations.},
  doi = {10.5194/hess-4-203-2000},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven1997,
  author = {Beven, K.},
  title = {{TOPMODEL}: A critique},
  journal = {Hydrological Processes},
  year = {1997},
  volume = {11},
  pages = {1069--1085},
  abstract = {TOPMODEL (a TOPography based hydrological MODEL) is now 20 years old
	and has been the subject of numerous applications to a wide variety
	of catchments. This paper represents a critical review of some of
	the issues involved in application of the TOPMODEL concepts, including
	the basic assumptions involved; the derivation of topographic index
	distributions from digital terrain data; additional model components;
	meaning and calibration of the model parameters; and issues involved
	in model validation and predictive uncertainty. The aim is to provoke
	a thoughtful approach to hydrological modelling and the interaction
	of modelling and field work. Some recommendations are made for future
	modelling practice. (C) 1997 by John Wiley \& Sons, Ltd.},
  doi = {10.1002/(SICI)1099-1085(199707)11:9<1069::AID-HYP545>3.0.CO;2-O},
  keywords = {TOPMODEL, distributed hydrological models, model calibration, GLUE,
	uncertainty, transmissivity, DIGITAL TERRAIN MODELS, HYDROLOGICAL
	MODELS, RUNOFF GENERATION, SWISS CATCHMENT, FLOW, SCALE, UNCERTAINTY,
	SENSITIVITY, VARIABILITY, PARAMETERS},
  tags = {conceptual model}
}

@ARTICLE{beven1993,
  author = {Beven, K.},
  title = {Prophecy, reality and uncertainty in distributed hydrological modelling},
  journal = {Advances in Water Resources},
  year = {1993},
  volume = {16},
  pages = {41--51},
  number = {1},
  abstract = {Difficulties in defining truly mechanistic model structures and difficulties
	of model calibration and validation suggest that the application
	of distributed hydrological models is more an exercise in prophecy
	than prediction. One response to these problems is outlined in terms
	of a realistic assessment of uncertainty in hydrological prophecy,
	together with a framework (GLUE) within which such ideas can be implemented.
	It is suggested that a post-modernistic hydrology will recognise
	the uncertainties inherent in hydrological modelling and will focus
	attention on the value of data in conditioning hydrological prophecies.},
  doi = {10.1016/0309-1708(93)90028-E},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven1989,
  author = {Beven, K.},
  title = {Changing ideas in hydrology - The case of physically-based models},
  journal = {Journal of Hydrology},
  year = {1989},
  volume = {105},
  pages = {157--172},
  number = {1-2},
  abstract = {This paper argues that there are fundamental problems in the application
	of physically-based models for practical prediction in hydrology.
	These problems result from limitations of the model equations relative
	to a heterogeneous reality; the lack of a theory of subgrid scale
	integration; practical constraints on solution methodologies; and
	problems of dimensionality in parameter calibration. It is suggested
	that most current applications of physically-based models use them
	as lumped conceptual models at the grid scale. Recent papers on physically-based
	models have misunderstood and misrepresented these limitations. There
	are practical hydrological problems requiring physically-based predictions,
	and there will continue to be a need for physically-based models
	but ideas about their capabilities must change so that future applications
	attempt to obtain realistic estimates of the uncertainty associated
	with their predictions, particularly in the case of evaluating future
	scenarios of the effects of management strategies.},
  doi = {10.1016/0022-1694(89)90101-7},
  tags = {conceptual model}
}

@ARTICLE{bevenbinley1992,
  author = {Beven, K. and Binley, A.},
  title = {The future of distributed models: {M}odel calibration and uncertainty
	prediction},
  journal = {Hydrological Processes},
  year = {1992},
  volume = {6},
  pages = {279--283},
  number = {5},
  abstract = {This paper describes a methodology for calibration and uncertainty
	estimation of distributed models based on generalized likelihood
	measures. the GLUE procedure works with multiple sets of parameter
	values and allows that, within the limitations of a given model structure
	and errors in boundary conditions and field observations, different
	sets of values May, be equally likely as simulators of a catchment.
	Procedures for incorporating different types of observations into
	the calibration; Bayesian updating of likelihood values and evaluating
	the value of additional observations to the calibration process are
	described. the procedure is computationally intensive but has been
	implemented on a local parallel processing computer. the methodology
	is illustrated by an application of the Institute of Hydrology Distributed
	Model to data from the Gwy experimental catchment at Plynlimon, mid-Wales.},
  doi = {10.1002/hyp.3360060305},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{bevenfreer2001,
  author = {Beven, K. and Freer, J.},
  title = {Equifinality, data assimilation, and uncertainty estimation in mechanistic
	modelling of complex environmental systems using the {GLUE} methodology},
  journal = {Journal of Hydrology},
  year = {2001},
  volume = {249},
  pages = {11--29},
  number = {1--4},
  abstract = {It may be endemic to mechanistic modelling of complex environmental
	systems that there are many different model structures and many different
	parameter sets within a chosen model structure that may be behavioural
	or acceptable in reproducing the observed behaviour of that system.
	This has been called the equifinality concept. The generalised likelihood
	uncertainty estimation (GLUE) methodology for model identification
	allowing for equifinality is described. Prediction within this methodology
	is a process of ensemble forecasting using a sample of parameter
	sets from the behavioural model space, with each sample weighted
	according to its likelihood measure to estimate prediction quantiles.
	This allows that different models may contribute to the ensemble
	prediction interval at different time steps and that the distributional
	form of the predictions may change over time. Any effects of model
	nonlinearity, covariation of parameter values and errors in model
	structure, input data or observed variables, with which the simulations
	are compared, are handled implicitly within this procedure. GLUE
	involves a number of choices that must be made explicit and can be
	therefore subjected to scrutiny and discussion. These include ways
	of combining information from different types of model evaluation
	or from different periods in a data assimilation context. An example
	application to rainfall-runoff modelling is used to illustrate the
	methodology, including the updating of likelihood measures. (C) 2001
	Elsevier Science B.V. All rights reserved.},
  doi = {10.1016/S0022-1694(01)00421-8},
  keywords = {GLUE, TOPMODEL, maimai catchment, rainfall-runoff modelling, parameter
	conditioning, prediction uncertainty, GLUE, PARAMETER UNCERTAINTY,
	BAYESIAN-ESTIMATION, DISTRIBUTED MODELS, SVAT MODEL, CALIBRATION,
	PREDICTION, CATCHMENTS, HYDROLOGY, TOPMODEL, SCALE},
  tags = {Uncertainty}
}

@ARTICLE{beven2008,
  author = {Beven, K. and Smith, P. and Freer, J.},
  title = {So just why would a modeller choose to be incoherent?},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {354},
  pages = {15--32},
  number = {1--4},
  abstract = {This article provides an extended response to the criticisms of the
	GLUE methodology by Mantovan and Todini [Mantovan, P., Todini, E.,
	2006. Hydrological forecasting uncertainty assessment: incoherence
	of the GLUE methodology. J. Hydrol. 330, 368–381]. It is shown that
	the formal Bayesian identification of models is a special case of
	GLUE that can be used where the modeller is prepared to make very
	strong assumptions about the nature of the modelling errors. Under
	such assumptions, GLUE can be coherent in the sense of Manotvan and
	Todini. In real applications, however, with multiple sources of uncertainty
	including model structural error, their strong definition of coherence
	is shown to be inapplicable to the extent that the choice of a formal
	likelihood function based on a simple error structure may be an incoherent
	choice. It is demonstrated by some relatively minor modifications
	of their hypothetical example that misspecification of the error
	model and the non-stationarities associated with the presence of
	input error and model structural error in the Bayes approach will
	then produce well-defined but incorrect parameter distributions.
	This empirical result is quite independent of GLUE, but the flexibility
	of the GLUE approach may then prove to be an advantage in providing
	more coherent and robust choices of model evaluation in these cases
	and, by analogy, in other non-ideal cases for real applications.
	At the current time it is difficult to make a reasoned choice between
	methods of uncertainty estimation for real applications because of
	a lack of understanding of the real information content of data in
	conditioning models.},
  doi = {10.1016/j.jhydrol.2008.02.007},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{bevensmithfreer2007,
  author = {Beven, K. and Smith, P. and Freer, J.},
  title = {Comment on ``{H}ydrological forecasting uncertainty assessment: Incoherence
	of the {GLUE} methodology'' by {P.} {M}antovan and {E.} {T}odini},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {338},
  pages = {315--318},
  number = {3--4},
  abstract = {This comment is a response to the criticisms of the GLUE methodology
	by [Mantovan, P., Todini, E., 2006. Hydrological forecasting uncertainty
	assessment: Incoherence of the GLUE methodology, J. Hydrology, 2006].
	In this comment it is shown that the formal Bayesian identification
	of models is a special case of GLUE that can be used where the modeller
	is prepared to make very strong assumptions about the nature of the
	modelling errors. For the hypothetical study of Mantovan and Todini,
	exact assumptions were assumed known for the formal Bayesian identification,
	but were then ignored in the application of GLUE to the same data.
	We show that a more reasonable application of GLUE to this problem
	using similar prior knowledge shows that gives equally coherent results
	to the formal Bayes identification. In real applications, subject
	to input and model structural error it is suggested that the coherency
	condition of MT06 cannot hold at the single observation level and
	that the choice of a formal Bayesian likelihood function may then
	be incoherent. In these (more interesting) cases, GLUE can be coherent
	in the application of likelihood measures based on blocks of data,
	but different choices of measures and blocks effectively represent
	different beliefs about the information content of data in real applications
	with input and model structural errors.},
  doi = {10.1016/j.jhydrol.2007.02.023},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{beven+al2011,
  author = {Beven, K. and Smith, P. and Wood, A.},
  title = {On the colour and spin of epistemic error (and what we might do about
	it)},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {3123--3133},
  number = {10},
  abstract = {Disinformation as a result of epistemic error is an issue in hydrological
	modelling. In particular the way in which the colour in model residuals
	resulting from epistemic errors should be expected to be non-stationary
	means that it is difficult to justify the spin that the structure
	of residuals can be properly represented by statistical likelihood
	functions. To do so would be to greatly overestimate the information
	content in a set of calibration data and increase the possibility
	of both Type I and Type II errors. Some principles of trying to identify
	periods of disinformative data prior to evaluation of a model structure
	of interest, are discussed. An example demonstrates the effect on
	the estimated parameter values of a hydrological model.},
  doi = {10.5194/hess-15-3123-2011},
  owner = {rojasro},
  timestamp = {2012.10.11}
}

@ARTICLE{bevenyoung2003,
  author = {Beven, K. and Young, P.},
  title = {Comment on "{B}ayesian recursive parameter estimation for hydrologic
	models" by {M}. {T}hiemann, {M}. {T}rosset, {H}. {G}upta, and {S}.
	{S}orooshian},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1116},
  doi = {10.1029/2001WR001183},
  tags = {Calibration, Uncertainty}
}

@INPROCEEDINGS{binkleyhagiwara2005,
  author = {Binkley, K. and Hagiwara, M.},
  title = {Particle swarm optimization with area of influence: increasing the
	effectiveness of the swarm},
  booktitle = {Swarm Intelligence Symposium, 2005. SIS 2005. Proceedings 2005 IEEE},
  year = {2005},
  pages = {45--52},
  month = {june},
  abstract = { In this paper we present a new definition of neighborhood for particle
	swarm optimization (PSO) methods called area of influence. Area of
	influence (AOI) derives from the observation that in nature the effective
	exchange of information between individuals of a society deteriorates
	as their physical distance increases. In PSO with AOI, the loss of
	information exchange, ability with distance is simulated by making
	the exchange of information a function of the physical distance between
	particles in hyperspace. In this paper, we compare the AOI method
	to the standard PSO neighborhood methods, global best, local best,
	and von Neumann. We also introduce a local search method using reinitialization
	of velocity components based on the current search range. We show
	that AOI along with local search and a time-varying constriction
	coefficient provides strong benefits to several PSO algorithms. Results
	are presented using the standard benchmark functions from the PSO
	literature.},
  doi = {10.1109/SIS.2005.1501601},
  issn = { },
  keywords = { AOI method, PSO neighborhood method, area of influence, information
	exchange, local search method, particle swarm optimization, time-varying
	constriction coefficient, velocity component reinitialization, evolutionary
	computation, particle swarm optimisation, search problems},
  tags = {Calibration, PSO}
}

@ARTICLE{binleybeven2003,
  author = {Binley, A. and Beven, K.},
  title = {Vadose zone flow model uncertainty as conditioned on geophysical
	data},
  journal = {Ground Water},
  year = {2003},
  volume = {41},
  pages = {119--127},
  number = {2},
  abstract = {An approach to estimating the uncertainty in model descriptions based
	on a landscape space to model space mapping concept is described.
	The approach is illustrated by an application making use of plot
	scale geophysical estimates of changes in water content profiles
	to condition a model of recharge to the Sherwood Sandstone Aquifer
	in the United Kingdom. It is demonstrated that the mapping is highly
	uncertain and that many different parameter sets give acceptable
	simulations of the observations. Multiple profile measurements over
	time offer only limited additional constraints on the mapping. The
	resulting mapping weights may be used to evaluate uncertainty in
	the predictions of vadose zone flow dynamics for the site.},
  doi = {10.1111/j.1745-6584.2003.tb02576.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{blasone2008a,
  author = {Blasone, {R.-S.} and Madsen, H. and Rosbjerg, D.},
  title = {Uncertainty assessment of integrated distributed hydrological models
	using {GLUE} with {M}arkov chain {M}onte {C}arlo sampling},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {353},
  pages = {18--32},
  number = {1--2},
  abstract = {In recent years, there has been an increase in the application of
	distributed, physically-based and integrated hydrological models.
	Many questions regarding how to properly calibrate and validate distributed
	models and assess the uncertainty of the estimated parameters and
	the spatially-distributed responses are, however, still quite unexplored.
	Especially for complex models, rigorous parameterization, reduction
	of the parameter space and use of efficient and effective algorithms
	are essential to facilitate the calibration process and make it more
	robust. Moreover, for these models multi-site validation must complement
	the usual time validation. In this study, we develop, through an
	application, a comprehensive framework for multi-criteria calibration
	and uncertainty assessment of distributed physically-based, integrated
	hydrological models. A revised version of the generalized likelihood
	uncertainty estimation (GLUE) procedure based on Markov chain Monte
	Carlo sampling is applied in order to improve the performance of
	the methodology in estimating parameters and posterior output distributions.
	The description of the spatial variations of the hydrological processes
	is accounted for by defining a measure of model performance that
	includes multiple criteria and spatially-distributed information.
	An initial sensitivity analysis is conducted on the model to avoid
	overparameterisation and to increase the robustness of the approach.
	It is demonstrated that the employed methodology increases the identifiability
	of the parameters and results in satisfactory multi-variable simulations
	and uncertainty estimates. However, the parameter uncertainty alone
	cannot explain the total uncertainty at all the sites, due to limitations
	in the distributed data included in the model calibration. The study
	also indicates that properly distributed information of discharge
	is particularly crucial in model calibration and validation.},
  doi = {10.1016/j.jhydrol.2007.12.026},
  owner = {RRojas},
  timestamp = {2008.07.11}
}

@ARTICLE{blasone2008b,
  author = {Blasone, {R.-S.} and Vrugt, J. and Madsen, H. and Rosbjerg, D. and
	Robinson, B. and Zyvoloski, G.},
  title = {Generalized likelihood uncertainty estimation ({GLUE}) using adaptive
	{M}arkov {C}hain {M}onte {C}arlo sampling},
  journal = {Advances in Water Resources},
  year = {2008},
  volume = {31},
  pages = {630--648},
  number = {4},
  abstract = {In the last few decades hydrologists have made tremendous progress
	in using dynamic simulation models for the analysis and understanding
	of hydrologic systems. However, predictions with these models are
	often deterministic and as such they focus on the most probable forecast,
	without an explicit estimate of the associated uncertainty. This
	uncertainty arises from incomplete process representation, uncertainty
	in initial conditions, input, output and parameter error. The generalized
	likelihood uncertainty estimation (GLUE) framework was one of the
	first attempts to represent prediction uncertainty within the context
	of Monte Carlo (MC) analysis coupled with Bayesian estimation and
	propagation of uncertainty. Because of its flexibility, ease of implementation
	and its suitability for parallel implementation on distributed computer
	systems, the GLUE method has been used in a wide variety of applications.
	However, the MC based sampling strategy of the prior parameter space
	typically utilized in GLUE is not particularly efficient in finding
	behavioral simulations. This becomes especially problematic for high-dimensional
	parameter estimation problems, and in the case of complex simulation
	models that require significant computational time to run and produce
	the desired output. In this paper we improve the computational efficiency
	of GLUE by sampling the prior parameter space using an adaptive Markov
	Chain Monte Carlo scheme (the Shuffled Complex Evolution Metropolis
	(SCEM-UA) algorithm). Moreover, we propose an alternative strategy
	to determine the value of the cutoff threshold based on the appropriate
	coverage of the resulting uncertainty bounds. We demonstrate the
	superiority of this revised GLUE method with three different conceptual
	watershed models of increasing complexity, using both synthetic and
	real-world streamflow data from two catchments with different hydrologic
	regimes.},
  doi = {10.1016/j.advwatres.2007.12.003},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{blenkinsopfowler2005,
  author = {Blenkinsop, S. and Fowler, H.},
  title = {Assessment of climatology in climate model},
  institution = {Newcastle University, School of Civil Engineering and Geosciences},
  year = {2005},
  type = {Aquaterra Deliverable {H}1.4},
  address = {Newcastle upon Tyne, NE1 7RU, UK.},
  tags = {Downscaling}
}

@ARTICLE{blenkinsopfowler2007,
  author = {Blenkinsop, S. and Fowler, H. J.},
  title = {Changes in European drought characteristics projected by the PRUDENCE
	regional climate models},
  journal = {International Journal of Climatology},
  year = {2007},
  volume = {27},
  pages = {1595--1610},
  number = {12},
  abstract = {One of the key features of global climate change will be perturbations
	to the hydrological regime across Europe. To date, assessments of
	the impacts of future change have generally used results from only
	one climate model, thus underestimating the range of possible change
	projected by different climate models. Here, the skill of six regional
	climate models (RCMs) in reproducing the mean precipitation for the
	1961–1990 period for six catchments across Europe is compared and
	their projections of changes in future precipitation are assessed.
	A simple drought index based on monthly precipitation anomalies is
	also described and used to assess the models. Considerable variation
	in model skill in reproducing monthly mean precipitation and drought
	statistics is observed, with model errors in the reproduction of
	drought events independent of those for the mean, suggesting that
	the models have difficulties in reproducing the observed persistence
	of low monthly rainfall totals. In broad terms, the models indicate
	decreases in summer and increases in winter precipitation across
	Europe. On the regional scales required for impacts analysis, considerable
	model uncertainty is demonstrated for future projections, particularly
	for drought frequency. Although increases in the frequency of long-duration
	droughts are identified for catchments in southern Europe, the magnitude
	of this change is not certain. In contrast, for a catchment in northern
	England, such events are likely to become less frequent. For shorter-duration
	droughts, future changes encompass the direction of change. For stakeholders
	in each of the regions, these changes and uncertainties pose different
	challenges for the management of water resources. For the scientific
	community, the challenge raised is how to incorporate this uncertainty
	in climate change projections in a way that allows those groups to
	make informed decisions based on model projections. It is suggested
	that probabilistic scenarios for specific hydrological impacts offer
	considerable potential to achieve this. Copyright © 2007 Royal Meteorological
	Society},
  doi = {10.1002/joc.1538},
  issn = {1097-0088},
  keywords = {drought, climate change, regional climate models, probabilistic scenarios,
	uncertainty, Europe},
  publisher = {John Wiley \& Sons, Ltd.}
}

@ARTICLE{block2009,
  author = {Block, P. and Filho, F. and Sun, L. and Kwon, {H.-H.}},
  title = {A streamflow forecasting framework using multiple climate and hydrological
	models},
  journal = {Journal of the American Water Resources Association},
  year = {2009},
  volume = {45},
  pages = {828--843},
  number = {4},
  abstract = {Water resources planning and management efficacy is subject to capturing
	inherent uncertainties stemming from climatic and hydrological inputs
	and models. Streamflow forecasts, critical in reservoir operation
	and water allocation decision making, fundamentally contain uncertainties
	arising from assumed initial conditions, model structure, and modeled
	processes. Accounting for these propagating uncertainties remains
	a formidable challenge. Recent enhancements in climate forecasting
	skill and hydrological modeling serve as an impetus for further pursuing
	models and model combinations capable of delivering improved streamflow
	forecasts. However, little consideration has been given to methodologies
	that include coupling both multiple climate and multiple hydrological
	models, increasing the pool of streamflow forecast ensemble members
	and accounting for cumulative sources of uncertainty. The framework
	presented here proposes integration and offline coupling of global
	climate models (GCMs), multiple regional climate models, and numerous
	water balance models to improve streamflow forecasting through generation
	of ensemble forecasts. For demonstration purposes, the framework
	is imposed on the Jaguaribe basin in northeastern Brazil for a hindcast
	of 1974-1996 monthly streamflow. The ECHAM 4.5 and the NCEP/MRF9
	GCMs and regional models, including dynamical and statistical models,
	are integrated with the ABCD and Soil Moisture Accounting Procedure
	water balance models. Precipitation hindcasts from the GCMs are downscaled
	via the regional models and fed into the water balance models, producing
	streamflow hindcasts. Multi-model ensemble combination techniques
	include pooling, linear regression weighting, and a kernel density
	estimator to evaluate streamflow hindcasts; the latter technique
	exhibits superior skill compared with any single coupled model ensemble
	hindcast.},
  doi = {10.1111/j.1752-1688.2009.00327.x},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@ARTICLE{boe2009a,
  author = {Bo\'e, J. and Terray, L. and Cassou, C. and Najac, J.},
  title = {Uncertainties in {E}uropean summer precipitation changes: role of
	large scale circulation},
  journal = {Climate Dynamics},
  year = {2009},
  volume = {33},
  pages = {265--276},
  number = {2--3},
  abstract = {Climate models suggest that anthropogenic emissions are likely to
	induce an important drying during summer over most of Europe in the
	late 21st century. However, the amplitude of the associated decrease
	in precipitation strongly varies among the different climate models.
	In order to reduce this spread, it is first necessary to identify
	its causes and the associated physical mechanisms. Consequently,
	the focus of this paper is to better estimate the role of large scale
	circulation (LSC) in precipitation changes over Europe using a multi-model
	framework and then to characterize the LSC changes using the weather
	regime paradigm. We show that LSC changes directly lead to a decrease
	of precipitation over northwestern Europe. This circulation-driven
	decrease in rainfall is mainly linked to an increase (decrease) of
	the occurrence of positive (negative) phase of the North Atlantic
	Oscillation regime. LSC is also responsible for a significant part
	of the models spread in precipitation changes over these regions.
	Over southern Europe, the role of LSC changes on multi-model mean
	precipitation changes is generally weak. We also show that the precipitation
	anomalies directly induced by LSC modifications seem to be further
	amplified through local feedbacks.},
  doi = {10.1007/s00382-008-0474-7},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{boe2007,
  author = {Bo\'e, J. and Terray, L. and Habets, F. and Martin, E.},
  title = {Statistical and dynamical downscaling of the {S}eine basin climate
	for hydro-meteorological studies},
  journal = {International Journal of Climatology},
  year = {2007},
  volume = {27},
  pages = {1643--1655},
  number = {12},
  abstract = {Two downscaling methods designed for the study of the hydrological
	impact of climate change on the Seine basin in France are tested
	for present climate. First, a multivariate statistical downscaling
	(SD) methodology based on weather typing and conditional resampling
	is described. Then, a bias correction technique for dynamical downscaling
	based on quantile–quantile mapping is introduced. To evaluate the
	end-to-end SD methodology, the atmospheric forcing derived from the
	large-scale circulation (LSC) of the ERA40 reanalysis by SD is used
	to force a hydrological model. Simulated discharges reproduce historical
	values reasonably well. Next, the dynamical and statistical approaches
	are compared using the M´et´eo–France ARPEGE general circulation
	model in a variable resolution configuration (resolution around 60
	km over France). The ARPEGE simulation is downscaled using the two
	methodologies, and hydrological simulations are performed. Regarding
	downscaled temperature and precipitation, the statistical approach
	is more efficient in reproducing the temporal and spatial autocorrelation
	properties. The simulated river discharges from the two approaches
	are nevertheless very similar: the two methods reproduce well the
	seasonal cycle and the daily distribution of streamflows. Finally,
	the results of the study are discussed from a practical impact study
	perspective.},
  doi = {10.1002/joc.1602},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@ARTICLE{boe2009b,
  author = {Bo\'e, J. and Terray, L. and Martin, E. and Habets, F.},
  title = {Projected changes in components of the hydrological cycle in {F}rench
	river basins during the 21st century},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {1--15},
  number = {W08426},
  abstract = {The main objective of this paper is to study the impacts of climate
	change on the hydrological cycle of the main French river basins,
	including the different uncertainties at stake. In particular, the
	relative importance of modeling uncertainty versus that of downscaling
	uncertainty is investigated. An ensemble of climate scenarios are
	statistically downscaled in order to force a hydrometeorological
	model over France. Then, the main changes in different variables
	of the hydrological cycle are studied. Despite large uncertainties
	linked to climate models, some robust signals already appear in the
	middle of the 21st century. In particular, a decrease in mean discharges
	in summer and fall, a decrease in soil moisture, and a decrease in
	snow cover, especially pronounced at the low and intermediate altitudes,
	are simulated. The low flows become more frequent but generally weak,
	and uncertain changes in the intensity of high flows are simulated.
	To evaluate downscaling uncertainties and assess the robustness of
	the results obtained with the statistical downscaling method, two
	other downscaling approaches are used. The first one is a dynamical
	downscaling methodology based on a variable resolution atmospheric
	model, with a quantile-quantile bias correction of the model variables.
	The second approach is based on the so-called anomaly method, that
	simply consists of perturbing present climate observations by the
	climatological change simulated by global climate models. After hydrological
	modeling, some discrepancies exist among the results from the different
	downscaling methods. However they remain limited and to a large extent
	smaller than climate model uncertainties, which raises important
	methodological considerations.},
  doi = {10.1029/2008WR007437},
  owner = {rojasro},
  timestamp = {2010.08.03}
}

@ARTICLE{boberg2009a,
  author = {Boberg, F. and Berg, P. and Thejil, P. and Gutowski, J. and Christensen,
	J.},
  title = {Improved confidence in climate change projections of precipitation
	further evaluated using daily statistics from {ENSEMBLES} models},
  journal = {Climate Dynamics},
  year = {2010},
  volume = {35},
  pages = {1509--1520},
  number = {7--8},
  abstract = {Probability density functions for daily precipitation data are used
	as a validation tool comparing station measurements to seven transient
	regional climate model runs, with a horizontal resolution of 25 km
	and driven by the SRES A1B scenario forcing, within the ENSEMBLES
	project. The validation is performed for the control period 1961–1990
	for eight predefined European subregions, and a ninth region enclosing
	all eight subregions, with different climate characteristics. Models
	that best match the observations are then used for making climate
	change projections of precipitation distributions during the twenty-first
	century for each subregion separately. We find, compared to the control
	period, a distinct decrease in the contribution to the total precipitation
	for days with moderate precipitation and a distinct increase for
	days with more intense precipitation. This change in contribution
	to the total precipitation is found to amplify with time during all
	of the twenty-first century with an average rate of 1.1% K?1. Furthermore,
	the crossover point separating the decreasing from the increasing
	contributions does not show any significant change with time for
	any specific subregion. These results are a confirmation and a specification
	of the results from a previous study using the same station measurements
	but with a regional climate model ensemble within the PRUDENCE project.},
  doi = {10.1007/s00382-009-0683-8},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@TECHREPORT{boberg2009b,
  author = {Boberg, F. and Berg, P. and Thejll, P.},
  title = {Comparing regional climate models with gridded observations within
	the {ENSEMBLES} project using probability density functions},
  institution = {Danish Meteorological Institute},
  year = {2009},
  number = {09-04},
  abstract = {Future precipitation over Europe is being modelled on the basis of
	present knowledge of observed precipitation and expectations for
	the future based on regional climate models. The observational data
	are vital for determining which of the models best represent current
	precipitation, but the precipitation data are available in two versions
	- as raw station data and as a specially gridded set. We investigate
	here whether these two representations are similar or not. A newly
	developed gridded data set, constructed using interpolations of station
	observations, is validated using regional climate model experiments
	within the ENSEMBLES project. The validation is made for daily precipitation
	and daily mean temperature during the period 1961-1990 using probability
	density functions for nine different European subregions with different
	climate characteristics. It is found that the gridded observations
	deviate from the models for precipitation, for all seasons and for
	most subregions, with an overestimation of days with moderate precipitation
	and an underestimation of days with more intense precipitation. For
	daily mean temperature, the gridded observations often underestimate
	the number of hot days, which is found for most subregions and most
	seasons. The cold day part of the model distribution is however relatively
	well matched by the gridded observations. We also include a publicly
	available station based observational data set in the study. This
	data set constitutes a relatively large fraction of the observations
	used to produce the gridded observational data set and it is, despite
	of its inhomogeneous spatial distribution, found to match the daily
	model precipitation distribution relatively well for most subregions
	and most seasons. However, compared with the gridded observations,
	the daily mean temperature distribution for the station observations
	is often poorly matching the model distributions, especially for
	colder days during summer.},
  owner = {rojasro},
  timestamp = {2010.06.21},
  url = {http://www.dmi.dk/dmi/dkc09-04.pdf}
}

@ARTICLE{boldetti+al2010,
  author = {Boldetti, G. and Riffard, M. and Andreassian, V. and Oudin, L.},
  title = {Data-set cleansing practices and hydrological regionalization: is
	there any valuable information among outliers?},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {941--951},
  number = {6},
  abstract = {In hydrological regionalization studies, where one attempts to transfer
	information from gauged (donor) stations to ungauged (target) ones,
	the problem of data quality and reliability is often raised. Should
	all the available data be used? Or should some donor stations be
	considered unreliable for some reason and therefore discarded? In
	this article, we address these questions by proposing a new method
	to detect potentially undesirable stations: this method to identify
	outliers is based on the detection of catchments which do not fit
	in their neighbourhood. We apply this approach to a case of simple
	regionalization involving reference flows and compare it with the
	traditional outlier detection method. As expected, different outlier
	definitions lead to considerably different results, and the proposed
	method appears to perform noticeably better than the traditional
	one. },
  doi = {10.1080/02626667.2010.505171},
  keywords = {regionalization, ungauged catchments, donor catchments, outliers },
  tags = {Calibration, Regionalization, Philosophical, Outliers}
}

@ARTICLE{booij2005,
  author = {Booij, M.},
  title = {Impact of climate change on river flooding assessed with different
	spatial model resolutions},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {303},
  pages = {176--198},
  number = {1--4},
  abstract = {The impact of climate change on ï¬‚ooding in the river Meuse is assessed
	on a daily basis using spatially and temporally changed climate patterns
	and a hydrological model with three different spatial resolutions.
	This is achieved by selecting a hydrological modelling framework
	and implementing appropriate model components, derived in an earlier
	study, into the selected framework (HBV). Additionally, two other
	spatial resolutions for the hydrological model are used to evaluate
	the sensitivity of the model results to spatial model resolution
	and to allow for a test of the model appropriateness procedure. Generations
	of a stochastic precipitation model under current and changed climate
	conditions have been used to assess the climate change impacts. The
	average and extreme discharge behaviour at the basin outlet is well
	reproduced by the three versions of the hydrological model in the
	calibration and validation, the results become somewhat better with
	increasing model resolution. The model results with synthetic precipitation
	under current climate conditions show a small overestimation of average
	discharge behaviour and a considerable underestimation of extreme
	discharge behaviour. The underestimation of extreme discharges is
	caused by the small scale character of the observed precipitation
	input at the sub-basin scale. The general trend with climate change
	is a small decrease of the average discharge and a small increase
	of discharge variability and extreme discharges. The variability
	in extreme discharges for climate change conditions increases with
	respect to the simulations for current climate conditions. This variability
	results both from the stochasticity of the precipitation process
	and the differences between the climate models. The total uncertainty
	in river ï¬‚ooding with climate change (over 40%) is much larger
	than the change with respect to current climate conditions (less
	than 10%). However, climate changes are systematic changes rather
	than random changes and thus the large uncertainty range will be
	shifted to another level corresponding to the changed average situation.
	},
  doi = {10.1016/j.jhydrol.2004.07.013},
  tags = {Impacts}
}

@ARTICLE{booijkrol2010,
  author = {Booij, M. and Krol, M.},
  title = {Balance between calibration objectives in a conceptual hydrological
	model},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {1017--1032},
  number = {6},
  abstract = {Three different measures to determine the optimum balance between
	calibration objectives are compared: the combined rank method, parameter
	identifiability and model validation. Four objectives (water balance,
	hydrograph shape, high flows, low flows) are included in each measure.
	The contributions of these objectives to the specific measure are
	varied to find the optimum balance between the objectives for each
	measure. The methods are applied to nine middle-sized catchments,
	using a typical conceptual hydrological model. The results indicate
	that differences in the optimum balance between the combined rank
	method and parameter identifiability on the one hand, and model validation
	on the other, are considerable. The theoretical optimum balance would
	be a situation without trade-off between single objectives. For some
	catchments and measures, this situation is closely approximated.
	On average, the performance of combined rank method is somewhat better
	than that of parameter identifiability (respectively 3.6\% and 5.0\%
	below the theoretical optimum), where the performance of model validation
	is considerably lower (22.4\% below the theoretical optimum). These
	results are supported by additional validation tests which gave robust
	results for the combined rank measure and the parameter identifiability
	measure, and less robust results for the model validation measure.
	},
  doi = {10.1080/02626667.2010.505892},
  keywords = {calibration objectives, combined rank measure, parameter identifiability,
	model validation, HBV model, Meuse basin },
  tags = {Calibration}
}

@ARTICLE{borahbera2004,
  author = {Borah, D. and Bera, M.},
  title = {Watershed-scale hydrologic and nonpoint-source pollution models:
	Review of applications},
  journal = {Transactions of the ASABE},
  year = {2004},
  volume = {47},
  pages = {789--803},
  number = {3},
  abstract = {Three watershed-scale hydrologic and nonpoint-source pollution models,
	all having the three major components (hydrology, sediment, and chemical),
	were selected based on a review of eleven models (AGNPS, AnnAGNPS,
	ANSWERS, ANSWERS-Continuous, CASC2D, DWSM, HSPF, KINEROS, MIKE SHE,
	PRMS, and SWAT) presented in a companion artide. Those selected were
	SWAT, a promising model for long-term continuous simulations in predominantly
	agricultural watersheds; HSPF, a promising model for long-term continuous
	simulations in mixed agricultural and urban watersheds; and DWSM,
	a promising storm event (rainfall) simulation model for agricultural
	and suburban watersheds. In this article, applications of these three
	models, as reported and found in the literature, are reviewed und
	discussed. Seventeen SWAT, twelve HSPF, and eighteen DWSM applications
	are compiled. SWAT and HSPF require a significant amount of data
	and empirical parameters for development and calibration. DWSM has
	efficient physically (process) based simulation routines and therefore
	has a small number of calibration parameters. SWAT and HSPF were
	found suitable for predicting yearly flow volumes, sediment. and
	nutrient loads. Monthly predictions were generally good, except for
	months having extreme storm events and hydrologic conditions. Daily
	simulations of extreme flow events were poor: DWSM reasonably predicted
	distributed flow hydrographs, and concentration or discharge graphs
	of sediment, nutrient, and pestides at small time intervals resulting
	from rainfall events. Combined use of these complementary models
	and perhaps other trtodels having different strengths is warranted
	to adequately address water quantity and quality problems and their
	solutions},
  keywords = {SWAT, REview of applications},
  tags = {SWAT},
  url = {http://asae.frymulti.com/abstract.asp?aid=16110&t=1}
}

@ARTICLE{bormann+al2009,
  author = {Bormann, H. and Breuer, L. and Gr\"aff, T. and Huisman, J. and Croke,
	B.},
  title = {Assessing the impact of land use change on hydrology by ensemble
	modelling ({LUCHEM}) {IV}: {M}odel sensitivity to data aggregation
	and spatial (re-)distribution},
  journal = {Advances in Water Resources},
  year = {2009},
  volume = {32},
  pages = {171--192},
  number = {2},
  abstract = {This paper analyses the effect of spatial resolution and distribution
	of model input data on the results of regional-scale land use scenarios
	using three different hydrological catchment models. A 25 m resolution
	data set of a mesoscale catchment and three land use scenarios are
	used. Data are systematically aggregated to resolutions up to 2 km.
	Land use scenarios are spatially redistributed, both randomly and
	topography based. Using these data, water fluxes are calculated on
	a daily time step for a 16 year time period without further calibration.
	Simulation results are used to identify grid size, distribution and
	model dependent scenario effects. In the case of data aggregation,
	all applied models react sensitively to grid size. WASIM and TOPLATS
	simulate constant water balances for grid sizes from 50 m to 300-500
	m, SWAT is more sensitive to input data aggregation, simulating constant
	water balances between 50 m and 200 m grid size. The calculation
	of scenario effects is less robust to data aggregation. The maximum
	acceptable grid size reduces to 200-300 m for TOPLATS and WASIM.
	In case of spatial distribution, SWAT and TOPLATS are slightly sensitive
	to a redistribution of land use (below 1.5% for water balance terms),
	whereas WASIM shows almost no reaction. Because the aggregation effects
	were stronger than the redistribution effects, it is concluded that
	spatial discretisation is more important than spatial distribution.
	As the aggregation effect was mainly associated with a change in
	land use fraction, it is concluded that accuracy of data sets is
	much more important than a high spatial resolution.},
  doi = {10.1016/j.advwatres.2008.01.002},
  keywords = {Hydrological catchment models, Model comparison, Land use change,
	Spatial distribution, Input data aggregation},
  tags = {Uncertainty, Climate Change}
}

@ARTICLE{bosch+al2004,
  author = {Bosch, D. and Sheridan, J. and Batten, H. and Arnold, J.},
  title = {Evaluation of the {SWAT} model on a coastal plain agricultural watershed},
  journal = {Transactions of the ASABE},
  year = {2004},
  volume = {47},
  pages = {1493--1506},
  number = {5},
  abstract = {The Better Assessment Science Integrating point and Nonpoint Sources
	(BASINS) system was developed by the U.S. Environmental Protection
	Agency to facilitate developing total maximum daily loads (TMDLs).
	The Soil Water Assessment Tool (SWAT) is one of the watershed-scale
	simulation models within BASINS. Because of the critical nature of
	the TMDL process, it is imperative that BASINS and SWAT be adequately
	validated for regions on which they are being applied. BASINS and
	SWAT were tested using six years of hydrologic data from a 22 km(2)
	subwatershed of the Little River in Georgia. Comparisons were made
	between water balance results obtained using high and low spatial
	resolution data as well as those obtained using default initial parameters
	versus those modified for existing groundwater conditions. In general,
	all scenarios simulated general trends in the observed flow data.
	However, for the years with lower precipitation, the total water
	yields simulated with the low spatial resolution data and the default
	initial conditions were overpredicted by up to 27\% of the annual
	precipitation input. Total water yields simulated using the high
	spatial resolution input data were within 20\% of the observed yields
	for each year of the assessment. Nash-Sutcliffe model efficiencies
	(E) for monthly total water yields were 0.80 using the high spatial
	resolution data with the modified initial conditions and 0.64 using
	the low spatial resolution data with the default initial conditions.
	While the model simulated general streamflow trends, discrepancies
	were observed between observed and simulated hydrograph peaks, time
	to peak, and hydrograph durations. A one-day time lag between the
	simulated and observed time to peak was the primary cause of large
	errors in daily flow simulations. Model modification and more extensive
	calibration may be necessary to increase the accuracy of the daily
	flow estimates for TMDL development.},
  keywords = {hydrologic modeling, streamflow, watersheds, STREAMFLOW, SIMULATION,
	SEDIMENT, WETLANDS, QUALITY, LOADS},
  tags = {SWAT, GIS related},
  url = {http://asae.frymulti.com/abstract.asp?aid=17629&t=1}
}

@ARTICLE{bouwer+al2010,
  author = {Laurens M. Bouwer and Philip Bubeck and Jeroen C.J.H. Aerts},
  title = {Changes in future flood risk due to climate and development in a
	Dutch polder area},
  journal = {Global Environmental Change},
  year = {2010},
  volume = {20},
  pages = {463--471},
  number = {3},
  note = {<ce:title>Governance, Complexity and Resilience</ce:title>},
  abstract = {Damages from weather related disasters are projected to increase,
	due to a combination of increasing exposure of people and assets,
	and expected changes in the global climate. Only few studies have
	assessed in detail the potential range of losses in the future and
	the factors contributing to the projected increase. Here we estimate
	future potential damage from river flooding, and analyse the relative
	role of land-use, asset value increase and climate change on these
	losses, for a case study area in The Netherlands. Projections of
	future socioeconomic change (land-use change and increase in the
	value of assets) are used in combination with flood scenarios, projections
	of flooding probabilities, and a simple damage model. It is found
	that due to socioeconomic change, annual expected losses may increase
	by between 35 and 172% by the year 2040, compared to the baseline
	situation in the year 2000. If no additional measures are taken to
	reduce flood probabilities or consequences, climate change may lead
	to an increase in expected losses of between 46 and 201%. A combination
	of climate and socioeconomic change may increase expected losses
	by between 96 and 719%. Asset value increase has a large role, as
	it may lead to a doubling of losses. The use of single loss estimates
	may lead to underestimation of the impact of extremely high losses.
	We therefore also present loss–probability curves for future risks,
	in order to assess the increase of the most extreme potential loss
	events. Our approach thus allows a more detailed and comprehensive
	assessment than previous studies that could also be applied in other
	study areas to generate flood risk projections. Adaptation through
	flood prevention measures according to currently planned strategies
	would counterbalance the increase in expected annual losses due to
	climate change under all scenarios.},
  doi = {10.1016/j.gloenvcha.2010.04.002},
  issn = {0959-3780},
  keywords = {Adaptation}
}

@TECHREPORT{bovolo+al2008,
  author = {Bovolo, C. and Blenkinsop, S. and Fowler, H. and B\"urger, C. and
	Majone, B.},
  title = {Application of climate change scenarios in {AT} case study catchments},
  institution = {University of Newcastle, School of Civil Engineering and Geosciences
	and University of T{\"u}bingen and University of Trento, Dep. of
	Civil and Environmental Engineering},
  year = {2008},
  type = {Aquaterra Deliverable {H}1.12},
  tags = {Downscaling}
}

@TECHREPORT{bovolo+al2009,
  author = {Bovolo, C. and Blenkinsop, S. and Fowler, H. and {Zambrano-Bigiarini},
	M. and Bellin, A.},
  title = {Production of climate change scenarios for large catchments},
  institution = {Newcastle University, School of Civil Engineering and Geosciences
	and University of Trento, Dep. of Civil and Environmental Engineering},
  year = {2009},
  type = {Aquaterra Deliverable {H}1.13},
  tags = {Downscaling}
}

@ARTICLE{box1980,
  author = {Box, G.E.P.},
  title = {Sampling and {B}ayes' inference in scientific modelling and robustness},
  journal = {Journal of the Royal Statistical Society Series A},
  year = {1980},
  volume = {143},
  pages = {383--430},
  number = {4},
  abstract = {Scientific learning is an iterative process employing Criticism and
	Estimation. Correspondingly the formulated model factors into two
	complementary parts a predictive part allowing model criticism, and
	a Bayes posterior part allowing estimation. Implications for significance
	tests, the theory of precise measurement and for ridge estimates
	are considered. Predictive checking functions for transformation,
	serial correlation, bad values, and their relation with Bayesian
	options are considered. Robustness is seen from a Bayesian viewpoint
	and examples are given. For the bad value problem a comparison with
	M estimators is made.},
  file = {:E\:\\rojasro\\My Documents\\articles\\Sampling and Bayes inference in scientific modelling and robustness (Box, 1980).pdf:PDF},
  owner = {rojasro},
  timestamp = {2009.09.10},
  url = {http://www.jstor.org/stable/2982063}
}

@ARTICLE{boyer+al2010,
  author = {Boyer, C. and Chaumont, D. and Chartier, I. and Roy, A.},
  title = {Impact of climate change on the hydrology of {S}t. {L}awrence tributaries},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {384},
  pages = {65--83},
  number = {1--2},
  abstract = {Changes in temperature and precipitation projected for the next century
	will induce important modifications into the hydrological regimes
	of the St. Lawrence tributaries (Quebec, Canada). The temperature
	increase anticipated during the winter and spring seasons will affect
	precipitation phase and consequently the snow/precipitation ratio
	and the water volume stored into snow cover. The impact on northern
	river hydrology and geomorphology will be significant. In this study
	we aim to assess the magnitude of the hydrological alteration associated
	with climate change; to model the projected temporal shift in the
	occurrence of winter/spring center-volume date; to assess the sensitivity
	of the winter/spring center-volume date to changes in climatic variables
	and to examine the latitudinal component of the projected changes
	through the use of five watersheds on both shores of the St. Lawrence.
	The study emphasizes changes in the winter and spring seasons. Projected
	river discharges for the next century were generated with the hydrological
	model HSAMI run with six climate series projections. Three General
	Circulation Models (HadCM3, CSIRO-Mk2 and ECHAM4) and two greenhouse
	gas emissions scenarios (A2 and B2) were used to create a range of
	plausible scenarios. The projected daily climate series were produced
	using the historical data of a reference period (1961--1990) with
	a perturbation factor equivalent to the monthly mean difference (temperature
	and precipitation) between a GCM in the future for three 30 year
	horizons (2010--2039, 2040--2069; 2070--2099) and the reference period.
	These climate projections represent an uncertainty envelope for the
	projected hydrologic data. Despite the differences due mainly to
	the GCM used, most of the hydrological simulations projected an increase
	in winter discharges and a decrease in spring discharges. The center-volume
	date is expected to be in advance by 22--34 days depending on the
	latitude of the watershed. The increase in mean temperature with
	the simultaneous decrease of the snow/precipitation ratio during
	the winter and spring period explain a large part of the projected
	hydrological changes. The latitude of the river governed the timing
	of occurrence of the maximum change (sooner for tributaries located
	south) and the duration of the period affected by marked changes
	in the temporal distribution of discharge (longer time scale for
	rivers located at higher latitudes). Higher winter discharges are
	expected to have an important geomorphological impact mostly because
	they may occur under ice-cover conditions. Lower spring discharges
	may promote sedimentation into the tributary and at their confluence
	with the St. Lawrence River. The combined effects of modifications
	in river hydrology and geomorphological processes will likely impact
	riparian ecosystems.},
  doi = {10.1016/j.jhydrol.2010.01.011},
  keywords = {River, Hydrology, Climate change, Stream flow, Variability, Modeling},
  tags = {Climate Change}
}

@ARTICLE{boyle+al2000,
  author = {Boyle, D. and Gupta, H. and Sorooshian, S.},
  title = {Toward Improved Calibration of Hydrologic Models: Combining the Strengths
	of Manual and Automatic Methods},
  journal = {Water Resources Research},
  year = {2000},
  volume = {36},
  pages = {3663--3674},
  number = {12},
  abstract = {Automatic methods for model calibration seek to take advantage of
	the speed and power of digital computers, while being objective and
	relatively easy to implement. However, they do not provide parameter
	estimates and hydrograph simulations that are considered acceptable
	by the hydrologists responsible for operational forecasting and have
	therefore not entered into widespread use. In contrast, the manual
	approach which has been developed and refined over the years to result
	in excellent model calibrations is complicated and highly labor-intensive,
	and the expertise acquired by one individual with a specific model
	is not easily transferred to another person (or model). In this paper,
	we propose a hybrid approach that combines the strengths of each.
	A multicriteria formulation is used to "model" the evaluation techniques
	and strategies used in manual calibration, and the resulting optimization
	problem is solved by means of a computerized algorithm. The new approach
	provides a stronger test of model performance than methods that use
	a single overall statistic to aggregate model errors over a large
	range of hydrologic behaviors. The power of the new approach is illustrated
	by means of a case study using the Sacramento Soil Moisture Accounting
	model.},
  doi = {10.1029/2000WR900207},
  keywords = {Manual calibration, automatic calibration, multi-criteria, multi-objective,
	RAINFALL-RUNOFF MODELS, IMPROVED PARAMETER INFERENCE, SHUFFLED COMPLEX
	EVOLUTION, GLOBAL OPTIMIZATION, CATCHMENT MODELS, UNCERTAINTY, ALGORITHMS,
	SCHEME},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{bredehoeft2005,
  author = {Bredehoeft, J.},
  title = {The conceptualization model problem--surprise},
  journal = {Hydrogeology Journal},
  year = {2005},
  volume = {13},
  pages = {37--46},
  number = {1},
  abstract = {The foundation of model analysis is the conceptual model. Surprise
	is defined as new data that renders the prevailing conceptual model
	invalid; as defined here it represents a paradigm shift. Limited
	empirical data indicate that surprises occur in 20–30% of model analyses.
	These data suggest that groundwater analysts have difficulty selecting
	the appropriate conceptual model. There is no ready remedy to the
	conceptual model problem other than (1) to collect as much data as
	is feasible, using all applicable methods—a complementary data collection
	methodology can lead to new information that changes the prevailing
	conceptual model, and (2) for the analyst to remain open to the fact
	that the conceptual model can change dramatically as more information
	is collected. In the final analysis, the hydrogeologist makes a subjective
	decision on the appropriate conceptual model. The conceptualization
	problem does not render models unusable. The problem introduces an
	uncertainty that often is not widely recognized. Conceptual model
	uncertainty is exacerbated in making long-term predictions of system
	performance.},
  doi = {10.1007/s10040-004-0430-5},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{bredehoeft2003,
  author = {Bredehoeft, J.},
  title = {From models to performance assessment: {T}he conceptualization problem},
  journal = {Ground Water},
  year = {2003},
  volume = {41},
  pages = {571--577},
  number = {5},
  abstract = {Today, models are ubiquitous tools for ground water analyses. The
	intent of this paper is to explore philosophically the role of the
	conceptual model in analysis. Selection of the appropriate conceptual
	model is an a priori decision by the analyst. Calibration is an integral
	part of the modeling process. Unfortunately a wrong or incomplete
	conceptual model can often be adequately calibrated; good calibration
	of a model does not ensure a correct conceptual model. Petroleum
	engineers have another term for calibration; they refer to it as
	history matching. A caveat to the idea of history matching is that
	we can make a prediction with some confidence equal to the period
	of the history match. In other words, if we have matched a 10-year
	history, we can predict for 10 years with reasonable confidence;
	beyond 10 years the confidence in the prediction diminishes rapidly.
	The same rule of thumb applies to ground water model analyses. Nuclear
	waste disposal poses a difficult problem because the time horizon,
	1000 years or longer, is well beyond the possibility of the history
	match (or period of calibration) in the traditional analysis. Nonetheless,
	numerical models appear to be the tool of choice for analyzing the
	safety of waste facilities. Models have a well-recognized inherent
	uncertainty. Performance assessment, the technique for assessing
	the safety of nuclear waste facilities, involves an ensemble of cascading
	models. Performance assessment with its ensemble of models multiplies
	the inherent uncertainty of the single model. The closer we can approach
	the idea of a long history with which to match the models, even models
	of nuclear waste facilities, the more confidence we will have in
	the analysis (and the models, including performance assessment).
	This thesis argues for prolonged periods of observation (perhaps
	as long as 300 to 1000 years) before a nuclear waste facility is
	finally closed.},
  doi = {10.1111/j.1745-6584.2003.tb02395.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{bronstert+al2007,
  author = {Bronstert, A. and Kolokotronis, V. and Schwandt, D. and Straub, H.},
  title = {Comparison and evaluation of regional climate scenarios for hydrological
	impact analysis: {G}eneral scheme and application example},
  journal = {International Journal of Climatology},
  year = {2007},
  volume = {27},
  pages = {1579--1594},
  number = {12},
  abstract = {We present a scheme and application example for evaluating simulations
	of possible future conditions of regional climate (‘regional climate
	change scenarios’) concerning their suitability for hydrological
	impact studies. The procedure is based on expert knowledge about
	the impacts of anthropogenic climate change on varying hydrological
	processes and different hydrological catchment status. A method to
	evaluate regional climate change scenarios for hydrological impact
	analysis is presented first, which consists basically of a two-step
	knowledge-based decision and evaluation procedure. The first step
	(‘climatic evaluation’) evaluates the capability of the climate scenarios
	to represent regional climate and the plausibility of the future
	climate conditions. The first step establishes the basis for the
	second step (‘hydrological evaluation’), which evaluates hydrologically
	relevant information of the climate scenarios to qualify information
	about the hydrological processes possibly altered by climate change.
	From this evaluation of the hydrological processes, an evaluation
	of regional catchment conditions is derived, such as long-term water
	availability, drought conditions or floods. In the second part of
	the paper, this method is applied systematically to three different
	regional climate change scenarios, which have been provided for south
	Germany. This evaluation results in different levels of adequacy,
	depending on the hydrological process under question. In general,
	processes which are governed by temperature conditions (e.g. evaporation,
	snowmelt) are evaluated as ‘more useful’ than the processes governed
	by precipitation characteristics (e.g. runoff generation, floods).
	All regional climate change scenario methods investigated are of
	rather limited value for extreme hydrological conditions. The proposed
	method can serve to systematically evaluate the usefulness of climate
	change scenarios for hydrological impact analysis. It becomes apparent
	that regional climate scenarios should only be applied for hydrological
	application if the spatial-temporal scale of variations of the governing
	hydrological processes is represented by the scenarios.},
  doi = {10.1002/joc.1621},
  owner = {rojasro},
  timestamp = {2010.07.30}
}

@ARTICLE{bronstert+al2002,
  author = {Bronstert, A. and Niehoff, D. and B\"urger, G.},
  title = {Effects of climate and land-use change on storm runoff generation:
	{P}resent knowledge and modelling capabilities},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {509},
  number = {2},
  abstract = {There are several indications that changes in land cover have influenced
	the hydrological regime of various river basins. In addition, the
	effects of climate change on the hydrological cycle and on the runoff
	behaviour of river catchments have been discussed extensively in
	recent years. However, it is at present rather uncertain how, how
	much and at which spatial scale these environmental changes are likely
	to affect the generation of storm runoff, and consequently the flood
	discharges of rivers. Firstly, this paper gives an overview of the
	possible effects of climatic and land-use change on storm runoff
	generation. Secondly, it discusses models dealing with the hydrological
	response to climate and land-use variations, including both the downscaling
	of climate information from global circulation models and the way
	flood forecasting models represent land-use conditions. Finally,
	two modelling studies of meso-scale catchments in Germany are presented:
	the first shows the possible impacts of climate change on storm runoff
	production, and the second the impacts of land-use changes.},
  doi = {10.1002/hyp.326},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{brookslerner1994,
  author = {Brooks, R. and Lerner, D. and Tobias, A.},
  title = {Determining the range of predictions of a groundwater model which
	arises from alternative calibrations},
  journal = {Water Resources Research},
  year = {1994},
  volume = {30},
  pages = {2993--3000},
  number = {11},
  abstract = {A major element in constructing a groundwater model is choosing the
	parameter values. The traditional approach is to aim for a single
	best set of values. The parameters used in a model are effective
	rather than measurable, and this combined with the inherent uncertainties
	in the modeling process means that there are often many plausible
	sets of values. A single prediction obtained from a single set of
	parameter values is not appropriate, but rather the range in predictions
	from the alternative calibrations should be used. A method is presented
	for finding the best case and worst case predictions among the plausible
	parameter sets and is applied to a real case study. Widely different
	feasible parameter sets were found giving significantly different
	predictions.},
  doi = {10.1029/94WR00947},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{brooks1998,
  author = {Brooks, S.},
  title = {Markov chain {M}onte {C}arlo method and its application},
  journal = {The Statistician},
  year = {1998},
  volume = {47},
  pages = {69--100},
  number = {1},
  abstract = {The Markov chain Monte Carlo (MCMC) method, as a computer-intensive
	statistical tool, has enjoyed an enormous upsurge in interest over
	the last few years. This paper provides a simple, comprehensive and
	tutorial review of some of the most common areas of research in this
	field. We begin by discussing how MCMC algorithms can be constructed
	from standard building- blocks to produce Markov chains with the
	desired stationary distribution. We also motivate and discuss more
	complex ideas that have been proposed in the literature, such as
	continuous time and dimension jumping methods. We discuss some implementational
	issues associated with MCMC methods. We take a look at the arguments
	for and against multiple replications, consider how long chains should
	be run for and how to determine suitable starting points. We also
	take a look at graphical models and how graphical approaches can
	be used to simplify MCMC implementation. Finally, we present a couple
	of examples, which we use as case-studies to highlight some of the
	points made earlier in the text. In particular, we use a simple changepoint
	model to illustrate how to tackle a typical Bayesian modelling problem
	via the MCMC method, before using mixture model problems to provide
	illustrations of good sampler output and of the implementation of
	a reversible jump MCMC algorithm.},
  owner = {RRojas},
  refid = {BROOKS1998},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2988428}
}

@ARTICLE{brooksgelman1998,
  author = {Brooks, S. and Gelman, A.},
  title = {General methods for monitoring convergence of iterative simulations},
  journal = {Journal of Computational and Graphical Statistics},
  year = {1998},
  volume = {7},
  pages = {434--455},
  number = {4},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for
	monitoring the convergence of iterative simulations by comparing
	between and within variances of multiple chains, in order to obtain
	a family of tests for convergence. We review methods of inference
	from simulations in order to develop convergence-monitoring summaries
	that are relevant for the purposes for which the simulations are
	used. We recommend applying a battery of tests for mixing based on
	the comparison of inferences from individual sequences and from the
	mixture of sequences. Finally, we discuss multivariate analogues,
	for assessing convergence of several parameters simultaneously.},
  owner = {RRojas},
  refid = {BROOKS1998A},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/1390675}
}

@ARTICLE{bubeck+al2011,
  author = {Bubeck, P. and de Moel, H. and Bouwer, L. M. and Aerts, J. C. J.
	H.},
  title = {How reliable are projections of future flood damage?},
  journal = {Natural Hazards and Earth System Science},
  year = {2011},
  volume = {11},
  pages = {3293--3306},
  number = {12},
  doi = {10.5194/nhess-11-3293-2011}
}

@INPROCEEDINGS{buckley1995,
  author = {Buckley, K. and Binley, A. and Beven, K.},
  title = {Calibration and predictive uncertainty estimation of groundwater
	quality models: {A}pplication to the {T}win {L}ake {T}racer {T}est},
  booktitle = {Groundwater {Q}uality {M}anagement: {P}roceedings of the {GQM}3 {C}onference},
  year = {1995},
  pages = {205--214},
  address = {Tallinn, Estonia},
  publisher = {vol, 220. IAHS Publ.},
  owner = {RRojas},
  timestamp = {2008.12.02}
}

@ARTICLE{buonomo+al2007,
  author = {Buonomo, E. and Jones, R. and Huntingford, C. and Hannaford, J.},
  title = {On the robustness of changes in extreme precipitation over {E}urope
	from two high resolution climate change simulations},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  year = {2007},
  volume = {133},
  pages = {65--81},
  number = {622},
  abstract = {Two Regional Climate Model (RCM) projections of changes in extreme
	precipitation over Europe are assessed and compared. This provides
	insight into the importance of RCM formulation in representing changes
	in climate extremes at high spatial resolution. The models concerned
	are two recent Hadley Centre RCMs, HadRM2 and HadRM3, and are applied
	at a horizontal resolution of approximately 50 km over Europe, nested
	within the Hadley Centre coupled Atmosphere Ocean General Circulation
	Model (AOGCM), HadCM2. The simulation periods are thirty years with
	fixed concentrations of greenhouse gases representing the climate
	of 1961-1990 and twenty years representing transient climate change
	for 2080-2100. The use of common boundary conditions to drive the
	two RCMs allows us to determine whether their different formulations
	significantly alter the downscaled projections. The RCM simulations
	of precipitation extremes are compared with observations from a dense
	rain-gauge network over Great Britain, aggregated to the grid used
	by the RCMs. Both RCMs simulate realistically extreme precipitation
	occurring over timescales of one to thirty days and for return periods
	of two to twenty years. In particular, relative errors in the magnitude
	of extreme precipitation are generally no larger than those in the
	mean. The two regional models show different patterns of errors for
	daily precipitation extremes, with the main difference in the western
	and upland areas of Great Britain where they are underestimated in
	HadRM2 and overestimated in HadRM3. Change in extremes over all land
	areas in the domain show increases in intensity everywhere (except
	for the Iberian peninsula and Mediterranean coast) with most of these
	significant at the 5% level. Projected increases are greatest for
	those extremes which are the rarest and shortest duration (i.e. the
	most intense), both in relative and thus absolute terms. The large-scale
	patterns of these changes are very similar in the two RCMs implying
	they are generally robust to the RCM formulation changes. Given the
	demonstrated quality of the models this enhances our confidence in
	the projected changes and suggests that they are mainly conditioned
	by the large-scale response in the driving GCM},
  doi = {10.1002/qj.13},
  keywords = {PRUDENCE},
  tags = {Multimodel - Ensambles}
}

@ARTICLE{burkebrown2008,
  author = {Burke, E. and Brown, S.},
  title = {Evaluating uncertainties in the projection of future drought},
  journal = {Journal of Hydrometeorology},
  year = {2008},
  volume = {9},
  pages = {292--299},
  number = {2},
  abstract = {The uncertainty in the projection of future drought occurrence was
	explored for four different drought indices using two model ensembles.
	The first ensemble expresses uncertainty in the parameter space of
	the third Hadley Centre climate model, and the second is a multimodel
	ensemble that additionally expresses structural uncertainty in the
	climate modeling process. The standardized precipitation index (SPI),
	the precipitation and potential evaporation anomaly (PPEA), the Palmer
	drought severity index (PDSI), and the soil moisture anomaly (SMA)
	were derived for both a single CO2 (1×CO2) and a double CO2 (2×CO2)
	climate. The change in moderate drought, defined by the 20th percentile
	of the relevant 1×CO2 distribution, was calculated. SPI, based solely
	on precipitation, shows little change in the proportion of the land
	surface in drought. All the other indices, which include a measure
	of the atmospheric demand for moisture, show a significant increase
	with an additional 5%–45% of the land surface in drought. There are
	large uncertainties in regional changes in drought. Regions where
	the precipitation decreases show a reproducible increase in drought
	across ensemble members and indices. In other regions the sign and
	magnitude of the change in drought is dependent on index definition
	and ensemble member, suggesting that the selection of appropriate
	drought indices is important for impact studies.},
  doi = {10.1175/2007JHM929.1},
  owner = {rojasro},
  timestamp = {2012.07.04}
}

@BOOK{burnhamanderson2002,
  title = {Model selection and multimodel inference. {A} practical information--theoretic
	approach},
  publisher = {Springer--Verlag},
  year = {2002},
  author = {Burnham, K. and Anderson, D.},
  pages = {496},
  address = {New York},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{burton+al2010,
  author = {Burton, A. and Fowler, H. and Blenkinsop, S. and Kilsby, C.},
  title = {Downscaling transient climate change using a {N}eyman--{S}cott {R}ectangular
	{P}ulses stochastic rainfall model},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {381},
  pages = {18--32},
  number = {1--2},
  abstract = {The future management of hydrological systems must be informed by
	climate change projections at relevant time horizons and at appropriate
	spatial scales. Furthermore, the robustness of such management decisions
	is dependent on both the uncertainty inherent in future climate change
	scenarios and the natural climate system. Addressing these needs,
	we present a new transient rainfall simulation methodology which
	combines dynamical and statistical downscaling techniques to produce
	transient (i.e. temporally non-stationary) climate change scenarios.
	This is used to generate a transient multi-model ensemble of simulated
	point-scale rainfall time series for 1997--2085 for the polluted
	Br{\'e}villes spring in Northern France. The recovery of this previously
	potable source may be affected by climatic changes and variability
	over the next few decades. The provision of locally-relevant transient
	climate change scenarios for use as input to hydrological models
	of both water quality and quantity will ultimately provide a valuable
	resource for planning and decision making. Observed rainfall from
	1988--2006 was characterised in terms of a set of statistics for
	each calendar month: the daily mean, variance, probability dry, lag-1
	autocorrelation and skew, and the monthly variance. The Neyman--Scott
	Rectangular Pulses (NSRP) stochastic rainfall model was fitted to
	these observed statistics and correctly simulated both monthly statistics
	and extreme rainfall properties. Multiplicative change factors which
	quantify the change in each statistic between the periods 1961--1990
	and 2071-2100 were estimated for each month and for each of 13 Regional
	Climate Models (RCMs) from the PRUDENCE ensemble. To produce transient
	climate change scenarios, pattern scaling factors were estimated
	and interpolated from four time-slice integrations of two General
	Circulation Models which condition the RCMs, ECHAM4/OPYC and HadCM3.
	Applying both factors to the observed statistics provided projected
	transient rainfall statistics (PTRS) to which piece-wise smoothly
	varying transient rainfall model parameterizations were fitted. These
	fits provided good representations of the PTRS for each RCM. An ensemble
	of 100 continuous daily rainfall time series, with steadily varying
	stochastic properties which model these projections of transient
	climate change, was then simulated using a new transient NSRP simulation
	methodology for each RCM. Together the ensembles form a 1300 member
	transient multi-model ensemble of rainfall time series.The simulated
	transient ensemble properties were investigated, identifying RCMs
	giving rise to unusual behaviour. For the Br{\'e}villes, annual rainfall
	is projected to decrease until 2085 but the change is highly sensitive
	to General Circulation Model forcing; ECHAM4-driven RCMs project
	larger annual decreases than HadCM3/HadAM3H/P driven RCMs. All RCMs
	project an increase in winter rainfall and a larger summer decrease.
	An increase of not, vert, similar10% in the 10-year return period
	annual maximum rainfall is projected by 2085, however both strong
	increasing trends and a slight decreasing trend are found for individual
	RCMs. Compared with transient RCMs, the new methodology provides
	a number of advantages: reduced biases, point scale scenarios relevant
	for local-scale impact studies, improved representation of natural
	variability and improved representation of extremes.},
  doi = {10.1016/j.jhydrol.2009.10.031},
  tags = {Downscaling}
}

@ARTICLE{butts2004,
  author = {Butts, M. and Payne, J. and Kristensen, M. and Madsen, H.},
  title = {An evaluation of the impact of model structure on hydrological modelling
	uncertainty for streamflow simulation},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {298},
  pages = {242--266},
  number = {1--4},
  abstract = {Operational flood management and warning requires the delivery of
	timely and accurate forecasts. The use of distributed and physically
	based forecasting models can provide improved streamflow forecasts.
	However, for operational modelling there is a trade-off between the
	complexity of the model descriptions necessary to represent the catchment
	processes, the accuracy and representativeness of the input data
	available for forecasting and the accuracy required to achieve reliable,
	operational flood management and warning. Four sources of uncertainty
	occur in deterministic flow modelling; random or systematic errors
	in the model inputs or boundary condition data, random or systematic
	errors in the recorded output data, uncertainty due to sub-optimal
	parameter values and errors due to incomplete or biased model structure.
	While many studies have addressed the issues of sub-optimal parameter
	estimation, parameter uncertainty and model calibration very few
	have examined the impact of model structure error and complexity
	on model performance and modelling uncertainty. In this study a general
	hydrological framework is described that allows the selection of
	different model structures within the same modelling tool. Using
	this tool a systematic investigation is carried out to determine
	the performance of different model structures for the DMIP study
	Blue River catchment using a split sample evaluation procedure. This
	investigation addresses two questions. First, different model structures
	are expected to perform differently, but is there a trade-off between
	model complexity and predictive ability? Secondly, how does the magnitude
	of model structure uncertainty compare to the other sources of uncertainty?
	The relative performance of different acceptable model structures
	is evaluated as a representation of structural uncertainty and compared
	to estimates of the uncertainty arising from measurement uncertainty,
	parametric uncertainty and the rainfall input. The results show first
	that model performance is strongly dependent on model structure.
	Distributed routing and to a lesser extent distributed rainfall were
	found to be the dominant processes controlling simulation accuracy
	in the Blue River basin. Secondly that the sensitivity to variations
	in acceptable model structure are of the same magnitude as uncertainties
	arising from the other evaluated sources. This suggests that for
	practical hydrological predictions there are important benefits in
	exploring different model structures as part of the overall modelling
	approach. Furthermore the model structural uncertainty should be
	considered in assessing model uncertainties. Finally our results
	show that combinations of several model structures can be a means
	of improving hydrological simulations.},
  doi = {10.1016/j.jhydrol.2004.03.042},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{buytaert2009,
  author = {Buytaert, W. and C\'elleri, R. and Timbe, L.},
  title = {Predicting climate change impacts on water resources in the tropical
	{A}ndes: {E}ffects of {GCM} uncertainty},
  journal = {Geophysical Research Letters},
  year = {2009},
  volume = {36},
  pages = {1--5},
  number = {L07406},
  abstract = {There is a strong demand from policy makers for predictions about
	the potential impacts of climate change on water resources. Integrated
	environmental models, combining climatic and hydrologic models, are
	often used for this purpose. This paper examines the impact of uncertainties
	related to GCMs in hydrological impact studies in the tropical Andes.
	A conceptual hydrological model is calibrated on data from four mesoscale,
	mountainous catchments in south Ecuador. The model inputs are then
	perturbed with anomalies projected by 20 GCMs available from the
	IPCC Data Distribution Centre. The results show that on average,
	the average monthly discharge is not expected to change dramatically.
	However, the simulated discharges driven by different global climate
	model forcing data can diverge widely, with prediction ranges often
	surpassing current discharge.},
  doi = {10.1029/2008GL037048},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{buytaert2010,
  author = {Buytaert, W. and Vuille, M. and Dewulf, A. and Urrutia, R. and Karmalkar,
	A. and C\'elleri, R.},
  title = {Uncertainties in climate change projections and regional downscalling:
	implications for water resources management},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2010},
  volume = {7},
  pages = {1821--1848},
  number = {2},
  abstract = {Climate change is expected to have a large impact on water resources
	worldwide. A major problem in assessing the potential impact of a
	changing climate on these resources is the difference in spatial
	scale between available climate change projections and water resources
	management. Regional climate models (RCMs) are often used for the
	spatial disaggregation of the outputs of global circulation models.
	However, RCMs are time-intensive to run and typically only a small
	number of model runs is available for a certain region of interest.
	This paper investigates the value of the improved representation
	of local climate processes by a regional climate model for water
	resources management in the tropical Andes of Ecuador. This region
	has a complex hydrology and its water resources are under pressure.
	Compared to the IPCC AR4 model ensemble, the regional climate model
	PRECIS does indeed capture local gradients better than global models,
	but locally the model is prone to large discrepancies between observed
	and modelled precipitation. It is concluded that a further increase
	in resolution is necessary to represent local gradients properly.
	Furthermore, to assess the uncertainty in downscaling, an ensemble
	of regional climate models should be implemented. Finally, translating
	the climate variables to streamflow using a hydrological model constitutes
	a smaller but not negligible source of uncertainty.},
  doi = {10.5194/hessd-7-1821-2010},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{caballero+al2007,
  author = {Caballero, Y. and {Voirin-Morel}, S. and Habets, F. and Noilhan,
	J. and {LeMoigne}, P. and Lehenaff, A. and Boone, A.},
  title = {Hydrological sensitivity of the {A}dour--{G}aronne river basin to
	climate change},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W07448},
  number = {7},
  abstract = {Output atmospheric fields from seven global climate models (GCMs)
	were extracted over a domain covering the Adour-Garonne basin in
	southwestern France in order to calculate precipitation and temperature
	anomalies for the decade 2050--2060 relative to the present climate.
	These anomalies showed a general trend of increasing precipitation
	in wintertime and decreasing precipitation in summertime, together
	with an increase in the annual average temperature of approximately
	2Â°C. The anomalies were used to create seven modified climate-forcing
	data sets, which were then used to drive the SAFRAN-ISBA-MODCOU (SIM)
	hydrometeorological modeling system. The river discharge simulated
	by the SIM model under each modified climate for the 2050--2060 decade
	was compared to the discharge simulated for the 1985--1995 reference
	decade. The results show a slight decrease in the low river flow,
	on the order of 11% Â± 8% on average for all of the climate-forcing
	data sets and the hydrometric stations. However, there was a significant
	impact on the snowpack in terms of reduced snow cover depth and duration.
	These changes provoked a discharge decrease in the spring and a large
	increase in winter due to the additional liquid precipitation. Considering
	the large range in climate conditions of the period studied, it appears
	that the hydrological sensitivity of the river basin is greater when
	applying the same climate modification to a wet year as opposed to
	a dry year. Finally, a transient climate forcing covering the 1985--2095
	period provokes a general tendency to decrease the river discharge
	for all seasons},
  doi = {10.1029/2005WR004192},
  tags = {Uncertainty, Impacts}
}

@BOOK{cacuci2003,
  title = {Sensitivity and uncertainty analysis: {T}heory},
  publisher = {Chapman \& Hall/CRC},
  year = {2003},
  author = {Cacuci, D.},
  pages = {304},
  address = {Florida},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{calanca2007,
  author = {Pierluigi Calanca},
  title = {Climate change and drought occurrence in the Alpine region: How severe
	are becoming the extremes?},
  journal = {Global and Planetary Change},
  year = {2007},
  volume = {57},
  pages = {151--160},
  number = {1–2},
  abstract = {There are clear indications that in the future the Alpine region will
	increasingly suffer from droughts. In this paper, a simple method
	is devised for determining probability of drought occurrence and
	exceedance of severity thresholds. A reconstruction of drought occurrence
	during the past 100 yr indicates that the method is able to detect
	major events and also to correctly gauge their relative severity.
	The procedure is used in conjunction with climate simulations for
	the European region valid for 2071–2100 to study the impact of climate
	change on the likelihood and severity of droughts. The climate scenario
	used for the analysis refers to a SRES A2 emission pathway and specifies
	in particular a decrease in the frequency of wet days of about 20%
	with respect to the growing season of summer crops (April to September).
	Under these conditions the frequency of droughts is shown to increase
	from about 15% to more than 50%. In addition, the results indicate
	an overall shift in the distribution toward higher severity scores.
	The average severity increases by a factor of two, but also at the
	upper end of the spectrum severity increases by more than 20%. It
	is argued that this will affect the perception of extreme droughts,
	i.e. of those events rarer than the 10th percentile of the distribution.
	If this scenario comes true, by the end of the 21st century droughts
	comparable in severity to the 2003 event would represent the norm
	rather than the exception.},
  doi = {10.1016/j.gloplacha.2006.11.001},
  issn = {0921-8181},
  keywords = {droughts}
}

@BOOK{calvetti2007,
  title = {An introduction to {B}ayesian scientific computing},
  publisher = {Springer-Verlag},
  year = {2007},
  author = {Calvetti, D. and Somersalo, E.},
  volume = {2},
  pages = {202},
  series = {Surveys and tutorials in the applied mathematical sciences},
  address = {Berlin, Germany},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.11.18}
}

@ARTICLE{cameron2006,
  author = {Cameron, D.},
  title = {An application of the {UKCIP02} climate change scenarios to flood
	estimation by continuous simulation for a gauged catchment in the
	northeast of {S}cotland, {UK} (with uncertainty)},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {328},
  pages = {212--226},
  number = {1--2},
  abstract = {This paper explores the potential impacts of climate change upon flood
	frequency for the gauged, Lossie catchment in the northeast of Scotland,
	UK. This catchment has significant flooding problems, but only limited
	data availability (particularly with respect to rainfall). A continuous
	simulation methodology, which uses a stochastic rainfall model to
	drive the rainfall-runoff model TOPMODEL, is utilised. Behavioural
	parameter sets for TOPMODEL are identified prior to the climate change
	runs using the Generalised Likelihood Uncertainty Estimation (GLUE)
	methodology. The {\grqq}Low Emissionsâ€?, {\grqq}Medium-Low Emissionsâ€?,
	{\grqq}Medium-High Emissionsâ€? and {\grqq}High Emissionsâ€? UKCIP02
	climate change scenarios, obtained from the HadCM3 global climate
	model (GCM) and HadRM3 regional climate model (RCM) simulations,
	are used at the catchment scale. Two further scenarios ({\grqq}H-Dryâ€?
	and {\grqq}H-Wetâ€?), based upon the model uncertainty margins available
	for the UKCIP02 {\grqq}High Emissionsâ€? scenario, are also developed
	in order to explore the possible range of changes to daily rainfall
	and temperature estimated from GCMs other than HadCM3. It is demonstrated
	that, while flood magnitude changes under all six of the climate
	change scenarios considered, the magnitude and direction of that
	change is dependent upon the choice of scenario. An overlap between
	the likelihood weighted uncertainty bounds estimated under the conditions
	of the current climate and those estimated under the four UKCIP02
	scenarios and the {\grqq}H-Dryâ€? scenario is also observed. These
	findings highlight the need to consider multiple climate change scenarios
	and account for model uncertainties when estimating the possible
	effects of climate change upon flood frequency},
  doi = {10.1016/j.jhydrol.2005.12.024},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{campolongo+al2000,
  author = {Campolongo, F. and Tarantola, S. and Saltelli, A.},
  title = {{Sensitivity Anaysis as an Ingredient of Modeling}},
  journal = {Statistical Science},
  year = {2000},
  volume = {15},
  pages = {377},
  number = {4},
  abstract = {We explore the tasks where sensitivity analysis (SA) can be useful
	and try to assess the relevance of SA within the modeling process.
	We suggest that SA could considerably assist in the use of models,
	by providing objective criteria of judgement for different phases
	of the model{-}building process: model identification and discrimination;
	model calibration; model corroboration. We review some new global
	quantitative SA methods and suggest that these might enlarge the
	scope for sensitivity analysis in computational and statistical modeling
	practice. Among the advantages of the new methods are their robustness,
	model independence and computational convenience. The discussion
	is based on worked examples.},
  doi = {10.1214/ss/1009213004},
  tags = {Sensitivity Analysis}
}

@ARTICLE{cancellieresalas2010,
  author = {Antonino Cancelliere and Jose D. Salas},
  title = {Drought probabilities and return period for annual streamflows series},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {391},
  pages = {77--89},
  number = {1--2},
  abstract = {Summary Probabilistic characterization of drought events is extremely
	important in water resources planning and management. In spite of
	the large number of studies that have been carried on the subject,
	the exact derivation of the probabilistic structure of drought characteristics
	is still an open problem, especially when both duration and accumulated
	deficit (or intensity) are taken into account. This study focuses
	on the derivation of the probability distribution of drought episodes
	considering both drought duration and accumulated deficit (or intensity)
	as well as of the ensuing return period, when the underlying hydrological
	variable is autocorrelated. First, the marginal distribution of drought
	length is investigated, showing that the simple Markov chain, usually
	adopted to model the sequence of deficits and surpluses, is not adequate
	when the underlying series exhibits a significant autocorrelation.
	Following previous studies, a Discrete Autoregressive Moving Average
	(DARMA) model is proposed to better model drought length. Then, the
	derivation of the moments of accumulated deficit conditioned on a
	fixed drought length is pursued. It is shown that the deficit series
	that is obtained by clipping an autocorrelated stationary series
	with a constant threshold is not i.i.d., and therefore not stationary.
	Thus, a fully multivariate approach, based on a truncated multivariate
	normal model, is applied to derive the moments of accumulated deficit
	as a function of the distribution of the underlying variable and
	of the threshold. In order to overcome numerical difficulties related
	to the integration of the multivariate distributions, empirical approximations
	are also proposed. Such moments are then used to derive approximate
	expressions of the bivariate distributions of accumulated deficit
	(or intensity) and length, based on the assumption that the distribution
	of accumulated deficit conditioned on a fixed length is beta, and
	expressions to compute the return period of drought events. The proposed
	procedure is illustrated by applying it to four streamflow data characterized
	by different levels of autocorrelation and skewness.},
  doi = {10.1016/j.jhydrol.2010.07.008},
  issn = {0022-1694},
  keywords = {Drought}
}

@ARTICLE{cao+al2006,
  author = {Cao, W. and Bowden, W. and Davie, T. and Fenemor, A.},
  title = {{Multi-variable and multi-site calibration and validation of SWAT
	in a large mountainous catchment with high spatial variability}},
  journal = {Hydrological Processes},
  year = {2006},
  volume = {20},
  pages = {1057--1073},
  number = {5},
  abstract = {Many methods developed for calibration and validation of physically
	based distributed hydrological models are time consuming and computationally
	intensive. Only a small set of input parameters can be optimized,
	and the optimization often results in unrealistic values. In this
	study we adopted a multi-variable and multi-site approach to calibration
	and validation of the Soil Water Assessment Tool (SWAT) model for
	the Motueka catchment, making use of extensive field measurements.
	Not only were a number of hydrological processes (model components)
	in a catchment evaluated, but also a number of subcatchments were
	used in the calibration. The internal variables used were PET, annual
	water yield, daily streamflow, baseflow, and soil moisture. The study
	was conducted using an 11-year historical flow record (1990-2000);
	1990-94 was used for calibration and 1995-2000 for validation. SWAT
	generally predicted well the PET, water yield and daily streamflow.
	The predicted daily strearnflow matched the observed values, with
	a Nash-Sutcliffe coefficient of 0.78 during calibration and 0.72
	during validation. However, values for subcatchments ranged from
	0.31 to 0.67 during calibration, and 0.36 to 0.52 during validation.
	The predicted soil moisture remained wet compared with the measurement.
	About 50\% of the extra soil water storage predicted by the model
	can be ascribed to overprediction of precipitation; the remaining
	50\% discrepancy was likely to be a result of poor representation
	of soil properties. Hydrological compensations in the modelling results
	are derived from water balances in the various pathways and storage
	(evaporation, strearnflow, surface runoff, soil moisture and groundwater)
	and the contributions to strearnflow from different geographic areas
	(hill slopes, variable source areas, sub-basins, and subcatchments).
	The use of an integrated multi-variable and multi-site method improved
	the model calibration and validation and highlighted the areas and
	hydrological processes requiring greater calibration effort. Copyright
	(c) 2005 John Wiley \& Sons, Ltd.},
  doi = {10.1002/hyp.5933},
  keywords = {physically based distributed hydrological models, calibration and
	validation, soil and water assessment tool, spatial variability,
	RAINFALL-RUNOFF MODELS, PARAMETER-ESTIMATION, HYDROLOGIC RESPONSE,
	SCALE, UNCERTAINTY, SYSTEM},
  tags = {SWAT, Calibration}
}

@ARTICLE{capilla2009,
  author = {Capilla, J. and {Llopis-Albert}, C.},
  title = {Gradual conditioning of non--{G}aussian transmissivity fields to
	flow and mass transport data: 1. {T}heory},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {371},
  pages = {66--74},
  number = {1--4},
  abstract = {The paper presents a new stochastic inverse method for the simulation
	of transmissivity (T) fields conditional to T measurements, secondary
	information obtained from expert judgement and geophysical surveys,
	transient piezometric and solute concentration measurements, and
	travel time data. The formulation of the method is simple and derived
	from the gradual deformation method. It basically consists of an
	iterative optimization procedure in which successive combinations
	of T fields, that honour T measurements and soft data (secondary
	data obtained from expert judgement and/or geophysical surveys),
	gradually lead to a simulated T field conditional to flow and mass
	transport data. Every combination of fields requires minimizing a
	penalty function that penalizes the difference between computed and
	measured conditioning data. This penalty function depends on only
	one parameter. Travel time conditioning data are considered by means
	of a backward-in-time probabilistic model, which extends the potential
	applications of the method to the characterization of groundwater
	contamination sources. In order to solve the mass transport equation,
	the method implements a Lagrangian approach that allows avoiding
	numerical problems usually found in Eulerian methods. Besides, to
	deal with highly heterogeneous and non-Gaussian media, being able
	to reproduce anomalous breakthrough curves, a dual-domain approach
	is implemented with a first-order mass transfer approach. To determine
	the particle distribution between the mobile domain and the immobile
	domain the method uses a Bernoulli trial on the appropriate phase
	transition probabilities, derived using the normalized zeroth spatial
	moments of the multirate transport equations. The presented method
	does not require assuming the classical multiGaussian hypothesis
	thus easing the reproduction of T spatial patterns where extreme
	values of T show high connectivity. This feature allows the reproduction
	of a property found in real formations, which is often crucial to
	obtain safe estimations of mass transport predictions. Furthermore,
	very few existing methods can afford with this stochastic property.
	In fact, this new approach gathers a set of capabilities so far not
	included in any existing method.},
  doi = {10.1016/j.jhydrol.2009.03.015},
  owner = {rojasro},
  timestamp = {2009.10.20}
}

@INPROCEEDINGS{carlisledozier2001,
  author = {Carlisle, A. and Dozier, G.},
  title = {An off-the-shelf {PSO}},
  booktitle = {Proceedings of the Particle Swarm Optimization Workshop},
  year = {2001},
  pages = {1--6},
  address = {Purdue Sch. of Eng. and Technol., Indianapolis, Indiana},
  tags = {Calibration, PSO}
}

@ARTICLE{carreraetal2005,
  author = {Carrera, J. and Alcolea, A. and Medina, A. and Hidalgo, J. and Slooten,
	L.},
  title = {Inverse problem in hydrogeology},
  journal = {Hydrogeology Journal},
  year = {2005},
  volume = {13},
  pages = {206--222},
  number = {1},
  abstract = {The state of the groundwater inverse problem is synthesized. Emphasis
	is placed on aquifer characterization, where modelers have to deal
	with conceptual model uncertainty (notably spatial and temporal variability),
	scale dependence, many types of unknown parameters (transmissivity,
	recharge, boundary conditions, etc.), nonlinearity, and often low
	sensitivity of state variables (typically heads and concentrations)
	to aquifer properties. Because of these difficulties, calibration
	cannot be separated from the modeling process, as it is sometimes
	done in other fields. Instead, it should be viewed as one step in
	the process of understanding aquifer behavior. In fact, it is shown
	that actual parameter estimation methods do not differ from each
	other in the essence, though they may differ in the computational
	details. It is argued that there is ample room for improvement in
	groundwater inversion: development of user-friendly codes, accommodation
	of variability through geostatistics, incorporation of geological
	information and different types of data (temperature, occurrence
	and concentration of isotopes, age, etc.), proper accounting of uncertainty,
	etc. Despite this, even with existing codes, automatic calibration
	facilitates enormously the task of modeling. Therefore, it is contended
	that its use should become standard practice.},
  doi = {10.1007/s10040-004-0404-7},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{carrera1986a,
  author = {Carrera, J. and Neuman, S.},
  title = {Estimation of aquifer parameters under transient and steady state
	conditions: 1. {M}aximum likelihood method incorporating prior information},
  journal = {Water Resources Research},
  year = {1986},
  volume = {22},
  pages = {199--210},
  number = {2},
  abstract = {In this series of three papers a method is presented to estimate the
	parameters of groundwater flow models under steady and nonsteady
	state conditions. The parameters include values and directions of
	principal hydraulic conductivities (or transmissivities) in anisotropic
	media, specific storage (or storativity), interior and boundary recharge
	or leakage rates, coefficients of head-dependent interior and boundary
	sources, and boundary heads. In transient situations, the initial
	head distribution can also be estimated if the system is originally
	at a steady state. Paper 1 of the series discusses some of the advantage
	in treating the inverse problem statistically and in regularizing
	its solution by means of penalty criteria based on prior estimates
	of the parameters. The inverse problem is posed in the framework
	of maximum likelihood theory cast in a manner that accounts for prior
	information about the parameters. Since not all the factors which
	contribute to the prior errors can be quantified statistically at
	the outset, the covariance matrices of these errors are expressed
	in terms of several parameters which, if unknown, can be estimated
	jointly with the hydraulic parameters by a stagewise optimization
	process. When transient head data are separated by a fixed time interval,
	the temporal structure of these data is approximated by a lag-one
	autoregressive model with a correlation coefficient that can be treated
	as another unknown parameter. Estimation errors are analyzed by examining
	the lower bound of their covariance matrix in the eigenspace. Paper
	1 concludes by suggesting that certain model identification criteria
	developed in the time series literature, all of which are based on
	the maximum likelihood concept, might be useful for selecting the
	best groundwater model (or the best method of parameterizing a particular
	model) among a number of given alternatives.},
  doi = {10.1029/WR022i002p00199},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{carrera1986b,
  author = {Carrera, J. and Neuman, S.},
  title = {Estimation of aquifer parameters under transient and steady state
	conditions: 2. {U}niqueness, stability, and solution algorithms},
  journal = {Water Resources Research},
  year = {1986},
  volume = {22},
  pages = {211--227},
  number = {2},
  abstract = {Paper 2 of this three-part series starts with a discussion of the
	question, Under what conditions is the aquifer inverse problem well-posed?
	After defining the terms uniqueness, identifiability, and stability,
	theoretical considerations and synthetic examples are used to demonstrate
	that ill-posedness can be mitigated by including prior information
	about the parameters in the estimation criterion to be minimized.
	At the same time, the inclusion of such information is shown to be
	insufficient to guarantee uniqueness and stability in all cases.
	Several test problems in the recent literature, which have resulted
	in pessimistic conclusions about the solvability of the aquifer inverse
	problem, are shown to be ill-posed; a question is thus raised about
	the validity of these conclusions in the general case. Various conjugate
	gradient algorithms, coupled with the adjoint state finite element
	method for computing the gradient of the estimation criterion, and
	with Newton's method for determining the optimum step size downgradient,
	are compared. A marked improvement in the rate at which these algorithms
	converge is shown to be achieved by switching from one method to
	another when the former slows down or fails to converge.},
  doi = {10.1029/WR022i002p00211},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{carrera1986c,
  author = {Carrera, J. and Neuman, S.},
  title = {Estimation of aquifer parameters under transient and steady state
	conditions: 3. {A}pplication to synthetic and field data},
  journal = {Water Resources Research},
  year = {1986},
  volume = {22},
  pages = {228--242},
  number = {2},
  abstract = {The last paper of this three-part series illustrates and explores
	various features of the methodology we have proposed in papers 1
	and 2 (J. Carrera and S. P. Neuman, this issue (a, b)) by applying
	it to a synthetic test case and to a set of field data from the southwestern
	United States. In addition to demonstrating the ability of our method
	to estimate model parameters under a variety of conditions, the synthetic
	example is used to investigate the relative worth of transient and
	steady state data in terms of their ability to bring about an improvement
	in the quality of the estimates. A similar investigation is performed
	with regard to the role that prior information may play in reducing
	the variance of the estimation errors. The paper demonstrates the
	potential utility of our inverse methodology to the optimum design
	of observation and measurement networks in space and time. Based
	on a synthetic example, the paper shows that the model structure
	identification criteria introduced in paper 1 (J. Carrera and S.
	P. Neuman, this issue (a)) can be used successfully to choose the
	best parameter zonation pattern among a number of given alternatives.
	In particular, a criterion due to R. L. Kashyap (1982) is found to
	be the most adequate for this purpose because it responds in the
	most convincing manner to noise in the data. The field example illustrates
	a case where one must account for temporal autocorrelation between
	water level data at a given observation point. By validating the
	model against data which have not been used for parameter estimation,
	one finds that the validation results improve when the temporal error
	structure of the water level data is represented by a lag-one autocorrelation
	model. Both the synthetic and the field examples are used to obviate
	the advantages of performing the analysis of the estimation errors
	in the eigenspace instead of the original parameter space and to
	relate the results of this analysis to the fundamental questions
	of identifiability, uniqueness, and stability where appropriate.},
  doi = {10.1029/WR022i002p00228},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{casella2001,
  author = {Casella, G. and Lavine, M. and Robert, C.},
  title = {Explaining the perfect sampler},
  journal = {The American Statistician},
  year = {2001},
  volume = {55},
  pages = {299--305},
  number = {4},
  abstract = {In 1996, Propp and Wilson introduced coupling from the past (CFTP),
	an algorithm for generating a sample from the exact stationary distribution
	of a Markov chain. In 1998, Fill proposed another so-called perfect
	sampling algorithm. These algorithms have enormous potential in Markov
	Chain Monte Carlo (MCMC) problems because they eliminate the need
	to monitor convergence and mixing of the chain. This article provides
	a brief introduction to the algorithms, with an emphasis on understanding
	rather than technical detail.},
  owner = {RRojas},
  timestamp = {2009.02.19},
  url = {http://www.jstor.org/stable/2685691}
}

@ARTICLE{castelletti+al2010,
  author = {Castelletti, A. and Pianosi, F. and {Soncini-Sessa}, R. and Antenucci,
	J.},
  title = {A multiobjective response surface approach for improved water quality
	planning in lakes and reservoirs},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W06502},
  number = {6},
  abstract = {Improved data collection techniques as well as increasing computing
	power are opening up new opportunities for the development of sophisticated
	models that can accurately reproduce hydrodynamic and biochemical
	conditions of water bodies. While increasing model complexity is
	considered a virtue for scientific purposes, it is a definite disadvantage
	for management (engineering) purposes, as it limits the model applicability
	to what-if analysis over a few, a priori defined interventions. In
	the recent past, this has become a significant limitation, particularly
	considering recent advances in water quality rehabilitation technologies
	(e.g., mixers or oxygenators) for which many design parameters have
	to be decided. In this paper, a novel approach toward integrating
	science-oriented and engineering-oriented models and improving water
	quality planning is presented. It is based on the use of a few appropriately
	designed simulations of a complex process-based model to iteratively
	identify the multidimensional function (response surface) that maps
	the rehabilitation interventions into the objective function. On
	the basis of the response surface (RS), a greater number of interventions
	can be quickly evaluated and the corresponding Pareto front can be
	approximated. Interesting points on the front are then selected and
	the corresponding interventions are simulated using the original
	process-based model, thus obtaining new decision-objective samples
	to refine the RS approximation. The approach is demonstrated in Googong
	Reservoir (Australia), which is periodically affected by high concentrations
	of manganese and cyanobacteria. Results indicate that significant
	improvements could be observed by simply changing the location of
	the two mixers installed in 2007. Furthermore, it also suggests the
	best location for an additional pair of mixers. },
  doi = {10.1029/2009WR008389},
  tags = {Calibration, Disturbed Catchments, Water Quality}
}

@ARTICLE{chang1990,
  author = {Chang, T.},
  title = {Effects of drought on streamflow characteristics},
  journal = {Journal of Irrigation and Drainage Engineering},
  year = {1990},
  volume = {116},
  pages = {332-341},
  number = {3},
  abstract = {Droughts are defined using daily streamflow series and different truncation
	levels including 30%, 50%, 60%, 70%, 80%, and 90% of recorded daily
	flows. Flow ratios, which are obtained by dividing subbasin flows
	of different truncation levels by their corresponding outlet flows,
	are calculated. It is found that the flow ratios of 17 selected subbasins
	decrease significantly with increasing truncation levels while their
	mean‐flow ratios are approximately equal to their corresponding area
	ratios. Therefore, the estimation of irrigation water using the drainage
	area ratio, which has been practiced in the studied drainage basin,
	is impractical in case of droughts. Furthermore, assuming the time‐dependent
	Poissonian behavior of drought series, the drought intensity function
	was developed to investigate drought severity in the basin. Based
	on plots of drought intensity function, it is found that the drought
	intensity has been increasing significantly for the basin investigated.},
  doi = {10.1061/(ASCE)0733-9437(1990)116:3(332)},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{chaplot+al2005,
  author = {Chaplot, V. and Saleh, A. and Jaynes, D.},
  title = {Effect of the accuracy of spatial rainfall information on the modeling
	of water, sediment, and {NO3-N} loads at the watershed level},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {312},
  pages = {223--234},
  number = {1-4},
  abstract = {In a given watershed, the accuracy of models in predicting the hydrologic
	and erosion behavior depends, to a large extent, on the quality of
	the knowledge in respect of the spatial rainfall. The hydrologic
	and erosion aspects of rainfall are often discussed without due regard
	to any resulting improvement in watershed modeling. Thus, there is
	a real need for streamlining raingauge networks in order to reflect
	rainfall variability and its effect on the prediction of water, sediment
	and nutrient fluxes at the watershed scale. In this study, such an
	impact was analyzed using 9-year data collected at the outlets of
	two watersheds encompassing a range of climates, surface areas and
	environmental conditions. The Soil and Water Assessment Tool (SWAT)
	was applied using as input data that collected from 1 to 15 precipitation
	gauges per watershed. At both sites the highest densities of raingauges
	were used for SWAT calibration. The differences between the highest
	gauge concentration and lower concentrations used for the estimation
	of sediment loads led to the conclusion that a high gauge concentration
	is necessary. At both watersheds, predictions using rainfall records
	from the national service stations produced inaccurate estimations.
	This was probably because the gauge concentration was too sparse.
	Finally, the general applicability of these results is proposed by
	displaying the possibilities of extrapolation to other watersheds
	or models},
  doi = {10.1016/j.jhydrol.2005.02.019},
  keywords = {Hydrologic modeling, SWAT, Spatial input data, Rainfall},
  tags = {SWAT, GIS related, Rainfall}
}

@BOOK{charbeneu2000,
  title = {Groundwater hydraulics and pollutant transport},
  publisher = {Prentice Hall, Inc.},
  year = {2000},
  author = {Charbeneau, R.},
  pages = {593},
  address = {New Jersey},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.25}
}

@ARTICLE{chatterjeesiarry2006,
  author = {Chatterjee, A. and Siarry, P.},
  title = {Nonlinear inertia weight variation for dynamic adpatation in particle
	swarm optimization},
  journal = {Computers \& Operations Research},
  year = {2006},
  volume = {33},
  pages = {859--871},
  number = {3},
  abstract = {The particle swarm optimization (PSO) is a relatively new generation
	of combinatorial metaheuristic algorithms which is based on a metaphor
	of social interaction, namely bird flocking or fish schooling. Although
	the algorithm has shown some important advances by providing high
	speed of convergence in specific problems it has also been reported
	that the algorithm has a tendency to get stuck in a near optimal
	solution and may find it difficult to improve solution accuracy by
	fine tuning. The present paper proposes a new variation of PSO model
	where we propose a new method of introducing nonlinear variation
	of inertia weight along with a particle's old velocity to improve
	the speed of convergence as well as fine tune the search in the multidimensional
	space. The paper also presents a new method of determining and setting
	a complete set of free parameters for any given problem, saving the
	user from a tedious trial and error based approach to determine them
	for each specific problem. The performance of the proposed PSO model,
	along with the fixed set of free parameters, is amply demonstrated
	by applying it for several benchmark problems and comparing it with
	several competing popular PSO and non-PSO combinatorial metaheuristic
	algorithms.},
  doi = {10.1016/j.cor.2004.08.012},
  keywords = {Combinatorial metaheuristics, Particle swarm, Nonlinear inertia weight,
	Fixed parameter set},
  tags = {Calibration, PSO}
}

@ARTICLE{chau2007,
  author = {Chau, K.},
  title = {A split-step particle swarm optimization algorithm in river stage
	forecasting},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {346},
  pages = {131--135},
  number = {3-4},
  abstract = {An accurate forecast of river stage is very significant so that there
	is ample time for the pertinent authority to issue a forewarning
	of the impending flood and to implement early evacuation measures
	as required. Since a variety of existing process-based hydrological
	models involve exogenous input and different assumptions, artificial
	neural networks have the potential to be a cost-effective solution.
	In this paper, a split-step particle swarm optimization (PSO) model
	is developed and applied to train multi-layer perceptrons for forecasting
	real-time water levels at Fo Tan in Shing Mun River of Hong Kong
	with different lead times on the basis of the upstream gauging station
	(Tin Sum) or at Fo Tan. This paradigm is able to combine the advantages
	of global search capability of PSO algorithm in the first step and
	local fast convergence of Levenberg{--}Marquardt algorithm in the
	second step. The results demonstrate that it is able to attain a
	higher accuracy in a much shorter time when compared with the benchmarking
	backward propagation algorithm as well as the standard PSO algorithm.},
  doi = {10.1016/j.jhydrol.2007.09.004},
  keywords = {River stage forecasting, Split-step, Particle swarm optimization,
	Levenberg{--}Marquardt algorithm, Artificial neural networks},
  tags = {PSO, Calibration}
}

@TECHREPORT{CHE2000,
  author = {CHE},
  title = {Los Aprovechamientos en la cuenca del Ebro: Afecci\'{o}n en el r\'{e}gimen
	hydrol\'{o}gico fluvial -2000-PH-24I [The Exploitation in the Ebro
	Basin: Alterations to the Hydrological Regime of the River - 2000-PH-24I]},
  institution = {Confederaci\'{o}n Hidrogr\'{a}fica del Ebro},
  year = {2000},
  note = {Available on http://oph.chebro.es/DOCUMENTACION/Hidrologicos/RegimenHidrologico.htm.
	[last accessed Oct-2011]},
  tags = {CHE},
  url = {http://oph.chebro.es/documentacion/TablaResumenCaudalEA/AlteracionesEA.htm}
}

@ARTICLE{chen2008,
  author = {Chen, {C.-T.} and Knutson, T.},
  title = {On the verification and comparison of extreme rainfall indices from
	climate models},
  journal = {Journal of Climate},
  year = {2008},
  volume = {21},
  pages = {1605--1621},
  number = {7},
  abstract = {The interpretation of model precipitation output (e.g., as a gridpoint
	estimate versus as an areal mean) has a large impact on the evaluation
	and comparison of simulated daily extreme rainfall indices from climate
	models. It is first argued that interpretation as a gridpoint estimate
	(i.e., corresponding to station data) is incorrect. The impacts of
	this interpretation versus the areal mean interpretation in the context
	of rainfall extremes are then illustrated. A high-resolution (0.25°
	× 0.25° grid) daily observed precipitation dataset for the United
	States [from Climate Prediction Center (CPC)] is used as idealized
	perfect model gridded data. Both 30-yr return levels of daily precipitation
	(P30) and a simple daily intensity index are substantially reduced
	in these data when estimated at coarser resolution compared to the
	estimation at finer resolution. The reduction of P30 averaged over
	the conterminous United States is about 9%, 15%, 28%, 33%, and 43%
	when the data were first interpolated to 0.5° × 0.5°, 1° × 1°, 2°
	× 2°, 3° × 3°, and 4° × 4° grid boxes, respectively, before the calculation
	of extremes. The differences resulting from the point estimate versus
	areal mean interpretation are sensitive to both the data grid size
	and to the particular extreme rainfall index analyzed. The differences
	are not as sensitive to the magnitude and regional distribution of
	the indices. Almost all Intergovernmental Panel on Climate Change
	(IPCC) Fourth Assessment Report (AR4) models underestimate U.S. mean
	P30 if it is compared directly with P30 estimated from the high-resolution
	CPC daily rainfall observation. On the other hand, if CPC daily data
	are first interpolated to various model resolutions before calculating
	the P30 (a more correct procedure in our view), about half of the
	models show good agreement with observations while most of the remaining
	models tend to overestimate the mean intensity of heavy rainfall
	events. A further implication of interpreting model precipitation
	output as an areal mean is that use of either simple multimodel ensemble
	averages of extreme rainfall or of intermodel variability measures
	of extreme rainfall to assess the common characteristics and range
	of uncertainties in current climate models is not appropriate if
	simulated extreme rainfall is analyzed at a model’s native resolution.
	Owing to the large sensitivity to the assumption used, the authors
	recommend that for analysis of precipitation extremes, investigators
	interpret model precipitation output as an area average as opposed
	to a point estimate and then ensure that various analysis steps remain
	consistent with that interpretation.},
  doi = {10.1175/2007JCLI1494.1},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@ARTICLE{chenchi2010,
  author = {Chen, {T-Y} and Chi, {T-M}},
  title = {On the improvements of the particle swarm optimization algorithm},
  journal = {Advances in Engineering Software},
  year = {2010},
  volume = {41},
  pages = {229--239},
  number = {2},
  abstract = {Since a particle swarm optimization (PSO) algorithm uses a coordinated
	search to find the optimum solution, it has a better chance of finding
	the global solution. Despite this advantage, it is also observed
	that some parameters used in PSO may affect the solution significantly.
	Following this observation, this research tries to tune some of the
	parameters and to add mechanisms to the PSO algorithm in order to
	improve its robustness in finding the global solution. The main approaches
	include using uniform design to ensure uniform distribution of the
	initial particles in the design space, adding a mutation operation
	to increase the diversity of particles, decreasing the maximum velocity
	limitation and the velocity inertia automatically to balance the
	local and the global search efforts, reducing velocity when constraints
	are violated, and using Gaussian distribution based local searches
	to escape local minima. Besides these efforts, an algorithm is also
	developed to find multiple solutions in a single run. The results
	show that the overall effect of these approaches can yield better
	results for most test problems},
  doi = {10.1016/j.advengsoft.2009.08.003},
  keywords = {Particle swarm optimization, Uniform design, Local search, Multiple
	solutions},
  tags = {Calibration, PSO}
}

@ARTICLE{cheng+al2002,
  author = {Cheng, C. and Ou, C. and Chau, K.},
  title = {Combining a fuzzy optimal model with a genetic algorithm to solve
	multi-objective rainfall-runoff model calibration},
  journal = {Journal of Hydrology},
  year = {2002},
  volume = {268},
  pages = {72--86},
  number = {1-4},
  abstract = {An automatic calibration methodology for the Xinanjiang model that
	has been successfully and widely applied in China is presented. The
	automatic calibration of the model consists of two parts: water balance
	parameter and runoff routing parameter calibration. The former is
	based on a simple genetic algorithm (GA). The latter is based on
	a new method which combines a fuzzy optimal model (FOM) with a GA
	for solving the multiple objective runoff routing parameters calibration
	problem. Except for the specific fitness where the membership degree
	of alternative obtained by FOM with limited alternatives and multi-objectives
	is employed, the GA with multiple objectives in this paper is otherwise
	the same as the simple GA. The parameter calibration includes optimization
	of multiple objectives: (1) peak discharge, (2) peak time and (3)
	total runoff volume. Thirty-four historical floods from 12 years
	in the Shuangpai Reservoir are applied to calibrate the model parameters
	whilst 11 floods in recent 2 years are utilized to verify these parameters.
	Results of this study and application show that the hybrid methodology
	of GAs and the FOM is not only capable of exploiting more the important
	characteristics of floods but also efficient and robust.},
  doi = {10.1016/S0022-1694(02)00122-1},
  keywords = {Rainfall-runoff model, Calibration, Genetic algorithms, Fuzzy optimal
	model, Multiple objectives},
  tags = {Calibration}
}

@ARTICLE{cheng+al2005,
  author = {Cheng, {C-T} and Wu, {X-Y} and Chau, K.},
  title = {Multiple criteria rainfall-runoff model calibration using a parallel
	genetic algorithm in a cluster of computers},
  journal = {Hydrological Sciences Journal},
  year = {2005},
  volume = {50},
  pages = {1069--1087},
  number = {6},
  abstract = {Genetic algorithms are among of the global optimization schemes that
	have gained popularity as a means to calibrate rainfall-runoff models.
	However, a conceptual rainfall-runoff model usually includes 10 or
	more parameters and these are interdependent, which makes the optimization
	procedure very time-consuming. This may result in the premature termination
	of the optimization process which will prejudice the quality of the
	results. Therefore, the speed of optimization procedure is crucial
	in order to improve the calibration quality and efficiency. A hybrid
	method that combines a parallel genetic algorithm with a fuzzy optimal
	model in a cluster of computers is proposed. The method uses the
	fuzzy optimal model to evaluate multiple alternatives with multiple
	criteria where chromosomes are the alternatives, whilst the criteria
	are flood performance measures. In order to easily distinguish the
	performance of different alternatives and to address the problem
	of non-uniqueness of optimum, two fuzzy ratios are defined. The new
	approach has been tested and compared with results obtained by using
	a two-stage calibration procedure. The current single procedure produces
	similar results, but is simpler and automatic. Comparison of results
	between the serial and parallel genetic algorithms showed that the
	current methodology can significantly reduce the overall optimization
	time and simultaneously improve the solution quality. },
  doi = {10.1623/hysj.2005.50.6.1069},
  keywords = {calibration, cluster, fuzzy evaluation, multiple criteria, parallel
	genetic algorithms, rainfall-runoff mode},
  tags = {Calibration}
}

@ARTICLE{chib1995,
  author = {Chib, S. and Greenberg, E.},
  title = {Understanding the {M}etropolis--{H}astings algorithm},
  journal = {The American Statistician},
  year = {1995},
  volume = {49},
  pages = {327--335},
  number = {4},
  abstract = {We provide a detailed, introductory exposition of the Metropolis-Hastings
	algorithm, a powerful Markov chain method to simulate multivariate
	distributions. A sim- ple, intuitive derivation of this method is
	given along with guidance on implementation. Also discussed are two
	applications of the algorithm, one for implementing acceptance-rejection
	sampling when a blanketing func- tion is not available and the other
	for implementing the al- gorithm with block-at-a-time scans. In the
	latter situation, many different algorithms, including the Gibbs
	sampler, are shown to be special cases of the Metropolis-Hastings
	algorithm. The methods are illustrated with examples.},
  owner = {RRojas},
  timestamp = {2008.07.16},
  url = {http://www.jstor.org/stable/2684568}
}

@ARTICLE{chiewetal2009a,
  author = {Chiew, F. and Teng, J. and Vaze, J. and Kirono, D.},
  title = {Influence of global climate model selection on runoff impact assessment},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {379},
  pages = {172--180},
  number = {1--2},
  abstract = {The future rainfall series used to drive hydrological models in many
	climate change impact on runoff studies are informed by rainfall
	simulated by global climate models (GCMs). This paper assesses how
	the choice of GCMs based on their abilities to reproduce the observed
	historical rainfall can affect runoff impact assessment. The 23 GCMs
	used in IPCC 4AR are considered together with 1961–2000 observed
	rainfall data over southeast Australia. The results indicate that
	most of the GCMs can reproduce the observed spatial mean annual rainfall
	pattern, but the errors in the mean seasonal and annual rainfall
	amounts can be significant. The future mean annual rainfall projections
	averaged across southeast Australia range from ?10% to +3% change
	per degree global warming, which is amplified as ?23% to +4% change
	in the future mean annual runoff. There is no clear difference in
	the future rainfall projections between the better and poorer GCMs
	based on their abilities to reproduce the observed historical rainfall,
	therefore using only the better GCMs or weights to favour the better
	GCMs give similar runoff impact assessment results as the use of
	all the 23 GCMs. The range of future runoff in impact assessment
	studies is probably best determined using future rainfall projections
	from the majority of available archived GCM simulations.},
  doi = {10.1016/j.jhydrol.2009.10.004},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{chiewal2009b,
  author = {Chiew, F. and Teng, J. and Vaze, J. and Post, D. and Perraud, J.
	and Kirono, D. and Viney, N.},
  title = {Estimating climate change impact on runoff across southeast {A}ustralia:
	{M}ethod, results, and implications of the modeling method},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W10414},
  number = {10},
  abstract = {This paper describes the modeling of climate change impact on runoff
	across southeast Australia using a conceptual rainfall-runoff model
	SIMHYD and presents the results and assesses the robustness of the
	modeling approach. The future climate series is obtained by scaling
	the historical series, informed by 15 global climate models (GCMs),
	to reflect a 0.9°C increase in global average surface air temperature,
	using a daily scaling method that considers changes in the future
	mean seasonal rainfall and potential evapotranspiration as well as
	in the daily rainfall distribution. The majority of the modeling
	results indicate that there will be less runoff in southeast Australia
	in the future. However, there is considerable uncertainty, with the
	results ranging from a 17% decrease to a 7% increase in the mean
	annual runoff averaged across the study area for the 0.9°C global
	warming. The model assessments indicate that the modeling approach
	is generally robust and can be used to estimate the climate impact
	on runoff. The modeled mean annual runoff is generally within 10–20%
	of the observed runoff. The modeling results for an independent test
	period are only slightly poorer than the calibration period, indicating
	that a satisfactorily calibrated rainfall-runoff model can be used
	to estimate runoff for another climate period. The modeled impact
	on various runoff characteristics as estimated by two rainfall-runoff
	models investigated here differ by less than 10%, which is relatively
	small compared to the range of modeled runoff results using rainfall
	projections from different GCMs.},
  doi = {10.1029/2008WR007338},
  tags = {Impacts}
}

@BOOK{chilesdelfiner1999,
  title = {Geostatistics: {M}odeling spatial uncertainty},
  publisher = {John Wiley \& Sons},
  year = {1999},
  author = {Chil\`es, {J-P}. and Delfiner, P.},
  pages = {720},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.02.23}
}

@BOOKLET{chipman2001,
  title = {The practical implementation of {B}ayesian model selection},
  author = {Chipman, H. and George, E. and McCulloch, R.},
  howpublished = {Lectures Notes Monograph Series},
  year = {2001},
  editor = {P. Lahiri},
  institution = {Institute of Mathematical Statistics},
  owner = {RRojas},
  timestamp = {2008.04.23},
  volume = {38}
}

@ARTICLE{choibeven2007,
  author = {Choi, {H-Y} and Beven, K.},
  title = {Multi-period and multi-criteria model conditioning to reduce prediction
	uncertainty in an application of {TOPMODEL} within the {GLUE} framework},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {332},
  pages = {316--336},
  number = {3-4},
  abstract = {A new approach to multi-criteria model evaluation is presented. The
	approach is consistent with the equifinality thesis and is developed
	within the Generalised Likelihood Uncertainty Estimation (GLUE) framework.
	The predictions of Monte Carlo realisations of TOPMODEL parameter
	sets are evaluated using a number of performance measures calibrated
	for both global (annual) and seasonal (30 day) periods. The seasonal
	periods were clustered using a Fuzzy C-means algorithm, into 15 types
	representing different hydrological conditions. The model shows good
	performance on a classical efficiency measure at the global level,
	but no model realizations were found that were behavioural over all
	multi-period clusters and all performance measures, raising questions
	about what should be considered as an acceptable model performance.
	Prediction uncertainties can still be calculated by allowing that
	different clusters require different parameter sets. Variations in
	parameter distributions between clusters, as well as examination
	of where observed discharges depart from model prediction bounds,
	give some indication of model structure deficiencies.},
  doi = {10.1016/j.jhydrol.2006.07.012},
  keywords = {TOPMODEL, GLUE, Seasonality, Multi-criteria evaluation, Fuzzy classification},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{christensen+al2008,
  author = {Christensen, J. and Boberg, F. and Christensen, O. and {Lucas-Picher},
	P.},
  title = {On the need for bias correction of regional climate change projections
	of temperature and precipitation},
  journal = {Geophysical Research Letters},
  year = {2008},
  volume = {35},
  pages = {1--6},
  number = {L20709},
  abstract = {Within the framework of the European project ENSEMBLES (ensembles-based
	predictions of climate changes and their impacts) we explore the
	systematic bias in simulated monthly mean temperature and precipitation
	for an ensemble of thirteen regional climate models (RCMs). The models
	have been forced with the European Centre for Medium Range Weather
	Forecasting Reanalysis (ERA40) and are compared to a new high resolution
	gridded observational data set. We find that each model has a distinct
	systematic bias relating both temperature and precipitation bias
	to the observed mean. By excluding the twenty-five percent warmest
	and wettest months, respectively, we find that a derived second-order
	fit from the remaining months can be used to estimate the values
	of the excluded months. We demonstrate that the common assumption
	of bias cancellation (invariance) in climate change projections can
	have significant limitations when temperatures in the warmest months
	exceed 4–6 °C above present day conditions.},
  doi = {10.1029/2008GL035694},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{christensen+al2007a,
  author = {Christensen, J. and Carter, T. and Rummukainen, M. and Amanatidis,
	G.},
  title = {Evaluating the performance and utility of climate models: {T}he {PRUDENCE}
	project},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {1--6},
  number = {1},
  abstract = {This special issue of Climatic Change contains a series of research
	articles documenting co-ordinated work carried out within a 3-year
	European Union project ‘Prediction of Regional scenarios and Uncertainties
	for Defining European Climate change risks and Effects’ (PRUDENCE).
	The main objective of the PRUDENCE project was to provide high resolution
	climate change scenarios for Europe at the end of the twenty-first
	century by means of dynamical downscaling (regional climate modelling)
	of global climate simulations. The first part of the issue comprises
	seven overarching PRUDENCE papers on: (1) the design of the model
	simulations and analyses of climate model performance, (2 and 3)
	evaluation and intercomparison of simulated climate changes, (4 and
	5) specialised analyses of impacts on water resources and on other
	sectors including agriculture, ecosystems, energy, and transport,
	(6) investigation of extreme weather events and (7) implications
	of the results for policy. A paper summarising the related MICE (Modelling
	the Impact of Climate Extremes) project is also included. The second
	part of the issue contains 12 articles that focus in more detail
	on some of the themes summarised in the overarching papers. The PRUDENCE
	results represent the first comprehensive, continental-scale intercomparison
	and evaluation of high resolution climate models and their applications,
	bringing together climate modelling, impact research and social sciences
	expertise on climate change.},
  doi = {10.1007/s10584-006-9211-6},
  keywords = {PRUDENCE},
  tags = {Climate Models, RCMs}
}

@ARTICLE{christensen+2007,
  author = {Christensen, J. and Christensen, O.},
  title = {A summary of the {PRUDENCE} model projections of changes in {E}uropean
	climate by the end of this century},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {7--30},
  number = {1},
  abstract = {An overview of the PRUDENCE fine resolution climate model experiments
	for Europe is presented in terms of their climate change signals,
	in particular 2-meter temperature and precipitation. A comparison
	is made with regard to the seasonal variation in climate change response
	of the different models participating in the project. In particular,
	it will be possible to check how representative a particular PRUDENCE
	regional experiment is of the overall set in terms of seasonal values
	of temperature and precipitation. This is of relevance for such further
	studies and impact models that for practical reasons cannot use all
	the PRUDENCE regional experiments. This paper also provides some
	guidelines for how to select subsets of the PRUDENCE regional experiments
	according to such main sources of uncertainty in regional climate
	simulations as the choice of the emission scenario and of the driving
	global climate model.},
  doi = {10.1007/s10584-006-9210-7},
  keywords = {BALTIC SEA, SURFACE TEMPERATURES, SIMULATIONS, UNCERTAINTY, VARIABILITY,
	ICE},
  tags = {RCMs}
}

@INCOLLECTION{christensen+al2007b,
  author = {Christensen, J. and Hewitson, B. and Busuioc, A. and Chen, A. and
	Gao, X. and Held, I. and Jones, R. and Kolli, R. and Kwon, {W.-T}.
	and Laprise, R. and {Maga{\~n}a Rueda}, V. and Mearns, L. and Men\'endez,
	C. and R\"ais\"anen, J. and Rinke, A. and Sarr, A. and Whetto, P.},
  title = {Regional Climate Projections},
  booktitle = {Climate Change 2007: The Physical Science Basis. Contribution of
	Working Group I to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {S. Solomon and D. Qin and M. Manning and Z. Chen and M. Marquis and
	K. B. Averyt and M. Tignor and H. L. Miller},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  tags = {RCMs}
}

@ARTICLE{christensen+al2010,
  author = {Christensen, J. and Kjellstr\"om, E. and Giogi, F. and Lenderink,
	G. and Rummukainen, M.},
  title = {Weight assignment in regional climate models},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {179-197},
  number = {2--3},
  abstract = {An important new development within the European ENSEMBLES project
	has been to explore performance-based weighting of regional climate
	models (RCMs). Until now, although no weighting has been applied
	in multi-RCM analyses, one could claim that an assumption of ‘equal
	weight’ was implicitly adopted. At the same time, different RCMs
	generate different results, e.g. for various types of extremes, and
	these results need to be combined when using the full RCM ensemble.
	The process of constructing, assigning and combining metrics of model
	performance is not straightforward. Rather, there is a considerable
	degree of subjectivity both in the choice of metrics and on how these
	may be combined into weights. We explore the applicability of combining
	a set of 6 specifically designed RCM performance metrics to produce
	one aggregated model weight with the purpose of combining climate
	change information from the range of RCMs used within ENSEMBLES.
	These metrics capture aspects of model performance in reproducing
	large-scale circulation patterns, meso-scale signals, daily temperature
	and precipitation distributions and extremes, trends and the annual
	cycle. We examine different aggregation procedures that generate
	different inter-model spreads of weights. The use of model weights
	is sensitive to the aggregation procedure and shows different sensitivities
	to the selected metrics. Generally, however, we do not find compelling
	evidence of an improved description of mean climate states using
	performance-based weights in comparison to the use of equal weights.
	We suggest that model weighting adds another level of uncertainty
	to the generation of ensemble-based climate projections, which should
	be suitably explored, although our results indicate that this uncertainty
	remains relatively small for the weighting procedures examined.},
  doi = {10.3354/cr00916},
  owner = {rojasro},
  timestamp = {2011.07.19}
}

@ARTICLE{christensen2007,
  author = {Christensen, N. and Lettenmaier, D.},
  title = {A multimodel ensemble approach to assessment of climate change impacts
	on the hydrology and water resources of th e{C}olorado river basin},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1417--1434},
  number = {4},
  abstract = {Implications of 21st century climate change on the hydrology and water
	resources of the Colorado River Basin were assessed using a multimodel
	ensemble approach in which downscaled and bias corrected output from
	11 General Circulation Models (GCMs) was used to drive macroscale
	hydrology and water resources models. Downscaled climate scenarios
	(ensembles) were used as forcings to the Variable Infiltration Capacity
	(VIC) macroscale hydrology model, which in turn forced the Colorado
	River Reservoir Model (CRMM). Ensembles of downscaled precipitation
	and temperature, and derived streamflows and reservoir system performance
	were assessed through comparison with current climate simulations
	for the 1950–1999 historical period. For each of the 11 GCMs, two
	emissions scenarios (IPCC SRES A2 and B1, corresponding to relatively
	unconstrained growth in emissions, and elimination of global emissions
	increases by 2100) were represented. Results for the A2 and B1 climate
	scenarios were divided into three periods: 2010–2039, 2040–2069,
	and 2070–2099. The mean temperature change averaged over the 11 ensembles
	for the Colorado basin for the A2 emission scenario ranged from 1.2
	to 4.4°C for periods 1–3, and for the B1 scenario from 1.3 to 2.7°C.
	Precipitation changes were modest, with ensemble mean changes ranging
	from ?1 to ?2% for the A2 scenario, and from +1 to ?1% for the B1
	scenario. An analysis of seasonal precipitation patterns showed that
	most GCMs had modest reductions in summer precipitation and increases
	in winter precipitation. Derived April 1 snow water equivalent declined
	for all ensemble members and time periods, with maximum (ensemble
	mean) reductions of 38% for the A2 scenario in period 3. Runoff changes
	were mostly the result of a dominance of increased evapotranspiration
	over the seasonal precipitation shifts, with ensemble mean runoff
	changes of ?1, ?6, and ?11% for the A2 ensembles, and 0, ?7, and
	?8% for the B1 ensembles. These hydrological changes were reflected
	in reservoir system performance. Average total basin reservoir storage
	and average hydropower production generally declined, however there
	was a large range across the ensembles. Releases from Glen Canyon
	Dam to the Lower Basin were reduced for all periods and both emissions
	scenarios in the ensemble mean. The fraction of years in which shortages
	occurred increased by approximately 20% by period 3 for both emissions
	scenarios.},
  doi = {10.5194/hess-11-1417-2007},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{christensen+al2004,
  author = {Christensen, N. and Wood, A. and Voisin, N. and Lettenmaier, D. and
	Palmer, R.},
  title = {The Effects of Climate Change on the Hydrology and Water Resources
	of the {C}olorado {R}iver Basin},
  journal = {Climatic Change},
  year = {2004},
  volume = {62},
  pages = {337--363},
  number = {1--3},
  abstract = {The potential effects of climate change on the hydrology and water
	resources of the Colorado River basin are assessed by comparing simulated
	hydrologic and water resources scenarios derived from downscaled
	climate simulations of the U.S. Department of Energy/National Center
	for Atmospheric Research Parallel Climate Model (PCM) to scenarios
	driven by observed historical (1950--1999) climate. PCM climate scenarios
	include an ensemble of three 105-year future climate simulations
	based on projected `business-as-usual'(BAU) greenhouse gas emissions
	and a control climate simulation based on static 1995 greenhouse
	gas concentrations. Downscaled transient temperature and precipitation
	sequences were extracted from PCM simulations, and were used to drive
	the Variable Infiltration Capacity (VIC) macroscale hydrology model
	to produce corresponding streamflow sequences. Results for the BAU
	scenarios were summarized into Periods 1, 2, and 3 (2010--2039,2040--2069,
	2070--2098). Average annual temperature changes for the Colorado
	Riverbasin were 0.5 Â°C warmer for control climate, and 1.0, 1.7,
	and 2.4 Â°C warmer for Periods 1--3, respectively, relative to the
	historicalclimate. Basin-average annual precipitation for the control
	climate was slightly(1%) less than for observed historical climate,
	and 3, 6, and 3%less for future Periods 1--3, respectively. Annual
	runoff in the controlrun was about 10% lower than for simulated historical
	conditions, and 14, 18, and 17% less for Periods 1--3, respectively.
	Analysis of watermanagement operations using a water management model
	driven by simulated streamflows showed that streamflows associated
	with control and future BAU climates would significantly degrade
	the performance of the water resourcessystem relative to historical
	conditions, with average total basin storage reduced by 7% for the
	control climate and 36, 32 and 40% for Periods 1--3, respectively.
	Releases from Glen Canyon Dam to the LowerBasin (mandated by the
	Colorado River Compact) were met in 80% of years for the control
	climate simulation (versus 92% in the historical climate simulation),
	and only in 59--75% of years for the future climate runs. Annual
	hydropower output was also significantly reduced for the control
	and future climate simulations. The high sensitivity of reservoir
	system performance for future climate is a reflection of the fragile
	equilibrium that now exists in operation of the system, with system
	demands only slightly less than long-term mean annual inflow},
  doi = {10.1023/B:CLIM.0000013684.13621.1f},
  tags = {Climate Change}
}

@TECHREPORT{hirham5,
  author = {Christensen, O. and Drews, M. and Christensen, J. and Dethloff, K.
	and Ketelsen, K. and Hebestadt, I. and Rinke, A.},
  title = {{The HIRHAM Regional Climate Model Version 5 (beta)}},
  institution = {Danish Meteorological Institute},
  year = {2007},
  number = {06-17},
  abstract = {This report describes version 5(?) of the HIRHAM regional climate
	model. The report is in two parts. The first part discusses the structure
	and main features of the upgraded version of the model system, while
	the second part offers a short tutorial on how to install and run
	the code, e.g. on NEC SX6.},
  owner = {rojasro},
  timestamp = {2010.08.09}
}

@ARTICLE{christensen2003,
  author = {Christensen, S.},
  title = {A synthetic groundwater modelling study of the accuracy of {GLUE}
	uncertainty intervals},
  journal = {Nordic Hydrology},
  year = {2004},
  volume = {35},
  pages = {45--59},
  number = {1},
  abstract = {Synthetic groundwater flow models with one unknown parameter, the
	average log transmissivity of the flow domain, and with Gaussian
	log-transmissivity error structure were used to study the nature
	and the accuracy of Generalized Likelihood Uncertainty Estimation
	(GLUE) intervals. The uniform prior distribution of log10 transmissivities
	was sampled uniformly and 1000 values per log10-transmissivity cycle
	were required to produce unbiased GLUE results. Because the errors
	in hydraulic head resulting from the log-transmissivity errors are
	known to be Gaussian for a linear model for heads, the Gaussian likelihood
	function was used as the GLUE goodness-of-fit function in most cases
	studied. The GLUE interval computed for the hydraulic head at different
	locations within the domain has the characteristics of a confidence
	interval for the hydraulic heads computed using the spatial average
	log transmissivity. The GLUE interval does not have the characteristics
	of a prediction interval, which is a probability interval for an
	uncertain observation of some variable such as the hydraulic head.
	The goodness-of-fit function can be corrected so that the resulting
	GLUE interval has the characteristics of a prediction interval. However,
	neither the original nor the corrected GLUE interval account for
	the uncertainty caused by small-scale model errors, which is always
	present in practical groundwater flow modelling. It is therefore
	concluded that one should be careful with using the GLUE interval
	to evaluate the validity of a model's structure. The structure may
	be valid even though observations fall significantly outside the
	GLUE interval if the observations are uncertain and the goodness-of-fit
	function is not corrected to account for this, or if small-scale
	model error is significant. If small-scale model error does not significantly
	bias model predictions, the predictions will be useful although uncertain.
	Small-scale model error did not bias the predictions in the examples
	studied here except near a strong sink. Changing the goodness-of-fit
	function from the Gaussian likelihood function to a similar but different
	function made the resulting GLUE intervals very inaccurate. Changing
	to a third function based on a fitted model rejection value produced
	results that were somewhat better than those obtained with the Gaussian
	likelihood function. However, the results were sensitive to the model
	rejection value, which in the ideal case should be adjusted for each
	predicted variable individually. Thus, the third function is not
	attractive for practical applications with the GLUE methodology.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.iwaponline.com/nh/035/nh0350045.htm}
}

@ARTICLE{christensen1999,
  author = {Christensen, S. and Cooley, R.},
  title = {Evaluation of prediction intervals for expressing uncertainties in
	groundwater flow model predictions},
  journal = {Water Resources Research},
  year = {1999},
  volume = {35},
  pages = {2627--2639},
  number = {9},
  abstract = {We tested the accuracy of 95% individual prediction intervals for
	hydraulic heads, streamflow gains, and effective transmissivities
	computed by groundwater models of two Danish aquifers. To compute
	the intervals, we assumed that each predicted value can be written
	as the sum of a computed dependent variable and a random error. Testing
	was accomplished by using a cross-validation method and by using
	new field measurements of hydraulic heads and transmissivities that
	were not used to develop or calibrate the models. The tested null
	hypotheses are that the coverage probability of the prediction intervals
	is not significantly smaller than the assumed probability (95%) and
	that each tail probability is not significantly different from the
	assumed probability (2.5%). In all cases tested, these hypotheses
	were accepted at the 5% level of significance. We therefore conclude
	that for the groundwater models of two real aquifers the individual
	prediction intervals appear to be accurate.},
  doi = {10.1029/1999WR900163},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{christiaens2002a,
  author = {Christiaens, K. and Feyen, J.},
  title = {Soil hydraulic parameter and output uncertainty of the distributed
	hydrological {MIKE SHE} model using the {GLUE} framework},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {373--391},
  number = {2},
  abstract = {Both calibration and uncertainty assessment are mandatory steps in
	today's modelling process. The former considers both the inputs (input
	variables and parameters) as well as model results. An exploratory
	investigation of the applicable parameter space results in a wide
	spectrum of values for a specific model output. By retaining only
	those model realizations that mimic reality in a sufficient way,
	inputs and associated response can be constrained, thereby quantifying
	the uncertainty involved. The generalized likelihood uncertainty
	estimation (GLUE) framework provides a structured methodology for
	this purpose. The study presented focuses on the applicability of
	the GLUE framework within the context of the distributed hydrological
	model MIKE SHE. Even though all significant processes involved are
	incorporated within the model, the problems of calibration and uncertainty
	assessment cannot be avoided. This has resulted in a quest for well-delineated
	effective parameters crucial for sound mechanistic model application.
	Being a complex model, the number of possible realizations using
	MIKE SHE is fairly small due to computing time, and an in-depth exploratory
	approach is impossible. On the other hand, several output variables
	are available aside from the hydrological river response, like water
	content in the soil profile or ground water level, which can be used
	for retaining realistic parameter sets. This study presents some
	preliminary results on the applicability of the GLUE-MIKE SHE framework
	and associated constrained preliminary parameter and output uncertainty
	values. Focus was given to the influence of soil hydraulic parameters
	on the hydrological behaviour of a small study basin. The soil hydraulic
	parameters were predicted using various pedo-transfer-function approaches
	and moisture retention measurements in the laboratory, and these
	distributions were then confronted with ranges of the constrained
	effective parameters. For the latter, the restrictions that were
	imposed based on behavioural acceptance were substantial. All the
	traditional methods show significant uncertainty, due to heterogeneity
	and model error, which cannot be disregarded. The first results suggest
	a reasonable match among effective parameters and laboratory measurements.
	The use of pedo-transfer-function distributions as effective parameters,
	however, may provide unacceptable results, even with liberal criteria.},
  doi = {10.1002/hyp.335},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{christiaens2002b,
  author = {Christiaens, K. and Feyen, J.},
  title = {Use of sensitivity and uncertainty measures in distributed hydrological
	modeling with an application to the {MIKE SHE} model},
  journal = {Water Resources Research},
  year = {2002},
  volume = {38},
  pages = {1169--1184},
  number = {9},
  abstract = {A methodology to qualify and quantify uncertainty and sensitivity
	measures, mathematically related to a representative metamodel and
	correlation coefficients, using the Latin hypercube approach, is
	evaluated in the context of the spatially distributed hydrological
	model MIKE SHE. The characteristics of various outputs, such as cumulative
	catchment discharge, average soil water content, and groundwater
	elevation, are examined at different time and space scales. The soil
	hydraulic parameters make up the varying input. The input uncertainties
	and the corresponding Latin hypercube parameter perturbations are
	based on U.S. Department of Agriculture texture figures. Results
	indicate important differences between the measures, even in ranking,
	making combined interpretation of the measures necessary. The real-world
	problem of correlation among parameters adds significant complexity
	to the assessment of uncertainty and sensitivity and cannot be disregarded
	without caution. The presented methodology, though still CPU intensive,
	allows the hydrological modeler to cope with this complexity.},
  doi = {10.1029/2001WR000478},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{christiaensfeyen2002,
  author = {Christiaens, K. and Feyen, J.},
  title = {{Constraining soil hydraulic parameter and output uncertainty of
	the distributed hydrological MIKE SHE model using the GLUE framework}},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {373--391},
  number = {2},
  abstract = {Both calibration and uncertainty assessment are mandatory steps in
	today's modelling process. The former considers both the inputs (input
	variables and parameters) as well as model results. An exploratory
	investigation of the applicable parameter space results in a wide
	spectrum of values for a specific model output. By retaining only
	those model realizations that mimic reality in a sufficient way,
	inputs and associated response can be constrained, thereby quantifying
	the uncertainty involved. The generalized likelihood uncertainty
	estimation (GLUE) framework provides a structured methodology for
	this purpose. The study presented focuses on the applicability of
	the GLUE framework within the context of the distributed hydrological
	model MIKE SHE. Even though all significant processes involved are
	incorporated within the model, the problems of calibration and uncertainty
	assessment cannot be avoided. This has resulted in a quest for well-delineated
	effective parameters crucial for sound mechanistic model application.
	Being a complex model, the number of possible realizations using
	MIKE SHE is fairly small due to computing time, and an in-depth exploratory
	approach is impossible. On the other hand, several output variables
	are available aside from the hydrological river response, like water
	content in the soil profile or ground water level, which can be used
	for retaining realistic parameter sets. This study presents some
	preliminary results on the applicability of the GLUE-MIKE SHE framework
	and associated constrained preliminary parameter and output uncertainty
	values. Focus was given to the influence of soil hydraulic parameters
	on the hydrological behaviour of a small study basin. The soil hydraulic
	parameters were predicted using various pedo-transfer-function approaches
	and moisture retention measurements in the laboratory, and these
	distributions were then confronted with ranges of the constrained
	effective parameters. For the latter, the restrictions that were
	imposed based on behavioural acceptance were substantial. All the
	traditional methods show significant uncertainty, due to heterogeneity
	and model error, which cannot be disregarded. The first results suggest
	a reasonable match among effective parameters and laboratory measurements.
	The use of pedo-transfer-function distributions as effective parameters,
	however, may provide unacceptable results, even with liberal criteria},
  doi = {10.1002/hyp.335},
  keywords = {GLUE, LHS},
  tags = {Uncertainty}
}

@INCOLLECTION{clerc2010,
  author = {Clerc, Maurice},
  title = {{From theory to practice in Particle Swarm Optimization}},
  booktitle = {Handbook of Swarm Intelligence},
  publisher = {Springer Berlin Heidelberg},
  year = {2010},
  editor = {Panigrahi, Bijaya Ketan and Shi, Yuhui and Lim, Meng-Hiot and Hiot,
	Lim Meng and Ong, Yew Soon},
  volume = {8},
  series = {Adaptation, Learning, and Optimization},
  pages = {3--36},
  abstract = {The purpose of this chapter is to draw attention to two points that
	are not always well understood, namely, a) the “balance” between
	exploitation and exploration may be not what we intuitively think,
	and b) a mean best result may be meaningless. The second point is
	obviously quite important when two algorithms are compared. These
	are discussed in the appendix. We believe that these points would
	be useful to researchers in the field for analysis and comparison
	of algorithms in a better and rigorous way, and help them design
	new powerful tools.},
  doi = {10.1007/978-3-642-17390-5_1},
  isbn = {978-3-642-17390-5},
  tags = {PSO},
  url = {http://dx.doi.org/10.1007/978-3-642-17390-51}
}

@TECHREPORT{clerc2012,
  author = {Clerc, M.},
  title = {{Standard Particle Swarm Optimisation}},
  institution = {Particle Swarm Central},
  year = {2012},
  owner = {rojasro},
  timestamp = {2012.03.12},
  url = {http://clerc.maurice.free.fr/pso/SPSO_descriptions.pdf}
}

@TECHREPORT{clerc2009,
  author = {Clerc, M.},
  title = {{A method to improve Standard PSO}},
  institution = {Particle Swarm Central},
  year = {2009},
  number = {MC2009-03-13},
  owner = {rojasro},
  timestamp = {2012.03.12},
  url = {http://clerc.maurice.free.fr/pso/Design_efficient_PSO.pdf}
}

@TECHREPORT{clerc2007,
  author = {Clerc, M.},
  title = {Back to random topology},
  institution = {Particle Swarm Central},
  year = {2007},
  owner = {rojasro},
  timestamp = {2011.11.23},
  url = {http://clerc.maurice.free.fr/pso/random_topology.pdf}
}

@BOOK{clerc2006,
  title = {{Particle Swarm Optimization}},
  publisher = {ISTE (International Scientific and Technical Encyclopedia)},
  year = {2006},
  author = {Clerc, Maurice},
  abstract = {This is the first book devoted entirely to Particle Swarm Optimization
	(PSO), which is a non-specific algorithm, similar to evolutionary
	algorithms, such as taboo search and ant colonies. Since its original
	development in 1995, PSO has mainly been applied to continuous-discrete
	heterogeneous strongly non-linear numerical optimization and it is
	thus used almost everywhere in the world. Its convergence rate also
	makes it a preferred tool in dynamic optimization. Particle Swarm
	Optimization explains the basic principles of the subject, particularly
	the concepts of particles, information link, memory and cooperation.
	Starting from a simple but efficient parametric version coded in
	a few lines, it shows how this can be gradually enhanced to lead
	to a fully adaptive version. All source programs are either included
	in the book or are downloadable for free.},
  isbn = {978-1-905209-04-0},
  owner = {morpionz},
  tags = {PSO},
  timestamp = {2008.04.20},
  url = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-1905209045.html}
}

@ELECTRONIC{SPSO2011,
  author = {Clerc, M. and Auger, A. and Blackwell, T. and Bratton, D. and Croussette,
	S. and Eberhart, R. and Hansen, N. and Keko, H. Kennedy, J. and Krohling,
	R. and Langdon, W. and Li, W. and Miranda, V. and Poli, R. and Serra,
	P. and Stickel, M.},
  title = {{Standard PSO 2011 - SPSO2011}},
  url = {http://www.particleswarm.info/Programs.html},
  year = {2011},
  comment = {Last time accessed on: 27-09-2012},
  owner = {rojasro},
  timestamp = {2011.11.23}
}

@ARTICLE{clerckennedy2002,
  author = {Clerc, M. and Kennedy, J.},
  title = {The particle swarm - Explosion, stability, and convergence in a multidimensional
	complex space},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2002},
  volume = {6},
  pages = {58--73},
  abstract = {The particle swarm is an algorithm for finding optimal regions of
	complex search spaces through the interaction of individuals in a
	population of particles. Even though the algorithm, which is based
	on a metaphor of social interaction, has been shown to perform well,
	researchers have not adequately explained how it works. Further,
	traditional versions of the algorithm have had some undesirable dynamical
	properties, notably the particles' velocities needed to be limited
	in order to control their trajectories. The present paper analyzes
	a particle's trajectory as it moves in discrete time (the algebraic
	view), then progresses to the view of it in continuous time (the
	analytical view). A five-dimensional depiction is developed, which
	describes the system completely. These analyses lead to a generalized
	model of the algorithm, containing a set of coefficients to control
	the system's convergence tendencies. Some results of the particle
	swarm optimizer, implementing modifications derived from the analysis,
	suggest methods for altering the original algorithm in ways that
	eliminate problems and increase the ability of the particle swarm
	to find optima of some well-studied test functions.},
  doi = {10.1109/4235.985692},
  keywords = {convergence, evolutionary computation, optimization, particle swarm,
	stability},
  tags = {Calibration, PSO}
}

@ARTICLE{clyde1999,
  author = {Clyde, M.},
  title = {Comment on ``{B}ayesian model averaging: {A} tutorial''},
  journal = {Statistical Science},
  year = {1999},
  volume = {14},
  pages = {401--404},
  number = {4},
  owner = {RRojas},
  timestamp = {2008.12.04},
  url = {http://www.jstor.org/stable/2676804}
}

@ARTICLE{clyde2004,
  author = {Clyde, M. and George, E.},
  title = {Model uncertainty},
  journal = {Statistical Science},
  year = {2004},
  volume = {19},
  pages = {81--94},
  number = {1},
  abstract = {The evolution of Bayesian approaches for model uncertainty over the
	past decade has been remarkable. Catalyzed by advances in methods
	and technology for posterior computation, the scope of these methods
	has widened substantially. Major thrusts of these developments have
	included new methods for semiautomatic prior specification and posterior
	exploration. To illustrate key aspects of this evolution, the highlights
	of some of these developments are described.},
  doi = {10.1214/088342304000000035},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/4144374}
}

@BOOK{coles2001,
  title = {An introduction to statistical modeling of extreme values},
  publisher = {Springer},
  year = {2001},
  author = {Coles, S.},
  pages = {224},
  series = {Springer series in statistics},
  address = {London},
  edition = {First},
  owner = {rojasro},
  timestamp = {2010.08.23}
}

@ARTICLE{collins2007,
  author = {Collins, M.},
  title = {Ensembles and probabilities: {A} new era in the prediction of climate
	change},
  journal = {Philosophical Transactions of the Royal Society A Mathematical, Physical
	\& Engineering Sciences},
  year = {2007},
  volume = {365},
  pages = {1957--1970},
  number = {1857},
  abstract = {Predictions of future climate are of central importance in determining
	actions to adapt to the impacts of climate change and in formulating
	targets to reduce emissions of greenhouse gases. In the absence of
	analogues of the future, physically based numerical climate models
	must be used to make predictions. New approaches are under development
	to deal with a number of sources of uncertainty that arise in the
	prediction process. This paper introduces some of the concepts and
	issues in these new approaches, which are discussed in more detail
	in the papers contained in this issue.},
  doi = {10.1098/rsta.2007.2068},
  keywords = {climate change, prediction, uncertainty, probability},
  tags = {Climate Change, Uncertainty}
}

@ARTICLE{collins+al2006,
  author = {Collins, M. and Booth, B. and Harris, G. and Murphy, J. and Sexton,
	D. and Webb, M.},
  title = {Towards quantifying uncertainty in transient climate change},
  journal = {Climate Dynamics},
  year = {2006},
  volume = {27},
  pages = {127--147},
  number = {2},
  abstract = {Ensembles of coupled atmosphere–ocean global circulation model simulations
	are required to make probabilistic predictions of future climate
	change. Perturbed physics ensembles provide a new approach in which
	modelling uncertainties are sampled systematically by perturbing
	uncertain parameters. The aim is to provide a basis for probabilistic
	predictions in which the impact of prior assumptions and observational
	constraints can be clearly distinguished. Here we report on the first
	perturbed physics coupled atmosphere–ocean model ensemble in which
	poorly constrained atmosphere, land and sea-ice component parameters
	are varied in the third version of the Hadley Centre model (the variation
	of ocean parameters will be the subject of future study). Flux adjustments
	are employed, both to reduce regional sea surface temperature (SST)
	and salinity biases and also to admit the use of combinations of
	model parameter values which give non-zero values for the global
	radiation balance. This improves the extent to which the ensemble
	provides a credible basis for the quantification of uncertainties
	in climate change, especially at a regional level. However, this
	particular implementation of flux-adjustments leads to a weakening
	of the Atlantic overturning circulation, resulting in the development
	of biases in SST and sea ice in the North Atlantic and Arctic Oceans.
	Nevertheless, model versions are produced which are of similar quality
	to the unperturbed and un-flux-adjusted version. The ensemble is
	used to simulate pre-industrial conditions and a simple scenario
	of a 1% per year compounded increase in CO2. The range of transient
	climate response (the 20 year averaged global warming at the time
	of CO2 doubling) is 1.5–2.6°C, similar to that found in multi-model
	studies. Measures of global and large scale climate change from the
	coupled models show simple relationships with associated measures
	computed from atmosphere-mixed-layer-ocean climate change experiments,
	suggesting that recent advances in computing the probability density
	function of climate change under equilibrium conditions using the
	perturbed physics approach may be extended to the transient case.},
  doi = {10.1007/s00382-006-0121-0},
  owner = {rojasro},
  timestamp = {2011.04.29}
}

@ARTICLE{confesorwhittaker2007,
  author = {Confesor, R. and Whittaker, G.},
  title = {Automatic calibration of hydrologic models with multi-objective evolutionary
	algorithm and {P}areto optimization},
  journal = {Journal of the American Water Resources Association},
  year = {2007},
  volume = {43},
  pages = {981--989},
  abstract = {In optimization problems with at least two conflicting objectives,
	a set of solutions rather than a unique one exists because of the
	trade-offs between these objectives. A Pareto optimal solution set
	is achieved when a solution cannot be improved upon without degrading
	at least one of its objective criteria. This study investigated the
	application of multi-objective evolutionary algorithm (MOEA) and
	Pareto ordering optimization in the automatic calibration of the
	Soil and Water Assessment Tool (SWAT), a process-based, semi-distributed,
	and continuous hydrologic model. The nondominated sorting genetic
	algorithm II (NSGA-II), a fast and recent MOEA, and SWAT were called
	in FORTRAN from a parallel genetic algorithm library (PGAPACK) to
	determine the Pareto optimal set. A total of 139 parameter values
	were simultaneously and explicitly optimized in the calibration.
	The calibrated SWAT model simulated well the daily streamflow of
	the Calapooia watershed for a 3-year period. The daily Nash-Sutcliffe
	coefficients were 0.86 at calibration and 0.81 at validation. Automatic
	multi-objective calibration of a complex watershed model was successfully
	implemented using Pareto ordering and MOEA. Future studies include
	simultaneous automatic calibration of water quality and quantity
	parameters and the application of Pareto optimization in decision
	and policy-making problems related to conflicting objectives of economics
	and environmental quality.},
  doi = {10.1111/j.1752-1688.2007.00080.x},
  keywords = {simulation, optimization, nonpoint source pollution, Pareto, multi-objective
	evolutionary algorithm, automatic calibration, GLOBAL OPTIMIZATION,
	SWAT},
  tags = {Calibration, SWAT}
}

@ARTICLE{congdon2005,
  author = {Congdon, P.},
  title = {Bayesian predictive model comparison via parallel sampling},
  journal = {Computational Statistics \& Data Analysis},
  year = {2005},
  volume = {48},
  pages = {735--753},
  number = {4},
  abstract = {Methods of model comparison and checking, and associated criteria,
	are proposed based on parallel sampling of two or more models subsequent
	to convergence. These complement Bayesian predictive criteria already
	proposed (e.g. error sum of squares and deviance based) but are on
	a scale that may be compared across applications. Penalised criteria
	for model comparison based on the AIC are also investigated, together
	with AIC model weights and evidence ratios. Parallel sampling enables
	posterior summaries to be obtained for continuous comparison measures
	(e.g. likelihood and evidence ratios). A forward selection procedure
	for regression is suggested as one possible extension, as well as
	procedures for model averaging and posterior predictive checking.
	Comparisons with the DIC are made together with implications of parallel
	sampling for assessing the density of the DIC. Three worked examples
	illustrate the working of the procedures in practice.},
  doi = {10.1016/j.csda.2004.03.016},
  keywords = {Predictive model comparison, Model checking, Parallel sampling, Predictor
	selection, Penalties for complexity},
  tags = {Calibration}
}

@ARTICLE{cooley1986,
  author = {Cooley, R. and Konikow, L. and Naff, R.},
  title = {Nonlinear regression groundwater flow modeling of a deep regional
	aquifer system},
  journal = {Water Resources Research},
  year = {1986},
  volume = {22},
  pages = {1759--1778},
  number = {13},
  abstract = {A nonlinear regression groundwater flow model, based on a Galerkin
	finite-element discretization, was used to analyze steady state two-dimensional
	groundwater flow in the areally extensive Madison aquifer in a 75,000
	mi2 area of the Northern Great Plains. Regression parameters estimated
	include intrinsic permeabilities of the main aquifer and separate
	lineament zones, discharges from eight major springs surrounding
	the Black Hills, and specified heads on the model boundaries. Aquifer
	thickness and temperature variations were included as specified functions.
	The regression model was applied using sequential F testing so that
	the fewest number and simplest zonation of intrinsic permeabilities,
	combined with the simplest overall model, were evaluated initially;
	additional complexities (such as subdivisions of zones and variations
	in temperature and thickness) were added in stages to evaluate the
	subsequent degree of improvement in the model results. It was found
	that only the eight major springs, a single main aquifer intrinsic
	permeability, two separate lineament intrinsic permeabilities of
	much smaller values, and temperature variations are warranted by
	the observed data (hydraulic heads and prior information on some
	parameters) for inclusion in a model that attempts to explain significant
	controls on groundwater flow. Addition of thickness variations did
	not significantly improve model results; however, thickness variations
	were included in the final model because they are fairly well defined.
	Effects on the observed head distribution from other features, such
	as vertical leakage and regional variations in intrinsic permeability,
	apparently were overshadowed by measurement errors in the observed
	heads. Estimates of the parameters correspond well to estimates obtained
	from other independent sources.},
  doi = {10.1029/WR022i013p01759},
  owner = {rojasro},
  timestamp = {2010.03.04}
}

@TECHREPORT{cooley1990,
  author = {Cooley, R. and Naff, R.},
  title = {Regression modeling of ground--water flow},
  institution = {United States Geological Survey},
  year = {1990},
  type = {{Techniques of Water-Resources Investigations}},
  number = {{Book 3, Chap. B4}},
  owner = {rojasro},
  timestamp = {2010.03.04}
}

@ARTICLE{cools2006,
  author = {Cools, J. and Meyus, Y. and Woldeamlak, S. and Batelaan, O. and {De
	Smedt}, F.},
  title = {Large--scale {GIS}--based hydrogeological modeling of {F}landers:
	{A} tool for groundwater management},
  journal = {Environmental Geology},
  year = {2006},
  volume = {50},
  pages = {1201--1209},
  number = {8},
  abstract = {For the implementation of the European Union Water Framework Directive
	(WFD), technological and scientific support are required. This paper
	presents a methodology to support a first step of the implementation
	of WFD, which is the delineation of groundwater bodies. The methodology
	consists of (1) the development of a complete and generally-accepted
	hydrogeological classification system for Flanders, named the HCOV
	code, (2) the development of a geographic information systems (GIS)-managed
	borehole database, and (3) the development of aquifer and aquitard
	models by means of a solid modeling approach. For each unit of the
	hydrogeological classification code for Flanders unit, GIS maps are
	generated for the three basic characteristics of hydrogeological
	layers: extent, base level and thickness, such that combined, the
	volume and extent of a hydrogeological layer is unambiguously defined.
	This GIS-based hydrogeological database has become a useful tool
	for groundwater management purposes and to provide the input for
	groundwater modeling.},
  doi = {10.1007/s00254-006-0292-3},
  owner = {RRojas},
  refid = {COOLS2006},
  timestamp = {2008.11.04}
}

@ARTICLE{cooper+al2007,
  author = {Cooper, V. and Nguyen, {V-T} and Nicell, J.},
  title = {Calibration of conceptual rainfall{--}runoff models using global
	optimisation methods with hydrologic process-based parameter constraints},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {334},
  pages = {455--466},
  number = {3-4},
  abstract = {The major difficulty associated with the use of conceptual rainfall{--}runoff
	(CRR) models in hydrology is their calibration since most of these
	models involve a large number of parameters. CRR model calibration
	is a global optimisation problem since its main objective is to find
	a set of optimal model parameter values that provides a best fit
	between observed and estimated flow hydrographs. However, even when
	using the superior search capabilities of modern global optimisation
	methods (GOM), the search for a set of optimal parameter values within
	an inflated multi-dimensional search space can result in an inefficient
	search and may lead to inaccurate parameter estimates. In addition,
	in most CRR model calibration studies to-date, no explicit constraint
	in the search procedure was used that could ensure the physical consistency
	of the estimated parameters. Improvements in parameter estimates
	could be achieved if the search space could be reduced through the
	incorporation of constraints that describe the logical interactions
	between the rainfall and runoff processes. A methodology is proposed
	herein for formulating constraints to improve the probability of
	success of calibration methods. More specifically, inequalities relating
	CRR model parameters with the available hydrologic data were developed
	and incorporated into a GOM to reduce the search space. The shuffled
	complex evolution (SCE) GOM showed that this approach resulted in
	significant improvements in the estimation of CRR model parameters
	for the case of synthetic streamflow data without errors as well
	as for data with heteroscesdastic errors. Furthermore, the suggested
	constrained SCE-based calibration procedure could provide CRR model
	parameter estimates that are consistent with the physically plausible
	interactions between the rainfall and runoff processes under consideration.},
  doi = {10.1016/j.jhydrol.2006.10.036},
  keywords = {Conceptual rainfall{--}runoff models, Global optimisation methods,
	Model calibration, Shuffled complex evolution technique, Constrained
	optimisation},
  tags = {SWAT, Calibration}
}

@ARTICLE{cooper+al1997,
  author = {Cooper, V. and Nguyen, {V-T} and Nicell, J.},
  title = {Evaluation of global optimization methods for conceptual rainfall-runoff
	model calibration},
  journal = {Water Science \& Technology},
  year = {1997},
  volume = {36},
  pages = {53--60},
  number = {5},
  abstract = {The calibration of conceptual rainfall runoff (CRR) models is an optimization
	problem whose objective is to determine the values of the model parameters
	which provide the best fit between observed and estimated flows.
	This study investigated the performance of three probabilistic optimization
	techniques for calibrating the Tank model, a hydrologic model typical
	of CRR models. These methods were the Shuffled Complex Evolution
	(SCE), genetic algorithms (GA) and simulated annealing (SA) methods.
	It was found that performances depended on the choice of the objective
	function considered and also on the position of the start of the
	optimization search relative to the global optimum. Of the three
	global optimization methods (GOM) in the study, the SCE method provided
	better estimates of the optimal solution than the GA and SA methods.
	Regarding the efficiency of the GOMs, as expressed by the number
	of iterations for convergence, the ranking in order of decreasing
	performance was the SCE, the GA and the SA methods. (C) 1997 IAWQ.
	Published by Elsevier Science Ltd.},
  keywords = {conceptual rainfall-runoff models, genetic algorithms, global optimization,
	model calibration, shuffled complex evolution method, simulated annealing},
  tags = {Calibration},
  url = {http://www.iwaponline.com/wst/03605/wst036050053.htm}
}

@ARTICLE{cooren+al2009,
  author = {Cooren, Y. and Clerc, M. and Siarry, P.},
  title = {{Performance evaluation of TRIBES, an adaptive particle swarm optimisation
	algorithm}},
  journal = {Swarm Intelligence},
  year = {2009},
  volume = {3},
  pages = {149--178},
  number = {2},
  doi = {10.1007/s11721-009-0026-8},
  owner = {rojasro},
  timestamp = {2012.09.26}
}

@ARTICLE{coppola+al2010,
  author = {Coppola, E. and Giorgi, F. and Rauscher, S. and Piani, C.},
  title = {Model weighting based on mesoscale structures in precipitation and
	temperature in an ensemble of regional climate models},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {121--134},
  number = {2--3},
  abstract = {We present a weighting scheme specifically designed for regional climate
	models (RCMs) in that it is based on the model performance in simulating
	the sub-global climate model (GCM) mesoscale climate signal. The
	functional form of the weights is based on multiple variables (temperature
	and precipitation) and metrics (correlation and root mean square
	error). The weighting scheme is applied to an ensemble of RCM simulations
	for the European region recently completed as part of the ENSEMBLES
	project. As a test of the successful implementation of the scheme,
	the weighting leads to an overall improvement of the performance
	of the ensemble when measured with the same metrics used in the weighting.
	The improvement is particularly pronounced over topographically complex
	regions (e.g. the Alps) in which a larger inter-model range of performance,
	and thus a more aggressive weighting, is found. When applied to the
	generation of probabilistic climate change projections, this scheme
	is designed to be used in conjunction with other RCM weighting metrics
	developed in the ENSEMBLES project and corresponding weighting schemes
	for the GCMs driving the regional models.},
  doi = {10.3354/cr00940},
  owner = {rojasro},
  timestamp = {2011.07.19}
}

@ARTICLE{hess-15-2963-2011,
  author = {{Corzo Perez}, G. A. and {van Huijgevoort}, M. H. J. and Vo{\ss},
	F. and {van Lanen}, H. A. J.},
  title = {On the spatio-temporal analysis of hydrological droughts from global
	hydrological models},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {2963--2978},
  number = {9},
  doi = {10.5194/hess-15-2963-2011},
  url = {http://www.hydrol-earth-syst-sci.net/15/2963/2011/}
}

@ARTICLE{cowles1996,
  author = {Cowles, M. and Carlin, B.},
  title = {Markov chain {M}onte {C}arlo convergence diagnostics: {A} comparative
	review},
  journal = {Journal of the American Statistical Association},
  year = {1996},
  volume = {91},
  pages = {883--904},
  number = {434},
  abstract = {A critical issue for users of Markov chain Monte Carlo (MCMC) methods
	in applications is how to determine when it is safe to stop sampling
	and use the samples to estimate characteristics of the distribution
	of interest. Research into methods of computing theoretical convergence
	bounds holds promise for the future but to date has yielded relatively
	little of practical use in applied work. Consequently, most MCMC
	users address the convergence problem by applying diagnostic tools
	to the output produced by running their samplers. After giving a
	brief overview of the area, we provide an expository review of 13
	convergence diagnostics, describing the theoretical basis and practical
	implementation of each. We then compare their performance in two
	simple models and conclude that all of the methods can fail to detect
	the sorts of convergence failure that they were designed to identify.
	We thus recommend a combination of strategies aimed at evaluating
	and accelerating MCMC sampler convergence, including applying diagnostic
	procedures to a small number of parallel chains, monitoring autocorrelations
	and cross-correlations, and modifying parameterizations or sampling
	algorithms appropriately. We emphasize, however, that it is not possible
	to say with certainty that a finite sample from an MCMC algorithm
	is representative of an underlying stationary distribution.},
  owner = {RRojas},
  refid = {COWLES1996},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2291683}
}

@ARTICLE{crisswinston2008,
  author = {Criss, R. and Winston, W.},
  title = {Do {N}ash values have value? {D}iscussion and alternate proposals},
  journal = {Hydrological Processes},
  year = {2008},
  volume = {22},
  pages = {2723--2725},
  number = {14},
  doi = {10.1002/hyp.7072},
  highlights = {Volumetric Efficiency (VE), similar to pabias (absolute pbias)},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{doll2002,
  author = {D\"oll, P.},
  title = {Impact of climate change and variability on irrigation requirements:
	{A} global perspective},
  journal = {Climatic Change},
  year = {2002},
  volume = {54},
  pages = {269--293},
  number = {3},
  abstract = {Anthropogenic climate change does not only affect water resources
	but also water demand. Future water and food security will depend,
	among other factors, on the impact of climate change on water demand
	for irrigation. Using a recently developed global irrigation model,
	with a spatial resolution of 0.5degrees by 0.5degrees, we present
	the first global analysis of the impact of climate change and climate
	variability on irrigation water requirements. We compute how long-term
	average irrigation requirements might change under the climatic conditions
	of the 2020s and the 2070s, as provided by two climate models, and
	relate these changes to the variations in irrigation requirements
	caused by long-term and interannual climate variability in the 20th
	century. Two-thirds of the global area equipped for irrigation in
	1995 will possibly suffer from increased water requirements, and
	on up to half of the total area (depending on the measure of variability),
	the negative impact of climate change is more significant than that
	of climate variability.},
  doi = {10.1023/A:1016124032231},
  keywords = {AGRICULTURE, DEMAND, MODEL},
  tags = {Agriculture, Impacts}
}

@ARTICLE{doll2008,
  author = {D\"oll, P. and Fiedler, K.},
  title = {Global-scale modeling of groundwater recharge},
  journal = {Hydrology and Earth System Sciences},
  year = {2008},
  volume = {12},
  pages = {863--885},
  number = {3},
  month = {May},
  abstract = {Long-term average groundwater recharge, which is equivalent to renewable
	groundwater resources, is the major limiting factor for the sustainable
	use of groundwater. Compared to surface water resources, groundwater
	resources are more protected from pollution, and their use is less
	restricted by seasonal and inter-annual flow variations. To support
	water management in a globalized world, it is necessary to estimate
	groundwater recharge at the global scale. Here, we present a best
	estimate of global-scale long-term average diffuse groundwater recharge
	(i.e. renewable groundwater resources) that has been calculated by
	the most recent version of the WaterGAP Global Hydrology Model WGHM
	(spatial resolution of 0.5° by 0.5°, daily time steps). The estimate
	was obtained using two state-of-the-art global data sets of gridded
	observed precipitation that we corrected for measurement errors,
	which also allowed to quantify the uncertainty due to these equally
	uncertain data sets. The standard WGHM groundwater recharge algorithm
	was modified for semi-arid and arid regions, based on independent
	estimates of diffuse groundwater recharge, which lead to an unbiased
	estimation of groundwater recharge in these regions. WGHM was tuned
	against observed long-term average river discharge at 1235 gauging
	stations by adjusting, individually for each basin, the partitioning
	of precipitation into evapotranspiration and total runoff. We estimate
	that global groundwater recharge was 12 666 km3/yr for the climate
	normal 1961–1990, i.e. 32% of total renewable water resources. In
	semi-arid and arid regions, mountainous regions, permafrost regions
	and in the Asian Monsoon region, groundwater recharge accounts for
	a lower fraction of total runoff, which makes these regions particularly
	vulnerable to seasonal and inter-annual precipitation variability
	and water pollution. Average per-capita renewable groundwater resources
	of countries vary between 8 m3/(capita yr) for Egypt to more than
	1 million m3/(capita yr) for the Falkland Islands, the global average
	in the year 2000 being 2091 m3/(capita yr). Regarding the uncertainty
	of estimated groundwater resources due to the two precipitation data
	sets, deviation from the mean is 1.1% for the global value, and less
	than 1% for 50 out of the 165 countries considered, between 1 and
	5% for 62, between 5 and 20% for 43 and between 20 and 80% for 10
	countries. Deviations at the grid scale can be much larger, ranging
	between 0 and 186 mm/yr.},
  doi = {10.5194/hess-12-863-2008},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{doll2003,
  author = {D\"oll, P. and Kaspar, F. and Lehner, B.},
  title = {A global hydrological model for deriving water availability indicators:
	model tuning and validation},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {270},
  pages = {105--134},
  number = {1--2},
  month = {January},
  abstract = {Freshwater availability has been recognized as a global issue, and
	its consistent quantification not only in individual river basins
	but also at the global scale is required to support the sustainable
	use of water. The WaterGAP Global Hydrology Model WGHM, which is
	a submodel of the global water use and availability model WaterGAP
	2, computes surface runoff, groundwater recharge and river discharge
	at a spatial resolution of 0.5°. WGHM is based on the best global
	data sets currently available, and simulates the reduction of river
	discharge by human water consumption. In order to obtain a reliable
	estimate of water availability, it is tuned against observed discharge
	at 724 gauging stations, which represent 50% of the global land area
	and 70% of the actively discharging area. For 50% of these stations,
	the tuning of one model parameter was sufficient to achieve that
	simulated and observed long-term average discharges agree within
	1%. For the rest, however, additional corrections had to be applied
	to the simulated runoff and discharge values. WGHM not only computes
	the long-term average water resources of a country or a drainage
	basin but also water availability indicators that take into account
	the interannual and seasonal variability of runoff and discharge.
	The reliability of the modeling results is assessed by comparing
	observed and simulated discharges at the tuning stations and at selected
	other stations. The comparison shows that WGHM is able to calculate
	reliable and meaningful indicators of water availability at a high
	spatial resolution. In particular, the 90% reliable monthly discharge
	is simulated well. Therefore, WGHM is suited for application in global
	assessments related to water security, food security and freshwater
	ecosystems.},
  doi = {10.1016/S0022-1694(02)00283-4},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{doll2010,
  author = {D\"oll, P. and Zhang, J.},
  title = {Impact of climate change on freshwater ecosystems: {A} global--scale
	analysis of ecologically relevant river flow alterations},
  journal = {Hydrology and Earth System Sciences},
  year = {2010},
  volume = {14},
  pages = {783--799},
  number = {5},
  abstract = {River flow regimes, including long-term average flows, seasonality,
	low flows, high flows and other types of flow variability, play an
	important role for freshwater ecosystems. Thus, climate change affects
	freshwater ecosystems not only by increased temperatures but also
	by altered river flow regimes. However, with one exception, transferable
	quantitative relations between flow alterations and ecological responses
	have not yet been derived. While discharge decreases are generally
	considered to be detrimental for ecosystems, the effect of future
	discharge increases is unclear. As a first step towards a global-scale
	analysis of climate change impacts on freshwater ecosystems, we quantified
	the impact of climate change on five ecologically relevant river
	flow indicators, using the global water model WaterGAP 2.1g to simulate
	monthly time series of river discharge with a spatial resolution
	of 0.5 degrees. Four climate change scenarios based on two global
	climate models and two greenhouse gas emissions scenarios were evaluated.
	We compared the impact of climate change by the 2050s to the impact
	of water withdrawals and dams on natural flow regimes that had occurred
	by 2002. Climate change was computed to alter seasonal flow regimes
	significantly (i.e. by more than 10%) on 90% of the global land area
	(excluding Greenland and Antarctica), as compared to only one quarter
	of the land area that had suffered from significant seasonal flow
	regime alterations due to dams and water withdrawals. Due to climate
	change, the timing of the maximum mean monthly river discharge will
	be shifted by at least one month on one third on the global land
	area, more often towards earlier months (mainly due to earlier snowmelt).
	Dams and withdrawals had caused comparable shifts on less than 5%
	of the land area only. Long-term average annual river discharge is
	predicted to significantly increase on one half of the land area,
	and to significantly decrease on one quarter. Dams and withdrawals
	had led to significant decreases on one sixth of the land area, and
	nowhere to increases. Thus, by the 2050s, climate change may have
	impacted ecologically relevant river flow characteristics more strongly
	than dams and water withdrawals have up to now. The only exception
	refers to the decrease of the statistical low flow Q90, with significant
	decreases both by past water withdrawals and future climate change
	on one quarter of the land area. However, dam impacts are likely
	underestimated by our study. Considering long-term average river
	discharge, only a few regions, including Spain, Italy, Iraq, Southern
	India, Western China, the Australian Murray Darling Basin and the
	High Plains Aquifer in the USA, all of them with extensive irrigation,
	are expected to be less affected by climate change than by past anthropogenic
	flow alterations. In some of these regions, climate change will exacerbate
	the discharge reductions, while in others climate change provides
	opportunities for reducing past reductions. Emissions scenario B2
	leads to only slightly reduced alterations of river flow regimes
	as compared to scenario A2 even though emissions are much smaller.
	The differences in alterations resulting from the two applied climate
	models are larger than those resulting from the two emissions scenarios.
	Based on general knowledge about ecosystem responses to flow alterations
	and data related to flow alterations by dams and water withdrawals,
	we expect that the computed climate change induced river flow alterations
	will impact freshwater ecosystems more strongly than past anthropogenic
	alterations.},
  doi = {10.5194/hess-14-783-2010},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{deque2007,
  author = {D\'equ\'e, M.},
  title = {Frequency of precipitation and temperature extremes over France in
	an anthropogenic scenario: Model results and statistical correction
	according to observed values},
  journal = {Global and Planetary Change},
  year = {2007},
  volume = {57},
  pages = {16--26},
  number = {1--2},
  abstract = {Météo-France atmospheric model ARPEGE/Climate has been used to simulate
	present climate (1961–1990) and a possible future climate (2071–2100)
	through two ensembles of three 30-year numerical experiments. In
	the scenario experiment, the greenhouse gas and aerosol concentrations
	are prescribed by the so-called SRES-A2 hypotheses, whereas the sea
	surface temperature and sea ice extent come from an earlier ocean–atmosphere
	coupled simulation. The model covers the whole globe, with a variable
	resolution reaching 50 to 60 km over France. Model responses on daily
	minimum and maximum temperature and precipitation are analyzed over
	France. The distribution of daily values is compared with observed
	data from the French climatological network. The extreme cold temperatures
	and summer heavy precipitations are underestimated by the model.
	A correction technique is proposed in order to adjust the simulated
	values according to the observed ones. This process is applied to
	both reference and scenario simulation. Synthetic indices of extreme
	events are calculated with corrected simulations. The number of heavy
	rain (N10 mm) days increases by one quarter in winter. The maximum
	length of summer dry episodes increases by one half in summer. The
	number of heat wave days is multiplied by 10. The response in precipitation
	is less when only the change in the mean is considered. Such a corrected
	simulation is useful to feed impact models which are sensitive to
	threshold values, but the correction does not reduce, and may enhance
	in some cases, the uncertainty about the climate projections. Using
	several models and scenarios is the appropriate technique to deal
	with uncertainty.},
  doi = {10.1016/j.gloplacha.2006.11.030},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@ARTICLE{deque+al2005,
  author = {D\'equ\'e, M. and Jones, R. and Wild, M. and Giorgi, F. and Christensen,
	J. and Hassell, D. and Vidale, P. and Rockel, B. and Jacob, D. and
	Kjellstr\"om, E. and {de Castro}, M. and Kucharski, F. and {van den
	Hurk}, B.},
  title = {Global high resolution versus {L}imited {A}rea {M}odel climate change
	projections over {E}urope: {Q}uantifying confidence level from {PRUDENCE}
	results},
  journal = {Climate Dynamics},
  year = {2005},
  volume = {25},
  pages = {653--670},
  number = {6},
  abstract = {Four high resolution atmospheric general circulation models (GCMs)
	have been integrated with the standard forcings of the PRUDENCE experiment:
	IPCC-SRES A2 radiative forcing and Hadley Centre sea surface temperature
	and sea-ice extent. The response over Europe, calculated as the difference
	between the 2071--2100 and the 1961--1990 means is compared with
	the same diagnostic obtained with nine Regional Climate Models (RCM)
	all driven by the Hadley Centre atmospheric GCM. The seasonal mean
	response for 2m temperature and precipitation is investigated. For
	temperature, GCMs and RCMs behave similarly, except that GCMs exhibit
	a larger spread. However, during summer, the spread of the RCMsâ€”in
	particular in terms of precipitationâ€”is larger than that of the
	GCMs. This indicates that the European summer climate is strongly
	controlled by parameterized physics and/or high-resolution processes.
	The temperature response is larger than the systematic error. The
	situation is different for precipitation. The model bias is twice
	as large as the climate response. The confidence in PRUDENCE results
	comes from the fact that the models have a similar response to the
	IPCC-SRES A2 forcing, whereas their systematic errors are more spread.
	In addition, GCM precipitation response is slightly but significantly
	different from that of the RCMs.},
  doi = {10.1007/s00382-005-0052-1},
  keywords = {PRUDENCE},
  tags = {Multimodel - Ensambles, RCMs}
}

@ARTICLE{deque+al2007,
  author = {D\'equ\'e, M. and Rowell, D. and L{\"u}thi, D. and Giorgi, F. and
	Christensen, J. and Rockel, B. and Jacob, D. and Kjellstr\"om, E.
	and {de Castro}, M. and {van den Hurk}, B.},
  title = {An intercomparison of regional climate simulations for {E}urope:
	{A}ssessing uncertainties in model projections},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {53--70},
  number = {1},
  abstract = {Ten regional climate models (RCM) have been integrated with the standard
	forcings of the PRUDENCE experiment: IPCC-SRES A2 radiative forcing
	and Hadley Centre boundary conditions. The response over Europe,
	calculated as the difference between the 2071--2100 and the 1961--1990
	means can be viewed as an expected value about which various uncertainties
	exist. Uncertainties are measured here by variance in eight sub-European
	boxes. Four sources of uncertainty can be evaluated with the material
	provided by the PRUDENCE project. Sampling uncertainty is due to
	the fact that the model climate is estimated as an average over a
	finite number of years (30). Model uncertainty is due to the fact
	that the models use different techniques to discretize the equations
	and to represent sub-grid effects. Radiative uncertainty is due to
	the fact that IPCC-SRES A2 is merely one hypothesis. Some RCMs have
	been run with another scenario of greenhouse gas concentration (IPCC-SRES
	B2). Boundary uncertainty is due to the fact that the regional models
	have been run under the constraint of the same global model. Some
	RCMs have been run with other boundary forcings. The contribution
	of the different sources varies according to the field, the region
	and the season, but the role of boundary forcing is generally greater
	than the role of the RCM, in particular for temperature. Maps of
	minimum expected 2m temperature and precipitation responses for the
	IPCC-A2 scenario show that, despite the above mentioned uncertainties,
	the signal from the PRUDENCE ensemble is significant},
  doi = {10.1007/s10584-006-9228-x},
  tags = {Multimodel - Ensambles, RCMs}
}

@ARTICLE{dequesomot2010,
  author = {D\'equ\'e, M. and Somot, S.},
  title = {{Weighted frequency distributions express modelling uncertainties
	in the ENSEMBLES regional climate experiments}},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {195--209},
  number = {2--3},
  abstract = {Fourteen regional climate models (RCMs) were driven by general circulation
	models (GCMs) in FP6-ENSEMBLES to provide 17 fine-scale (25 km) climate
	change scenarios for the period 2021–2050. In a preliminary exercise,
	these RCMs were driven by gridded observations (ERA40 reanalysis)
	to simulate as accurately as possible the 1961–2000 period. The quality
	of this reproduction was used to calculate a weight for each model.
	Each individual model climate had an uncertainty due to the finite
	sampling (30 yr). These spreads were combined by those weights to
	produce an ensemble uncertainty. We illustrate here the daily and
	climatological frequency distributions for winter and summer temperature
	and precipitation in 3 European cities (Budapest, Dublin and Lisbon).
	The distribution obtained by ENSEMBLES weights was compared with
	a distribution using equal weights, distributions using random weights
	and distributions based on a single model. As far as the reproduction
	of the observed distribution (1961–1990) is concerned, there is no
	evidence that the ENSEMBLES weight system provides results closer
	to observation than equal weights or weights drawn at random. A single
	model taken at random yields a quality score not better than ENSEMBLES
	in the case of precipitation, and worse than ENSEMBLES in the case
	of temperature. As far as climate change for 2021–2050 is concerned,
	the use of ENSEMBLES weights instead of equal weights also leads
	to a similar response at daily as well as 30 yr mean time scales.},
  doi = {10.3354/cr00866},
  owner = {rojasro},
  timestamp = {2011.07.19}
}

@ARTICLE{deque+al2011,
  author = {D\'equ\'e, M. and Somot, S. and {Sanchez-Gomez}, E. and Goodess,
	C. and Jacob, D. and Lenderink, G. and Christensen, O.},
  title = {{The spread amongst ENSEMBLES regional scenarios: regional climate
	models, driving general circulation models and interannual variability}},
  journal = {Climate Dynamics},
  year = {2011},
  pages = {1--14},
  abstract = {Various combinations of thirteen regional climate models (RCM) and
	six general circulation models (GCM) were used in FP6-ENSEMBLES.
	The response to the SRES-A1B greenhouse gas concentration scenario
	over Europe, calculated as the difference between the 2021–2050 and
	the 1961–1990 means can be viewed as an expected value about which
	various uncertainties exist. Uncertainties are measured here by variance
	explained for temperature and precipitation changes over eight European
	sub-areas. Three sources of uncertainty can be evaluated from the
	ENSEMBLES database. Sampling uncertainty is due to the fact that
	the model climate is estimated as an average over a finite number
	of years (30) despite a non-negligible interannual variability. Regional
	model uncertainty is due to the fact that the RCMs use different
	techniques to discretize the equations and to represent sub-grid
	effects. Global model uncertainty is due to the fact that the RCMs
	have been driven by different GCMs. Two methods are presented to
	fill the many empty cells of the ENSEMBLES RCM × GCM matrix. The
	first one is based on the same approach as in FP5-PRUDENCE. The second
	one uses the concept of weather regimes to attempt to separate the
	contribution of the GCM and the RCM. The variance of the climate
	response is analyzed with respect to the contribution of the GCM
	and the RCM. The two filling methods agree that the main contributor
	to the spread is the choice of the GCM, except for summer precipitation
	where the choice of the RCM dominates the uncertainty. Of course
	the implication of the GCM to the spread varies with the region,
	being maximum in the South-western part of Europe, whereas the continental
	parts are more sensitive to the choice of the RCM. The third cause
	of spread is systematically the interannual variability. The total
	uncertainty about temperature is not large enough to mask the 2021–2050
	response which shows a similar pattern to the one obtained for 2071–2100
	in PRUDENCE. The uncertainty about precipitation prevents any quantitative
	assessment on the response at grid point level for the 2021–2050
	period. One can however see, as in PRUDENCE, a positive response
	in winter (more rain in the scenario than in the reference) in northern
	Europe and a negative summer response in southern Europe.},
  doi = {10.1007/s00382-011-1053-x},
  owner = {rojasro},
  timestamp = {2011.05.26}
}

@BOOK{dagan1989,
  title = {Flow and transport in porous formations},
  publisher = {Springer-Verlag},
  year = {1989},
  author = {Dagan, G.},
  pages = {470},
  address = {Berlin},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{dagan1985,
  author = {Dagan, G.},
  title = {Stochastic modelling of groundwater flow by unconditional and conditional
	probabilities: {T}he inverse problem},
  journal = {Water Resources Research},
  year = {1985},
  volume = {21},
  pages = {65-72},
  number = {1},
  abstract = {The inverse problem is defined here as follows : determine the transmissivity
	at varius points, given the shape and boundary of the aquifer and
	recharge intensity and given a set of measured log-transmissivity
	Y and head H values at a few points. The log-transmissivity distribution
	is regarded as a realization of a random function of normal and stationary
	unconditional probability density function (pdf). The solution of
	the inverse problem is the conditional normal pdf of Y, conditioned
	on measured H and Y, which is expressed in terms of the unconditional
	joint pdf of Y and H. The problem is reduced to determining the unconditional
	head-log-transmissivity covariance and head variogram for a selected
	Y covariance which depends on a few unknown parameters. This is achieved
	by solving a first-order approximation of the flow equations. The
	method is illustrated for an exponential Y covariance, and the effect
	of head and transmissivity measurements upon the reduction of uncertainty
	of Y is investigated systematically. It is shown that measurement
	of H has a lesser impact than those of Y, but a judicious combination
	may lead to significant reduction of the predicted variance of Y.
	Possible applications to real aquifers are outlined.},
  doi = {10.1029/WR021i001p00065},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@BOOK{daganneuman1997,
  title = {Subsurface flow and transport: {A} stochastic approach},
  publisher = {Cambridge University Press},
  year = {1997},
  author = {Dagan, G. and Neuman, S.},
  pages = {256},
  address = {Cambridge},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{dai2011,
  author = {Dai, A.},
  title = {Drought under global warming: a review},
  journal = {{Wiley Interdisciplinary Reviews: Climate Change}},
  year = {2011},
  volume = {2},
  pages = {45--65},
  number = {1},
  abstract = {This article reviews recent literature on drought of the last millennium,
	followed by an update on global aridity changes from 1950 to 2008.
	Projected future aridity is presented based on recent studies and
	our analysis of model simulations. Dry periods lasting for years
	to decades have occurred many times during the last millennium over,
	for example, North America, West Africa, and East Asia. These droughts
	were likely triggered by anomalous tropical sea surface temperatures
	(SSTs), with La Niña-like SST anomalies leading to drought in North
	America, and El-Niño-like SSTs causing drought in East China. Over
	Africa, the southward shift of the warmest SSTs in the Atlantic and
	warming in the Indian Ocean are responsible for the recent Sahel
	droughts. Local feedbacks may enhance and prolong drought. Global
	aridity has increased substantially since the 1970s due to recent
	drying over Africa, southern Europe, East and South Asia, and eastern
	Australia. Although El Niño-Southern Oscillation (ENSO), tropical
	Atlantic SSTs, and Asian monsoons have played a large role in the
	recent drying, recent warming has increased atmospheric moisture
	demand and likely altered atmospheric circulation patterns, both
	contributing to the drying. Climate models project increased aridity
	in the 21st century over most of Africa, southern Europe and the
	Middle East, most of the Americas, Australia, and Southeast Asia.
	Regions like the United States have avoided prolonged droughts during
	the last 50 years due to natural climate variations, but might see
	persistent droughts in the next 20–50 years. Future efforts to predict
	drought will depend on models' ability to predict tropical SSTs.},
  doi = {10.1002/wcc.81},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{dankers+al2007,
  author = {Dankers, R. and Christensen, O. and Feyen, L. and Kalas, M. and {de
	Roo}, A.},
  title = {Evaluation of very high-resolution climate model data for simulating
	flood hazards in the {U}pper {D}anube basin},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {347},
  pages = {319--331},
  number = {3--4},
  abstract = {For the purpose of assessing flood hazard in the Upper Danube Basin
	in Central Europe under current and projected future climate conditions,
	we evaluated data from a recent experiment with the regional climate
	model HIRHAM at a horizontal resolution of approximately 12 km. The
	climate simulations were used to drive the hydrological model LISFLOOD
	and the results were compared with observations of precipitation
	and river discharge in the area. To explore the benefits of using
	these very high-resolution data, we also included the results of
	two HIRHAM experiments at a lower resolution of not, vert, similar50
	km in our comparison. It was found that the 12-km data represent
	the orographic precipitation patterns and the extreme rainfall events
	over the Upper Danube Basin better than the low-resolution 50-km
	data. However, the average precipitation rates are generally higher
	than observed, while the extreme precipitation levels are mostly
	underestimated. Using the HIRHAM data as input into the LISFLOOD
	model resulted in a realistic simulation of the average discharge
	regime in the Upper Danube. In most rivers the 12-km data also led
	to a better representation of extreme discharge levels, although
	the performance was still poor in two relatively small rivers originating
	in the Alps. At larger spatial scales much of the differences and
	uncertainties between the high- and low-resolution climate data and
	the observations are averaged out, resulting in a more or less similar
	performance of the hydrological model, but at the local and sub-basin
	scale the 12-km data yield better results. The scenario simulations
	suggest that future climate changes will have an influence on the
	discharge regime and may increase the flood hazard in large parts
	of the Upper Danube Basin.},
  doi = {10.1016/j.jhydrol.2007.09.055},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{dankersfeyen2009,
  author = {Dankers, R. and Feyen, L.},
  title = {Flood hazard in {E}urope in an ensemble of regional climate scenarios},
  journal = {Journal of Geophysical Research},
  year = {2009},
  volume = {114},
  pages = {1--16},
  number = {D16108},
  abstract = {We analyze changes in flood hazard in Europe by examining extreme
	discharge levels as simulated by the hydrological model LISFLOOD
	when driven by a multimodel ensemble of climate simulations. The
	ensemble consists of simulations from two regional climate models
	(RCMs), both run with boundary conditions from two global models,
	and for two scenarios of greenhouse gas emissions. In northeastern
	Europe, a general decrease in extreme river discharge is observed
	in the scenario period, suggesting a reduction in the hazard of extreme
	snowmelt floods. Elsewhere, we find a consistent tendency toward
	a higher flood hazard in the majority of the model experiments in
	several major European rivers. These changes can partly be attributed
	to large, decadal-scale variability in the simulated climate and
	can be expected to occur naturally when comparing two 30-year time
	periods, even without a change in greenhouse gas forcing. We furthermore
	find evidence for a considerable influence of especially the global
	model that is used to drive the RCMs. At the scale of individual
	river basins, using a different combination of climate models or
	assuming a different emissions scenario sometimes results in a very
	different or even opposite climate change signal in flood hazard.
	We therefore believe that a multimodel approach as adopted in the
	present paper provides the best way to address the various uncertainties
	in impact studies of hydrometeorological extremes. Probabilistic
	scenarios that consist of multiple realizations of the current and
	future climate state are indispensable to better identify the climate
	signal amidst large variability.},
  doi = {10.1029/2008JD011523},
  owner = {rojasro},
  timestamp = {2010.08.09}
}

@ARTICLE{dankersfeyen2008,
  author = {Dankers, R. and Feyen, L.},
  title = {Climate change impact on flood hazard in {E}urope: {A}n assessment
	based on high--resolution climate simulations},
  journal = {Journal of Geophysical Research},
  year = {2008},
  volume = {113},
  pages = {D19105},
  number = {D19},
  abstract = {Global warming is generally expected to increase the magnitude and
	frequency of extreme precipitation events, which may lead to more
	intense and frequent river flooding. This work assesses the implications
	of climate change for future flood hazard in Europe. Regional climate
	simulations from the HIRHAM model with 12-km horizontal resolution
	were used to drive the hydrological model LISFLOOD, and extreme value
	techniques were applied to the results to estimate the probability
	of extreme discharges. It was found that by the end of this century
	under the Special Report on Emission Scenarios (SRES) A2 emissions
	scenario in many European rivers extreme discharge levels may increase
	in magnitude and frequency. In several rivers, most notably in the
	west and parts of eastern Europe, the return period of what is currently
	a 100-year flood may in the future decrease to 50 years or less.
	A considerable decrease in flood hazard was found in the northeast,
	where warmer winters and a shorter snow season reduce the magnitude
	of the spring snowmelt peak. Also in other rivers in central and
	southern Europe a decrease in extreme river flows was simulated.
	The results were compared with those obtained with two HIRHAM experiments
	at 50-km resolution for the SRES A2 and B2 scenarios. Disagreements
	between the various model experiments indicate that the effect of
	the horizontal resolution of the regional climate model is comparable
	in magnitude to the greenhouse gas scenario. Also, the choice of
	extreme value distribution to estimate discharge extremes influences
	the results, especially for events with higher return periods},
  doi = {10.1029/2007JD009719},
  tags = {Impacts, RCMs}
}

@ARTICLE{das+al2008,
  author = {Das, T. and Bardossy, A. and Zehe, E. and He, Y.},
  title = {Comparison of conceptual model performance using different representations
	of spatial variability},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {356},
  pages = {106--118},
  abstract = {The objective in this study is to explore a solution to the question
	whether model input data having higher spatial resolution and higher
	model resolution, as most people assume, lead to better model performance
	within a given modelling objective. An attempt was made to modify
	the conceptual rainfall-runoff model HBV to incorporate a spatially
	distributed structure. Additionally, three more model structures
	based on the HBV model concept were designed: lumped, semi-lumped
	and semi-distributed. An automatic calibration procedure based on
	simulated annealing optimization algorithm was followed for maximizing
	an objective function composed of Nash-Sutcliffe coefficients of
	several temporal aggregation steps. The predictive performance from
	each model was then assessed and compared with other model structures
	with respect to stream flow prediction at the catchment outlet. The
	spatial variation of the meteorotogical input was produced using
	external drift kriging method from available limited point measurements.
	The models were applied to a mesoscale catchment located in central
	Europe. The simulated hydrographs obtained using different model.
	structures were analyzed through comparison of their Nash-Sutcliffe
	coefficients and other goodness-of-fit indices. For the present study,
	semi-distributed and semi-lumped model structures outperformed the
	distributed and fully-lumped model structures. A possible explanation
	why the distributed model did not perform better than the simpler
	model structures is the use of limited available spatial information.
	The models use interpolated precipitation and temperature as input,
	which probably cannot reflect the true spatial variability. Another
	possible explanation is that only discharge at the catchment outlet
	was predicted; which is the purpose for which lumped and semi-distributed
	models were actually designed. (C) 2008 Elsevier B.V. All rights
	reserved.},
  doi = {10.1016/j.jhydrol.2008.04.008},
  keywords = {distributed modelling, spatial variability, rainfalt-runoff model,
	predictive uncertainty, DISTRIBUTED HYDROLOGICAL MODEL, CLIMATE-CHANGE,
	CATCHMENT MODEL, RUNOFF RESPONSE, SOIL-MOISTURE, WATER-BALANCE, RIVER-BASIN,
	IMPACT, PRECIPITATION, UNCERTAINTY},
  tags = {conceptual model}
}

@TECHREPORT{debecker1997,
  author = {{De Becker}, P. and Huybrechts, W.},
  title = {Het {W}alenbos--{E}cohydrologische {A}tlas},
  institution = {Instituut voor Natuurbehoud},
  year = {1997},
  type = {Rep. Nr IN 97/03},
  address = {Brussels, Belgium},
  owner = {RRojas},
  pages = {--76 pp},
  refid = {DEBECKER1997},
  timestamp = {2008.11.04}
}

@INPROCEEDINGS{desmedt2007,
  author = {{De Smedt}, F.},
  title = {Two-- and three--dimensional flow of groundwater},
  booktitle = {The handbook of groundwater engineering, chapter 4.},
  year = {2007},
  editor = {Delleur, J.},
  pages = {1--36},
  address = {Boca Raton, Florida},
  publisher = {CRC Press},
  chapter = {4},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.06.02}
}

@ARTICLE{detoffol2008,
  author = {{De Toffol}, S. and Engelhard, C. and Rauch, W.},
  title = {Influence of climate change on the water resources in an alpine region},
  journal = {Water Science \& Technology},
  year = {2008},
  volume = {58},
  pages = {839--846},
  number = {4},
  month = {April},
  abstract = {It is widely accepted that the global warming will impact on water
	resources. This study investigates the possible influence of climate
	change on the water resources in an alpine region. A description
	of the actual situation with emphasis on the water resources from
	the one side and on the water consuming factors, here called stressors,
	is given. The probable effects of climate change in the region and
	their influence on its water resources are then described. The main
	outcome is that in the analysed region the climate change will rather
	have positive influence on the water balance by inducing higher precipitations
	during the rivers' natural low flow period (winter). This outcome
	contradicts many common predictions, however, this due to the specifics
	induced by the alpine nature of the catchment.},
  doi = {10.2166/wst.2008.705},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{delhomme1979,
  author = {Delhomme, J.},
  title = {Spatial variability and uncertainty in groundwater flow model parameters:
	A geostatistical approach},
  journal = {Water Resources Research},
  year = {1979},
  volume = {15},
  pages = {269--280},
  number = {2},
  abstract = {A geostatistical approach is proposed for characterizing the uncertainty
	about the transmissivity field of an aquifer and analyzing its effect
	on predicted head values. A new methodology is developed, which couples
	conditional simulation and groundwater flow modeling. Conditional
	simulation is used for generating different two-dimensional transmissivity
	fields that all have the same spatial variability as the true field
	and are consistent with the measured T values at well locations.
	Two case studies are presented in order to illustrate the method,
	and conclusions are drawn for future investigation.},
  doi = {10.1029/WR015i002p00269},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@BOOK{delleur2007,
  title = {The handbook of groundwater engineering},
  publisher = {CRC Press},
  year = {2007},
  author = {Delleur, J.},
  pages = {1320},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.03.24}
}

@ARTICLE{dempster1977,
  author = {Dempster, A. and Laird, N. and Rubin, D.},
  title = {Maximum likelihood from incomplete data via the {EM} algorithm},
  journal = {Journal of the Royal Statistical Society Series B},
  year = {1977},
  volume = {39},
  pages = {1--38},
  number = {1},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates
	from incomplete data is presented at various levels of generality.
	Theory showing the monotone behaviour of the likelihood and convergence
	of the algorithm is derived. Many examples are sketched, including
	missing value situations, applications to grouped, censored or truncated
	data, finite mixture models, variance component estimation, hyperparameter
	estimation, iteratively reweighted least squares and factor analysis.},
  owner = {RRojas},
  timestamp = {2009.03.13},
  url = {http://www.jstor.org/stable/2984875}
}

@ARTICLE{dessaihulme2007,
  author = {Dessai, S. and Hulme, M.},
  title = {Assessing the robustness of adaptation decisions to climate change
	uncertainties: {A} case study on water resources management in the
	{E}ast of {E}ngland},
  journal = {Global Environmental Change},
  year = {2007},
  volume = {17},
  pages = {59--72},
  number = {1},
  abstract = {Projections of future climate change are plagued with uncertainties,
	causing difficulties for planners taking decisions on adaptation
	measures. This paper presents an assessment framework that allows
	the identification of adaptation strategies that are robust (i.e.
	insensitive) to climate change uncertainties. The framework is applied
	to a case study of water resources management in the East of England,
	more specifically to the Anglian Water Servicesâ€™ 25 year Water
	Resource Plan (WRP). The paper presents a local sensitivity analysis
	(a â€˜one-at-a-timeâ€™ experiment) of the various elements of the
	modelling framework (e.g., emissions of greenhouse gases, climate
	sensitivity and global climate models) in order to determine whether
	or not a decision to adapt to climate change is sensitive to uncertainty
	in those elements. Water resources are found to be sensitive to uncertainties
	in regional climate response (from general circulation models and
	dynamical downscaling), in climate sensitivity and in climate impacts.
	Aerosol forcing and greenhouse gas emissions uncertainties are also
	important, whereas uncertainties from ocean mixing and the carbon
	cycle are not. Despite these large uncertainties, Anglian Water Servicesâ€™
	WRP remains robust to the climate change uncertainties sampled because
	of the adaptation options being considered (e.g. extension of water
	treatment works), because the climate model used for their planning
	(HadCM3) predicts drier conditions than other models, and because
	â€˜one-at-a-timeâ€™ experiments do not sample the combination of
	different extremes in the uncertainty range of parameters. This research
	raises the question of how much certainty is required in climate
	change projections to justify investment in adaptation measures,
	and whether such certainty can be delivered.},
  doi = {10.1016/j.gloenvcha.2006.11.005},
  keywords = {Climate change, Adaptation, Uncertainty, Robustness, Sensitivity analysis,
	Water resources, East of England},
  tags = {Uncertainty}
}

@TECHREPORT{dessaihulme2003,
  author = {Dessai, S. and Hulme, M.},
  title = {Does climate policy need probabilities?},
  institution = {Tyndall Centre Working Papers},
  year = {2003},
  type = {Working Paper},
  abstract = {Estimating the likelihood of future climate change has become a topical
	matter within the research community. This is the case because of
	the advancement of science, user demand and the central role played
	by prediction in guiding policy. But are probabilities what climate
	policy really needs? This paper reviews three key questions: (1)
	why might we need probabilities of climate change? (2) what are the
	problems in estimating probabilities? (3) how are researchers estimating
	probabilities? The first question is primarily analysed within the
	context of adaptation to climate change, but mitigation and integrated
	assessment are also briefly discussed. The second question explores
	the types and sources of uncertainties involved in estimating probabilities
	of climate change and how their characterisation can be controversial.
	For the third question, an extensive review of the literature is
	conducted on research that is creating the building blocks towards
	estimating the likelihood of climate change. Overall, we conclude
	that the jury is still out on whether probabilities are useful for
	climate policy. The answer is highly context dependent and thus is
	a function of the goals and motivation of the policy analysis, the
	unit of analysis, timescale and the training of the analyst. There
	are various problems in estimating the probability of future climate
	change, but human reflexive uncertainty is largely intractable in
	the context of prediction. Nonetheless, there is considerable scope
	to develop novel methodologies that combine conditional probabilities
	with scenarios and which are relevant for climate },
  tags = {Uncertainty}
}

@ARTICLE{dessai+al2007,
  author = {Dessai, S. and {O'Brien}, K. and Hulme, M.},
  title = {Editorial: {O}n uncertainty and climate change},
  journal = {Global Environmental Change},
  year = {2007},
  volume = {17},
  pages = {1--3},
  number = {1},
  abstract = {Climate change has emerged as one of the most multi-faceted manifestations
	of global change of our time. As emphasized in the Third Assessment
	Report of the Intergovernmental Panel on Climate Change (IPCC, 2001),
	it is virtually certain that the Earth's climate is changing, with
	most of the warming over the last 50 years likely to be attributable
	to the increase in atmospheric greenhouse gas concentrations. The
	Fourth Assessment Report of the IPCC due later this year will further
	reinforce these conclusions. There is very high confidence that further
	emissions of greenhouse gases and aerosols due to human activities
	will continue to change atmospheric composition throughout the twenty-first
	century. There is, however, less confidence about exactly how the
	climate will change in the future, and lesser confidence still about
	the adjustments it will induce to natural and human systems. Given
	the wide range of uncertainties associated with future climate change,
	it is not surprising that debates within the two domains of human
	response to climate change—adaptation and mitigation—remain deeply
	contentious and irresolvable.
	
	Uncertainty, then, is pervasive in the climate change debate, but
	uncertainty is not unique to climate change. There is uncertainty
	associated with other global phenomena—the economy, geopolitics and
	health—whether in relation to economic crises, terrorism, or influenzas
	and pandemics. In fact, uncertainty is a multi-dimensional concept
	that is omnipresent in our society. A number of different uncertainty
	typologies have been proposed and used in the literature, but there
	is no agreement on the best uncertainty classification. For example,
	in the context of model-based decision support, Walker et al. (2003)
	classify uncertainties according to three dimensions: their ‘location’,
	where uncertainty manifests itself in the model complex; their ‘level’,
	where uncertainty manifests itself on the gradual spectrum between
	deterministic knowledge and total ignorance; and their ‘nature’,
	whether uncertainty primarily stems from imperfect knowledge or due
	to inherent variability. Mehta et al. (1999) discuss how ecological
	uncertainties, livelihood uncertainties and knowledge uncertainties
	are addressed by institutions, and they call for a more sophisticated
	understanding of the relationship between institutions and uncertainty.
	In terms of climate change, knowledge uncertainties have received
	the most attention, followed by ecological uncertainties and livelihood
	uncertainties. Nevertheless, all three types of uncertainties, and
	indeed many others, have implications for climate policy and for
	human security in the context of a changing climate.
	
	Some have argued that climate policy needs ‘robust’ science (Patrinos
	and Bamzai, 2005)—an argument that favours more scientific research
	over policy action. Others have argued that uncertainty should not
	be used as a justification to do nothing, instead arguing that it
	provides a reason to take specific policy action in the near term
	(Yohe et al., 2004). Between these two positions, there are a range
	of views about the implications of uncertainties for different types
	of policy responses, ranging from mitigation to adaptation (Congressional
	Budget Office, 2005; Stern, 2006). Uncertainties about climate change
	not only shape international, national and local climate policy,
	but they also influence perceptions of and responses to climate change
	at the level of individuals, communities and businesses. As Heal
	and Kriström (2002, p. 34) emphasize, “climate change involves uncertainties
	in a breathtaking number of dimensions, including, but not limited
	to, the fields of natural science and economics”.
	
	The issue of uncertainty is clearly not trivial, nor are the uncertainties
	themselves, yet little effort has been made to systematically assess
	what uncertainty means for the many dimensions of climate change
	analysis and action. A wider understanding of uncertainty must, as
	a minimum, include perspectives from psychology, ethics, decision
	sciences and law. Although uncertainty about climate change has received
	growing attention in recent years, much of this has focused on the
	description of scientific uncertainties in the climate system (Carter
	et al., 1999), and to a lesser extent in climate change impact assessments
	(Jones, 2000). The only peer-reviewed special journal issue on the
	topic of climate change uncertainty was published in 2005 in Comptes
	Rendus Geoscience (edited by Michel Petit (2005)) on “Scientific
	uncertainties and climate risk”. This special issue was the result
	of presentations made at an IPCC workshop in Ireland in 2004 on “Describing
	scientific uncertainties in climate change to support analysis of
	risk and of options”. This mainly focused on a fairly narrowly drawn
	definition of uncertainty around issues of climate science and prediction.
	What is still missing, however, is a wider examination of the many
	ways that uncertainty affects and how individuals, organisations
	and societies respond to climate change, and what this means for
	future sustainability.
	
	This special issue of Global Environmental Change brings together
	a wider range of perspectives—from geography, psychology, communication
	science and decision making—on the way uncertainty affects possible
	responses to climate change. The six research papers in this special
	issue are developed from presentations made at a session (with the
	same title) at the Sixth Open Meeting of the Human Dimensions of
	Global Environmental Change Research Community, which took place
	in Bonn from 9 to 13 October 2005. Although not a comprehensive discussion
	of all the many dimensions of uncertainty, the papers raise some
	important issues about how uncertainty relates to both adaptation
	(e.g. indicators of adaptive capacity) and mitigation (e.g. energy
	scenarios), as well as to how individuals perceive (e.g. risk and
	uncertainty communication) and respond to (e.g. decision-making frameworks)
	climate change uncertainty.
	
	We have also included two invited editorial essays on the topic of
	this special issue. The editorial by Moss (2007) makes an interesting
	observation about the asymmetry between US public concern on the
	issue of climate change (high) and public perception of scientific
	“certainties” and consensus (low). He discusses the evolution of
	the treatment of uncertainty in the IPCC and its importance in decision
	making. Moss (2007) concludes by arguing for uncertainty analyses
	that are decision focused, instead of being presented in a vacuum.
	The editorial by Ha-Duong et al. (2007) discusses the history of
	uncertainty management in IPCC assessments and examines why different
	Working Groups have used different methods to assess and communicate
	uncertainty. They argue that uncertainties related to human choice
	explain why Working Group III of the IPCC (dealing with mitigation)
	did not follow the IPCC “Guidance paper” on addressing uncertainties.
	Ha-Duong et al. (2007) make a case for a multi-dimensional approach
	to uncertainty communication in IPCC assessments.
	
	In the first research paper of the special issue, Vincent (2007) examines
	uncertainty in the concept of adaptive capacity with a particular
	focus on the scale of the analysis, comparing a national level index
	for cross-country comparison in Africa and a household index for
	cross-household comparison in a village in South Africa. She shows
	that the development of robust indicators are complicated by the
	contextual nature of adaptive capacity, by considerations of timescale,
	and by the difficulties of validation. Nevertheless, there is some
	convergence among the central elements of such indices around notions
	of institutional collective response and access to resources.
	
	In relation to mitigation, Mander et al. (2007) use a backcasting
	approach to explore the possibilities of energy pathways for meeting
	the UK Government's 60% carbon emissions reduction target by 2050.
	Their participatory scenario methodology allows for the identification
	of key uncertainties using a formal typology which differentiates
	between uncertainties due to ‘limited knowledge’ and those due to
	‘social variability’. The principal message in relation to uncertainty
	is that making the transition to a low carbon energy system is more
	securely achieved through a focus on a reduction in energy demand
	than on an increase in low carbon supply.
	
	Patt (2007) and Marx et al. (2007) examine different psychological
	dimensions of uncertainty in climate change perception and communication.
	Patt (2007) conducts an experimental study to compare the effects
	on citizens of uncertainty that is quantified using models and uncertainty
	that emerges from expert disagreement. This is an area that has been
	overlooked by the IPCC and other assessment panels working under
	a standard expected utility paradigm. Although the results of the
	experiment were inconclusive, they were consistent with the psychological
	and social models of decision making in which problem framing matters
	and in which people are not assumed to optimize on the basis of probability
	estimates. Patt (2007) concludes with some recommendations for the
	IPCC and other assessment panels.
	
	Using different case studies, Marx et al. (2007) examine how uncertainty
	about climate information can best be communicated. They focus on
	two ways in which people process information: analytically and experientially.
	They draw heavily upon two projects: one that examines the impact
	of vicarious, experiential information (as exemplified in the movie
	The Day After Tomorrow) on climate change risk perception and policy;
	and the other that looks at the comprehension of statistical information
	through experiential retranslation by Ugandan farmers’ groups. Marx
	et al. (2007) conclude that in order to best communicate climate
	risks both analytic and experiential approaches need to be considered
	through participatory decision making.
	
	The last two papers consider the place of uncertainty in decision
	making about adaptation to climate change. Dessai and Hulme (2007)
	and Groves and Lempert (2007) develop analytic frameworks to assess
	robust adaptation strategies under deep uncertainties. Dessai and
	Hulme (2007) assess the robustness of an adaptation strategy—a company's
	Water Resource Plan in the East of England—to a number of climate
	change uncertainties. They find that adaptation decisions about future
	water resource investments are sensitive to some uncertainties, but
	that the water company plan remains robust overall because of the
	sampling used, the adaptation options considered, and the climate
	model used in planning. This reveals a mixture of both planned and
	serendipitous factors in designing robust adaptation. Groves and
	Lempert (2007) apply a robust decision-making approach (Lempert et
	al., 2003 R.J. Lempert, S.W. Popper and S.C. Bankes, Shaping the
	Next One Hundred Years: New Methods for Quantitative, Long-Term Policy
	Analysis, RAND, Santa Monica, CA (2003).Lempert et al., 2003) to
	identify policy relevant scenarios in the context of water resources
	planning in California. The resulting scenarios can communicate quantitative
	judgements about uncertainty, as well as support a well-defined decision
	process without many of the drawbacks of approaches which take a
	less structured view of uncertainty.
	
	Climate is changing in response to human perturbations of the atmosphere
	and future climate is uncertain—both because our scientific understanding
	of the workings of the climate system remains incomplete and because
	the evolution of future climate is (partly) contingent upon human
	actions. This uncertainty about the course of future climate—on all
	scales—and uncertainty about how societies will respond to this change
	in climate presents new challenges for the way individuals, organizations
	and societies make decisions. It also makes it difficult to design
	effective, equitable and efficient policies which will achieve the
	goals of the UN Framework Convention on Climate Change. It is our
	hope that, as a collection, these papers will inspire a wider debate
	about what uncertainty means for both mitigation and adaptation responses
	to climate change.
	
	We would like to thank the contributors and reviewers of the papers
	of this special issue. Neil Jennings is thanked for all his help
	in putting together this special issue.},
  doi = {10.1016/j.gloenvcha.2006.12.001},
  tags = {Uncertainty}
}

@BOOK{deutsh2002,
  title = {Encyclopedia of physical science and technologies},
  publisher = {Academic Press},
  year = {2002},
  author = {Deutsch, C.},
  volume = {6},
  pages = {697--707},
  edition = {Third},
  owner = {RRojas},
  timestamp = {2009.03.27}
}

@BOOK{deutsch1998,
  title = {G{SLIB}: {G}eostatistical {S}oftware {L}ibrary and {U}ser's {G}uide},
  publisher = {Oxford University Press},
  year = {1998},
  author = {Deutsch, C. and Journel, A.},
  pages = {384},
  address = {New York},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{DGA1996,
  author = {DGA},
  title = {Determinaci\'on de la disponibilidad de recursos h\'idricos para
	constituir nuevos derechos de aprovechamiento de aguas subterr\'aneas
	en el sector del acu\'ifero de la {Pampa del Tamarugal}},
  institution = {{Direcci\'on General de Aguas}},
  year = {1996},
  type = {{S.D.T. Nr. 68}, 27 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@TECHREPORT{DGAUchile1988,
  author = {DGA},
  title = {Modelo de simulaci\'on hidrogeol\'ogico de la {Pampa} del {Tamarugal}},
  institution = {{Direcci\'on General de Aguas}},
  year = {1988},
  type = {98 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@TECHREPORT{JICA1995,
  author = {JICA-DGA-PCI},
  title = {The study on the develpment of water resources in northern {C}hile.
	{S}upporting report {B}: {G}eology and groundwater},
  institution = {{Japanese International Cooperation Agency, Direcci\'on General de
	Aguas, and Pacific Consultants International}},
  year = {1995},
  type = {216 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{diluzio+al2004,
  author = {Di Luzio, M. and Srinivasan, R. and Arnold, J.},
  title = {A {GIS}-Coupled Hydrological Model System for the Watershed Assessment
	of Agricultural Nonpoint and Point Sources of Pollution},
  journal = {Transactions in GIS},
  year = {2004},
  volume = {8},
  pages = {113-136},
  number = {1},
  abstract = {This paper introduces AVSWAT, a GIS based hydrological system linking
	the Soil and Water Assessment Tool (SWAT) water quality model and
	ArcView" Geographic Information System software. The main purpose
	of AVSWAT is the combined assess-ment of nonpoint and point pollution
	loading at the watershed scale. The GIS component of the system,
	in addition to the traditional functions of data acquisition, storage,
	organization and display, implements advanced analytical methods
	with enhanced flexibility to improve the hydrological characterization
	of a study watershed. Intuitive user friendly graphic interfaces,
	also part of the GIS component, have been developed to provide an
	efficient interaction with the model and the associated parameter
	databases, and ultimately to simplify water quality assessments,
	while maintaining and increasing their reliability. This is also
	supported by SWAT, the core of the system, a complex, conceptual,
	hydrologic, continuous model with spatially e xplicit parameterization,
	building upon the United State Department of Agriculture (USDA) modeling
	experience. A step-by-step example application for a watershed in
	Central Texas is also included to verify the capability and illustrate
	some of the characteristics of the system which has been adopted
	by many users around the world},
  doi = {10.1111/j.1467-9671.2004.00170.x},
  keywords = {AVSWAT-X, SWAT},
  tags = {SWAT}
}

@ARTICLE{diaz-nietowilby2005,
  author = {{Diaz-Nieto}, J. and Wilby, R.},
  title = {{A comparison of statistical downscaling and climate change factor
	methods: impacts on low flows in the River Thames, United Kingdom}},
  journal = {Climatic Change},
  year = {2005},
  volume = {69},
  pages = {245--268},
  number = {2--3},
  abstract = {Strategic-scale assessments of climate change impacts are often undertaken
	using the change factor (CF) methodology whereby future changes in
	climate projected by General Circulation Models (GCMs) are applied
	to a baseline climatology. Alternatively, statistical downscaling
	(SD) methods apply climate variables from GCMs to statistical transfer
	functions to estimate point-scale meteorological series. This paper
	explores the relative merits of the CF and SD methods using a case
	study of low flows in the River Thames under baseline (1961–1990)
	and climate change conditions (centred on the 2020s, 2050s and 2080s).
	Archived model outputs for the UK Climate Impacts Programme (UKCIP02)
	scenarios are used to generate daily precipitation and potential
	evaporation (PE) for two climate change scenarios via the CF and
	SD methods. Both signal substantial reductions in summer precipitation
	accompanied by increased PE throughout the year, leading to reduced
	flows in the Thames in late summer and autumn. However, changes in
	flow associated with the SD scenarios are generally more conservative
	and complex than that arising from CFs. These departures are explained
	in terms of the different treatment of multidecadal natural variability,
	temporal structuring of daily climate variables and large-scale forcing
	of local precipitation and PE by the two downscaling methods.},
  doi = {10.1007/s10584-005-1157-6},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{dibikecoulibaly2005,
  author = {Dibike, Y. and Coulibaly, P.},
  title = {Hydrologic impact of climate change in the {S}aguenay watershed:
	comparison of downscaling methods and hydrologic models},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {307},
  pages = {145--163},
  number = {1--4},
  abstract = {Changes in global climate will have significant impact on local and
	regional hydrological regimes, which will in turn affect ecological,
	social and economical systems. However, climate-change impact studies
	on hydrologic regime have been relatively rare until recently, mainly
	because Global Circulation Models, which are widely used to simulate
	future climate scenarios, do not provide hourly or daily rainfall
	reliable enough for hydrological modeling. Nevertheless, more reliable
	rainfall series corresponding to future climate scenarios can be
	derived from GCM outputs using the so called â€˜downscaling techniquesâ€™.
	This study applies two types of statistical (a stochastic and a regression
	based) downscaling techniques to generate the possible future values
	of local meteorological variables such as precipitation and temperature
	in the Chute-du-Diable sub-basin of the Saguenay watershed in northern
	Qu{\'e}bec, Canada. The downscaled data is used as input to two different
	hydrologic models to simulate the corresponding future flow regime
	in the catchment. In addition to assessing the relative potential
	of the downscaling methods, the paper also provides comparative study
	results of the possible impact of climate change on river flow and
	total reservoir inflow in the Chute-du-Diable basin. Although the
	two downscaling techniques do not provide identical results, the
	time series generated by both methods indicates a general increasing
	trend in the mean daily temperature values. While the regression
	based downscaling technique resulted in an increasing trend in the
	mean and variability of daily precipitation values, such a trend
	is not obvious in the case of precipitation time series downscaled
	with the stochastic weather generator. Moreover, the hydrologic impact
	analysis made with the downscaled precipitation and temperature time
	series as input to the two hydrological models suggest an overall
	increasing trend in mean annual river flow and reservoir inflow as
	well as earlier spring peak flows in the basin},
  doi = {10.1016/j.jhydrol.2004.10.012},
  tags = {Impacts, Downscaling}
}

@TECHREPORT{DICTUC2008,
  author = {DICTUC},
  title = {E{IA Proyecto Pampa Hermosa. Anexo VIII.2 Hidrogeolog\'ia}},
  institution = {{Direcci\'on de Investigaciones Cient\'ificas y Tecnol\'ogicas Universidad
	Cat\'olica de Chile}},
  year = {2008},
  type = {169 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07},
  url = {https://www.e-seia.cl/expediente/expedientes.php?modo=ficha&id_expediente=3083858&idExpediente=3083858}
}

@TECHREPORT{DICTUC2007,
  author = {DICTUC},
  title = {E{IA Proyecto Pampa Hermosa. Anexo IX.1 Geolog\'ia del acu\'ifero
	de la Pampa del Tamarugal}},
  institution = {{Direcci\'on de Investigaciones Cient\'ificas y Tecnol\'ogicas Universidad
	Cat\'olica de Chile}},
  year = {2007},
  type = {99 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07},
  url = {https://www.e-seia.cl/expediente/expedientes.php?modo=ficha&id_expediente=3083858&idExpediente=3083858}
}

@TECHREPORT{DICTUC2005,
  author = {DICTUC},
  title = {An\'alisis t\'ecnico de solicitud de derechos de aprovechamiento
	de {ACF Minera. Acu\'ifero Pampa del Tamarugal}},
  institution = {{Direcci\'on de Investigaciones Cient\'ificas y Tecnol\'ogicas Universidad
	Cat\'olica de Chile}},
  year = {2005},
  type = {107 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@TECHREPORT{DICTUC1995,
  author = {DICTUC},
  title = {Evaluaci\'on del impacto del proyecto de extracci\'on de aguas subterr\'aneas
	en la zona sur de la {Pampa del Tamarugal}},
  institution = {{Direcci\'on de Investigaciones Cient\'ificas y Tecnol\'ogicas Universidad
	Cat\'olica de Chile}},
  year = {1995},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@CONFERENCE{digert2003,
  author = {Digert, F. and Hoke, G. and Jordan, T. and Isaaks, B.},
  title = {Subsurface stratigraphy of the neogene {P}ampa del {T}amarugal basin,
	northern {C}hile},
  booktitle = {X Congreso Geol\'ogico Chileno},
  year = {2003},
  address = {Concepci\'on, Chile},
  owner = {RRojas},
  timestamp = {2009.04.08}
}

@ARTICLE{diks2010,
  author = {Diks, C. and Vrugt, J.},
  title = {Comaprison of point forecast accuracy of model averaging methods
	in hydrologic applications},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2010},
  volume = {24},
  pages = {809--820},
  number = {6},
  abstract = {Multi-model averaging is currently receiving a surge of attention
	in the atmospheric, hydrologic, and statistical literature to explicitly
	handle conceptual model uncertainty in the analysis of environmental
	systems and derive predictive distributions of model output. Such
	density forecasts are necessary to help analyze which parts of the
	model are well resolved, and which parts are subject to considerable
	uncertainty. Yet, accurate point predictors are still desired in
	many practical applications. In this paper, we compare a suite of
	different model averaging techniques by their ability to improve
	forecast accuracy of environmental systems. We compare equal weights
	averaging (EWA), Bates-Granger model averaging (BGA), averaging using
	Akaike’s information criterion (AICA), and Bayes’ Information Criterion
	(BICA), Bayesian model averaging (BMA), Mallows model averaging (MMA),
	and Granger-Ramanathan averaging (GRA) for two different hydrologic
	systems involving water flow through a 1950 km2 watershed and 5 m
	deep vadose zone. Averaging methods with weights restricted to the
	multi-dimensional simplex (positive weights summing up to one) are
	shown to have considerably larger forecast errors than approaches
	with unconstrained weights. Whereas various sophisticated model averaging
	approaches have recently emerged in the literature, our results convincingly
	demonstrate the advantages of GRA for hydrologic applications. This
	method achieves similar performance as MMA and BMA, but is much simpler
	to implement and use, and computationally much less demanding.},
  doi = {10.1007/s00477-010-0378-z},
  owner = {rojasro},
  timestamp = {2012.10.23}
}

@TECHREPORT{dingman1965,
  author = {Dingman, R. and Galli, C.},
  title = {Geology and groundwater resources of the {P}ica area, {T}arapaca
	{P}rovince, {C}hile},
  institution = {U.S. Geological Survey},
  year = {1965},
  address = {Virginia, USA.},
  owner = {RRojas},
  timestamp = {2009.04.08}
}

@MANUAL{pest2010,
  title = {P{EST}: {M}odel-independent parameter estimation. {U}ser manual:
	5th edition},
  author = {Doherty, J.},
  organization = {Watermark {N}umerical {C}omputing},
  edition = {Fifth},
  year = {2010},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{doherty2003,
  author = {Doherty, J.},
  title = {Ground water model calibration using pilot points and regularization},
  journal = {Ground Water},
  year = {2003},
  volume = {41},
  pages = {170--177},
  number = {2},
  abstract = {Use of nonlinear parameter estimation techniques is now commonplace
	in ground water model calibration. However, there is still ample
	room for further development of these techniques in order to enable
	them to extract more information from calibration datasets, to more
	thoroughly explore the uncertainty associated with model predictions,
	and to make them easier to implement in various modeling contexts.
	This paper describes the use of "pilot points" as a methodology for
	spatial hydraulic property characterization. When used in conjunction
	with nonlinear parameter estimation software that incorporates advanced
	regularization functionality (such as PEST), use of pilot points
	can add a great deal of flexibility to the calibration process at
	the same time as it makes this process easier to implement. Pilot
	points can be used either as a substitute for zones of piecewise
	parameter uniformity, or in conjunction with such zones. In either
	case, they allow the disposition of areas of high and low hydraulic
	property value to be inferred through the calibration process, without
	the need for the modeler to guess the geometry of such areas prior
	to estimating the parameters that pertain to them. Pilot points and
	regularization can also be used as an adjunct to geostatistically
	based stochastic parameterization methods. Using the techniques described
	herein, a series of hydraulic property fields can be generated, all
	of which recognize the stochastic characterization of an area at
	the same time that they satisfy the constraints imposed on hydraulic
	property values by the need to ensure that model outputs match field
	measurements. Model predictions can then be made using all of these
	fields as a mechanism for exploring predictive uncertainty.},
  doi = {10.1111/j.1745-6584.2003.tb02580.x},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{doherty2010,
  author = {Doherty, J. and Welter, D.},
  title = {A short exploration of structural noise},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {1--14},
  number = {W05525},
  abstract = {“Structural noise” is a term often used to describe model?to?measurement
	misfit that cannot be ascribed to measurement noise and therefore
	must be ascribed to the imperfect nature of a numerical model as
	a simulator of reality. As such, it is often the dominant contributor
	to model?to?measurement misfit. As the name “structural noise” implies,
	this type of misfit is often treated as an additive term to measurement
	noise when assessing model parameter and predictive uncertainty.
	This paper inquires into the nature of defect?induced model?to?measurement
	misfit and provides a conceptual basis for accommodating it. It is
	shown that inasmuch as defect?induced model?to?measurement misfit
	can be characterized as “noise,” this noise is likely to show a high
	degree of spatial and temporal correlation; furthermore, its covariance
	matrix may approach singularity. However, the deleterious impact
	of structural noise on the model calibration process may be mitigated
	in a variety of ways. These include adoption of a highly parameterized
	approach to model construction and calibration (including the strategic
	use of compensatory parameters where appropriate), processing of
	observations and their model?generated counterparts in ways that
	are able to filter out structural noise prior to fitting one to the
	other, and/or through implementation of a weighting strategy that
	gives prominence to observations that most resemble predictions required
	of a model.},
  doi = {10.1029/2009WR008377},
  owner = {rojasro},
  timestamp = {2010.08.10}
}

@BOOK{dorigo2004,
  title = {Ant colony optimization},
  publisher = {MIT Press},
  year = {2004},
  author = {Dorigo, M. and Stutzle, T.},
  address = {Cambridge (MA)},
  owner = {rojasro},
  timestamp = {2011.10.11}
}

@ARTICLE{dorner+al2008,
  author = {Dorner, W. and Porter, M. and Metzka, R.},
  title = {Are floods in part a form of land use externality?},
  journal = {Natural Hazards and Earth System Science},
  year = {2008},
  volume = {8},
  pages = {523--532},
  number = {3},
  doi = {10.5194/nhess-8-523-2008}
}

@ARTICLE{dosio2011,
  author = {Dosio, A. and Paruolo, P.},
  title = {{Bias correction of the ENSEMBLES high resolution climate change
	projections for use by impact models: evaluation on the present cimate}},
  journal = {Journal of Geophysical Research},
  year = {2011},
  volume = {116},
  pages = {1--22},
  number = {D16106},
  abstract = {A statistical bias correction technique is applied to a set of high
	resolution climate change simulations for Europe from state-of-the-art
	regional climate models (RCMs) from the project ENSEMBLES. Modelled
	and observed daily values of mean, minimum and maximum temperature
	and total precipitation are used to construct transfer functions
	for the period 1961-1990, which are then applied to the decade 1991-2000,
	where the results are evaluated. By using a large ensembles of model
	runs and a long construction period, we take into account both inter-model
	variability, and longer (e.g. decadal) natural climate variability.
	Results show that the technique performs successfully for all variables
	over large part of the European continent, for all seasons. In particular,
	the probability distribution functions (PDFs) of both temperature
	and precipitation are greatly improved, especially in the tails,
	i.e., increasing the capability of reproducing extreme events. When
	the statistics of bias corrected results are ensemble-averaged, the
	result is very close to the observed ones. The bias correction technique
	is also able to improve statistics that depend strongly on the temporal
	sequence of the original field, such as the number of consecutive
	dry days and the total amount of precipitation in consecutive heavy
	precipitation episodes, which are quantities that may have a large
	influence on e.g. hydrological or crop impact models. Bias-corrected
	projections of RCMs are hence found to be potentially useful for
	the assessment of impacts of climate change over Europe.},
  doi = {10.1029/2011JD015934},
  owner = {rojasro},
  timestamp = {2011.04.29}
}

@ARTICLE{dosio+al2012,
  author = {Dosio, A. and Paruolo, P. and Rojas, R.},
  title = {{Bias correction of the ENSEMBLES high resolution climate change
	projections for use by impact models: analysis of the climate change
	signal}},
  journal = {Journal of Geophysical Research},
  year = {2012},
  volume = {in press},
  doi = {10.1029/2012JD017968},
  owner = {rojasro},
  timestamp = {2012.07.30}
}

@ARTICLE{DOV2008,
  author = {DOV},
  title = {Databank {O}ndergrond {V}laanderen},
  year = {2008},
  owner = {RRojas},
  timestamp = {2008.11.18},
  url = {http://dov.vlaanderen.be/dovweb/html/engels.html}
}

@ARTICLE{draper1995,
  author = {Draper, D.},
  title = {Assessment and propagation of model uncertainty},
  journal = {Journal of the Royal Statistical Society Series B},
  year = {1995},
  volume = {57},
  pages = {45--97},
  number = {1},
  abstract = {In most examples of inference and prediction, the expression of uncertainty
	about unknown quantities y on the basis of known quantities x is
	based on a model M that formalizes assumptions about how x and y
	are related. M will typically have two parts: structural assumptions
	S, such as the form of the link function and the choice of error
	distribution in a generalized linear model, and parameters 0 whose
	meaning is specific to a given choice of S. It is common in statistical
	theory and practice to acknowledge parametric uncertainty about 0
	given a particular assumed structure S; it is less common to acknowledge
	structural uncertainty about S itself. A widely used approach involves
	enlisting the aid of x to specify a plausible single 'best' choice
	S* for S, and then proceeding as if S* were known to be correct.
	In general this approach fails to assess and propagate structural
	uncertainty fully and may lead to miscalibrated uncertainty assessments
	about y given x. When miscalibration occurs it will often result
	in understatement of inferential or predictive uncertainty about
	y, leading to inaccurate scientific summaries and overconfident decisions
	that do not incorporate sufficient hedging against uncertainty. In
	this paper I discuss a Bayesian approach to solving this problem
	that has long been available in principle but is only now becoming
	routinely feasible, by virtue of recent computational advances, and
	examine its im- plementation in examples that involve forecasting
	the price of oil and estimating the chance of catastrophic failure
	of the US space shuttle.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2346087}
}

@ARTICLE{driessen+al2010,
  author = {Driessen, T. and Hurkmans, R. and Terink, W. and Hazenberg, P. and
	Torfs, P. and Uijlenhoet, R.},
  title = {The hydrological response of the {O}urthe catchment to climate change
	as modelled by the {HBV} model},
  journal = {Hydrology and Earth System Sciences},
  year = {2010},
  volume = {14},
  pages = {651--665},
  number = {4},
  abstract = {The Meuse is an important river in Western Europe, which is almost
	exclusively rain-fed. Projected changes in precipitation characteristics
	due to climate change, therefore, are expected to have a considerable
	effect on the hydrological regime of the river Meuse. We focus on
	an important tributary of the Meuse, the Ourthe, measuring about
	1600 km2. The well-known hydrological model HBV is forced with three
	high-resolution (0.088°) regional climate scenarios, each based on
	one of three different IPCC CO2 emission scenarios: A1B, A2 and B1.
	To represent the current climate, a reference model run at the same
	resolution is used. Prior to running the hydrological model, the
	biases in the climate model output are investigated and corrected
	for. Different approaches to correct the distributed climate model
	output using single-site observations are compared. Correcting the
	spatially averaged temperature and precipitation is found to give
	the best results, but still large differences exist between observations
	and simulations. The bias corrected data are then used to force HBV.
	Results indicate a small increase in overall discharge, especially
	for the B1 scenario during the beginning of the 21st century. Towards
	the end of the century, all scenarios show a decrease in summer discharge,
	partially because of the diminished buffering effect by the snow
	pack, and an increased discharge in winter. It should be stressed,
	however, that we used results from only one GCM (the only one available
	at such a high resolution). It would be interesting to repeat the
	analysis with multiple models.},
  doi = {10.5194/hess-14-651-2010},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{drogue+al2004,
  author = {Drogue, G. and Pfister, L. and Leviandier, T. and {El Idrissi}, A.
	and Iffly, {J.-F.} and Matgen, P. and Humbert, J. and Hoffmann, L.},
  title = {Simulating the spatio-temporal variability of streamflow response
	to climate change scenarios in a mesoscale basin},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {293},
  pages = {255--269},
  number = {1--4},
  abstract = {A continuous rainfall-runoff simulation was performed to assess the
	potential effect of climate changes on the streamflow regimes and
	water resources of tributaries of the Alzette river basin extending
	over 1176 km(2), mainly in the Grand Duchy of Luxembourg. characterized
	by various hydrological patterns. Global climate change scenarios
	for the 2050 horizon, based on GCM projections from the KNMI and
	UKHI synoptic runs, were disaggregated into mesoscale daily PET and
	rainfall series. Seasonal expected PET changes were proportionally
	applied to present daily values, whereas future hyetographs were
	empirically constructed according to observed trends in rainfall
	time series for the study area. The various ways of applying the
	mesoscale rainfall scenarios exert a significant influence on the
	magnitude and spatial distribution of streamflow responses. The comparison
	of future and present hydrographs also shows that the impact of mesoscale
	climate change is extremely variable with regard to the considered
	hydrological variable. The spatial variability of streamflow responses
	is largely conditioned by climatic and physiographical characteristics
	of the sub-basins. The winter period is most affected by altered
	climate conditions and some sub-re ions appear to be particularly
	sensitive in terms of changes in low or high flows. (C) 2004 Elsevier
	B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2004.02.009},
  keywords = {climate change, mesoscale variability, rainfall scenario, streamflow
	modeling, Grand Duchy of Luxembourg, LAND-USE, RUNOFF, PRECIPITATION},
  tags = {Impacts}
}

@TECHREPORT{DSM2002,
  author = {{DSM}},
  title = {E{staci\'on Cerro Gordo. Modelo de flujo num\'erico de aguas subterr\'aneas}},
  institution = {{DSM Minera S.A.}},
  year = {2002},
  type = {70 pp.},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{duan2007,
  author = {Duan, Q. and Ajami, N. and Gao, X. and Sorooshian, S.},
  title = {Multi--model ensemble hydrologic prediction using {B}ayesian model
	averaging},
  journal = {Advances in Water Resources},
  year = {2007},
  volume = {30},
  pages = {1371--1386},
  number = {5},
  abstract = {Multi-model ensemble strategy is a means to exploit the diversity
	of skillful predictions from different models. This paper studies
	the use of Bayesian model averaging (BMA) scheme to develop more
	skillful and reliable probabilistic hydrologic predictions from multiple
	competing predictions made by several hydrologic models. BMA is a
	statistical procedure that infers consensus predictions by weighing
	individual predictions based on their probabilistic likelihood measures,
	with the better performing predictions receiving higher weights than
	the worse performing ones. Furthermore, BMA provides a more reliable
	description of the total predictive uncertainty than the original
	ensemble, leading to a sharper and better calibrated probability
	density function (PDF) for the probabilistic predictions. In this
	study, a nine-member ensemble of hydrologic predictions was used
	to test and evaluate the BMA scheme. This ensemble was generated
	by calibrating three different hydrologic models using three distinct
	objective functions. These objective functions were chosen in a way
	that forces the models to capture certain aspects of the hydrograph
	well (e.g., peaks, mid-flows and low flows). Two sets of numerical
	experiments were carried out on three test basins in the US to explore
	the best way of using the BMA scheme. In the first set, a single
	set of BMA weights was computed to obtain BMA predictions, while
	the second set employed multiple sets of weights, with distinct sets
	corresponding to different flow intervals. In both sets, the streamflow
	values were transformed using Box–Cox transformation to ensure that
	the probability distribution of the prediction errors is approximately
	Gaussian. A split sample approach was used to obtain and validate
	the BMA predictions. The test results showed that BMA scheme has
	the advantage of generating more skillful and equally reliable probabilistic
	predictions than original ensemble. The performance of the expected
	BMA predictions in terms of daily root mean square error (DRMS) and
	daily absolute mean error (DABS) is generally superior to that of
	the best individual predictions. Furthermore, the BMA predictions
	employing multiple sets of weights are generally better than those
	using single set of weights.},
  doi = {10.1016/j.advwatres.2006.11.014},
  owner = {RRojas},
  timestamp = {2008.12.08}
}

@ARTICLE{duan+al1992,
  author = {Duan, Q. and Sorooshian, S. and Gupta, H.},
  title = {Effective and efficient global optimization for conceptual ranfall--runoff
	models},
  journal = {Water Resources Research},
  year = {1992},
  volume = {28},
  pages = {1015--1031},
  number = {4},
  abstract = {The successful application of a conceptual rainfall-runoff (CRR) model
	depends on how well it is calibrated. Despite the popularity of CRR
	models, reports in the literature indicate that it is typically difficult,
	if not impossible, to obtain unique optimal values for their parameters
	using automatic calibration methods. Unless the best set of parameters
	associated with a given calibration data set can be found, it is
	difficult to determine how sensitive the parameter estimates (and
	hence the model forecasts) are to factors such as input and output
	data error, model error, quantity and quality of data, objective
	function used, and so on. Results are presented that establish clearly
	the nature of the multiple optima problem for the research CRR model
	SIXPAR. These results suggest that the CRR model optimization problem
	is more difficult than had been previously thought and that currently
	used local search procedures have a very low probability of successfully
	finding the optimal parameter sets. Next, the performance of three
	existing global search procedures are evaluated on the model SIXPAR.
	Finally, a powerful new global optimization procedure is presented,
	entitled the shuffled complex evolution (SCE-UA) method, which was
	able to consistently locate the global optimum of the SIXPAR model,
	and appears to be capable of efficiently and effectively solving
	the CRR model optimization problem.},
  doi = {10.1029/91WR02985},
  owner = {rojasro},
  timestamp = {2011.06.21}
}

@ARTICLE{duan+al1994,
  author = {Duan, Q. and Sorooshian, S. and Gupta, V.},
  title = {Optimal Use Of The {SCE-UA} Global Optimization Method For Calibrating
	Watershed Models},
  journal = {Journal of Hydrology},
  year = {1994},
  volume = {158},
  pages = {265--284},
  note = {Gives recommended values for the use of the SCE-UA algorithm:},
  abstract = {The difficulties involved in calibrating conceptual watershed models
	have, in the past, been partly attributable to the lack of robust
	optimization tools. Recently, a global optimization method known
	as the SCE-UA (shuffled complex evolution method developed at The
	University of Arizona) has shown promise as an effective and efficient
	optimization technique for calibrating watershed models. Experience
	with the method has indicated that the effectiveness and efficiency
	of the algorithm are influenced by the choice of the algorithmic
	parameters. This paper first reviews the essential concepts of the
	SCE-UA method and then presents the results of several experimental
	studies in which the National Weather Service river forecast system-soil
	moisture accounting (NWSRFS-SMA) model, used by the National Weather
	Service for river and flood forecasting, was calibrated using different
	algorithmic parameter setups. On the basis of these results, the
	recommended values for the algorithmic parameters are given. These
	values should also help to provide guidelines for other users of
	the SCE-UA method.},
  doi = {10.1016/0022-1694(94)90057-4},
  keywords = {RAINFALL-RUNOFF MODELS, IMPROVED PARAMETER INFERENCE, CATCHMENT MODELS,
	SEARCH, ERRORS},
  tags = {Calibration}
}

@ARTICLE{dunkley2005,
  author = {Dunkley, J. and Bucher, M. and Ferreira, P. and Moodley, K. and Skordis,
	C.},
  title = {Fast and reliabe {MCMC} for cosmological parameter estimation},
  journal = {Monthly Notices of the Royal Astronomical Society},
  year = {2005},
  volume = {356},
  pages = {925--936},
  abstract = {Markov Chain Monte Carlo (MCMC) techniques are now widely used for
	cosmological parameter estimation. Chains are generated to sample
	the posterior probability distribution obtained following the Bayesian
	approach. An important issue is how to optimize the efficiency of
	such sampling and how to diagnose whether a finite-length chain has
	adequately sampled the underlying posterior probability distribution.
	We show how the power spectrum of a single such finite chain may
	be used as a convergence diagnostic by means of a fitting function,
	and discuss strategies for optimizing the distribution for the proposed
	steps. The methods developed are applied to current CMB and LSS data
	interpreted using both a pure adiabatic cosmological model and a
	mixed adiabatic/isocurvature cosmological model including possible
	correlations between modes. For the latter application, because of
	the increased dimensionality and the presence of degeneracies, the
	need for tuning MCMC methods for maximum efficiency becomes particularly
	acute.},
  owner = {RRojas},
  timestamp = {2008.07.16},
  url = {http://www.citebase.org/abstract?id=oai:arXiv.org:astro-ph/0405462}
}

@ARTICLE{durman+al2001,
  author = {Durman, C. and Gregory, J. and Kassell, D. and Jones, R. and Murphy,
	J.},
  title = {A comparison of extreme {E}uropean daily precipitation simulated
	by a global and a regional climate model for present and future climates},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  year = {2001},
  volume = {127},
  pages = {1005--1015},
  number = {573},
  abstract = {The intensity and distribution of daily precipitation is predicted
	to change under scenarios of increased greenhouse gases (GHGs). In
	this paper, we analyse the ability of HadCM2, a general circulation
	model (GCM), and a high-resolution regional climate model (RCM),
	both developed at the Met Office's Hadley Centre, to simulate extreme
	daily precipitation by reference to observations. A detailed analysis
	of daily precipitation is made at two UK grid boxes, where probabilities
	of reaching daily thresholds in the GCM and RCM are compared with
	observations. We find that the RCM generally overpredicts probabilities
	of extreme daily precipitation but that, when the GCM and RCM simulated
	values are scaled to have the same mean as the observations, the
	RCM captures the upper-tail distribution more realistically. To compare
	regional changes in daily precipitation in the GHG-forced period
	2080-2100 in the GCM and the RCM, we develop two methods. The first
	considers the fractional changes in probability of local daily precipitation
	reaching or exceeding a fixed 15 mm threshold in the anomaly climate
	compared with the control. The second method uses the upper one-percentile
	of the control at each point as the threshold. Agreement between
	the models is better in both seasons with the latter method, which
	we suggest may be more useful when considering larger scale spatial
	changes. On average, the probability of precipitation exceeding the
	1% threshold increases by a factor of 2.5 (GCM and RCM) in winter
	and by 1.7 (GCM) or 1.3 (RCM) in summer.},
  doi = {10.1002/qj.49712757316},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{dvorak+al1997,
  author = {Dvorak, V. and Hladny, J. and Kasparek, L.},
  title = {Climate change impacts and water resources impact and adaptation
	for selected river basins in the {Czech Republic}},
  journal = {Climatic Change},
  year = {1997},
  volume = {36},
  pages = {93--106},
  number = {1--2},
  abstract = {The Czech Republic has a northern hemisphere Atlantic-continental
	type of moderate climate. Mean annual temperature ranges between
	1.0 and 9.4 °C (between 8.8 and 18.5 °C in summer and between –6.8
	and 0.2 °C in winter). Annual precipitation ranges between 450 mm
	in dry regions and 1300 mm in mountainous regions of the country.
	With its 2000 m3 per capita fresh water availability, the Czech Republic
	is slightly below average. Occasional water shortages do not usually
	result from general unavailability of water resources but rather
	from time or space variability of water supply/demand and high degree
	of water resources exploitation. To study potential impacts of climate
	change on hydrological system and water resources, four river basins
	have been selected in the territory of the Czech Republic: the Elbe
	River at Decin (50761.7 km2), the Zelivka River at Soutice (1188.6
	km2), the Upa River at Ceska Skalice (460.7 km2) and the Metuje River
	at Marsov n. M. (93.9 km2). To simulate potential changes in runoff,
	three hydrological models have been applied using incremental and
	GCM (GISS, GFDL and CCCM) scenarios: the BILAN water balance model,
	the SACRAMENTO (SAC-SMA) conceptual model and the CLIRUN water balance
	model. The paper reviews methods applied in the study, results of
	the assessments and concludes with suggestions for possible general
	adaptation policy options where the preference is for nonstructural
	measures such as water conservation, efficient water demand management
	and protection of water resources.},
  doi = {10.1023/A:1005384120954},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{dynesius1994,
  author = {Dynesius, M. and Nilsson, C.},
  title = {Fragmentation and flow regulation of river systems in the northern
	third of the world},
  journal = {Science},
  year = {1994},
  volume = {266},
  pages = {753--762},
  number = {5186},
  abstract = {Seventy-seven percent of the total water discharge of the 139 largest
	river systems in North America north of Mexico, in Europe, and in
	the republics of the former Soviet Union is strongly or moderately
	affected by fragmentation of the river channels by dams and by water
	regulation resulting from reservoir operation, interbasin diversion,
	and irrigation. The remaining free-flowing large river systems are
	relatively small and nearly all situated in the far north, as are
	the 59 medium-sized river systems of Norway, Sweden, Finland, and
	Denmark. These conditions indicate that many types of river ecosystems
	have been lost and that the populations of many riverine species
	have become highly fragmented. To improve the conservation of biodiversity
	and the sustainable use of biological resources, immediate action
	is called for to create an international preservation network of
	free-flowing river systems and to rehabilitate exploited rivers in
	areas that lack unaffected watercourses.},
  doi = {10.1126/science.266.5186.753},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@INPROCEEDINGS{eberhartkennedy1995,
  author = {Eberhart, R. and Kennedy, J.},
  title = {A new optimizer using particle swarm theory},
  booktitle = {Micro Machine and Human Science, 1995. MHS {'}95., Proceedings of
	the Sixth International Symposium on},
  year = {1995},
  pages = {39--43},
  month = {oct},
  abstract = {The optimization of nonlinear functions using particle swarm methodology
	is described. Implementations of two paradigms are discussed and
	compared, including a recently developed locally oriented paradigm.
	Benchmark testing of both paradigms is described, and applications,
	including neural network training and robot task learning, are proposed.
	Relationships between particle swarm optimization and both artificial
	life and evolutionary computation are reviewed},
  doi = {10.1109/MHS.1995.494215},
  keywords = {artificial life, benchmark testing, bird flocks, evolutionary computation,
	gbest, genetic algorithms, globally oriented concept, hyperspace,
	lbest, locally oriented paradigm, multilayer perceptron, neural network
	training, nonlinear functions, optimization, particle swarm theory,
	pbest, robot task learning, algorithm theory, feedforward neural
	nets, genetic algorithms, intelligent control, learning (artificial
	intelligence), multilayer perceptrons, optimisation},
  tags = {Calibration, PSO}
}

@INCOLLECTION{eberhartshi1998,
  author = {Eberhart, R. and Shi, Y.},
  title = {Comparison between genetic algorithms and particle swarm optimization},
  booktitle = {Evolutionary Programming VII},
  publisher = {Springer Berlin / Heidelberg},
  year = {1998},
  editor = {V. Porto and N. Saravanan and D. Waagen and A. Eiben},
  volume = {1447},
  pages = {611--616},
  abstract = {This paper compares two evolutionary computation paradigms: genetic
	algorithms and particle swarm optimization. The operators of each
	paradigm are reviewed, focusing on how each affects search behavior
	in the problem space. The goals of the paper are to provide additional
	insights into how each paradigm works, and to suggest ways in which
	performance might be improved by incorporating features from one
	paradigm into the other. },
  doi = {10.1007/BFb0040812},
  tags = {PSO, Calibration}
}

@INPROCEEDINGS{eberhartshi2001,
  author = {Eberhart, R. and Shi, Y.},
  title = {Particle swarm optimization: developments, applications and resources},
  booktitle = {Proceedings of the 2001 Congress on Evolutionary Computation},
  year = {2001},
  volume = {1},
  pages = {81--86},
  abstract = {This paper focuses on the engineering and computer science aspects
	of developments, applications, and resources related to particle
	swarm optimization. Developments in the particle swarm algorithm
	since its origin in 1995 are reviewed. Included are brief discussions
	of constriction factors, inertia weights, and tracking dynamic systems.
	Applications, both those already developed, and promising future
	application areas, are reviewed. Finally, resources related to particle
	swarm optimization are listed, including books, Web sites, and software.
	A particle swarm optimization bibliography is at the end of the pape},
  doi = {10.1109/CEC.2001.934374},
  tags = {PSO}
}

@INPROCEEDINGS{eberhartshi2000,
  author = {Eberhart, R. and Shi, Y.},
  title = {Comparing inertia weights and constriction factors in particle swarm
	optimization},
  booktitle = {Evolutionary Computation, 2000. Proceedings of the 2000 Congress
	on},
  year = {2000},
  volume = {1},
  pages = {84--88},
  abstract = {The performance of particle swarm optimization using an inertia weight
	is compared with performance using a constriction factor. Five benchmark
	functions are used for the comparison. It is concluded that the best
	approach is to use the constriction factor while limiting the maximum
	velocity Vmax to the dynamic range of the variable Xmax on each dimension.
	This approach provides performance on the benchmark functions superior
	to any other published results known by the authors},
  doi = {10.1109/CEC.2000.870279},
  keywords = {benchmark functions, constriction factors, inertia weights, particle
	swarm optimization, evolutionary computation},
  tags = {Calibration, PSO}
}

@BOOK{eberhart+al1996,
  title = {Computational {I}ntelligence {PC} {T}ools},
  publisher = {Academic Press Professional, Inc.},
  year = {1996},
  author = {Eberhart, R. and Simpson, P. and Dobbins, R.},
  address = {San Diego, CA, USA},
  isbn = {0-12-228630-8},
  tags = {Calibration, PSO}
}

@ARTICLE{ebtehaj+al2010,
  author = {Ebtehaj, M. and Moradkhani, H. and Gupta, H.},
  title = {Improving robustness of hydrologic parameter estimation by the use
	of moving block bootstrap resampling},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W07515},
  abstract = {Modeling of natural systems typically involves conceptualization and
	parameterization to simplify the representations of the underlying
	process. Objective methods for estimation of the model parameters
	then require optimization of a cost function, representing a measure
	of distance between the observations and the corresponding model
	predictions, typically by calibration in a static batch mode and/or
	via some dynamic recursive optimization approach. Recently, there
	has been a focus on the development of parameter estimation methods
	that appropriately account for different sources of uncertainty.
	In this context, we introduce an approach to sample the optimal parameter
	space that uses nonparametric block bootstrapping coupled with global
	optimization. We demonstrate the applicability of this procedure
	via a case study, in which we estimate the parameter uncertainty
	resulting from uncertainty in the forcing data and evaluate its impacts
	on the resulting streamflow simulations. },
  doi = {10.1029/2009WR007981},
  tags = {Uncertainty, Calibration}
}

@ARTICLE{eckhardtarnold2001,
  author = {Eckhardt, K. and Arnold, J.},
  title = {Automatic calibration of a distributed catchment model},
  journal = {Journal of Hydrology},
  year = {2001},
  volume = {251},
  pages = {103--109},
  number = {1-2},
  abstract = {Parameters of hydrologic models often are not exactly known and therefore
	have to be determined by calibration. A manual calibration depends
	on the subjective assessment of the modeler and can be very time-consuming
	though. Methods of automatic calibration can improve these shortcomings.
	Yet, the high number of parameters in distributed models makes special
	demands on the optimization. In this paper a strategy of imposing
	constraints on the parameters to limit the number of independently
	calibrated values is outlined. Subsequently, an automatic calibration
	of the version SWAT-G of the model SWAT (Soil and Water Assessment
	Tool) with a stochastic global optimization algorithm, the Shuffled
	Complex Evolution algorithm. is presented for a mesoscale catchment.
	(C) 2001 Elsevier Science B.V. All rights reserved.},
  doi = {10.1016/S0022-1694(01)00429-2},
  keywords = {distributed models, calibration, parameter estimation, SWAT, SCE-UA,
	RAINFALL-RUNOFF MODELS, GLOBAL OPTIMIZATION},
  tags = {SWAT, Calibration}
}

@ARTICLE{eckhardt+al2005,
  author = {Eckhardt, K. and Fohrer, N. and Frede, {Hans-Georg}},
  title = {Automatic model calibration},
  journal = {Hydrological Processes},
  year = {2005},
  volume = {19},
  pages = {651--658},
  number = {3},
  abstract = {Model calibration allows reducing parameter uncertainty and, therefore,
	uncertainty in simulation results. In the present study, automatic
	model calibration with the shuffled complex evolution algorithm is
	demonstrated using the example of the distributed conceptual model
	SWAT-G, a derivative of the Soil and Water Assessment Toot (SWAT).
	The special challenge in calibrating SWAT is that it describes many
	physical processes and therefore is highly parameterized. Moreover,
	as SWAT or SWAT-G, respectively, is a distributed model, the parameters
	may take on different values in different spatial subunits of a catchment
	model. A strategy of imposing constraints on the parameters to limit
	the number of independently calibrated values is outlined and applied
	to the model of a mesoscale low mountain range catchment. Copyright
	(C) 2005 John Wiley Sons, Ltd.},
  doi = {10.1002/hyp.5613},
  keywords = {distributed models, calibration, parameter estimation, SWAT, SOIL},
  tags = {SWAT, Calibration}
}

@ARTICLE{eckhardt+al2002,
  author = {Eckhardt, K. and Haverkamp, S. and Fohrer, N. and Frede, {Hans-Georg}},
  title = {{SWAT-G}, a version of {SWAT99}.2 modified for application to low
	mountain range catchments},
  journal = {Physics and Chemistry of the Earth},
  year = {2002},
  volume = {27},
  pages = {641--644},
  number = {9-10},
  abstract = {The Soil and Water Assessment Tool (SWAT) is a well established distributed
	eco-hydrologic model. However, using the example of a mesoscale catchment
	in Germany it is shown that the version SWAT99.2 is not able to correctly
	reproduce the runoff generation in a low mountain region. The calculated
	contribution of the baseflow to the streamflow is far too high whereas
	the interflow is strongly underestimated. Alternatively, the modified
	version SWAT-G can be used which, as is demonstrated in this paper,
	yields far better results for catchments with predominantly steep
	slopes and shallow soils over hard rock aquifers. In the example,
	calibrating the model over three hydrologic years of daily streamflow,
	the model efficiency increases from -0.17 to +0.76. The modifications
	in SWAT-G allow hydrological processes to be modelled in low mountain
	ranges while not restraining the applicability of the model to catchments
	with other characteristics. (C) 2002 Elsevier Science Ltd. All rights
	reserved.},
  doi = {S1474-7065(02)00048-7},
  keywords = {distributed models, model verification, SWAT, MODEL, CALIBRATION},
  tags = {SWAT}
}

@TECHREPORT{EC2007,
  author = {{EC}},
  title = {{Addressing the challenge of water scarcity and droughts in the European
	Union}},
  institution = {{Communication from the Commission to the European Parliament and
	the Council}},
  year = {2007},
  number = {Com(2007) 0414 final},
  owner = {rojasro},
  timestamp = {2012.06.26}
}

@TECHREPORT{corine2000,
  author = {EEA},
  title = {Corine Land Cover 2000},
  institution = {European Commission, European Environment Agency (EEA)},
  year = {2002},
  number = {89},
  owner = {rojasro},
  timestamp = {2010.08.17},
  url = {http://dataservice.eea.europa.eu/}
}

@ARTICLE{efstratiadiskoutsoyiannis2010,
  author = {Efstratiadis, A. and Koutsoyiannis, D.},
  title = {One decade of multi-objective calibration approaches in hydrological
	modelling: a review},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {58--78},
  number = {1},
  abstract = {One decade after the first publications on multi-objective calibration
	of hydrological models, we summarize the experience gained so far
	by underlining the key perspectives offered by such approaches to
	improve parameter identification. After reviewing the fundamentals
	of vector optimization theory and the algorithmic issues, we link
	the multi-criteria calibration approach with the concepts of uncertainty
	and equifinality. Specifically, the multi-criteria framework enables
	recognition and handling of errors and uncertainties, and detection
	of prominent behavioural solutions with acceptable trade-offs. Particularly
	in models of complex parameterization, a multi-objective approach
	becomes essential for improving the identifiability of parameters
	and augmenting the information contained in calibration by means
	of both multi-response measurements and empirical metrics ({``}soft{''}
	data), which account for the hydrological expertise. Based on the
	literature review, we also provide alternative techniques for dealing
	with conflicting and non-commeasurable criteria, and hybrid strategies
	to utilize the information gained towards identifying promising compromise
	solutions that ensure consistent and reliable calibrations. },
  doi = {10.1080/02626660903526292},
  keywords = {multi-objective evolutionary algorithms, multiple responses, uncertainty,
	equifinality, hybrid calibration, soft data },
  tags = {Calibration}
}

@ARTICLE{ehretzehe2011,
  author = {Ehret, U. and Zehe, E.},
  title = {Series distance - an intuitive metric to quantify hydrograph similarity
	in terms of occurrence, amplitude and timing of hydrological events},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {877--896},
  number = {3},
  abstract = {Applying metrics to quantify the similarity or dissimilarity of hydrographs
	is a central task in hydrological modelling, used both in model calibration
	and the evaluation of simulations or forecasts. Motivated by the
	shortcomings of standard objective metrics such as the Root Mean
	Square Error (RMSE) or the Mean Absolute Peak Time Error (MAPTE)
	and the advantages of visual inspection as a powerful tool for simultaneous,
	case-specific and multi-criteria (yet subjective) evaluation, we
	propose a new objective metric termed Series Distance, which is in
	close accordance with visual evaluation. The Series Distance quantifies
	the similarity of two hydrographs neither in a time-aggregated nor
	in a point-by-point manner, but on the scale of hydrological events.
	It consists of three parts, namely a Threat Score which evaluates
	overall agreement of event occurrence, and the overall distance of
	matching observed and simulated events with respect to amplitude
	and timing. The novelty of the latter two is the way in which matching
	point pairs on the observed and simulated hydrographs are identified:
	not by equality in time (as is the case with the RMSE), but by the
	same relative position in matching segments (rise or recession) of
	the event, indicating the same underlying hydrological process. Thus,
	amplitude and timing errors are calculated simultaneously but separately,
	from point pairs that also match visually, considering complete events
	rather than only individual points (as is the case with MAPTE). Relative
	weights can freely be assigned to each component of the Series Distance,
	which allows (subjective) customization of the metric to various
	fields of application, but in a traceable way. Each of the three
	components of the Series Distance can be used in an aggregated or
	non-aggregated way, which makes the Series Distance a suitable tool
	for differentiated, process-based model diagnostics. After discussing
	the applicability of established time series metrics for hydrographs,
	we present the Series Distance theory, discuss its properties and
	compare it to those of standard metrics used in Hydrology, both at
	the example of simple, artificial hydrographs and an ensemble of
	realistic forecasts. The results suggest that the Series Distance
	quantifies the degree of similarity of two hydrographs in a way comparable
	to visual inspection, but in an objective, reproducible way.},
  doi = {10.5194/hess-15-877-2011},
  tags = {Regionalization, Goodness-of-Fit}
}

@ARTICLE{el-nasr+al2005,
  author = {{El-Nasr}, A. and Arnold, J. and Feyen, J. and Berlamont, J.},
  title = {Modelling the hydrology of a catchment using a distributed and a
	semi-distributed model},
  journal = {Hydrological Processes},
  year = {2005},
  volume = {19},
  pages = {573--587},
  number = {3},
  abstract = {Various hydrological models exist that describe the phases in the
	hydrologic cycle either in an empirical, semi-mechanistic or fully
	mechanistic way. The way and level of detail for the different processes
	of the hydrologic cycle that needs to be described depends on the
	objective, the application and the availability of data. In this
	study the performance of two different models, the fully distributed
	MIKE SHE model and the semi-distributed SWAT model, was assessed.
	The aim of the comparative study was to examine if both models are
	equally able to describe the different phases in the hydrologic cycle
	of a catchment, given the availability of hydrologic data in the
	catchment. For the comparison, historic data of the Jeker river basin,
	situated in the loamy belt region of Belgium, was used. The size
	of the catchment is 465 km2. The landscape is rolling, the dominant
	land use is farmland, and the soils vary from sandy-loam to clay-loam.
	The daily data of a continuous period of 6 years were used for the
	calibration and validation of both models. The results were obtained
	by comparing the performance of the two models using a qualitative
	(graphical) and quantitative (statistical) assessment, such as graphical
	representation of the observed and simulated river discharge, performance
	indices, the hydrograph maxima, the baseflow minima, the total accumulated
	volumes and the extreme value distribution of river flow data. The
	analysis revealed that both models are able to simulate the hydrology
	of the catchment in an acceptable way. The calibration results of
	the two tested models, although they differ in concept and spatial
	distribution, are quite similar. However, the MIKE SHE model predicts
	slightly better the overall variation of the river flow},
  doi = {10.1002/hyp.5610},
  keywords = {hydrology, semi- and fully distributed model, performance indices,
	SWAT, MIKE SHE},
  tags = {SWAT, Goodness-of-Fit, conceptual model}
}

@ARTICLE{engeland+al2006,
  author = {Engeland, K. and Braud, I. and Gottschalk, L. and Leblois, E.},
  title = {Multi-objective regional modelling},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {327},
  pages = {339--351},
  number = {3-4},
  abstract = {Regional rainfall runoff models are used to estimate the hydrological
	dynamics, the water balance, and the statistics of hydrological variables
	at ungauged sites. Most hydrological models are calibrated in order
	to get a good fit between observed and simulated variables. One or
	several objectives might be used for the calibration, and the modelling
	uncertainty is seen in at least two phenomena: (1) The surface defined
	by a objective function is often non-smooth, multi-modal and even
	discontinuous. (2) Different objectives prefer different parameter
	sets. The aim of this study is to obtain regional parameter sets
	for the Ecomag model applied to the Saone catchment, and to assess
	the modelling uncertainty based on the trade-offs between the Reff
	criterion for daily streamflows in different catchments. This aim
	was achieved by applying a multi-objective calibration method with
	streamflow data from seven catchments as the objectives. A Pareto-set
	with 2330 parameter values were obtained. The 2330 parameter sets
	were used to obtain envelopes for the simulated streamflow. The results
	show that all the parameters might be considered as sensitive to
	the output results, that the trade-off uncertainty between the different
	catchments is important, and that this uncertainty can only explain
	parts of the complex uncertainties in catchment modelling. There
	is a need for finding new concepts describing the complex uncertainties
	in catchment models operating on daily time step},
  doi = {10.1016/j.jhydrol.2005.11.022},
  keywords = {Rainfall runoff modelling, Distributed hydrological models, Regionalisation,
	Ungauged catchments, Parameter uncertainty, Multiple objectives},
  tags = {Calibration, Large Scale}
}

@ARTICLE{engeland2002,
  author = {Engeland, K. and Gottschalk, L.},
  title = {Bayesian estimation of parameters in a regional hydrological model},
  journal = {Hydrology and Earth System Sciences},
  year = {2002},
  volume = {6},
  pages = {883--898},
  number = {5},
  abstract = {This study evaluates the applicability of the distributed, process-oriented
	Ecomag model for prediction of daily streamflow in ungauged basins.
	The Ecomag model is applied as a regional model to nine catchments
	in the NOPEX area, using Bayesian statistics to estimate the posterior
	distribution of the model parameters conditioned on the observed
	streamflow. The distribution is calculated by Markov Chain Monte
	Carlo (MCMC) analysis. The Bayesian method requires formulation of
	a likelihood function for the parameters and three alternative formulations
	are used. The first is a subjectively chosen objective function that
	describes the goodness of fit between the simulated and observed
	streamflow, as defined in the GLUE framework. The second and third
	formulations are more statistically correct likelihood models that
	describe the simulation errors. The full statistical likelihood model
	describes the simulation errors as an AR(1) process, whereas the
	simple model excludes the auto-regressive part. The statistical parameters
	depend on the catchments and the hydrological processes and the statistical
	and the hydrological parameters are estimated simultaneously. The
	results show that the simple likelihood model gives the most robust
	parameter estimates. The simulation error may be explained to a large
	extent by the catchment characteristics and climatic conditions,
	so it is possible to transfer knowledge about them to ungauged catchments.
	The statistical models for the simulation errors indicate that structural
	errors in the model are more important than parameter uncertainties.},
  doi = {10.5194/hess-6-883-2002},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{engen2007,
  author = {{Engen-Skaugen}, T.},
  title = {Refinement of dynamically downscaled precipitation and temperature
	scenarios},
  journal = {Climatic Change},
  year = {2007},
  volume = {84},
  pages = {365--382},
  number = {3--4},
  abstract = {A method for adjusting dynamically downscaled precipitation and temperature
	scenarios representing specific sites is presented. The method reproduces
	mean monthly values and standard deviations based on daily observations.
	The trend obtained in the regional climate model both for temperature
	and precipitation is maintained, and the frequency of modelled and
	observed rainy days shows better agreement. Thus, the method is appropriate
	for tailoring dynamically downscaled temperature and precipitation
	values for climate change impact studies. One precipitation and temperature
	scenario dynamically downscaled with HIRHAM from the Atmospheric-Ocean
	General Circulation Model at the Max-Planck Institute in Hamburg,
	ECHAM4/OPYC4 GSDIO with emission scenario IS92a, is chosen to illustrate
	the adjustment method.},
  doi = {10.1007/s10584-007-9251-6},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@TECHREPORT{watch40-2011,
  author = {Estifanos, S. and {van Huijgevoort}, M. and Hazenberg, P. and Weedon,
	G. and Bertrand, N. and Folwell, S. and Gomes, S. and Vo{\ss}, F.
	and {van Lanen}, H.},
  title = {{Multi-model analysis of drought at the global scale: Differences
	in hydrological drought between the first and second part of the
	20th century}},
  institution = {WATCH - Water and global Change},
  year = {2011},
  month = {August},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{estrelavargas2012,
  author = {Estrela, T. and Vargas, E.},
  title = {{Drought management plans in the European Union. The case of Spain}},
  journal = {Water Resources Management},
  year = {2012},
  volume = {26},
  pages = {1537--1553},
  number = {6},
  abstract = {Water is a strategic resource for the economic, social and environmental
	development. However, water scarcity and droughts are current challenges
	to this growth, as it is reflected in European Union (EU) water policies,
	and in national and regional growing initiatives. In addition, these
	water related issues could worsen by climate change effects, adding
	pressure to already water stressed areas. This paper presents a general
	overview of drought management in the European Union, reviews scientific
	and technical advances, the status of implementation of policy tools
	and focuses on drought management plans. It analyses the specific
	case of Spain, a country characterised by presenting a high irregularity
	in temporal and spatial distribution of water resources and numerous
	areas affected by water scarcity and droughts. Details are presented
	on the National Drought Indicator System and drought management plans
	approved in 2007 in Spain, which represent strategic tools with positive
	results in drought warning and impact mitigation respectively.},
  doi = {10.1007/s11269-011-9971-2},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{etchevers2002,
  author = {Etchevers, P. and Golaz, C. and Habets, F. and Noilhan, J.},
  title = {Impact of a climate change on the {R}hone river catchment hydrology},
  journal = {Journal of Geophysical Research},
  year = {2002},
  volume = {107},
  pages = {4293},
  number = {D16},
  abstract = {Within the framework of the Global Energy and Water Cycle Experiment
	(GEWEX)/Rhone project, a system has been built to estimate the hydrological
	budget of the Rhone, one of the major European rivers (with a 86,500
	km2 surface area for the French part of the catchment). The methodology
	is based on three models, one for each component of the hydrometeorological
	system: The atmospheric parameter analysis, the snow cover, the surface
	water and energy budget, and the underground water transfer and discharge
	estimation. This tool has been validated for 14 years (from 1981
	to 1994) by comparing the daily river flows simulated by the models
	with measurements from 145 gauging stations. In this study, the results
	of the ARPEGE-Climat general circulation model (GCM) have been used
	to estimate the climate of the Rhone catchment in 60 years. The perturbation
	of the air temperature and precipitation amount has been used to
	modify the actual set of meteorological parameters in order to simulate
	the hydrological budget of the Rhone river. Vegetation and soil structure
	are supposed to be identical to current values, which is a strong
	hypothesis. The river discharge and soil water resources under the
	climatic scenario are compared with the results of the actual simulation.
	Strong contrasts in the hydrological response of the catchment are
	noticeable, depending on the location of the subcatchments and the
	amount of precipitation. Snow cover is the most sensitive hydrological
	component to the air temperature increase, and the high mountainous
	river regimes are strongly modified. When considering the soil water
	content, it appears that the northern part of the domain stays quite
	wet, whereas drying is enhanced in the south. The uncertainties in
	the results are estimated by analyzing the model sensitivity to different
	simple climatic scenarios. In particular, the analysis brings into
	light the impact of the downscaling of the GCM results},
  doi = {10.1029/2001JD000490},
  tags = {Impacts}
}

@ARTICLE{evans2003,
  author = {Evans, J.},
  title = {Improving the characteristics of streamflow modeled by regional climate
	models},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {284},
  pages = {211--227},
  number = {1--4},
  month = {December},
  abstract = {The introduction of complex land surface parameterization schemes
	into regional climate models (RCMs) has been focused on improving
	the modeling of land surface feedbacks to the atmosphere. As such
	the modeling of streamflow was considered a by-product of the water
	balance and until recently it received relatively little attention.
	Comparison of four RCMs (RegCM2, MM5/BATS, MM5/SHEELS and MM5/OSU)
	and a simple hydrology model, Catchment Moisture Deficit–Identification
	of unit Hydrographs And Component flows from Rainfall, Evaporation
	and Streamflow data (CMD-IHACRES) demonstrates the improvement in
	the characteristics of the streamflow prediction, which may be achieved
	using CMD-IHACRES. The conceptual structure of CMD-IHACRES allows
	it to be ‘incorporated’ into the RCMs, improving their streamflow
	predictions, as is demonstrated for the FIFE region of central USA.},
  doi = {10.1016/j.jhydrol.2003.08.003},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@INPROCEEDINGS{eversghalia2009,
  author = {Evers, G. and Ghalia, M.},
  title = {Regrouping Particle Swarm Optimization: A New Global Optimization
	Algorithm with Improved Performance Consistency Across Benchmarks},
  booktitle = {IEEE International Conference on Systems, Man and Cybernetics, 2009.
	SMC 2009},
  year = {2009},
  pages = {3901--3908},
  doi = {10.1109/ICSMC.2009.5346625},
  tags = {Calibration, PSO}
}

@ARTICLE{ewen+al2006,
  author = {Ewen, J. and O'Donnell, G. and Burton, A. and O'Connell, E.},
  title = {Errors and uncertainty in physically--based rainfall--runoff modelling
	of catchment change effects},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {330},
  pages = {641},
  number = {3--4},
  abstract = {The error in physically-based rainfall-runoff modelling is broken
	into components, and these components are assigned to three groups:
	(1) model structure error, associated with the model’s equations;
	(2) parameter error, associated with the parameter values used in
	the equations; and (3) run time error, associated with rainfall and
	other forcing data. The error components all contribute to “integrated”
	errors, such as the difference between simulated and observed runoff,
	but their individual contributions cannot usually be isolated because
	the modelling process is complex and there is a lack of knowledge
	about the catchment and its hydrological responses. A simple model
	of the Slapton Wood Catchment is developed within a theoretical framework
	in which the catchment and its responses are assumed to be known
	perfectly. This makes it possible to analyse the contributions of
	the error components when predicting the effects of a physical change
	in the catchment. The standard approach to predicting change effects
	involves: (1) running “unchanged” simulations using current parameter
	sets; (2) making adjustments to the sets to allow for physical change;
	and (3) running “changed” simulations. Calibration or uncertainty-handling
	methods such as GLUE are used to obtain the current sets based on
	forcing and runoff data for a calibration period, by minimising or
	creating statistical bounds for the “integrated” errors in simulations
	of runoff. It is shown that current parameter sets derived in this
	fashion are unreliable for predicting change effects, because of
	model structure error and its interaction with parameter error, so
	caution is needed if the standard approach is to be used when making
	management decisions about change in catchments.},
  doi = {10.1016/j.jhydrol.2006.04.024},
  tags = {Uncertainty}
}

@ARTICLE{ewenparkin1996,
  author = {Ewen, J. and Parkin, G.},
  title = {Validation of catchment models for predicting land-use and climate
	change impacts .1. Method},
  journal = {Journal of Hydrology},
  year = {1996},
  volume = {175},
  pages = {583--594},
  number = {1--4},
  abstract = {Computer simulation models are increasingly being proposed as tools
	capable of giving water resource managers accurate predictions of
	the impact of changes in land-use and climate. Previous validation
	testing of catchment models is reviewed, and it is concluded that
	the methods used do not clearly test a model's fitness for such a
	purpose. A new generally applicable method is proposed. This involves
	the direct testing of fitness for purpose, uses established scientific
	techniques, and may be implemented within a quality assured programme
	of work. The new method is applied in Part 2 of this study (Parkin
	et al., J. Hydrol., 175: 595-613, 1996).},
  doi = {10.1016/S0022-1694(96)80026-6},
  keywords = {SYSTEME HYDROLOGIQUE EUROPEEN, WATER-BALANCE, SIMULATION, RUNOFF,
	ACIDIFICATION, UPLANDS, SHE}
}

@ARTICLE{ezzedine1996,
  author = {Ezzedine, S. and Rubin, Y.},
  title = {A geostatistical approach to the conditional estimation of spatially
	distributed solute concentration and notes on the use of tracer data
	in the inverse problem},
  journal = {Water Resources Research},
  year = {1996},
  volume = {32},
  pages = {853--861},
  number = {4},
  abstract = {This paper presents a theoretical framework for deriving the moments
	of the concentration, based on the Lagrangian approach and using
	a stochastic framework, conditional to measurements of conductivity
	and hydraulic head. The method consists of deriving the spatial correlations
	between concentration and travel time and hydrologic variables such
	as the conductivity and the hydraulic head. These correlations allow
	the conditioning of the moments of the concentration on measurements.
	By conditioning the concentration the uncertainty associated with
	its estimation can be reduced substantially. Consequently, difficulties
	associated with estimation of the extent of contamination can be
	alleviated. The theoretical framework and derivations may also be
	used to condition the moments of the conductivity on tracer data
	such as concentrations, travel times, and displacements. An application
	of such an approach would require a configuration of sources and
	samplers. We show that measured concentration is inferior to travel
	time and displacements in terms of efficient conditioning.},
  doi = {10.1029/95WR02285},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{falloon2009,
  author = {Falloon, P. and Betts, R.},
  title = {Climate impacts on {E}uropean agriculture and water management in
	the context of adaptation and mitigation--{T}he importance of an
	integrated approach},
  journal = {Science of the Total Environment},
  year = {2009},
  volume = {408},
  pages = {5667--5687},
  number = {23},
  abstract = {We review and qualitatively assess the importance of interactions
	and feedbacks in assessing climate change impacts on water and agriculture
	in Europe. We focus particularly on the impact of future hydrological
	changes on agricultural greenhouse gas (GHG) mitigation and adaptation
	options. Future projected trends in European agriculture include
	northward movement of crop suitability zones and increasing crop
	productivity in Northern Europe, but declining productivity and suitability
	in Southern Europe. This may be accompanied by a widening of water
	resource differences between the North and South, and an increase
	in extreme rainfall events and droughts. Changes in future hydrology
	and water management practices will influence agricultural adaptation
	measures and alter the effectiveness of agricultural mitigation strategies.
	These interactions are often highly complex and influenced by a number
	of factors which are themselves influenced by climate. Mainly positive
	impacts may be anticipated for Northern Europe, where agricultural
	adaptation may be shaped by reduced vulnerability of production,
	increased water supply and reduced water demand. However, increasing
	flood hazards may present challenges for agriculture, and summer
	irrigation shortages may result from earlier spring runoff peaks
	in some regions. Conversely, the need for effective adaptation will
	be greatest in Southern Europe as a result of increased production
	vulnerability, reduced water supply and increased demands for irrigation.
	Increasing flood and drought risks will further contribute to the
	need for robust management practices. The impacts of future hydrological
	changes on agricultural mitigation in Europe will depend on the balance
	between changes in productivity and rates of decomposition and GHG
	emission, both of which depend on climatic, land and management factors.
	Small increases in European soil organic carbon (SOC) stocks per
	unit land area are anticipated considering changes in climate, management
	and land use, although an overall reduction in the total stock may
	result from a smaller agricultural land area. Adaptation in the water
	sector could potentially provide additional benefits to agricultural
	production such as reduced flood risk and increased drought resilience.
	The two main sources of uncertainty in climate impacts on European
	agriculture and water management are projections of future climate
	and their resulting impacts on water and agriculture. Since changes
	in climate, agricultural ecosystems and hydrometeorology depend on
	complex interactions between the atmosphere, biosphere and hydrological
	cycle there is a need for more integrated approaches to climate impacts
	assessments. Methods for assessing options which “moderate” the impact
	of agriculture in the wider sense will also need to consider cross-sectoral
	impacts and socio-economic aspects.},
  doi = {10.1016/j.scitotenv.2009.05.002},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@TECHREPORT{FAO1989,
  author = {FAO},
  title = {Role of forestry in combating desertification: {Proceedings of the
	FAO Expert Consultation on the Role of Forestry in Combating Desertification
	held in Saltillo, Mexico 24-28 June 1985}},
  institution = {FAO},
  year = {1989},
  address = {Rome, Italy},
  owner = {RRojas},
  timestamp = {2009.04.08},
  url = {http://www.fao.org/docrep/T0115E/T0115E00.htm#Contents}
}

@ARTICLE{fenicia+al2007b,
  author = {Fenicia, F. and Savenije, H. and Matgen, P. and Pfister, L.},
  title = {A comparison of alternative multiobjective calibration strategies
	for hydrological modeling},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W03434},
  abstract = {A conceptual hydrological model structure contains several parameters
	that have to be estimated through matching observed and modeled watershed
	behavior in a calibration process. The requirement that a model simulation
	matches different aspects of system response at the same time has
	led the calibration problem toward a multiobjective approach. In
	this work we compare two multiobjective calibration approaches, each
	of which represents a different calibration philosophy. The first
	calibration approach is based on the concept of Pareto optimality
	and consists of calibrating all parameters with respect to a common
	set of objectives in one calibration stage. This approach results
	in a set of Pareto-optimal solutions representing the trade-offs
	between the selected calibration objectives. The second is a stepped
	calibration approach (SCA), which implies a stepwise calibration
	of sets of parameters that are associated with specific aspects of
	the system response. This approach replicates the steps followed
	by a hydrologist in manual calibration and develops a single solution.
	The comparison is performed considering the same set of objectives
	for the two approaches and two model structures of a different level
	of complexity. The difference in the two approaches, their reciprocal
	utility, and the practical implications involved in their application
	are analyzed and discussed using the Hesperange catchment case, an
	experimental basin in the Alzette River basin in Luxembourg. We show
	that the two approaches are not necessarily conflicting but can be
	complementary. The first approach provides useful information about
	the deficiencies of a model structure and therefore helps the model
	development, while the second attempts at determining a solution
	that is consistent with the data available. We also show that with
	increasing model complexity it becomes possible to reproduce the
	observations more accurately. As a result, the solutions for the
	different calibration objectives become less distinguishable from
	each other, indicating that calibration results become less dependent
	on the objective functions used when the model is a better representation
	of reality and has a higher potential to reproduce the observations},
  doi = {10.1029/2006WR005098},
  tags = {Calibration}
}

@ARTICLE{fenicia+al2007a,
  author = {Fenicia, F. and Solomatine, D. and Savenije, H. and Matgen, P.},
  title = {Soft combination of local models in a multi-objective framework},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1797--1809},
  number = {11},
  abstract = {Conceptual hydrologic models are useful tools as they provide an interpretable
	representation of the hydrologic behaviour of a catchment. Their
	representation of catchment's hydrological processes and physical
	characteristics, however, implies a simplification of the complexity
	and heterogeneity of reality. As a result, these models may show
	a lack of flexibility in reproducing the vast spectrum of catchment
	responses. Hence, the accuracy in reproducing certain aspects of
	the system behaviour may be paid in terms of a lack of accuracy in
	the representation of other aspects.},
  doi = {10.5194/hess-11-1797-2007},
  keywords = {ARTIFICIAL NEURAL-NETWORK, RAINFALL-RUNOFF MODELS, 2 CONTRASTING CATCHMENTS,
	HYDROLOGIC-MODELS, HIERARCHICAL MIXTURES, IMPROVED CALIBRATION, UNCERTAINTY,
	PREDICTION, OPTIMIZATION, EQUIFINALITY},
  tags = {Calibration}
}

@ARTICLE{fernandezsalas1999a,
  author = {Fern\'andez, B. and Salas, J.},
  title = {{Return period and risk of hydrologic events.I: Mathematical formulation}},
  journal = {Journal of Hydrologic Engineering},
  year = {1999},
  volume = {4},
  pages = {297--307},
  number = {4},
  abstract = {The estimation of return periods of hydrological events and the corresponding
	risks of failure of hydraulic structures that are associated with
	such events are important aspects in many water resources studies.
	For simple hydrologic events such as those related to independent
	annual floods, both the return period and the risk of failure can
	be readily calculated. However, no general applicable methods are
	available for the estimation of return periods, risk of failure,
	and reliability of service in cases of more complex hydrological
	events such as those related to dependent annual flows and droughts.
	In this paper, the definitions commonly employed for return period
	and risk of failure are reexamined and a general procedure for their
	estimation are presented, which may be applicable to a wide range
	of hydrological events related to floods, droughts, minimum flows,
	aquifer levels, and reservoir levels and outflows. Part II of this
	paper includes numerical examples and applications.},
  doi = {10.1061/(ASCE)1084-0699(1999)4:4(297)},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{fernandezsalas1999b,
  author = {Fern\'andez, B. and Salas, J.},
  title = {{Return period and risk of hydrologic events. II: Applications}},
  journal = {Journal of Hydrologic Engineering},
  year = {1999},
  volume = {4},
  pages = {308--316},
  number = {4},
  abstract = {A mathematical formulation to estimate return periods and risks of
	failure of complex hydrologic events such as those arising from dependent
	floods and droughts have been examined in the first part of this
	paper. Specifically, some relationships and algorithms for computing
	return periods and associated risks for runs arising from independent
	and dependent events assuming that dependence is represented by a
	two-state Markov chain have been proposed. The applicability of these
	procedures is illustrated herein, considering several types of hydrological
	events with emphasis on those where dependence is important. First,
	meteorological droughts based on annual precipitation are considered
	as examples of events consisting of runs in independent trials. Then,
	minimum streamflows and maximum annual lake outflows are included
	as examples of dependent events, assuming that dependence is represented
	by a simple Markov chain. Also, hydrological droughts based on annual
	streamflow series are considered. In addition, the estimation of
	return period and risk are illustrated by data generation.},
  doi = {10.1061/(ASCE)1084-0699(1999)4:4(308)},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{feyen+al2008,
  author = {Feyen, J. and Kalas, M. and Vrugt, J.},
  title = {Semi-distributed parameters for large-scale streamflow simulation
	using global optimization},
  journal = {Hydrological Sciences Journal},
  year = {2008},
  volume = {53},
  pages = {293--308},
  number = {2},
  abstract = {In catchments characterized by spatially varying hydrological processes
	and responses, the optimal parameter values or regions of attraction
	in parameter space may differ with location-specific characteristics
	and dominating processes. This paper evaluates the value of semi-distributed
	calibration parameters for large-scale streamflow simulation using
	the spatially distributed LISFLOOD model. We employ the Shuffled
	Complex Evolution Metropolis (SCEM-UA) global optimization algorithm
	to infer the calibration parameters using daily discharge observations.
	The resulting posterior parameter distribution reflects the uncertainty
	about the model parameters and forms the basis for making probabilistic
	flow predictions. We assess the value of semi-distributing the calibration
	parameters by comparing three different calibration strategies. In
	the first calibration strategy uniform values over the entire area
	of interest are adopted for the unknown parameters, which are calibrated
	against discharge observations at the downstream outlet of the catchment.
	In the second calibration strategy the parameters are also uniformly
	distributed, but they are calibrated against observed discharges
	at the catchment outlet and at internal stations. In the third strategy
	a semi-distributed approach is adopted. Starting from upstream, parameters
	in each subcatchment are calibrated against the observed discharges
	at the outlet of the subcatchment. In order not to propagate upstream
	errors in the calibration process, observed discharges at upstream
	catchment outlets are used as inflow when calibrating downstream
	subcatchments. As an illustrative example, we demonstrate the methodology
	for a part of the Morava catchment, covering an area of approximately
	10 000 km2. The calibration results reveal that the additional value
	of the internal discharge stations is limited when applying a lumped
	parameter approach. Moving from a lumped to a semi-distributed parameter
	approach: (i) improves the accuracy of the flow predictions, especially
	in the upstream subcatchments; and (ii) results in a more correct
	representation of flow prediction uncertainty. The results show the
	clear need to distribute the calibration parameters, especially in
	large catchments characterized by spatially varying hydrological
	processes and responses.},
  doi = {10.1623/hysj.53.2.293},
  owner = {rojasro},
  timestamp = {2010.08.17}
}

@ARTICLE{feyen2001,
  author = {Feyen, L. and Beven, K. and {De Smedt}, F. and Freer, J.},
  title = {Stochastic capture zone delineation within the {GLUE}--methodology:
	{C}onditioning on head observations},
  journal = {Water Resources Research},
  year = {2001},
  volume = {37},
  pages = {625--638},
  number = {3},
  abstract = {A stochastic methodology to evaluate the predictive uncertainty in
	well capture zones in heterogeneous aquifers with uncertain parameters
	is presented. The approach is based on the generalized likelihood
	uncertainty estimation methodology. The hydraulic conductivity is
	modeled as a random space function allowing for the uncertainty that
	stems from the imperfect knowledge of the parameters of the correlation
	structure. Parameters are sampled from prior distributions and are
	used for the generation of a large number of hydraulic conductivity
	fields, which are subsequently used to solve the groundwater flow
	equation. A likelihood is calculated for every simulation, based
	on some goodness-of-fit measure between simulated heads and available
	observations. Using inverse particle tracking, a capture zone is
	determined which is assigned the likelihood calculated for that particular
	simulation. Statistical analysis of the ensemble of all simulations
	enables the predictive uncertainty of the well capture zones to be
	defined. Results are presented for a hypothetical test case and different
	likelihood definitions used in the conditioning process. The results
	show that the delineated capture zones are most sensitive to the
	mean hydraulic conductivity and the variance, whereas the integral
	scale of the variogram is the parameter with the smallest influence.
	For all likelihood measures the prior uncertainty is reduced considerably
	by introducing the observation heads, but the reduction is most effective
	for the very selective likelihood definition. The method presented
	can be used in real applications to quantify the uncertainty in the
	location and extent of well capture zones when little or no information
	is available about the hydraulic properties, through the conditioning
	on head observations.},
  doi = {10.1029/2000WR900351},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen2005,
  author = {Feyen, L. and Caers, J.},
  title = {Quantifying geological uncertainty for flow and transport modeling
	in multi--modal heterogeneous formations},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {912--929},
  number = {6},
  abstract = {In this work, we address the problem of characterizing the heterogeneity
	and uncertainty of hydraulic properties for complex geological settings.
	Hereby, we distinguish between two scales of heterogeneity, namely
	the hydrofacies structure and the intrafacies variability of the
	hydraulic properties. We employ multiple-point geostatistics to characterize
	the hydrofacies architecture. The multiple-point statistics are borrowed
	from a training image that is designed to reflect the prior geological
	conceptualization. The intrafacies variability of the hydraulic properties
	is represented using conventional two-point correlation methods,
	more precisely, spatial covariance models under a multi-Gaussian
	spatial law. We address the different levels and sources of uncertainty
	in characterizing the subsurface heterogeneity, and explore their
	effect on groundwater flow and transport predictions. Typically,
	uncertainty is assessed by way of many images, termed realizations,
	of a fixed statistical model. However, in many cases, sampling from
	a fixed stochastic model does not adequately represent the space
	of uncertainty. It neglects the uncertainty related to the selection
	of the stochastic model and the estimation of its input parameters.
	We acknowledge the uncertainty inherent in the definition of the
	prior conceptual model of aquifer architecture and in the estimation
	of global statistics, anisotropy, and correlation scales. Spatial
	bootstrap is used to assess the uncertainty of the unknown statistical
	parameters. As an illustrative example, we employ a synthetic field
	that represents a fluvial setting consisting of an interconnected
	network of channel sands embedded within finer-grained floodplain
	material. For this highly non-stationary setting we quantify the
	groundwater flow and transport model prediction uncertainty for various
	levels of hydrogeological uncertainty. Results indicate the importance
	of accurately describing the facies geometry, especially for transport
	predictions.},
  doi = {10.1016/j.advwatres.2005.08.002},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyendankers2009,
  author = {Feyen, L. and Dankers, R.},
  title = {Impact of global warming on streamflow drought in {E}urope},
  journal = {Journal of Geophysical Research},
  year = {2009},
  volume = {114},
  pages = {1--17},
  number = {D17116},
  abstract = {Recent developments in climate modeling suggest that global warming
	is likely to favor conditions for the development of droughts in
	many regions of Europe. Studies evaluating possible changes in drought
	hazard typically have employed indices that are derived solely from
	climate variables such as temperature and precipitation, whereas
	many of the impacts of droughts are more related to hydrological
	variables such as river flow. This study examines the impact of global
	warming on streamflow drought in Europe by comparing low-flow predictions
	of a hydrological model driven by high-resolution regional climate
	simulations for the end of the previous century and for the end of
	this century based on the Special Report on Emissions Scenarios A2
	greenhouse gas emission scenario. For both time slices, low-flow
	characteristics were derived from the simulated streamflow series
	using extreme value analysis. More specifically, we employed the
	methods of block maxima and partial duration series to obtain minimum
	flows and flow deficits and fitted extreme value distributions by
	the maximum likelihood method. In order not to mix drought events
	with different physical causes the analysis was performed separately
	for the frost and nonfrost season. Results show that in the frost-free
	season streamflow droughts will become more severe and persistent
	in most parts of Europe by the end of this century, except in the
	most northern and northeastern regions. In the frost season, streamflow
	drought conditions will be of less importance under future climate
	conditions},
  doi = {10.1029/2008JD011438},
  tags = {Impacts}
}

@ARTICLE{feyen+al2012,
  author = {Feyen, L. and Dankers, R. and Bodis, K. and Salamon, P. and Barredo,
	J.},
  title = {{Fluvial flood risk in Europe in present and future climates}},
  journal = {Climatic Change},
  year = {2012},
  volume = {112},
  pages = {47--62},
  number = {1},
  abstract = {In this work we evaluate the implications of climate change for future
	fluvial flood risk in Europe, considering climate developments under
	the SRES A2 (high emission) and B2 (low emission) scenario. We define
	flood risk as the product of flood probability (or hazard), exposure
	of capital and population, and vulnerability to the effect of flooding.
	From the European flood hazard simulations of Dankers and Feyen (J
	Geophys Res 114:D16108. doi:10.1029/2008JD011523, 2009) discharges
	with return periods of 2, 5, 10, 20, 50, 100, 250 and 500 years were
	extracted and converted into flood inundation extents and depths
	using a planar approximation approach. Flood inundation extents and
	depths were transformed into direct monetary damage using country
	specific flood depth-damage functions and land use information. Population
	exposure was assessed by overlaying the flood inundation information
	with data on population density. By linearly interpolating damages
	and population exposed between the different return periods, we constructed
	damage and population exposure probability functions under present
	and future climate. From the latter expected annual damages (EAD)
	and expected annual population exposed (EAP) were calculated. To
	account for flood protection the damage and population exposure probability
	functions were truncated at design return periods based on the country
	GDP/capita. Results indicate that flood damages are projected to
	rise across much of Western Europe. Decreases in flood damage are
	consistently projected for north-eastern parts of Europe. For EU27
	as a whole, current EAD of approximately €6.4 billion is projected
	to amount to €14–21.5 billion (in constant prices of 2006) by the
	end of this century, depending on the scenario. The number of people
	affected by flooding is projected to rise by approximately 250,000
	to 400,000. Notwithstanding these numbers are subject to uncertainty,
	they provide an indication of potential future developments in flood
	risk in a changing climate.},
  doi = {10.1007/s10584-011-0339-7},
  owner = {rojasro},
  timestamp = {2012.10.17}
}

@ARTICLE{feyen2004,
  author = {Feyen, L. and Dessalegn, A. and {De Smedt}, F. and Gebremeskel, S.
	and Batelaan, O.},
  title = {Application of a {B}ayesian approach to stochastic delineation of
	capture zones},
  journal = {Ground Water},
  year = {2004},
  volume = {42},
  pages = {542--551},
  number = {4},
  abstract = {This paper presents a Bayesian Monte Carlo method for evaluating the
	uncertainty in the delineation of well capture zones and its application
	to a wellfield in a heterogeneous, multiaquifer system. In the method
	presented, Bayes' rule is used to update prior distributions for
	the unknown parameters of the stochastic model for the hydraulic
	conductivity, and to calculate probability-based weights for parameter
	realizations using head residuals. These weights are then assigned
	to the corresponding capture zones obtained using forward particle
	tracking. Statistical analysis of the set of weighted protection
	zones results in a probability distribution for the capture zones.
	The suitability of the Bayesian stochastic method for a multilayered
	system is investigated, using the wellfield Het Rot at Nieuwrode,
	Belgium, located in a three-layered aquifer system, as an example.
	The hydraulic conductivity of the production aquifer is modeled as
	a spatially correlated random function with uncertain parameters.
	The aquitard and overlying uncon-fined aquifer are assigned random,
	homogeneous conductivities. The stochastic results are compared with
	deterministic capture zones obtained with a calibrated model for
	the area. The predictions of the stochastic approach are more conservative
	and indicate that parameter uncertainty should be taken into account
	in the delineation of well capture zones.},
  doi = {10.1111/j.1745-6584.2004.tb02623.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen2003a,
  author = {Feyen, L. and G\'omez-Hern\'andez, J. and Ribeiro, P. and Beven,
	K. and {De Smedt}, F.},
  title = {A Bayesian approach to stochastic capture zone delineation incorporating
	tracer arrival times, conductivity measurements, and hydraulic head
	observations},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1126--1138},
  number = {5},
  abstract = {This paper presents a methodology to invert tracer arrival times and
	to incorporate travel time data in the delineation of well capture
	zones. Within a Bayesian framework the observed arrival times are
	used to obtain probability-based weights for each realization of
	the hydraulic conductivity field. Realizations that closely reproduce
	the observed arrival times are more likely to represent the “true”
	or real conductivity field than realizations yielding worse predictions,
	and are consequently characterized by a higher conditional probability.
	In the prediction of the capture zones the realizations are weighted
	by their respective probability. We combine the arrival time data
	with conductivity measurements and head observations to delineate
	stochastic capture zones. The conductivity measurements update the
	prior distributions specified for the unknown structural parameters
	of the conductivity field, and are used as conditioning data in the
	generation of conditional conductivity fields. The parameter values
	used to generate the conductivity realizations are sampled by Monte
	Carlo from the updated parameter distributions. The head and travel
	time observations are subsequently used to assign probability-based
	weights to the conductivity realizations by applying Bayes' theorem.
	The hyperparameters of the error model are assumed unknown, and their
	effect is accounted for by marginalization. A synthetic flow setup
	consisting of a single extraction well in a fully confined aquifer
	under a regional gradient is used to illustrate the method. We evaluate
	the relative worth of the different data types by introducing them
	sequentially in the inverse methodology. Results indicate that the
	different data types are complementary and that the travel time data
	are essential to improve the predictions of the capture zones. This
	is confirmed by the results for the case where uncertainty in the
	homogeneous porosity is accounted for.},
  doi = {10.1029/2002WR001544},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen2003c,
  author = {Feyen, L. and Ribeiro, P. and {De Smedt}, F. and Diggle, P.},
  title = {Stochastic delineation of capture zones: {C}lassical versus {B}ayesian
	approach},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {281},
  pages = {313--324},
  number = {4},
  abstract = {A Bayesian approach to characterize the predictive uncertainty in
	the delineation of time-related well capture zones in heterogeneous
	formations is presented and compared with the classical or non-Bayesian
	approach. The transmissivity field is modelled as a random space
	function and conditioned on distributed measurements of the transmissivity.
	In conventional geostatistical methods the mean value of the log
	transmissivity and the functional form of the covariance and its
	parameters are estimated from the available measurements, and then
	entered into the prediction equations as if they are the true values.
	However, this classical approach accounts only for the uncertainty
	that stems from the lack of ability to exactly predict the transmissivity
	at unmeasured locations. In reality, the number of measurements used
	to infer the statistical properties of the transmissvity field is
	often limited, which introduces error in the estimation of the structural
	parameters. The method presented accounts for the uncertainty that
	originates from the imperfect knowledge of the parameters by treating
	them as random variables. In particular, we use Bayesian methods
	of inference so as to make proper allowance for the uncertainty associated
	with estimating the unknown values of the parameters. The classical
	and Bayesian approach to stochastic capture zone delineation are
	detailed and applied to a hypothetical flow field. Two different
	sampling densities on a regular grid are considered to evaluate the
	effect of data density in both methods. Results indicate that the
	predictions of the Bayesian approach are more conservative.},
  doi = {10.1016/S0022-1694(03)00193-8},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen2002,
  author = {Feyen, L. and Ribeiro, P. and {De Smedt}, F. and Diggle, P.},
  title = {Bayesian methodology to stochastic capture zone determination: {C}onditioning
	on transmissivity measurements},
  journal = {Water Resources Research},
  year = {2002},
  volume = {38},
  pages = {1164--1174},
  number = {9},
  abstract = {A methodology to determine the uncertainty associated with the delineation
	of well capture zones in heterogeneous aquifers is presented. The
	log transmissivity field is modeled as a random space function and
	the Bayesian paradigm accounts for the uncertainty that stems from
	the imperfect knowledge about the parameters of the stochastic model.
	Unknown parameters are treated as random quantities and characterized
	by a prior probability distribution. Log transmissivity measurements
	are incorporated into Bayes' theorem, updating the prior distribution
	and yielding posterior estimates of the mean value and the covariance
	parameters of the log transmissivity. Conditional simulations of
	the log transmissivity field are generated using samples from the
	posterior distribution of the parameters, yielding samples from the
	predictive distribution of the log transmissivity field. The uncertainty
	in the model parameters is propagated to the predictive uncertainty
	of the capture zone by solving numerically the groundwater flow equation,
	followed by a semianalytical particle-tracking algorithm. The method
	is applied to a set of hypothetical flow fields for various sampling
	densities and assuming different levels of parameter uncertainty.
	Simulation results for all the sampling densities show no univocal
	relation between the predictive uncertainty of the capture zones
	and the level of parameter uncertainty. However, in general, the
	predictive uncertainty increases when parameter uncertainty is taken
	into account.},
  doi = {10.1029/2001WR000950},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen2003b,
  author = {Feyen, L. and Ribeiro, P. and G\'omez-Hern\'andez, J. and Beven,
	K. and {De Smedt}, F.},
  title = {Bayesian methodology for stochastic capture zone delineation incorporating
	transmissivity measurements and hydraulic head observations},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {271},
  pages = {156--170},
  number = {1--4},
  abstract = {The paper presents a methodology to estimate the uncertainty in the
	prediction of capture zones using transmissivity measurements and
	hydraulic head observations. We use a stochastic approach to parameterise
	the transmissivity field. By treating the parameters of the stochastic
	model as random variables we account for the fact that they are unknown
	and that a simple deterministic process cannot model their genesis.
	The method requires the definition of a prior probability density
	function for the mean value and for the covariance parameters of
	the log transmissivity field. In a first phase, log transmissivity
	measurements are incorporated into Bayes' theorem updating the prior
	densities to yield the posterior densities for the model parameters.
	Conditional realisations of the log transmissivity field are generated
	using parameter sets obtained by Monte Carlo sampling from the posterior
	parameter distributions. The second phase consists of updating the
	posterior probabilities of the conditional log transmissivity fields
	using the hydraulic head observations through a second application
	of Bayes' theorem. Then, for each realisation of the log transmissivity
	field, the capture zone is determined using particle tracking, which
	is weighted by the posterior probabilities of the respective log
	transmissivity field. The set of weighted capture zones results in
	a capture zone probability distribution. We show an application to
	a hypothetical flow system consisting of a single abstraction well
	in a confined aquifer under a regional background gradient and compare
	the worth of log transmissivity measurements and head observations
	by incorporating the different data types into the procedure sequentially.
	Results show that head observations are more effective in reducing
	the spread of the predictive capture zone distribution, whereas the
	transmissivity measurements are more valuable in predicting the actual
	location of the true capture zone.},
  doi = {10.1016/S0022-1694(02)00314-1},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{feyen+al2007,
  author = {Feyen, L. and Vrugt, J. and {\'O Nuall\'ain}, B. and {van der Knijff},
	J. and {de Roo}, A.},
  title = {Parameter optimization and uncertainty assessment for large-scale
	streamflow simulation with the {LISFLOOD} model},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {332},
  pages = {276--289},
  number = {3--4},
  month = {January},
  abstract = {This work addresses the calibration of the distributed rainfall-runoff
	model LISFLOOD and, in particular, the realistic quantification of
	parameter uncertainty and its effect on the prediction of river discharges
	for large European catchments. LISFLOOD is driven by meteorological
	input data and simulates river discharge in large drainage basins
	as a function of spatial information on topography, soils and land
	cover. Even though LISFLOOD is physically based to a certain extent,
	some processes are only represented in a lumped conceptual way. As
	a result, some parameters lack physical basis and cannot be directly
	inferred from quantities that can be measured. In the current LISFLOOD
	version five parameters need to be determined by calibration. We
	employ the Shuffled Complex Evolution Metropolis (SCEM-UA) global
	optimisation algorithm to automatically calibrate the model against
	daily discharge observations. The resulting posterior parameter distribution
	reflects the uncertainty about the model parameters after taking
	into account the discharge observations, and forms the basis for
	making probabilistic flow predictions. To overcome the computational
	burden the optimisation has been implemented using parallel computing.
	As an illustrative example, we demonstrate the methodology for the
	Meuse catchment upstream of Borgharen, covering approximately 21,000
	km2. Results demonstrate the capabilities of the SCEM-UA algorithm
	to efficiently evolve to the target posterior distribution and to
	identify, except for the lower groundwater zone time constant, the
	LISFLOOD calibration parameters using daily discharge observations.
	It should be noted that the posterior parameter distributions are
	based on the assumption of independent errors. Analysis of the residuals
	revealed there is a strong evidence to reject this assumption. This
	will impact on the parameter posterior distributions and also lead
	to underestimation of the prediction limits.},
  doi = {10.1016/j.jhydrol.2006.07.004},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{fienen2008,
  author = {Fienen, M. and Clemo, T. and Kitanidis, P.},
  title = {An interactive {B}ayesian geostatistical inverse protocol for hydraulic
	tomography},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W00B01},
  abstract = {Hydraulic tomography is a powerful technique for characterizing heterogeneous
	hydrogeologic parameters. An explicit trade-off between characterization
	based on measurement misfit and subjective characterization using
	prior information is presented. We apply a Bayesian geostatistical
	inverse approach that is well suited to accommodate a flexible model
	with the level of complexity driven by the data and explicitly considering
	uncertainty. Prior information is incorporated through the selection
	of a parameter covariance model characterizing continuity and providing
	stability. Often, discontinuities in the parameter field, typically
	caused by geologic contacts between contrasting lithologic units,
	necessitate subdivision into zones across which there is no correlation
	among hydraulic parameters. We propose an interactive protocol in
	which zonation candidates are implied from the data and are evaluated
	using cross validation and expert knowledge. Uncertainty introduced
	by limited knowledge of dynamic regional conditions is mitigated
	by using drawdown rather than native head values. An adjoint state
	formulation of MODFLOW-2000 is used to calculate sensitivities which
	are used both for the solution to the inverse problem and to guide
	protocol decisions. The protocol is tested using synthetic two-dimensional
	steady state examples in which the wells are located at the edge
	of the region of interest.},
  doi = {10.1029/2007WR006730},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{fienen2009,
  author = {Fienen, M. and Hunt, R. and Krabbenhoft, D. and Clemo, T.},
  title = {Obtaining parsimonious hydraulic conductivity fields using head and
	transport observations: {A Bayesian} geostatistical parameter estimation
	approach},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W08405},
  abstract = {Flow path delineation is a valuable tool for interpreting the subsurface
	hydrogeochemical environment. Different types of data, such as groundwater
	flow and transport, inform different aspects of hydrogeologic parameter
	values (hydraulic conductivity in this case) which, in turn, determine
	flow paths. This work combines flow and transport information to
	estimate a unified set of hydrogeologic parameters using the Bayesian
	geostatistical inverse approach. Parameter flexibility is allowed
	by using a highly parameterized approach with the level of complexity
	informed by the data. Despite the effort to adhere to the ideal of
	minimal a priori structure imposed on the problem, extreme contrasts
	in parameters can result in the need to censor correlation across
	hydrostratigraphic bounding surfaces. These partitions segregate
	parameters into facies associations. With an iterative approach in
	which partitions are based on inspection of initial estimates, flow
	path interpretation is progressively refined through the inclusion
	of more types of data. Head observations, stable oxygen isotopes
	(18O/16O ratios), and tritium are all used to progressively refine
	flow path delineation on an isthmus between two lakes in the Trout
	Lake watershed, northern Wisconsin, United States. Despite allowing
	significant parameter freedom by estimating many distributed parameter
	values, a smooth field is obtained.},
  doi = {10.1029/2008WR007431},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{fleig+al2006,
  author = {Fleig, A. K. and Tallaksen, L. M. and Hisdal, H. and Demuth, S.},
  title = {A global evaluation of streamflow drought characteristics},
  journal = {Hydrology and Earth System Sciences},
  year = {2006},
  volume = {10},
  pages = {535--552},
  number = {4},
  abstract = {How drought is characterised depends on the purpose and region of
	the study and the available data. In case of regional applications
	or global comparison a standardisation of the methodology to characterise
	drought is preferable. In this study the threshold level method in
	combination with three common pooling procedures is applied to daily
	streamflow series from a wide range of hydrological regimes. Drought
	deficit characteristics, such as drought duration and deficit volume,
	are derived, and the methods are evaluated for their applicability
	for regional studies. Three different pooling procedures are evaluated:
	the moving-average procedure (MA-procedure), the inter-event time
	method (IT-method), and the sequent peak algorithm (SPA). The MA-procedure
	proved to be a flexible approach for the different series, and its
	parameter, the averaging interval, can easily be optimised for each
	stream. However, it modifies the discharge series and might introduce
	dependency between drought events. For the IT-method it is more difficult
	to find an optimal value for its parameter, the length of the excess
	period, in particular for flashy streams. The SPA can only be recommended
	as pooling procedure for the selection of annual maximum series of
	deficit characteristics and for very low threshold levels to ensure
	that events occurring shortly after major events are recognized.
	Furthermore, a frequency analysis of deficit volume and duration
	is conducted based on partial duration series of drought events.
	According to extreme value theory, excesses over a certain limit
	are Generalized Pareto (GP) distributed. It was found that this model
	indeed performed better than or equally to other distribution models.
	In general, the GP-model could be used for streams of all regime
	types. However, for intermittent streams, zero-flow periods should
	be treated as censored data. For catchments with frost during the
	winter season, summer and winter droughts have to be analysed separately.},
  doi = {10.5194/hess-10-535-2006}
}

@ARTICLE{foglia2009,
  author = {Foglia, L. and Hill, M. and Mehl, S. and Burlando, P.},
  title = {Sensitivity analysis, calibration, and testing of a distributed hydrological
	model using error--based weighting and one objective function},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W06427},
  abstract = {We evaluate the utility of three interrelated means of using data
	to calibrate the fully distributed rainfall-runoff model TOPKAPI
	as applied to the Maggia Valley drainage area in Switzerland. The
	use of error-based weighting of observation and prior information
	data, local sensitivity analysis, and single-objective function nonlinear
	regression provides quantitative evaluation of sensitivity of the
	35 model parameters to the data, identification of data types most
	important to the calibration, and identification of correlations
	among parameters that contribute to nonuniqueness. Sensitivity analysis
	required only 71 model runs, and regression required about 50 model
	runs. The approach presented appears to be ideal for evaluation of
	models with long run times or as a preliminary step to more computationally
	demanding methods. The statistics used include composite scaled sensitivities,
	parameter correlation coefficients, leverage, Cook's D, and DFBETAS.
	Tests suggest predictive ability of the calibrated model typical
	of hydrologic models.},
  doi = {10.1029/2008WR007255},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{foglia2007,
  author = {Foglia, L. and Mehl, S. and Hill, M. and Perona, P. and Burlando,
	P.},
  title = {Alternative ground water models using cross--validation and other
	methods},
  journal = {Ground Water},
  year = {2007},
  volume = {45},
  pages = {627--641},
  number = {5},
  abstract = {Many methods can be used to test alternative ground water models.
	Of concern in this work are methods able to (1) rank alternative
	models (also called model discrimination) and (2) identify observations
	important to parameter estimates and predictions (equivalent to the
	purpose served by some types of sensitivity analysis). Some of the
	measures investigated are computationally efficient; others are computationally
	demanding. The latter are generally needed to account for model nonlinearity.
	The efficient model discrimination methods investigated include the
	information criteria: the corrected Akaike information criterion,
	Bayesian information criterion, and generalized cross-validation.
	The efficient sensitivity analysis measures used are dimensionless
	scaled sensitivity (DSS), composite scaled sensitivity, and parameter
	correlation coefficient (PCC); the other statistics are DFBETAS,
	Cook's D, and observation-prediction statistic. Acronyms are explained
	in the introduction. Cross-validation (CV) is a computationally intensive
	nonlinear method that is used for both model discrimination and sensitivity
	analysis. The methods are tested using up to five alternative parsimoniously
	constructed models of the ground water system of the Maggia Valley
	in southern Switzerland. The alternative models differ in their representation
	of hydraulic conductivity. A new method for graphically representing
	CV and sensitivity analysis results for complex models is presented
	and used to evaluate the utility of the efficient statistics. The
	results indicate that for model selection, the information criteria
	produce similar results at much smaller computational cost than CV.
	For identifying important observations, the only obviously inferior
	linear measure is DSS; the poor performance was expected because
	DSS does not include the effects of parameter correlation and PCC
	reveals large parameter correlations.},
  doi = {10.1111/j.1745-6584.2007.00341.x},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{forest+al2002,
  author = {Forest, C. and Stone, P. and Sokolov, A. and Allen, M. and Webster,
	M.},
  title = {Quantifying uncertainties in climate system properties with the use
	of recent climate observations},
  journal = {Science},
  year = {2002},
  volume = {295},
  pages = {113--117},
  number = {5552},
  abstract = {We derive joint probability density distributions for three key uncertain
	properties of the climate system, using an optimal fingerprinting
	approach to compare simulations of an intermediate complexity climate
	model with three distinct diagnostics of recent climate observations.
	On the basis of the marginal probability distributions, the 5 to
	95% confidence intervals are 1.4 to 7.7 kelvin for climate sensitivity
	and -0.30 to -0.95 watt per square meter for the net aerosol forcing.
	The oceanic heat uptake is not well constrained, but ocean temperature
	observations do help to constrain climate sensitivity. The uncertainty
	in the net aerosol forcing is much smaller than the uncertainty range
	for the indirect aerosol forcing alone given in the Intergovernmental
	Panel on Climate Change Third Assessment Report.},
  doi = {10.1126/science.1064419},
  file = {:E\:\\rojasro\\My Documents\\articles\\Quantifying uncertainties in climate system properties with the use of recent climate observations (Forest et al. 2002).pdf:PDF},
  keywords = {SURFACE AIR-TEMPERATURE, MODEL, OCEAN},
  tags = {Uncertainty}
}

@TECHREPORT{fowler+al2005,
  author = {Fowler, H. and Baran, N. and Mouvet, C. and Gutierrez, A.},
  title = {Report detailing selection of {GCM} outputs and pre--processed observed
	rainfall and {PE} data for calibration of catchment models},
  institution = {University of Newcastle, School of Civil Engineering and Geosciences},
  year = {2005},
  type = {Aquaterra Deliverable {H}1.1},
  address = { Newcastle upon Tyne, NE1 7RU, UK},
  tags = {Downscaling}
}

@ARTICLE{fowler+al2007a,
  author = {Fowler, H. and Blenkinsop, S. and Tebaldi, C.},
  title = {Linking climate change modelling to impacts studies: {R}ecent advances
	in downscaling techniques for hydrological modelling},
  journal = {International Journal of Climatology},
  year = {2007},
  volume = {27},
  pages = {1547--1578},
  number = {12},
  abstract = {There is now a large published literature on the strengths and weaknesses
	of downscaling methods for different climatic variables, in different
	regions and seasons. However, little attention is given to the choice
	of downscaling method when examining the impacts of climate change
	on hydrological systems. This review paper assesses the current downscaling
	literature, examining new developments in the downscaling field specifically
	for hydrological impacts. Sections focus on the downscaling concept;
	new methods; comparative methodological studies; the modelling of
	extremes; and the application to hydrological impacts.Consideration
	is then given to new developments in climate scenario construction
	which may offer the most potential for advancement within the lsquodownscaling
	for hydrological impactsrsquo community, such as probabilistic modelling,
	pattern scaling and downscaling of multiple variables and suggests
	ways that they can be merged with downscaling techniques in a probabilistic
	climate change scenario framework to assess the uncertainties associated
	with future projections. Within hydrological impact studies there
	is still little consideration given to applied research; how the
	results can be best used to enable stakeholders and managers to make
	informed, robust decisions on adaptation and mitigation strategies
	in the face of many uncertainties about the future. It is suggested
	that there is a need for a move away from comparison studies into
	the provision of decision-making tools for planning and management
	that are robust to future uncertainties; with examination and understanding
	of uncertainties within the modelling system. Copyright \copyright
	2007 Royal Meteorological Society},
  address = {Water Resource Systems Research Laboratory, School of Civil Engineering
	and Geosciences, Newcastle University, UK; Institute for the Study
	of Society and Environment, National Center for Atmospheric Research,
	Boulder, CO, USA},
  citeulike-article-id = {3089284},
  doi = {10.1002/joc.1556},
  keywords = {climate, downscaling, ensemble, extreme, hydrology, impacts, intercomparison,
	model, policy, precipitation, regional},
  posted-at = {2008-08-05 19:22:35},
  priority = {2},
  tags = {Downscaling}
}

@ARTICLE{fowlerekstrom2009,
  author = {Fowler, H. and Ekstr\"om, M.},
  title = {Multi--model ensemble estimates of climate change impacts on {UK}
	seasonal precipitation extremes},
  journal = {International Journal of Climatology},
  year = {2009},
  volume = {29},
  pages = {385--416},
  number = {3},
  abstract = {Thirteen regional climate model (RCM) integrations from the Prediction
	of Regional Scenarios and Uncertainties for Defining European Climate
	change risks and Effects (PRUDENCE) ensemble are used together with
	extreme value analysis to assess changes to seasonal precipitation
	extremes in nine UK rainfall regions by 2070-2100 under the SRES
	A2 emissions scenario. Model weights are based on similarities between
	observed and modelled UK extreme precipitation calculated for a combination
	of (1) spatial characteristics: the semi-variogram parameters sill
	and range, and (2) the discrepancy in the regional median seasonal
	maxima. These weights are used to combine individual RCM bootstrap
	samples to provide multi-model ensemble estimates of percent change
	in the return value magnitudes of regional extremes. The contribution
	of global climate model (GCM) and RCM combinations to model structural
	uncertainty is also investigated. The multi-model ensembles project
	increases across the UK in winter, spring and autumn extreme precipitation;
	although there is uncertainty in the absolute magnitude of increases,
	these range from 5 to 30% depending upon region and season. In summer,
	model predictions span the zero change line, although there is low
	confidence due to poor model performance. RCM performance is shown
	to be highly variable; extremes are well simulated in winter and
	very poorly simulated in summer. The ensemble distributions are wider
	(projections are more uncertain) for shorter duration extremes (e.g.
	1 day) and higher return periods (e.g. 25 year). There are rather
	limited differences in the weighted and unweighted multi-model ensembles,
	perhaps a consequence of the lack of model independence between ensemble
	members. The largest contribution to uncertainty in the multi-model
	ensembles comes from the lateral boundary conditions used by RCMs
	included in the ensemble. Therefore, the uncertainty bounds shown
	here are conservative despite the relatively large number of RCMs
	contributing to the multi-model ensemble distribution},
  doi = {10.1002/joc.1827},
  keywords = {PRUDENCE},
  tags = {Multimodel - Ensambles, Precipitation Changes}
}

@ARTICLE{fowler+al2007c,
  author = {Fowler, H. and Ekstr\"om, M. and Blenkinsop, S. and Smith, A.},
  title = {Estimating change in extreme European precipitation using a multi-model
	ensemble},
  journal = {Journal of Geophysical Research},
  year = {2007},
  volume = {112},
  pages = {D18104},
  abstract = {Using the results from multi-model ensembles enables the assessment
	of model uncertainty in present and future estimates of extremes
	and the production of probabilities for regional or local scale change.
	Six regional climate model (RCM) integrations from the PRUDENCE ensemble
	are used together with extreme value analysis to assess changes to
	precipitation extremes over Europe by 2070-2100 under the SRES A2
	emissions scenario, investigating the contribution of the formulations
	of global (GCM) and regional climate models to scenario uncertainty.
	RCM ability to simulate precipitation extremes is evaluated for a
	UK case study. RCMs are shown to underestimate 1 day return values
	but reasonably simulate longer duration (5 or 10 day) extremes. A
	multi-model approach by which probabilities can be produced for regional
	or local scale change in extremes is then developed. A key result
	is that all RCMs project increases in the magnitude of short and
	long duration extreme precipitation for most of Europe. Individual
	model projections vary considerably but are independent of changes
	in mean precipitation. The magnitude of change is strongly influenced
	by the driving GCM but moderated by the RCM, which also influences
	spatial pattern. Therefore, when designing future ensemble experiments
	(a) the number of GCMs should at least equal the number of RCMs;
	(b) if spatial pattern is important then integrations from different
	RCMs should be incorporated. For impact studies, both the resolution
	and number of models in the ensemble will influence projections of
	change. The use of a multi-model approach therefore provides more
	robust estimates.},
  doi = {10.1029/2007JD008619},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{fowlerkilsby2007,
  author = {Fowler, H. and Kilsby, C.},
  title = {Using regional climate model data to simulate historical and future
	river flows in northwest {E}ngland},
  journal = {Climatic Change},
  year = {2007},
  volume = {80},
  pages = {337--367},
  number = {3--4},
  month = {February},
  abstract = {Daily rainfall and temperature data were extracted from the multi-ensemble
	HadRM3H regional climate model (RCM) integrations for control (1960--1990)
	and future (2070--2100) time-slices. This dynamically downscaled
	output was bias-corrected on observed mean statistics and used as
	input to hydrological models calibrated for eight catchments which
	are critical water resources in northwest England. Simulated daily
	flow distributions matched observed from Q95 to Q5, suggesting that
	RCM data can be used with some confidence to examine future changes
	in flow regime. Under the SRES A2 (UKCIP02 Medium-High) scenario,
	annual runoff is projected to increase slightly at high elevation
	catchments, but reduce by 16% at lower elevations. Impacts on monthly
	flow distribution are significant, with summer reductions of 40--80%
	of 1961--90 mean flow, and winter increases of up to 20%. This changing
	seasonality has a large impact on low flows, with Q95 projected to
	decrease in magnitude by 40--80% in summer months, with serious consequences
	for water abstractions and river ecology. In contrast, high flows
	(> Q5) are projected to increase in magnitude by up to 25%, particularly
	at high elevation catchments, providing an increased risk of flooding
	during winter months. These changes will have implications for management
	of water resources and ecologically important areas under the EU
	Water Framework Directive},
  citeulike-article-id = {1131457},
  doi = {10.1007/s10584-006-9117-3},
  issn = {0165-0009},
  posted-at = {2007-02-28 20:45:36},
  publisher = {Springer},
  tags = {Downscaling}
}

@ARTICLE{fowler+al2007b,
  author = {Fowler, H. and Kilsby, C. and Stunell, J.},
  title = {Modelling the impacts of projected future climate change on water
	resources in north--west {E}ngland},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1115--1124},
  number = {3},
  abstract = {Over the last two decades, the frequency of water resource drought
	in the UK, coupled with the more recent pan-European drought of 2003,
	has increased concern over changes in climate. Using the UKClP02
	Medium-High (SRES A2) scenario for 2070-2100, this study investigates
	the impact of climate change on the operation of the Integrated Resource
	Zone (IRZ), a complex conjunctive-use water supply system in north-western
	England. The results indicate that the contribution of individual
	sources to yield may change substantially but that overall yield
	is reduced by only 18%. Notwithstanding this significant effect on
	water supply, the flexibility of the system enables it to meet modelled
	demand for much of the time under the future climate scenario, even
	without a change in system management, but at significant expense
	for pumping additional abstraction from lake and borehole sources.
	This research provides a basis for the future planning and management
	of the complex water resource system in the north-west of England.},
  doi = {10.5194/hess-11-1115-2007},
  keywords = {water resources, drought, north-west England, UKCIP02, regional climate
	model, climate change impacts, REGIONAL CLIMATE, CHANGE SCENARIOS,
	BASIN HYDROLOGY, RIVER-BASIN, EXTREME RAINFALL, UNITED-STATES, ARNO
	RIVER, VARIABILITY, PRECIPITATION, RUNOFF},
  tags = {Impacts}
}

@ARTICLE{fowlerwilby2010,
  author = {Fowler, H. and Wilby, R.},
  title = {Detecting changes in seasonal precipitation extremes using regional
	climate model projections: {I}mplications for managing fluvial flood
	risk},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W03525},
  number = {3},
  abstract = {There is growing evidence of coherent, global patterns of change in
	annual precipitation and runoff with high latitudes experiencing
	increases consistent with climate model projections. This paper describes
	a methodology for estimating detection times for changes in seasonal
	precipitation extremes. The approach is illustrated using changes
	in UK precipitation projected by the European Union PRUDENCE climate
	model ensemble. We show that because of high variability from year
	to year and confounding factors, detection of anthropogenic climate
	change at regional scales is not generally expected for decades to
	come. Overall, the earliest detection times were found for 10 day
	winter precipitation totals with 10 year return period in SW England.
	In this case, formal detection could be possible within a decade
	from now if the climate model projections are realized. The outlook
	for changes in summer flash flood risk is highly uncertain. Our analysis
	further demonstrates that existing precautionary allowances for climate
	change used for flood management may not be sufficiently robust in
	NE England and east Scotland. These findings imply that for certain
	types of flood mechanism, adaptation decisions might have to be taken
	in advance of formally detected changes in flood risk. This reinforces
	the case for long-term environmental monitoring and reporting of
	climate change indices at {\grqq}sentinelâ€? locations},
  doi = {10.1029/2008WR007636},
  tags = {Precipitation Changes}
}

@ARTICLE{fox2009,
  author = {Fox, J.},
  title = {{Aspects of the Social Organization and Trajectory of the R Project}},
  journal = {The R Journal},
  year = {2009},
  volume = {1},
  pages = {5--13},
  number = {2},
  month = {December},
  url = {http://journal.r-project.org/archive/2009-2/RJournal_2009-2_Fox.pdf}
}

@ARTICLE{franchini+al1998,
  author = {Franchini, M. and Galeati, G. and Berra, S.},
  title = {Global optimization techniques for the calibration of conceptual
	rainfall-runoff models},
  journal = {Hydrological Sciences Journal},
  year = {1998},
  volume = {43},
  pages = {443--458},
  number = {3},
  abstract = {In this study we present the results of the comparison of three different
	algorithms: the Genetic Algorithm coupled with Sequential Quadratic
	Programming (GA-SQP), the Pattern Search also coupled with SQP (PS-SQP)
	and the Shuffled Complex Evolution (SCE-UA), The analyses were conducted
	using a conceptual rainfall-runoff model applied both to a single
	basin and to a complex basin, For both types of basin, a theoretical
	case without model and data errors was considered, in which the true
	values of the parameters are known a priori, and several real-world
	cases where model and data errors exist. With reference to the single
	basin, the SCE-UA algorithm was the most reliable while the other
	two algorithms gave solutions equivalent to those of the SCE-UA in
	the theoretical case, but in the real-world cases they showed an
	increasing tendency (particularly the PS-SQP) to be trapped in local
	minima. With reference to the complex basin, none of the three algorithms
	identified the exact solution in the theoretical case. However, the
	SCE-UA was the one which systematically approximated it better than
	the others, In the real-world case its solutions were stable but
	characterized by many parameter values set at the boundary of their
	own range. The other two algorithms produced a very unstable set
	of parameters.},
  doi = {10.1080/02626669809492137},
  keywords = {GENETIC ALGORITHM},
  tags = {Calibration}
}

@ARTICLE{francos+al2003,
  author = {Francos, A. Elorza, F. and Bouraoui, F. and Bidoglio, G. and Galbiati,
	L.},
  title = {Sensitivity analysis of distributed environmental simulation models:
	understanding the model behaviour in hydrological studies at the
	catchment scale},
  journal = {Reliability Engineering \& System Safety},
  year = {2003},
  volume = {79},
  pages = {205--218},
  number = {2},
  abstract = {The development of new hydrological simulation tools allows for the
	modelling of large hydrological catchments, with the aim of comprehensive
	management of the water resources, control of diffuse pollution processes,
	such as the fate of agricultural fertilizants and finally, with purposes
	of economical optimization of the crop yields as a function of the
	expected climate, the watershed characteristics and the socio-economical
	conditions of the region where the catchment is located. This paper
	describes the sensitivity analysis of a hydrological distributed
	model applied in one large European watershed by using a two-step
	procedure. Firstly, it allows for the consideration of a huge input
	parameter data set by using an implementation of the Morris screening
	procedure, eschewing the huge computational requirements arising
	from the necessary repetitive simulations. In the second step it
	provides quantitative estimations of sensitivity in terms of variance
	decomposition procedures based upon the FAST method for both the
	hydrological and the water quality determinants.},
  doi = {10.1016/S0951-8320(02)00231-4},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{franks1998,
  author = {Franks, S. and Gineste, P. and Beven, K. and Merot, P.},
  title = {On constraining the predcitions of a distributed model: {T}he incorporation
	of fuzzy estimates of saturated areas into the calibration process},
  journal = {Water Resources Research},
  year = {1998},
  volume = {34},
  pages = {787--797},
  number = {4},
  abstract = {Distributed hydrological models are generally overparameterized, resulting
	in the possibility of multiple parameterizations from many areas
	of the parameter space providing acceptable fits to observed data.
	In this study, TOPMODEL parameterizations are conditioned on discharges,
	and then further conditioned on estimates of saturated areas derived
	from ERS-I synthetic aperture radar (SAR) images combined with the
	In (?/tan ?) topographic index, and compared to ground truth saturation
	measurements made in one small subcatchment. The uncertainty associated
	with the catchment-wide predictions of saturated area is explicitly
	incorporated into the conditioning through the weighting of estimates
	within a fuzzy set framework. The predictive uncertainty associated
	with the parameterizations is then assessed using the generalized
	likelihood uncertainty estimation (GLUE) methodology. It is shown
	that despite the uncertainty in the predictions of saturated area
	the methodology can reject many previously acceptable parameterizations
	with the consequence of a marked reduction in the acceptable range
	of a catchment average transmissivity parameter and of improved predictions
	of some discharge events.},
  doi = {10.1029/97WR03041},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{freer1996,
  author = {Freer, J. and Beven, K. and Ambroise, B.},
  title = {Bayesian estimation of uncertainty in runoff prediction and the value
	of data: {A}n application of the {GLUE} approach},
  journal = {Water Resources Research},
  year = {1996},
  volume = {32},
  pages = {2161--2173},
  number = {7},
  abstract = {This paper addresses the problem of evaluating the predictive uncertainty
	of TOPMODEL using the Bayesian Generalised Likelihood Uncertainty
	Estimation (GLUE) methodology in an application to the small Ringelbach
	research catchment in the Vosges, France. The wide range of parameter
	sets giving acceptable simulations is demonstrated, and uncertainty
	bands are presented based on different likelihood measures. It is
	shown how the distributions of predicted discharges are non-Gaussian
	and vary in shape through time and with discharge. Updating of the
	likelihood weights using Bayes equation is demonstrated after each
	year of record and it is shown how the additional data can be evaluated
	in terms of the way they constrain the uncertainty bands.},
  doi = {10.1029/95WR03723},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{freeze1975,
  author = {Freeze, R.},
  title = {A stochastic--conceptual analysis of one-dimensional groundwater
	flow in non-uniform, homogeneus media},
  journal = {Water Resources Research},
  year = {1975},
  volume = {11},
  pages = {725-741},
  number = {5},
  abstract = {The most realistic representation of a naturally occurring porous
	medium is a stochastic set of macroscopic elements in which the values
	of the three basic hydrogeologic parameters (hydraulic conductivity
	K, compressibility ?, and porosity n) are defined by frequency distributions.
	A homogeneous formation under this representation is one in which
	the frequency distributions do not change through space. All soils
	and geologic formations, even the ones that are homogeneous, show
	random variations in the values of the hydrogeological parameters
	through space; that is, they are nonuniform, and a measure of the
	nonuniformity is provided by the standard deviation of the frequency
	distributions. If K and ? are log normally distributed and n is normally
	distributed, and if we define Y = log K and C = log ?, then the parameters
	Y, C, and n can be generated from a multivariate normal density function
	with means ? y , ? c , and ? n , standard deviations ? y , ? c ,
	and ? n , and correlation coefficients ? yc , ? yn , and ? cn The
	analysis of groundwater flow in nonuniform media requires a stochastic-conceptual
	approach in which the effects of stochastic parameter distributions
	on predicted hydraulic heads are analyzed with the aid of a set of
	Monte Carlo solutions to the pertinent boundary value problems. In
	this study, two one-dimensional saturated flow problems are analyzed:
	steady state flow between two specified heads and transient consolidation
	of a clay layer. The primary output is the statistical distribution
	of hydraulic head ?¯, through space and time, as indicated by the
	mean values ? ¯ ( x , t ) ) and their standard deviations S ?¯(x,
	t) Results show that the standard deviations of the input hydrogeologic
	parameters, particularly ? y and ? c , are important index properties;
	changes in their values lead to different responses for ? ¯ ( x ,
	t ) ) even when the means ? y , ? c , and ? n are fixed. The degree
	of uncertainty associated with hydraulic head predictions increases
	as the degree of nonuniformity of the porous medium increases. For
	large values of ? y and ? c it becomes virtually impossible to obtain
	meaningful hydraulic head predictions. For transient flow the output
	distribution of hydraulic head values is almost never normal; in
	some cases it approaches a uniform distribution. The results of this
	study throw into question the validity of the hidden assumption that
	underlies all deterministic groundwater modeling; namely, that it
	is possible to select a single value for each flow parameter in a
	homogeneous but nonuniform medium that is somehow representative
	and hence define an ‘equivalent’ uniform porous medium. For transient
	flow there may be no way to define an equivalent medium. The fact
	that nine index parameters rather than three are required to describe
	a nonuniform geologic formation, the large uncertainties in predicted
	hydraulic heads for relatively simple flow problems in nonuniform
	soils, and the contention that there may be no simple way to define
	an equivalent uniform porous medium all have important implications
	in the development of groundwater flow theory and in its most fundamental
	applications.},
  doi = {10.1029/WR011i005p00725},
  owner = {rojasro},
  timestamp = {2009.09.15}
}

@ARTICLE{freni2012,
  author = {Gabriele Freni and Giorgio Mannina},
  title = {Uncertainty estimation of a complex water quality model: The influence
	of Box–Cox transformation on Bayesian approaches and comparison with
	a non-Bayesian method},
  journal = {Physics and Chemistry of the Earth, Parts A/B/C},
  year = {2012},
  volume = {42–-44},
  pages = {31--41},
  number = {0},
  abstract = {In urban drainage modelling, uncertainty analysis is of undoubted
	necessity. However, uncertainty analysis in urban water-quality modelling
	is still in its infancy and only few studies have been carried out.
	Therefore, several methodological aspects still need to be experienced
	and clarified especially regarding water quality modelling. The use
	of the Bayesian approach for uncertainty analysis has been stimulated
	by its rigorous theoretical framework and by the possibility of evaluating
	the impact of new knowledge on the modelling predictions. Nevertheless,
	the Bayesian approach relies on some restrictive hypotheses that
	are not present in less formal methods like the Generalised Likelihood
	Uncertainty Estimation (GLUE). One crucial point in the application
	of Bayesian method is the formulation of a likelihood function that
	is conditioned by the hypotheses made regarding model residuals.
	Statistical transformations, such as the use of Box–Cox equation,
	are generally used to ensure the homoscedasticity of residuals. However,
	this practice may affect the reliability of the analysis leading
	to a wrong uncertainty estimation. The present paper aims to explore
	the influence of the Box–Cox equation for environmental water quality
	models. To this end, five cases were considered one of which was
	the “real” residuals distributions (i.e. drawn from available data).
	The analysis was applied to the Nocella experimental catchment (Italy)
	which is an agricultural and semi-urbanised basin where two sewer
	systems, two wastewater treatment plants and a river reach were monitored
	during both dry and wet weather periods. The results show that the
	uncertainty estimation is greatly affected by residual transformation
	and a wrong assumption may also affect the evaluation of model uncertainty.
	The use of less formal methods always provide an overestimation of
	modelling uncertainty with respect to Bayesian method but such effect
	is reduced if a wrong assumption is made regarding the residuals
	distribution. If residuals are not normally distributed, the uncertainty
	is over-estimated if Box–Cox transformation is not applied or non-calibrated
	parameter is used.},
  doi = {10.1016/j.pce.2011.08.024},
  issn = {1474-7065},
  keywords = {Bayesian inference}
}

@ARTICLE{freni+al2009,
  author = {Gabriele Freni and Giorgio Mannina and Gaspare Viviani},
  title = {Urban runoff modelling uncertainty: Comparison among Bayesian and
	pseudo-Bayesian methods},
  journal = {Environmental Modelling \& Software},
  year = {2009},
  volume = {24},
  pages = {1100--1111},
  number = {9},
  abstract = {Urban stormwater quality modelling plays a central role in evaluation
	of the quality of the receiving water body. However, the complexity
	of the physical processes that must be simulated and the limited
	amount of data available for calibration may lead to high uncertainty
	in the model results. This study was conducted to assess modelling
	uncertainty associated with catchment surface pollution evaluation.
	Eight models were compared based on the results of a case study in
	which there was limited data available for calibration. Uncertainty
	analysis was then conducted using three different methods: the Bayesian
	Monte Carlo method, the GLUE pseudo-Bayesian method and the GLUE
	method revised by means of a formal distribution of residuals between
	the model and measured data (GLUE_f). The uncertainty assessment
	of the models enabled evaluation of the advantages and limitations
	of the three methodologies adopted. The models were then tested using
	the quantity–quality data gathered for the Fossolo catchment in Bologna,
	Italy. The results revealed that all of the models evaluated here
	provided good calibration results, even if the model reliability
	(in terms of related uncertainty) varied, which suggests the adoption
	of a specific modelling approach with respect to the others. Additionally,
	a comparison of uncertainty analysis approaches showed that, regarding
	the models evaluated here, the classical Bayesian method is more
	effective at discriminating models according to their uncertainty,
	but the GLUE approach performs similarly when it is based on the
	same founding assumptions as the Bayesian method.},
  doi = {10.1016/j.envsoft.2009.03.003},
  issn = {1364-8152},
  keywords = {Urban stormwater modelling}
}

@ARTICLE{fritz1981,
  author = {Fritz, P. and Suzuki, O. and Silva, C. and Salati, E.},
  title = {Isotope hydrology of groundwaters in the {P}ampa del {T}amarugal,
	{C}hile},
  journal = {Journal of Hydrology},
  year = {1981},
  volume = {53},
  pages = {161--184},
  number = {1--2},
  abstract = {The water resources in northern Chile are extremely scarce and development
	is limited by this. This paper discusses the isotope hydrology of
	groundwaters in the Pampa del Tamarugal — an area of extreme aridity
	(rainfall < 1 mm/yr.) — whose groundwater resources are mined for
	the town of Iquique, as well as for industrial and agricultural purposes.
	The aim of the project was to obtain information on modern recharge
	and to delineate, if possible recharge environments. To obtain the
	necessary background information a precipitation survey in the high
	Andes, as well as spring- and surface-water studies were carried
	out. The results show that a well-defined meteoric water line exists
	where ?2H = (7.8 ? 18O + 10.3)%, and altitude effects depend on air-mass
	movements and cannot be defined without a broader regional and detailed
	sampling programme. However, it is still possible to assign maximum
	altitudes of recharge to springs in the Andes and at the eastern
	border of the Pampa del Tamarugal. Comparison of these data with
	groundwater compositions show, that these groundwaters originate
	from infiltrating surface water rather than directly infiltrated
	precipitation. A dependence of individual groundwater systems on
	specific quebradas (river valleys) is recognized. However, low 14C
	activities indicate that most of the waters pumped today are fossil
	and at least several hundreds if not thousands of years old. Some
	minor subsurface recharge does occur at the foot of the Andes, especially
	at Pica where high-altitude waters discharge, but even there groundwater
	appears to be a diminishing resource.},
  doi = {10.1016/0022-1694(81)90043-3},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{gomezhernandez2006,
  author = {G\'omez-Hern\'andez, J.},
  title = {Complexity},
  journal = {Ground Water},
  year = {2006},
  volume = {44},
  pages = {782--785},
  number = {6},
  abstract = {It is difficult to define complexity in modeling. Complexity is often
	associated with uncertainty since modeling uncertainty is an intrinsically
	difficult task. However, modeling uncertainty does not require, necessarily,
	complex models, in the sense of a model requiring an unmanageable
	number of degrees of freedom to characterize the aquifer. The relationship
	between complexity, uncertainty, heterogeneity, and stochastic modeling
	is not simple. Aquifer models should be able to quantify the uncertainty
	of their predictions, which can be done using stochastic models that
	produce heterogeneous realizations of aquifer parameters. This is
	the type of complexity addressed in this article.},
  doi = {10.1111/j.1745-6584.2006.00222.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{gomezhernandez1989,
  author = {G\'omez-Hern\'andez, J. and Gorelick, S.},
  title = {Effective groundwater model parameter values: {I}nfluence of spatial
	variability of hydraulic conductivity, leakance, and recharge},
  journal = {Water Resources Research},
  year = {1989},
  volume = {25},
  pages = {405--419},
  number = {3},
  abstract = {A stochastic simulation approach was used to inspect the influence
	of spatial variability in aquifer and recharge properties on effective
	(averaged) groundwater model parameters. A two-dimensional unconfined
	aquifer model was set up for an area similar to that near Livermore,
	California. Monte Carlo simulations were generated for different
	sets of spatial correlation structures assuming stationarity and
	an exponential semivariogram. For each Monte Carlo realization, groundwater
	flow was simulated, and estimates of the means and standard deviations
	of the hydraulic head were obtained. Nine different cases were studied
	involving different correlation lengths of hydraulic conductivity,
	conditional simulations, parameter zonation, and spatial recharge
	distributions. Results indicate that there is no single uniform value
	of the hydraulic conductivity that reproduces the expected head values.
	Spatial averaging to obtain effective values was based on the p norm
	(power norm) which ranges from 1 for the arithmetic mean to ?1 for
	the harmonic mean, with 0 representing the geometric mean. For the
	unconditional case the ?0.4 p norm seems to best reproduce the expected
	values. The ?0.2 p norm gave the best result for the conditional
	simulation case. If the geometric mean were used instead, the heads
	would deviate from the expected heads by about 2 m near the wells.
	Some value between the arithmetic and the geometric means of riverbed
	leakance will give results close to the expected head values. The
	arithmetic mean of aerially distributed recharge produced results
	with small deviations from the expected head values. These particular
	results may only apply to this modeled system, as the pumping pattern
	and magnitudes exhibited strong influences on zones of maximum head
	deviation. The implication of this work is that if an effective hydraulic
	conductivity is used in a simulation model, it must be selected for
	a particular set of wells and pumping rates. In an aquifer with significant
	pumping centers the best effective value for a two-dimensional unconfined
	flow model is most likely between the geometric and harmonic mean.
	If the wells are turned off, the effective hydraulic conductivity
	for that flow model reverts to the geometric mean.},
  doi = {10.1029/WR025i003p00405},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{gomezhernandez1997,
  author = {G\'omez-Hern\'andez, J. and Sahuquillo, A. and Capilla, J.},
  title = {Stochastic simulation of transmissivity fields conditional to both
	transmissivity and piezometric data: {I}. {T}heory},
  journal = {Journal of Hydrology},
  year = {1997},
  volume = {203},
  pages = {162--174},
  number = {1--4},
  abstract = {The generation of equally likely realizations of transmissivity fields
	conditional to both transmissivity and piezometric data is achieved
	using an iterative technique that couples geostatistics and optimization.
	By conditioning to both transmissivity fields and piezometric data
	we mean that the transmissivity field generated honours the transmissivity
	data and that the solution to the flow equation reproduces the measured
	piezometric heads. By equally likely realizations we mean that all
	realizations display the same patterns of spatial variability as
	observed in the field, as opposed to the transmissivity fields obtained
	by interpolation or by inverse modelling which are oversmooth, unrealistic,
	representations of the real field. In this regard, any realization
	could be the real but unknown transmissivity field. To achieve this
	goal, first a transmissivity field is generated conditional only
	to the transmissivity data, then the transmissivity field (and possibly
	the boundary conditions), are modified without destroying the spatial
	patterns of variability of transmissivity until the piezometric data
	are also honoured. The methodology is presented for steady-state
	flow in a confined two-dimensional aquifer. The details of the implementation
	are given for finite-differences solution over an aquifer discretized
	in square blocks.},
  doi = {doi:10.1016/S0022-1694(97)00098-X},
  owner = {RRojas},
  timestamp = {2009.03.13}
}

@ARTICLE{gaganis2006,
  author = {Gaganis, P. and Smith, L.},
  title = {Evaluation of the uncertainty of groundwater model predictions associated
	with conceptual errors: {A} per--datum approach to model calibration},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {503-514},
  number = {4},
  abstract = {The effect of systematic model error on the model predictions varies
	in space and time, and differs for the flow and solute transport
	components of a groundwater model. The classical single-objective
	formulation of the inverse problem by its nature cannot capture these
	characteristics of model error. We introduce an inverse approach
	that allows the spatial and temporal variability of model error to
	be evaluated in the parameter space. A set of solutions for model
	parameters are obtained by this new method that almost exactly satisfies
	the model equation at each observation point (per-datum calibration).
	This set of parameter estimates are then used to define a posterior
	parameter space that may be translated into a probabilistic description
	of model output to represent the level of confidence in model performance.
	It is shown that this approach can provide useful information regarding
	the strengths and limitations of a model as well as the performance
	of classical calibration procedures.},
  doi = {10.1016/j.advwatres.2005.06.006},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{gaganis2001,
  author = {Gaganis, P. and Smith, L.},
  title = {A {B}ayesian approach to the quantification of the effect of model
	error on the predictions of groundwater models},
  journal = {Water Resources Research},
  year = {2001},
  volume = {37},
  pages = {2309--2322},
  number = {9},
  abstract = {Errors arising from the imperfect mathematical representation of the
	structure of a hydrologic system (model error) are not random but
	systematic. Their effect on model predictions varies in space and
	time and differs for the flow and solute transport components of
	a groundwater model. Such errors do not necessarily have any probabilistic
	properties that can be easily exploited in the construction of a
	model performance criterion. A Bayesian approach is presented for
	quantifying model error in the presence of parameter uncertainty.
	Insight gained in updating the prior information on the model parameters
	is used to assess the correctness of the model structure, which is
	defined relative to the accuracy required of the model predictions.
	Model error is evaluated for each measurement of the dependent variable
	through an examination of the correctness of the model structure
	for different accuracy levels. The effect of model error on each
	dependent variable, which is quantified as a function of location
	and time, represents a measure of the reliability of the model in
	terms of each model prediction. This method can be used in identifying
	possible causes of model error and in discriminating among models
	in terms of the correctness of the model structure. It also offers
	an improved description of the uncertainties associated with a modeling
	exercise that may be useful in risk assessments and decision analyses.},
  doi = {10.1029/2000WR000001},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{gallagher2007,
  author = {Gallagher, M. and Doherty, J.},
  title = {Parameter estimation and uncertainty analysis for a watershed model},
  journal = {Environmental Modelling \& Software},
  year = {2007},
  volume = {22},
  pages = {1000-1020},
  number = {7},
  abstract = {Where numerical models are employed as an aid to environmental management,
	the uncertainty associated with predictions made by such models must
	be assessed. A number of different methods are available to make
	such an assessment. This paper explores the use of three such methods,
	and compares their performance when used in conjunction with a lumped
	parameter model for surface water flow (HSPF) in a large watershed.
	Linear (or first-order) uncertainty analysis has the advantage that
	it can be implemented with virtually no computational burden. While
	the results of such an analysis can be extremely useful for assessing
	parameter uncertainty in a relative sense, and ascertaining the degree
	of correlation between model parameters, its use in analyzing predictive
	uncertainty is often limited. Markov Chain Monte Carlo (MCMC) methods
	are far more robust, and can produce reliable estimates of parameter
	and predictive uncertainty. As well as this, they can provide the
	modeler with valuable qualitative information on the shape of parameter
	and predictive probability distributions; these shapes can be quite
	complex, especially where local objective function optima lie within
	those parts of parameter space that are considered probable after
	calibration has been undertaken. Nonlinear calibration-constrained
	optimization can also provide good estimates of parameter and predictive
	uncertainty, even in situations where the objective function surface
	is complex. Furthermore, they can achieve these estimates using far
	fewer model runs than MCMC methods. However, they do not provide
	the same amount of qualitative information on the probability structure
	of parameter space as do MCMC methods, a situation that can be partially
	rectified by combining their use with an efficient gradient-based
	search method that is specifically designed to locate different local
	optima. All methods of parameter and predictive uncertainty analysis
	discussed herein are implemented using freely-available software.
	Hence similar studies, or extensions of the present study, can be
	easily undertaken in other modeling contexts by other modelers.},
  doi = {10.1016/j.envsoft.2006.06.007},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{ganbiftu1996,
  author = {Gan, {T-Y} and Biftu, G.},
  title = {Automatic Calibration of Conceptual Rainfall-Runoff Models: Optimization
	Algorithms, Catchment Conditions, and Model Structure},
  journal = {Water Resources Research},
  year = {1996},
  volume = {32},
  pages = {3513--3524},
  number = {12},
  abstract = {From 32 CRR-catchment cases (combinations from four conceptual rainfall-runoff
	models (CRR) and eight catchments) calibrated with either two or
	three optimization methods, (1) the shuffle complex evolution method
	(SCE-UA), (2) the multiple start Simplex (MSX), and (3) the local
	Simplex, it seems that all three methods produced parameter sets
	of comparable, local-optimum quality. Even with comparable performance
	among the models, some parameter values derived by the three optimization
	methods for the same CRR-catchment cases are surprisingly different
	from each other. In addition, parameter sets of SCE-UA or MSX, which
	often produce marginally better results than the local Simplex at
	the calibration stage, could end up with worse results at the validation
	stage. Apparently, given the inherent limitations of calibration
	data, model inadequacies, and identifiability problems, it is impossible
	to achieve global convergence in the parameter search. However, other
	than those for dry catchments such as Ihimbu or Bird Creek, the parameter
	sets obtained are generally feasible. Both SCE-UA and the local Simplex
	are viable optimization tools, while MSX is inefficient computationally.
	SCE-UA can complete the parameter search in one run, while the local
	Simplex often requires multirun operations to get good results},
  doi = {10.1029/95WR02195},
  tags = {Calibration}
}

@ARTICLE{gan+al1997,
  author = {Gan, {T-Y} and Dlamini, E. and Biftu, G.},
  title = {Effects of model complexity and structure, data quality, and objective
	functions on hydrologic modeling},
  journal = {Journal of Hydrology},
  year = {1997},
  volume = {192},
  pages = {81--103},
  abstract = {Three medium sized, dry catchments located in Africa and USA were
	modeled with four or five conceptual rainfall-runoff (CRR) models
	of different complexity. The models were the Pitman model of South
	Africa (16 parameters), the Sacramento model of USA (21 parameters),
	the NAM model of Europe (15 parameters), the Xinanjiang model of
	China (15 parameters), and the SMAR model of Ireland (nine parameters).
	Between these models, the Xinanjiang model has been consistently
	doing better mainly because it is the only model that considers the
	non-uniform distribution of runoff producing areas to simulate the
	runoff. On the whole, it seems that standard, good quality hydrologic
	data can still support modeling of dry catchments with traditional
	CRR models. The model performance depends more on the model structure,
	the objective function used in automatic calibration, and data quality,
	than on model complexity or calibration data length. For relatively
	dry catchments such as the great Usuthu catchment, wet years should
	be preferred over dry years for the calibration data. (C) 1997 Elsevier
	Science B.V.},
  doi = {10.1016/S0022-1694(96)03114-9},
  keywords = {RAINFALL-RUNOFF MODELS, AUTOMATIC CALIBRATION, GLOBAL OPTIMIZATION,
	CATCHMENTS, ALGORITHMS},
  tags = {Calibration, conceptual model}
}

@ARTICLE{gassman+al2007,
  author = {Gassman, P. and Reyes, M. and Green, C. and Arnold, J.},
  title = {The Soil and Water Assessment Tool: Historical Development, Applications,
	and Future Research Directions},
  journal = {Transactions of the ASABE},
  year = {2007},
  volume = {50},
  pages = {1211--1250},
  number = {4},
  note = {Complete document in: http://ideas.repec.org/p/isu/genres/12744.html},
  abstract = {The Soil and Water Assessment Tool (SWAT) model is a continuation
	of nearly 30 years of modeling efforts conducted by the USDA Agricultural
	Research Service (ARS). SWAT has gained international acceptance
	as a robust interdisciplinary watershed modeling tool as evidenced
	by international SWAT conferences, hundreds of SWAT related papers
	presented at numerous other scientific meetings, and dozens of articles
	published in peer reviewed journals. The model has also been adopted
	as part of the U.S. Environmental Protection Agency (USEPA) Better
	Assessment Science Integrating Point and Nonpoint Sources (BASINS)
	software package and is being used by many U.S. federal and state
	agencies, including the USDA within the Conservation Effects Assessment
	Project (CEAP). At present, over 250 peer reviewed published articles
	have been identified that report SWAT applications, reviews of SWAT
	components, or other research that includes SWAT. Many of these peer
	reviewed articles are summarized here according to relevant application
	categories such as streamflow calibration and related hydrologic
	analyses, climate change impacts on hydrology, pollutant load assessments,
	comparisons with other models, and sensitivity analyses and calibration
	techniques. Strengths and weaknesses of the model are presented,
	and recommended research needs for SWAT are also provided.},
  doi = {10.1007/s00477-006{-}0057-2 ASABE},
  keywords = {Developmental history, Flow analysis, Modeling, SWAT, Water quality},
  tags = {SWAT}
}

@BOOK{gelhar1993,
  title = {Stochastic subsurface hydrology},
  publisher = {Prentice-Hall, Inc.},
  year = {1993},
  author = {Gelhar, L.},
  pages = {390},
  address = {New Jersey},
  edition = {First},
  owner = {rojasro},
  timestamp = {2009.09.15}
}

@BOOK{gelman2004,
  title = {Bayesian data analysis},
  publisher = {Chapman \& Hall/CRC},
  year = {2004},
  author = {Gelman, A. and Carlin, J. and Stern, H. and Rubin, D.},
  pages = {696},
  address = {New York},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{georgakakos2004,
  author = {Georgakakos, K. and Seo, D.J. and Gupta, H. and Schaake, J. and Butts,
	M.},
  title = {Towards the characterization of streamflow simulation uncertainty
	through multimodel ensembles},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {298},
  pages = {222-241},
  number = {1--4},
  abstract = {Distributed hydrologic modeling holds significant promise for improved
	estimates of streamflow with high spatial resolution. However, uncertainty
	in model structure and parameters, which are distributed in space,
	and in operational weather radar rainfall estimates, which comprise
	the main input to the models, contributes to significant uncertainty
	in distributed model streamflow simulations over a wide range of
	space and time scales. Using the simulations produced for the Distributed
	Model Intercomparison Project (DMIP), this paper develops and applies
	sample-path methods to characterize streamflow simulation uncertainty
	by diverse distributed hydrologic models. The emphasis in this paper
	is on the model parameter and structure uncertainty given radar rainfall
	forcing. Multimodel ensembles are analyzed for six application catchments
	in the Central US to characterize model structure uncertainty within
	the sample of models (both calibrated and uncalibrated) participating
	in DMIP. Ensembles from single distributed and lumped models are
	also used for one of the catchments to provide a basis to characterize
	the impact of parametric uncertainty versus model structure uncertainty
	in flow simulation statistics. Two main science questions are addressed:
	(a) what is the value of multimodel streamflow ensembles in terms
	of the probabilistic characterization of simulation uncertainty?
	And (b) how do probabilistic skill measures of multimodel versus
	single-model ensembles compare? Discussed also are implications for
	the operational use of streamflow ensembles generated by distributed
	hydrologic models. The results support the serious consideration
	of ensemble simulations and predictions created by diverse models
	in real time flow prediction.},
  doi = {10.1016/j.jhydrol.2004.03.037},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{george1999,
  author = {George, E.},
  title = {Comment to ``{B}ayesian model averaging: {A} tutorial'' by {J}ennifer
	{H}oeting},
  journal = {Statistical Science},
  year = {1999},
  volume = {14},
  pages = {409--412},
  number = {4},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2676806}
}

@ARTICLE{geyer1992,
  author = {Geyer, C.},
  title = {Practical {M}arkov chain {M}onte {C}arlo},
  journal = {Statistical Science},
  year = {1992},
  volume = {7},
  pages = {473--483},
  number = {4},
  abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is
	a general method for the simulation of stochastic processes having
	probability densities known up to a constant of proportionality.
	Despite recent advances in its theory, the practice has remained
	contro- versial. This article makes the case for basing all inference
	on one long run of the Markov chain and estimating the Monte Carlo
	error by standard nonparametric methods well-known in the time-series
	and oper- ations research literature. In passing it touches on the
	Kipnis-Varadhan central limit theorem for reversible Markov chains,
	on some new variance estimators, on judging the relative efficiency
	of competing Monte Carlo schemes, on methods for constructing more
	rapidly mixing Markov chains and on diagnostics for Markov chain
	Monte Carlo.},
  owner = {RRojas},
  refid = {GEYER1992},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2246094}
}

@BOOK{ghosh2006,
  title = {An introduction to {B}ayesian analysis--{T}heory and methods},
  publisher = {Springer},
  year = {2006},
  author = {Ghosh, J. and Delampady, M. and Samanta, T.},
  pages = {352},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{giannini2008,
  author = {Giannini, A. and Biasutti, M. and Verstraete, M.},
  title = {A climate model-based review of drought in the {Sahel: Desertification,
	the re-greening and climate change}},
  journal = {Global and Planetary Change},
  year = {2008},
  volume = {64},
  pages = {119--128},
  number = {3--4},
  month = {December},
  abstract = {We review the evidence that connects drought and desertification in
	the Sahel with climate change past, present and future. Advances
	in climate modeling point to the oceans, not land, as the cause of
	the recent persistence of drought in the Sahel. The current generation
	of global climate models reproduces the spatial extent, continental
	in scale, and the timing and duration of the shift to dry conditions
	that occurred in the late 1960's given knowledge of observed surface
	oceanic conditions only. The pattern statistically and dynamically
	associated with drought is one of warming of the tropical oceans,
	especially the Pacific and Indian Oceans, superimposed on an enhanced
	warming of the southern compared to the northern hemisphere most
	evident in the Atlantic. These models, which include a prognostic
	description of land surface and/or vegetation, albeit crude, indicate
	that positive feedbacks between precipitation and land surface/cover
	may act to amplify the ocean-forced component of continental climate.
	Despite the advances made in understanding the recent past, uncertainty
	dominates as we move forward in time, to the present, partial greening
	of the Sahel, and to the future of climate change projections.},
  doi = {10.1016/j.gloplacha.2008.05.004},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{gibelin2003,
  author = {Gibelin, A. and D\'equ\'e, M.},
  title = {Anthropogenic climate change over the {Mediterranean region simulated
	by a global variable resolution model}},
  journal = {Climate Dynamics},
  year = {2003},
  volume = {20},
  pages = {327--339},
  number = {4},
  month = {February},
  abstract = {Two 30-year simulations corresponding to 1960-1989 and 2070-2099 have
	been performed with a variable resolution atmospheric model. The
	model has a maximum horizontal resolution of 0.5° over the Mediterranean
	Sea. Simulations are driven by IPCC-B2 scenario radiative forcing.
	Sea surface temperatures (SSTs) are prescribed from monthly observations
	for the present climate simulation, and from a blend of observations
	and coupled simulations for the scenario. Another pair of forced
	atmospheric simulations has been run under these forcings with the
	same uniform low resolution as the coupled model. Comparisons with
	observations show that the variable resolution model realistically
	reproduces the main climate characteristics of the Mediterranean
	region. At a global scale, changes in latitudinal temperature profiles
	are similar for the forced and coupled models, justifying the time-slice
	approach. The 2 m temperature and precipitation responses predict
	a warming and drying of the Mediterranean region. A comparison with
	the coupled simulation and forced low-resolution simulation shows
	that this pattern is robust. The decrease in mean precipitation is
	associated with a significant decrease in soil wetness, and could
	involve considerable impact on water resources around the Mediterranean
	basin.},
  doi = {10.1007/s00382-002-0277-1},
  owner = {rojasro},
  timestamp = {2009.08.14}
}

@BOOK{gilks1995,
  title = {Markov {C}hain {M}onte {C}arlo in practice},
  publisher = {Chapman \& Hall/CRC},
  year = {1995},
  author = {Gilks, W. and Richardson, S. and Spiegelhalter, D.},
  pages = {512},
  address = {Boca Raton, Florida, USA},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.11.18}
}

@ARTICLE{gill+al2006,
  author = {Gill, M. and Kaheil, Y. and Khalil, A. and McKee, M. and Bastidas,
	L.},
  title = {Multiobjective particle swarm optimization for parameter estimation
	in hydrology},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W07417},
  number = {7},
  abstract = {Modeling of complex hydrologic processes has resulted in models that
	themselves exhibit a high degree of complexity and that require the
	determination of various parameters through calibration. In the current
	application we introduce a relatively new global optimization tool,
	called particle swarm optimization (PSO), that has already been applied
	in various other fields and has been reported to show effective and
	efficient performance. The PSO approach initially dealt with a single-objective
	function but has been extended to deal with multiobjectives in a
	form called multiobjective particle swarm optimization (MOPSO). The
	algorithm is modified to account for multiobjective problems by introducing
	the Pareto rank concept. The new MOPSO algorithm is tested on three
	case studies. Two test functions are used as the first case study
	to generate the true Pareto fronts. The approach is further tested
	for parameter estimation of a well-known conceptual rainfall-runoff
	model, the Sacramento soil moisture accounting model having 13 parameters,
	for which the results are very encouraging. We also tested the MOPSO
	algorithm to calibrate a three-parameter support vector machine model
	for soil moisture prediction},
  doi = {10.1029/2005WR004528},
  tags = {Calibration, PSO}
}

@ARTICLE{giorgi2006,
  author = {Giorgi, F.},
  title = {Regional climate modeling: {S}tatus and perspectives},
  journal = {Journal de Physique IV France},
  year = {2006},
  volume = {139},
  pages = {101--118},
  number = {7},
  abstract = {This paper is presents a concise review of regional climate modeling,
	from its ensuing stages in the late 1980s to the most recent developments.
	A tremendous progress has been achieved in improving the performance
	of regional climate models, which are currently used by a growing
	research community for a wide range of applications, from process
	studies to paleoclimate and future climate simulations. Basic concepts
	underlying the nested modeling technique, along with the current
	debate on outstanding issues in regional climate modeling, are discussed.
	Finally, perspectives of future developments in this rapidly evolving
	research area are briefly outlined. An extensive reference list is
	provided to support the discussion.},
  doi = {10.1051/jp4:2006139008},
  tags = {Multimodel - Ensambles, RCMs}
}

@ARTICLE{giorgi2005,
  author = {Giorgi, F.},
  title = {Climate change prediction},
  journal = {Climatic Change},
  year = {2005},
  volume = {73},
  pages = {239--265},
  number = {3},
  abstract = {The concept of climate change prediction in response to anthropogenic
	forcings at multi-decadal time scales is reviewed. This is identified
	as a predictability problem with characteristics of both first kind
	and second kind (due to the slow components of the climate system).
	It is argued that, because of the non-linear and stochastic aspects
	of the climate system and of the anthropogenic and natural forcings,
	climate change contains an intrinsic level of uncertainty. As a result,
	climate change prediction needs to be approached in a probabilistic
	way. This requires a characterization and quantification of the uncertainties
	associated with the sequence of steps involved in a climate change
	prediction. A review is presented of different approaches recently
	proposed to produce probabilistic climate change predictions. The
	additional difficulties found when extending the prediction from
	the global to the regional scale and the implications that these
	have on the choice of prediction strategy are finally discussed.},
  doi = {10.1007/s10584-005-6857-4},
  tags = {Uncertainty, Multimodel - Ensambles}
}

@ARTICLE{giorgi2004,
  author = {Giorgi, F. and Bi, X. and Pal, J.},
  title = {Mean, interannual variability and trends in a regional climate change
	experiment over Europe. {II: cimate change scenarios (2071--2100)}},
  journal = {Climate Dynamics},
  year = {2004},
  volume = {23},
  pages = {839--858},
  number = {7--8},
  month = {December},
  abstract = {We present an analysis of climate change over Europe as simulated
	by a regional climate model (RCM) nested within time-slice atmospheric
	general circulation model (AGCM) experiments. Changes in mean and
	interannual variability are discussed for the 30-year period of 2071–2100
	with respect to the present day period of 1961–1990 under forcing
	from the A2 and B2 IPCC emission scenarios. In both scenarios, the
	European region undergoes substantial warming in all seasons, in
	the range of 1–5.5°C, with the warming being 1–2°C lower in the B2
	than in the A2 scenario. The spatial patterns of warming are similar
	in the two scenarios, with a maximum over eastern Europe in winter
	and over western and southern Europe in summer. The precipitation
	changes in the two scenarios also show similar spatial patterns.
	In winter, precipitation increases over most of Europe (except for
	the southern Mediterranean regions) due to increased storm activity
	and higher atmospheric water vapor loadings. In summer, a decrease
	in precipitation is found over most of western and southern Europe
	in response to a blocking-like anticyclonic circulation over the
	northeastern Atlantic which deflects summer storms northward. The
	precipitation changes in the intermediate seasons (spring and fall)
	are less pronounced than in winter and summer. Overall, the intensity
	of daily precipitation events predominantly increases, often also
	in regions where the mean precipitation decreases. Conversely the
	number of wet days decreases (leading to longer dry periods) except
	in the winter over western and central Europe. Cloudiness, snow cover
	and soil water content show predominant decreases, in many cases
	also in regions where precipitation increases. Interannual variability
	of both temperature and precipitation increases substantially in
	the summer and shows only small changes in the other seasons. A number
	of statistically significant regional trends are found throughout
	the scenario simulations, especially for temperature and for the
	A2 scenario. The results from the forcing AGCM simulations and the
	nested RCM simulations are generally consistent with each other at
	the broad scale. However, significant differences in the simulated
	surface climate changes are found between the two models in the summer,
	when local physics processes are more important. In addition, substantial
	fine scale detail in the RCM-produced change signal is found in response
	to local topographical and coastline features.},
  doi = {10.1007/s00382-004-0467-0},
  owner = {rojasro},
  timestamp = {2009.08.14}
}

@ARTICLE{giorgi+al1994,
  author = {Giorgi, F. and Hostetler, S. and {Shields Brodeur}, C.},
  title = {Analysis of the surface hydrology in a regional climate model},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  year = {1994},
  volume = {120},
  pages = {161--183},
  number = {515},
  abstract = {This paper discusses the surface hydrology of a multi-year simulation
	of present day climate over the United States (US) conducted with
	a regional climate model (RegCM) nested within a general circulation
	model (GCM). The RegCM, which is run with a 60 km gridpoint spacing
	is interactively coupled with a state-of-the-art surface physics
	package that includes full surface hydrology calculations (the Biosphere-Atmosphere
	Transfer Scheme or BATS). The hydrologic budgets of ten regional
	drainage basins in the US are analysed. Model results are compared
	with available observations and with results from previous modelling
	experiments to evaluate the feasibility of using nested RegCM/GCM
	models for hydrology studies. In our experiment, the model captures
	the basic seasonality of the basin hydrologic budgets, although the
	simulated precipitation amounts are too high over the western US
	and too low over the eastern US. As a result, runoff, snow cover
	and soil water content are underestimated over the eastern US basins,
	while evaporation and runoff are overestimated in some of the western
	US basins. Topographically induced characteristics of precipitation,
	snow cover and runoff are well simulated over the mountainous western
	regions. Also well captured is the inter-basin variation of hydrologic
	budgets which occurs in response to different climatic settings.
	The springtime snowmelt and peak runoff season generally occurs in
	the model earlier in the year than is observed. Although our work
	indicates that the coupled regional modelling system can be useful
	in applications to hydrological studies, results from this experiment
	indicate that better accuracy in the simulation of regional climatic
	variables and more detailed representation of some hydrologic processes
	would be required before the coupled modelling system could be used
	to provide accurate assessments of hydrologic responses to climate
	change},
  doi = {10.1002/qj.49712051510},
  tags = {RCMs}
}

@ARTICLE{giorgilionello2008,
  author = {Giorgi, F. and Lionello, P.},
  title = {Climate change projections for the {M}editerranean region},
  journal = {Global and Planetary Change},
  year = {2008},
  volume = {63},
  pages = {90--104},
  number = {2--3},
  abstract = {We present a review of climate change projections over the Mediterranean
	region based on the most recent and comprehensive ensembles of global
	and regional climate change simulations completed as part of international
	collaborative projects. A robust and consistent picture of climate
	change over the Mediterranean emerges, consisting of a pronounced
	decrease in precipitation, especially in the warm season, except
	for the northern Mediterranean areas (e.g. the Alps) in winter. This
	drying is due to increased anticyclonic circulation that yields increasingly
	stable conditions and is associated with a northward shift of the
	Atlantic storm track. A pronounced warming is also projected, maximum
	in the summer season. Inter-annual variability is projected to mostly
	increase especially in summer, which, along with the mean warming,
	would lead to a greater occurrence of extremely high temperature
	events. The projections by the global and regional model simulations
	are generally consistent with each other at the broad scale. However,
	the precipitation change signal produced by the regional models shows
	substantial orographically-induced fine scale structure absent in
	the global models. Overall, these change signals are robust across
	forcing scenarios and future time periods, with the magnitude of
	the signal increasing with the intensity of the forcing. The intensity
	and robustness of the climate change signals produced by a range
	of global and regional climate models suggest that the Mediterranean
	might be an especially vulnerable region to global change},
  doi = {10.1016/j.gloplacha.2007.09.005},
  keywords = {PRUDENCE comparison},
  tags = {Multimodel - Ensambles, Precipitation Changes}
}

@ARTICLE{giorgimearns2003,
  author = {Giorgi, F. and Mearns, L.},
  title = {Probability of regional climate change based on the {R}eliability
	{E}nsemble {A}veraging ({REA}) method},
  journal = {Geophysical Research Letters},
  year = {2003},
  volume = {30},
  pages = {1629},
  number = {12},
  abstract = {We present an extension of the Reliability Ensemble Averaging, or
	REA, method [Giorgi and Mearns, 2002] to calculate the probability
	of regional climate change exceeding given thresholds based on ensembles
	of different model simulations. The method is applied to a recent
	set of transient experiments for the A2 and B2 IPCC emission scenarios
	with 9 different atmosphere-ocean General Circulation Models (AOGCMs).
	Probabilities of surface air temperature and precipitation change
	are calculated for 10 regions of subcontinental scale spanning a
	range of latitudes and climatic settings. The results obtained from
	the REA method are compared with those obtained with a simpler but
	conceptually similar approach [R{\"a}is{\"a}nen and Palmer, 2001].
	It is shown that the REA method can provide a simple and flexible
	tool to estimate probabilities of regional climate change from ensembles
	of model simulations for use in risk and cost assessment studies},
  doi = {10.1029/2003GL017130},
  tags = {Thesis, Multimodel - Ensambles}
}

@ARTICLE{giorgimearns1991,
  author = {Giorgi, F. and Mearns, L.},
  title = {Approaches to regional climate change simulation: {A} review},
  journal = {Reviews of Geophysics},
  year = {1991},
  volume = {29},
  pages = {191--216},
  number = {2},
  abstract = {The increasing demand by the scientific community, policy makers,
	and the public for realistic projections of possible regional impacts
	of future climate changes has rendered the issue of regional climate
	simulation critically important. The problem of projecting regional
	climate changes can be identified as that of representing effects
	of atmospheric forcings on two different spatial scales: large-scale
	forcings, i.e., forcings which modify the general circulation and
	determine the sequence of weather events which characterize the climate
	regime of a given region (for example, greenhouse gas abundance),
	and mesoscale forcings, i.e., forcings which modify the local circulations,
	thereby regulating the regional distribution of climatic variables
	(for example, complex mountainous systems). General circulation models
	(GCMs) are the main tools available today for climate simulation.
	However, they are run and will likely be run for the next several
	years at resolutions which are too coarse to adequately describe
	mesoscale forcings and yield accurate regional climate detail. This
	paper presents a review of these approaches. They can be divided
	in three broad categories: (1) Purely empirical approaches, in which
	the forcings are not explicitly accounted for, but regional climate
	scenarios are constructed by using instrumental data records or paleoclimatic
	analogues; (2) semiempirical approaches, in which GCMs are used to
	describe the atmospheric response to large-scale forcings of relevance
	to climate changes, and empirical techniques account for the effect
	of mesoscale forcings; and (3) modeling approaches, in which mesoscale
	forcings are described by increasing the model resolution only over
	areas of interest. Since they are computationally inexpensive, empirical
	and semiempirical techniques have been so far more widely used. Their
	application to regional climate change projection is, however, limited
	by their own empiricism and by the availability of data sets of adequate
	quality. More recently, a nested GCM-limited area model methodology
	for regional climate simulation has been developed, with encouraging
	preliminary results. As it is physically, rather than empirically,
	based, the nested modeling framework has a wide range of applications.},
  doi = {10.1029/90RG02636},
  tags = {RCMs, Downscaling}
}

@ARTICLE{gleick1986,
  author = {Gleick, P.},
  title = {Methods for evaluating the regional hydrologic impacts of global
	climatic changes},
  journal = {Journal of Hydrology},
  year = {1986},
  volume = {88},
  pages = {97--116},
  number = {1--2},
  abstract = {Concern over changes in global climate caused by rising atmospheric
	concentrations of carbon dioxide and other trace gases has increased
	in recent years as our understanding of atmospheric dynamics and
	global climate systems has improved. Yet despite a better understanding
	of climatic processes, many of the effects of human-induced climatic
	changes are still poorly understood. The most profound effect of
	such climatic changes may be major alterations in regional hydrologic
	cycles and changes in regional water availability. Unfortunately,
	these are among the least well-understood impacts. This paper reviews
	approaches for evaluating the regional hydrologic impacts of global
	climatic changes and presents a series of criteria for choosing among
	the different methods. One approach â€” the use of modified water-balance
	models â€” appears to offer significant advantages over other methods
	in accuracy, flexibility, and ease of use. Water-balance models are
	especially useful for identifying the regional hydrologic consequences
	of changes in temeprature, precipitation, and other climatic variables.
	The ability of water-balance models to incorporate month-to-month
	or seasonal variations in climate, snowfall and snowmelt algorithms,
	groundwater fluctuations, soil moisture characteristics, and natural
	climatic variability makes them especially attractive for water-resource
	studies of climatic changes. Furthermore, such methods can be combined
	with state-of-the-art information from general circulation models
	of the climate and with plausible hypothetical climate-change scenarios
	to generate information on the water-resource implications of future
	climatic changes},
  doi = {10.1016/0022-1694(86)90199-X},
  tags = {Impacts}
}

@ARTICLE{goderniaux+al2009,
  author = {Goderniaux, P. and Brouy\`ere, S. and Fowler, H. and Blenkinsop,
	S. and Therrien, R. and Orban, P. and Dassargues, A.},
  title = {Large scale surface--subsurface hydrological model to assess climate
	change impacts on groundwater reserves},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {373},
  pages = {122--138},
  number = {1--2},
  abstract = {Estimating the impacts of climate change on groundwater represents
	one of the most difficult challenges faced by water resources specialists.
	One difficulty is that simplifying the representation of the hydrological
	system often leads to discrepancies in projections. This study provides
	an improved methodology for the estimation of the impacts of climate
	change on groundwater reserves, where a physically-based surface--subsurface
	flow model is combined with advanced climate change scenarios for
	the Geer basin (465 km2), Belgium. Coupled surface--subsurface flow
	is simulated with the finite element model HydroGeoSphere. The simultaneous
	solution of surface and subsurface flow equations in HydroGeoSphere,
	as well as the internal calculation of the actual evapotranspiration
	as a function of the soil moisture at each node of the defined evaporative
	zone, improve the representation of interdependent processes like
	recharge, which is crucial in the context of climate change. More
	simple models or externally coupled models do not provide the same
	level of realism. Fully-integrated surface--subsurface flow models
	have recently gained attention, but have not been used in the context
	of climate change impact studies. Climate change simulations were
	obtained from six regional climate model (RCM) scenarios assuming
	the SRES A2 emission (medium--high) scenario. These RCM scenarios
	were downscaled using a quantile mapping bias-correction technique
	that, rather than applying a correction only to the mean, forces
	the probability distributions of the control simulations of daily
	temperature and precipitation to match the observed distributions.
	The same corrections are then applied to RCM scenarios for the future.
	Climate change scenarios predict hotter and drier summer and warmer
	and wetter winters. The combined use of an integrated surface--subsurface
	modelling approach, a spatial representation of the evapotranspiration
	processes and sophisticated climate change scenarios improves the
	model realism and projections of climate change impacts on groundwater
	reserves. For the climatic scenarios considered, the integrated flow
	simulations show that significant decreases are expected in the groundwater
	levels (up to 8 m) and in the surface water flow rates (between 9%
	and 33%) by 2080},
  doi = {10.1016/j.jhydrol.2009.04.017},
  issn = {0022-1694},
  keywords = {Groundwater, Climate change, Integrated model, HydroGeoSphere},
  tags = {Climate Change, application, Thesis}
}

@BOOK{goldberg1989,
  title = {Genetic algorithms in search, optimization and machine learning},
  publisher = {Addisson-Wesley},
  year = {1989},
  author = {Goldberg, D.},
  address = {Reading (MA)},
  owner = {rojasro},
  timestamp = {2011.10.11}
}

@BOOK{goovaerts1997,
  title = {Geostatistics for natural resources evaluation},
  publisher = {Oxford University Press},
  year = {1997},
  author = {Goovaerts, P.},
  pages = {483},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.27}
}

@ARTICLE{gordon+al2000,
  author = {Gordon, C. and Cooper, C. and Senior, C. and Banks, H. and Gregory,
	J. and Johns, T. and Mitchell, J. and Wood, R.},
  title = {The simulation of {SST}, sea ice extents and ocean heat transports
	in a version of the {H}adley {C}entre coupled model without flux
	adjustments},
  journal = {Climate Dynamics},
  year = {2000},
  volume = {16},
  pages = {147--168},
  number = {2--3},
  abstract = {Results are presented from a new version of the Hadley Centre coupled
	model (HadCM3) that does not require flux adjustments to prevent
	large climate drifts in the simulation. The model has both an improved
	atmosphere and ocean component. In particular, the ocean has a 1.25Â°Ã—1.25Â°
	degree horizontal resolution and leads to a considerably improved
	simulation of ocean heat transports compared to earlier versions
	with a coarser resolution ocean component. The model does not have
	any spin up procedure prior to coupling and the simulation has been
	run for over 400 years starting from observed initial conditions.
	The sea surface temperature (SST) and sea ice simulation are shown
	to be stable and realistic. The trend in global mean SST is less
	than 0.009Â°C per century. In part, the improved simulation is a
	consequence of a greater compatibility of the atmosphere and ocean
	model heat budgets. The atmospheric model surface heat and momentum
	budget are evaluated by comparing with climatological ship-based
	estimates. Similarly the ocean model simulation of poleward heat
	transports is compared with direct ship-based observations for a
	number of sections across the globe. Despite the limitations of the
	observed datasets, it is shown that the coupled model is able to
	reproduce many aspects of the observed heat budget.},
  doi = {10.1007/s003820050010},
  keywords = {HadCM3},
  tags = {Climate Models}
}

@ARTICLE{gosain+al2006,
  author = {Gosain, A. and Rao, S. and Basuray, D.},
  title = {Climate change impact assessment on hydrology of {I}ndian river basin},
  journal = {Current Science},
  year = {2006},
  volume = {90},
  pages = {346--353},
  number = {3},
  tags = {SWAT, Climate Change},
  url = {http://www.ias.ac.in/currsci/feb102006/346.pdf}
}

@ARTICLE{gosling+al2011,
  author = {Gosling, S. and Taylor, R. and Arnell, N. and Todd, M.},
  title = {A comparative analysis of projected impacts of climate change on
	river runoff from global and ctachment-scale hydrological models},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {279--294},
  number = {1},
  abstract = {We present a comparative analysis of projected impacts of climate
	change on river runoff from two types of distributed hydrological
	model, a global hydrological model (GHM) and catchment-scale hydrological
	models (CHM). Analyses are conducted for six catchments that are
	global in coverage and feature strong contrasts in spatial scale
	as well as climatic and developmental conditions. These include the
	Liard (Canada), Mekong (SE Asia), Okavango (SW Africa), Rio Grande
	(Brazil), Xiangxi (China) and Harper's Brook (UK). A single GHM (Mac-PDM.09)
	is applied to all catchments whilst different CHMs are applied for
	each catchment. The CHMs include SLURP v. 12.2 (Liard), SLURP v.
	12.7 (Mekong), Pitman (Okavango), MGB-IPH (Rio Grande), AV-SWAT-X
	2005 (Xiangxi) and Cat-PDM (Harper's Brook). The CHMs typically simulate
	water resource impacts based on a more explicit representation of
	catchment water resources than that available from the GHM and the
	CHMs include river routing, whereas the GHM does not. Simulations
	of mean annual runoff, mean monthly runoff and high (Q5) and low
	(Q95) monthly runoff under baseline (1961–1990) and climate change
	scenarios are presented. We compare the simulated runoff response
	of each hydrological model to (1) prescribed increases in global-mean
	air temperature of 1.0, 2.0, 3.0, 4.0, 5.0 and 6.0 °C relative to
	baseline from the UKMO HadCM3 Global Climate Model (GCM) to explore
	response to different amounts of climate forcing, and (2) a prescribed
	increase in global-mean air temperature of 2.0 °C relative to baseline
	for seven GCMs to explore response to climate model structural uncertainty.
	We find that the differences in projected changes of mean annual
	runoff between the two types of hydrological model can be substantial
	for a given GCM (e.g. an absolute GHM-CHM difference in mean annual
	runoff percentage change for UKMO HadCM3 2 °C warming of up to 25%),
	and they are generally larger for indicators of high and low monthly
	runoff. However, they are relatively small in comparison to the range
	of projections across the seven GCMs. Hence, for the six catchments
	and seven GCMs we considered, climate model structural uncertainty
	is greater than the uncertainty associated with the type of hydrological
	model applied. Moreover, shifts in the seasonal cycle of runoff with
	climate change are represented similarly by both hydrological models,
	although for some catchments the monthly timing of high and low flows
	differs. This implies that for studies that seek to quantify and
	assess the role of climate model uncertainty on catchment-scale runoff,
	it may be equally as feasible to apply a GHM (Mac-PDM.09 here) as
	it is to apply a CHM, especially when climate modelling uncertainty
	across the range of available GCMs is as large as it currently is.
	Whilst the GHM is able to represent the broad climate change signal
	that is represented by the CHMs, we find however, that for some catchments
	there are differences between GHMs and CHMs in mean annual runoff
	due to differences in potential evapotranspiration estimation methods,
	in the representation of the seasonality of runoff, and in the magnitude
	of changes in extreme (Q5, Q95) monthly runoff, all of which have
	implications for future water management issues.},
  doi = {10 5194/hess-15-279-2011},
  owner = {rojasro},
  timestamp = {2011.04.27}
}

@ARTICLE{goswamioconnor2007,
  author = {Goswami, M. and O'connor, K.},
  title = {Comparative assessment of six automatic optimization techniques for
	calibration of a conceptual rainfall-runoff model},
  journal = {Hydrological Sciences Journal},
  year = {2007},
  volume = {52},
  pages = {432--449},
  abstract = {In this application-based study, six automated strategies of parameter
	optimization are used for calibration of the conceptual soil moisture
	accounting and routing (SMAR) model for rainfall-runoff simulation
	in two catchments, one small and the other large. The methods used
	are: the genetic algorithm, particle swarm optimization, Rosenbrock's
	technique, shuffled complex evolution of the University of Arizona,
	simplex search, and simulated annealing. A comparative assessment
	is made using the Nash-Sutcliffe model efficiency index and the mean
	relative error (MRE) to evaluate the performance of each optimization
	method. It is found that the degree of variation of the values of
	the water balance parameters is generally less for the small catchment
	than for the large one. In the case of both catchments, the probabilistic
	global population-based optimization method of simulated annealing
	is considered best in terms of having the least variability of parameter
	values in successive tests, thereby alleviating the phenomenon of
	equifinality in parameter optimization, and also in producing the
	lowest MRE in verification.},
  doi = {10.1029/2005WR004528.},
  keywords = {optimization, SMAR, genetic algorithm, Rosenbrock, simplex, particle
	swarm optimization, simulated annealing, shuffled complex evolution,
	PARTICLE SWARM OPTIMIZATION, SHUFFLED COMPLEX EVOLUTION, GLOBAL OPTIMIZATION,
	GENETIC ALGORITHM, RIVER, MINIMIZATION, IDENTIFICATION, SIMULATION,
	STRATEGIES, CATCHMENTS},
  tags = {Calibration, PSO}
}

@ARTICLE{govendereverson2005,
  author = {Govender, M. and Everson, C.},
  title = {Modelling streamflow from two small {S}outh {A}frican experimental
	catchments using the {SWAT} model},
  journal = {Hydrological Processes},
  year = {2005},
  volume = {19},
  pages = {683--692},
  number = {3},
  abstract = {Increasing demand for timber products results in the expansion of
	commercial afforestation in South Africa. The conversion of indigenous
	seasonally dormant grassland to evergreen forests results in increased
	transpiration and ultimately a reduction in catchment runoff, creating
	a negative impact on the country's scarce water supplies. In order
	to assist managers in the decision-making processes it is important
	to be able to accurately assess and predict hydrological processes,
	and the impact that land use change will have on water resources.
	The Soil and Water Assessment Tool (SWAT) provides a means of performing
	these assessments. One of the key strengths of the SWAT model lies
	in its ability to model the relative impacts of changes in management
	practices, climate and vegetation on water quantity and quality.
	The aim of this study was to determine if the SWAT model could reasonably
	simulate hydrological processes in daily time steps from two small
	South African catchments. To verify the SWAT model a grassland (C
	VIgrass) and Pinus patula afforested catchment (C IIpine) were selected
	from the Cathedral Peak hydrological research station in the KwaZulu
	Natal Drakensberg mountains. These catchments were chosen because
	of the availability of detailed hydrological records and suitable
	land use. Observed and simulated streamflow for C VIgrass and C IIpine
	were compared. When model fits of observed and simulated streamflow
	for C VIgrass were acceptable, this parameter set was then used in
	the configuration of C IIpine. Results show that the model performs
	well for C VIgrass with reasonable agreement between modelled and
	observed data (R2 = 0{$\cdot$}68). Comparisons for C IIpine show
	a total oversimulation of streamflow for the period 1950 to 1965,
	with deviations between observed and modelled data increasing from
	1959 to 1965, due to the model not accounting for the increase in
	ET brought about by the maturing pine plantation},
  doi = {10.1002/hyp.5621},
  keywords = {SWAT model, streamflow, evaporation, land use change, forest growth,
	verification, PEST},
  tags = {Calibration, SWAT}
}

@ARTICLE{graham+al2007a,
  author = {Graham, L. and Andr\'easson, J. and Carlsson, B.},
  title = {Assessing climate change impacts on hydrology from an ensemble of
	regional climate models, model scales and linking methods--{A} case
	study on the {L}ule {R}iver basin},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {293--307},
  number = {1},
  abstract = {This paper investigates how using different regional climate model
	(RCM) simulations affects climate change impacts on hydrology in
	northern Europe using an offline hydrological model. Climate change
	scenarios from an ensemble of seven RCMs, two global climate models
	(GCMs), two global emissions scenarios and two RCMs of varying resolution
	were used. A total of 15 climate change simulations were included
	in studies on the Lule River basin in Northern Sweden. Two different
	approaches to transfer climate change from the RCMs to hydrological
	models were tested. A rudimentary estimate of change in laydropower
	potential on the Lule River due to climate change was also made.
	The results indicate an overall increase in river flow, earlier spring
	peak flows and an increase in hydropower potential. The two approaches
	for transferring the signal of climate change to the hydrological
	impacts model gave similar mean results, but considerably different
	seasonal dynamics, a result that is highly relevant for other types
	of climate change impacts studies.},
  doi = {10.1007/s10584-006-9215-2},
  keywords = {WATER-RESOURCES MANAGEMENT, SOUTHERN SWEDEN, PERSPECTIVE},
  tags = {Impacts}
}

@ARTICLE{graham+al2007b,
  author = {Graham, L. and Hagemann, S. and Jaun, S. and Beniston, M.},
  title = {On interpreting hydrological change from regional climate models},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {97--122},
  number = {1},
  abstract = {Although representation of hydrology is included in all regional climate
	models (RCMs), the utility of hydrological results from RCMs varies
	considerably from model to model. Studies to evaluate and compare
	the hydrological components of a suite of RCMs and their use in assessing
	hydrological impacts from future climate change were carried out
	over Europe. This included using different methods to transfer RCM
	runoff directly to river discharge and coupling different RCMs to
	offline hydrological models using different methods to transfer the
	climate change signal between models. The work focused on drainage
	areas to the Baltic Basin, the Bothnian Bay Basin and the Rhine Basin.
	A total of 20 anthropogenic climate change scenario simulations from
	11 different RCMs were used. One conclusion is that choice of GCM
	(global climate model) has a larger impact on projected hydrological
	change than either selection of emissions scenario or RCM used for
	downscaling.},
  doi = {10.1007/s10584-006-9217-0},
  tags = {Climate Models}
}

@ARTICLE{graham1989a,
  author = {Graham, W. and {McLaughlin}, D.},
  title = {Stochastic analysis of nonstationary subsurface solute transport.
	1. {U}nconditional moments},
  journal = {Water Resources Research},
  year = {1989},
  volume = {25},
  pages = {215--232},
  number = {2},
  abstract = {This paper applies stochastic methods to the analysis and prediction
	of solute transport in heterogeneous saturated porous media. Partial
	differential equations for three unconditional ensemble moments (the
	concentration mean, concentration covariance, and velocity concentration
	cross covariance) are derived by applying perturbation techniques
	to the governing transport equation for a conservative solute. Concentration
	uncertainty is assumed to be the result of unmodeled small-scale
	fluctuations in a steady state velocity field. The moment expressions,
	which describe how each moment evolves over time and space, resemble
	the classic deterministic advection-dispersion equation and can be
	solved using similar methods. A solution procedure based on a Galerkin
	finite element algorithm is illustrated with a hypothetical two-dimensional
	example. For this example the required steady state velocity statistics
	are obtained from an infinite domain spectral solution of the stochastic
	groundwater flow equation. The perturbation solution is shown to
	reproduce the statistics obtained from a Monte Carlo simulation quite
	well for a natural log conductivity standard deviation of 0.5 and
	moderately well for a natural log conductivity standard deviation
	of 1.0. The computational effort required for a perturbation solution
	is significantly less than that required for a Monte Carlo solution
	of acceptable accuracy. Sensitivity analyses conducted with the perturbation
	approach provide qualitative confirmation of a number of results
	obtained by other investigators for more restrictive special cases.},
  doi = {10.1029/WR025i002p00215},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@ARTICLE{graham1989b,
  author = {Graham, W. and {McLaughlin}, D.},
  title = {Stochastic analysis of nonstationary subsurface solute transport.
	2. {C}onditional moments},
  journal = {Water Resources Research},
  year = {1989},
  volume = {25},
  pages = {2331--2355},
  number = {11},
  abstract = {Stochastic analyses of subsurface transport indicate that the concentration
	distributions of individual solute plumes may deviate significantly
	from those predicted by unconditional ensemble statistics, particularly
	in near-source regions. This paper presents a method for developing
	improved concentration predictions which are tailored to site-specific
	conditions. The improved predictions are obtained by conditioning
	ensemble moments on field observations of log hydraulic conductivity,
	head, and solute concentration. The conditional moments are obtained
	from a distributed parameter Kaiman filter which is recursively linearized
	about the most recent estimates of solute concentration and velocity.
	The conditioning procedure is illustrated for two synthetic random
	solute plumes. Reasonably good estimates of the solute concentration
	distributions are obtained by conditioning the ensemble moments on
	a small number of measurements located in regions of high concentration
	uncertainty. The sampling networks adapt to the unique characteristics
	of each plume as they evolve over time. The example indicates that
	it is important to capture the dominant trends of the velocity field
	at as early a time as possible. As more measurements become available,
	advection accounts for a greater portion of small-scale velocity
	variability, and the magnitude of the macrodispersion term diminishes.
	This is reflected in the behavior of the conditional ensemble moments.},
  doi = {10.1029/WR025i011p02331},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@ARTICLE{grayson+al2002,
  author = {Grayson, R. and Bloschl, G. and Western, A. and Mcmahon, T.},
  title = {Advances in the use of observed spatial patterns of catchment hydrological
	response},
  journal = {Advances In Water Resources},
  year = {2002},
  volume = {25},
  pages = {1313--1334},
  abstract = {Over the past two decades there have been repeated calls for the collection
	of new data for use in developing hydrological science. The last
	few years have begun to bear fruit from the seeds sown by these calls,
	through increases in the availability and utility of remote sensing
	data, as well as the execution of campaigns in research catchments
	aimed at providing new data for advancing hydrological understanding
	and predictive capability. In this paper we discuss some philosophical
	considerations related to model complexity, data availability and
	predictive performance, highlighting the potential of observed patterns
	in moving the science and practice of catchment hydrology forward.
	We then review advances that have arisen from recent work on spatial
	patterns, including in the characterisation of spatial structure
	and heterogeneity, and the use of patterns for developing, calibrating
	and testing distributed hydrological models. We illustrate progress
	via examples using observed patterns of snow cover, runoff occurrence
	and soil moisture. Methods for the comparison of patterns are presented,
	illustrating how they can be used to assess hydrologically important
	characteristics of model performance. These methods include point-to-point
	comparisons, Spatial relationships between errors and landscape parameters,
	transects, and optimal local alignment. It is argued that the progress
	made to date augers well for future developments, but there is scope
	for improvements in several areas. These include better quantitative
	methods for pattern comparisons, better use of pattern information
	in data assimilation and modelling, and a call for improved archiving
	of data from field studies to assist in comparative studies for generalising
	results and developing fundamental understanding. (C) 2002 Elsevier
	Science Ltd. All rights reserved.},
  doi = {S0309-1708(02)00060-X},
  keywords = {SOIL-MOISTURE PATTERNS, RAIN-FOREST CATCHMENT, SNOW COVER PATTERNS,
	IMPROVED CALIBRATION, BAYESIAN-ESTIMATION, RUNOFF PREDICTION, FLUX
	MEASUREMENTS, ALPINE CATCHMENT, SOURCE AREAS, DATA SET},
  tags = {Applications}
}

@ARTICLE{greenvangriensven2008,
  author = {Green, C. and {van Griensven}, A.},
  title = {Autocalibration in hydrologic modeling: Using {SWAT2005} in small-scale
	watersheds},
  journal = {Environmental Modelling \& Software},
  year = {2008},
  volume = {23},
  pages = {422--434},
  number = {4},
  abstract = {SWAT is a physically based model that can simulate water quality and
	quantity at the watershed scale. Due to many of the processes involved
	in the manual- or autocalibration of model parameters and the knowledge
	of realistic input values, calibration can become difficult. An autocalibration-
	sensitivity analysis procedure was embedded in SWAT version 2005
	(SWAT2005) to optimize parameter processing. This embedded procedure
	is applied to six small-scale watersheds (subwatersheds) in the central
	Texas Blackland Prairie. The objective of this study is to evaluate
	the effectiveness of the autocalibration-sensitivity analysis procedures
	at small-scale watersheds (4.0-8.4 ha). Model simulations are completed
	using two data scenarios: (1) 1 year used for parameter calibration;
	(2) 5 years used for parameter calibration. The impact of manual
	parameter calibration versus autocalibration with manual adjustment
	on model simulation results is tested. The combination of autocalibration
	tool parameter values and manually adjusted parameters for the 2000-2004
	simulation period resulted in the highest E-NS and R-2 values for
	discharge; however, the same 5-year period yielded better overall
	E-NS, R-2 and P-values for the simulation values that were manually
	adjusted. The disparity is most likely due to the limited number
	of parameters that are included in this version of the autocalibration
	tool (i.e. Nperco, Pperco. and nitrate). Overall, SWAT2005 simulated
	the hydrology and the water quality constituents at the subwatershed-scale
	more adequately when all of the available observed data were used
	for model simulation as evidenced by statistical measure when both
	the autocalibration and manually adjusted parameters were used in
	the simulation. Published by Elsevier Ltd.},
  doi = {10.1016/j.envsoft.2007.06.002},
  keywords = {SWAT, hydrologic modeling, autocalibration, watershed, nutrients,
	sediment, POULTRY LITTER FERTILIZATION, RAINFALL-RUNOFF MODELS, SENSITIVITY-ANALYSIS,
	AGRICULTURAL PHOSPHORUS, GLOBAL OPTIMIZATION, CATCHMENT SCALE, ASSESSMENT-TOOL,
	QUALITY, RIVER, MANAGEMENT},
  tags = {SWAT, Calibration}
}

@ARTICLE{groves+al2008,
  author = {Groves, D. and Yates, D. and Tebaldi, C.},
  title = {Developing and applying uncertain global climate change projections
	for regional water management planning},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W12413},
  abstract = {The authors of this report are grateful for the financial support
	received from the National Science Foundation (grant SES-0345925)
	and from NCAR Weather and Climate Impacts Assessment program. NCAR
	is sponsored by the National Science Foundation. The authors also
	acknowledge the modeling groups, the Program for Climate Model Diagnosis
	and Intercomparison (PCMDI), and the WCRP's Working Group on Coupled
	Modelling (WGCM) for their roles in making available the WCRP CMIP3
	multimodel data set. Support of this data set is provided by the
	Office of Science, U. S. Department of Energy. This paper reports
	on work that received substantive assistance from RAND colleagues
	Robert Lempert, Debra Knopman, Sandra Berry, and Lynne Wainfan. The
	case study analysis would not have been possible without the support
	of Martha Davis and Richard Atwater of the IEUA.},
  doi = {10.1029/2008WR006964},
  keywords = {PRIORITY-DRIVEN, CHANGE IMPACTS, DEMAND-DRIVEN, SCENARIOS, CALIFORNIA,
	RESOURCES, DECISIONS, VARIABLES, WEAP21},
  tags = {Uncertainty, Multimodel - Ensambles}
}

@TECHREPORT{gubian+al2012,
  author = {Gubian, S. and Xiang, Y. and Suomela, B. and Hoeng, J.},
  title = {{Generalized Simulated Annealing - GenSA Package 1.0.3 }},
  institution = {{R Foundation for Statistical Computing}},
  year = {2012},
  abstract = {This package implements a function that searches for global minimum
	of a very complex non-linear objective function with a very large
	number of optima.},
  owner = {rojasro},
  timestamp = {2012.09.18},
  url = {http://cran.r-project.org/web/packages/GenSA/GenSA.pdf}
}

@ARTICLE{gudmundsson+al2011b,
  author = {Gudmundsson, L. and Tallaksen, L. and Stahl, K.},
  title = {{Spatial cross-correlation patterns of European low, mean and high
	flows}},
  journal = {Hydrological Processes},
  year = {2011},
  volume = {25},
  pages = {1035--1045},
  number = {7},
  abstract = {Low and high flows are associated with different hydrological processes.
	High flows correspond to the direct response of catchments to water
	input, whereas low flows occur in pronged dry periods and are governed
	by depleting storages. Therefore, the inter-annual dynamics of high
	and low flows are often considered to be independent. To shed light
	on this assumption, we analysed a pan-European dataset of 615 streamflow
	records, summarized as time series of annual streamflow percentiles
	(5th, 10th, …, 95th). The analysis was based on comparing the spatial
	cross-correlation patterns derived from the different percentile
	series. Their interrelation was visualized by projecting them into
	a low-dimensional space. We found that large parts of the cross-correlations
	of the percentile series can be summarized by one dominating component.
	This component represents geographical continuous regions in Europe
	of correlated streamflow. Departures from this mean pattern occurred
	for low and high flows and were characterized by the corresponding
	spatial correlation functions. Generally, spatial correlations appear
	to be stronger for high flows than for mean flows, particularly for
	short distances (<400 km). Low flows, on the other hand, have the
	lowest spatial correlations for short distances. For longer distances
	(>800 km), this pattern reverses and the spatial correlation of low
	flows become largest. This discrepancy between low and high flows
	suggests that hydrological systems are more homogeneously linked
	to climatic fluctuations under wet conditions. Under dry conditions,
	local catchment properties appear to play a larger role in translating
	climatic fluctuations into hydrological response.},
  doi = {10.1002/hyp.7807},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{gudmundsson+al2011a,
  author = {Gudmundsson, L. and Tallaksen, L. M. and Stahl, K. and Fleig, A.
	K.},
  title = {Low-frequency variability of European runoff},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {2853--2869},
  number = {9},
  doi = {10.5194/hess-15-2853-2011}
}

@ARTICLE{guinot+al2011,
  author = {Guinot, V. and Cappelaere, B. and Delenne, C. and Ruelland, D.},
  title = {Objective Functions for Conceptual Hydrological Model Calibration:
	Theoretical Analysis of Distance- and Weak Form-Based Functions},
  journal = {Journal of Hydrology},
  year = {2011},
  volume = {401},
  pages = {1--13},
  number = {1--2},
  note = {ARTICLE in Press, Accepted Manuscript -},
  abstract = {Calibrating conceptual hydrological models is often done via the optimization
	of objective functions serving as a measure of model performance.
	Most of the objective functions used in the hydrological literature
	can be classified into distance- and weak form-based objective functions.
	Distance- and weak form-based objective functions can be seen respectively
	as generalizations of the square error and balance error. An analysis
	of the objective functions shows that: (i) the calibration problem
	is transformed from an optimization problem with distance-based objective
	functions into a root finding problem for weak form-based functions;
	(ii) weak form-based objective functions are essentially less prone
	to local extrema than distance-based functions; (iii) consequently,
	they allow simple gradient-based methods to be used; (iv) parameter
	redundancy can be assessed very simply by superimposing the contour
	lines or comparing the gradients of two objective functions of similar
	nature in the parameter space; (v) simple guidelines can be defined
	for the selection of the calibration variables in a conceptual hydrological
	model. The theoretical results are illustrated by two simple test
	cases. Weak form-based approaches offer the potential for better-posed
	calibration problems, through the use of a number of independent
	criteria that matches the dimension of the identification problem.
	In contrast with distance-based objective functions, they do not
	have the inconvenience of solution non-uniqueness. Finally, the need
	for models with internal variables bearing a physical meaning is
	acknowledged.},
  doi = {10.1016/j.jhydrol.2011.02.004},
  highlights = {A theoretical analysis of distance-based and weak form-based objective
	functions for conceptual hydrological models is carried out * Weak
	form-based objective functions are shown to yield better-posed calibration
	problems than distance-based functions * Practical guidelines are
	proposed for optimal model calibration * A simple parameter redundancy
	test is proposed * An estimate is proposed for the length of the
	simulation warm-up period},
  tags = {Calibration}
}

@ARTICLE{gullentops2001,
  author = {Gullentops, F. and Bogemans, F. and {De Moor}, G. and Palissen, E.
	and Pissart, A.},
  title = {Quaternary lithostratigraphic units ({B}elgium)},
  journal = {Geologica Belgica},
  year = {2001},
  volume = {4},
  pages = {153--164},
  number = {1--2},
  abstract = {The lithostratigraphic classification of the Quaternary deposits is
	based on the genesis of the sediments. The distinguished environments
	are marine - estuarine, fluvial, eolian and slope. The marine - estuarine
	deposits are restricted to certain time-intervals within the Quaternary
	and are limited to the northern part of Belgium. Fluvial deposits
	are found throughout the Quaternary. On the basis of the sedimentological
	- lithological differentials within the Meuse basin and the Schelde
	basin a bipartite subdivision of the fluvial deposits is introduced.
	Eolian deposits are differentiated on the basis of their grain size
	distribution, namely sand and silt. The sandy deposits are accumulated
	in the northern part of Belgium, whereas loess is deposited in the
	more southern part of the country. Slope deposits are not restricted
	regionally neither temporally.},
  owner = {RRojas},
  refid = {GULLENTOPS2001},
  timestamp = {2008.11.04},
  url = {http://popups.ulg.ac.be/Geol/document.php?id=1965}
}

@ARTICLE{gupta+al1999,
  author = {Gupta, H. and Bastidas, L. and Sorooshian, S. and Shuttleworth, W.
	and Yang, Z.},
  title = {Parameter estimation of a land surface scheme using multicriteria
	methods},
  journal = {Journal of Geophysical Research},
  year = {1999},
  volume = {104},
  pages = {19491--19503},
  number = {D16},
  abstract = {Attempts to create models of surface-atmosphere interactions with
	greater physical realism have resulted in land surface schemes (LSS)
	with large numbers of parameters. The hope has been that these parameters
	can be assigned typical values by inspecting the literature. The
	potential for using the various observational data sets that are
	now available to extract plot-scale estimates for the parameters
	of a complex LSS via advanced parameter estimation methods developed
	for hydrological models is explored in this paper. Results are reported
	for two case studies using data sets of typical quality but very
	different location and climatological regime (ARM-CART and Tucson).
	The traditional single-criterion methods were found to be of limited
	value. However, a multicriteria approach was found to be effective
	in constraining the parameter estimates into physically plausible
	ranges when observations on at least one appropriate heat flux and
	one properly selected state variable are available.},
  doi = {10.1029/1999JD900154},
  keywords = {RAINFALL-RUNOFF MODELS, ATMOSPHERE TRANSFER SCHEME, GLOBAL OPTIMIZATION,
	HYDROLOGIC-MODELS, SENSITIVITY ANALYSIS, CALIBRATION, PREDICTIONS,
	SITE},
  tags = {Calibration}
}

@INBOOK{gupta+al2005,
  author = {Gupta, H. and Beven, K. and Wagener, T.},
  editor = {M.G. Anderson},
  title = {Model calibration and uncertainty estimation},
  year = {2005},
  abstract = {All rainfall-runoff models are, by definition, simplifications of
	the real-world system under investigation. The model components are
	aggregated descriptions of real-world hydrologic processes. One consequence
	of this is that the model parameters often do not represent directly
	measurable entities, but must be estimated using measurements of
	the system response through a process known as model calibration.
	The objective of this calibration process is to obtain a model with
	the following characteristics: (i) the input-state-output behavior
	of the model is consistent with the measurements of catchment behavior,
	(ii) the model predictions are accurate (i.e. they have negligible
	bias) and precise (i.e. the prediction uncertainty is relatively
	small), and (iii) the model structure and behavior are consistent
	with current hydrologic understanding of reality. This article describes
	the historic development leading to current views on model calibration,
	and the algorithms and techniques that have been developed for estimating
	parameters, thereby enabling the model to mimic the behavior of the
	hydrologic system. Manual techniques as well as automatic algorithms
	are addressed. The automatic approaches range from purely random
	techniques, to local and global search algorithms. An overview of
	multiobjective and recursive algorithms is also presented. Although
	it would be desirable to reduce the total output prediction error
	to zero (i.e. the difference between observed and simulated system
	behavior) this is generally impossible owing to the unavoidable uncertainties
	inherent in any rainfall-runoff modeling procedure. These uncertainties
	stem mainly from the inability of calibration procedures to uniquely
	identify a single optimal parameter set, from measurement errors
	associated with the system input and output, and from model structural
	errors arising from the aggregation of real-world processes into
	a mathematical model. Some commonly used approaches to estimate these
	uncertainties and their impacts on the model predictions are discussed.
	The article ends with a brief discussion about the current status
	of calibration and how well we are able to represent the effects
	of uncertainty in the modeling process, and some potential directions.},
  keywords = {rainfall-runoff modeling, model calibration, model identification,
	optimization, uncertainty estimation, parameter uncertainty, structural
	uncertainty, data uncertainty},
  optannote = {Model calibration and uncertainty estimation. In: Anderson, M.G. (Ed.),
	Encyclopedia of Hydrological Sciences, John Wiley \& Sons Ltd., Chichester,
	UK.},
  tags = {Calibration}
}

@ARTICLE{gupta+al2009,
  author = {Gupta, H. and Kling, H. and Yilmaz, K. and Martinez, G.},
  title = {Decomposition of the mean squared error and {NSE} performance criteria:
	Implications for improving hydrological modelling},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {377},
  pages = {80--91},
  number = {1-2},
  abstract = {The mean squared error (MSE) and the related normalization, the Nash{--}Sutcliffe
	efficiency (NSE), are the two criteria most widely used for calibration
	and evaluation of hydrological models with observed data. Here, we
	present a diagnostically interesting decomposition of NSE (and hence
	MSE), which facilitates analysis of the relative importance of its
	different components in the context of hydrological modelling, and
	show how model calibration problems can arise due to interactions
	among these components. The analysis is illustrated by calibrating
	a simple conceptual precipitation-runoff model to daily data for
	a number of Austrian basins having a broad range of hydro-meteorological
	characteristics. Evaluation of the results clearly demonstrates the
	problems that can be associated with any calibration based on the
	NSE (or MSE) criterion. While we propose and test an alternative
	criterion that can help to reduce model calibration problems, the
	primary purpose of this study is not to present an improved measure
	of model performance. Instead, we seek to show that there are systematic
	problems inherent with any optimization based on formulations related
	to the MSE. The analysis and results have implications to the manner
	in which we calibrate and evaluate environmental models; we discuss
	these and suggest possible ways forward that may move us towards
	an improved and diagnostically meaningful approach to model performance
	evaluation and identification.},
  doi = {10.1016/j.jhydrol.2009.08.003},
  keywords = {KGE, Mean squared error, Nash{--}Sutcliffe efficiency, Model performance
	evaluation, Calibration, Multiple criteria, Criteria decomposition},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{guptasorooshianyapo1999,
  author = {Gupta, H. and Sorooshian, S. and Yapo, P.},
  title = {Status of Automatic Calibration for Hydrologic Models: Comparison
	with Multilevel Expert Calibration},
  journal = {Journal of Hydrologic Engineering},
  year = {1999},
  volume = {4},
  pages = {135--143},
  number = {2},
  abstract = {The usefulness of a hydrologic model depends on how well the model
	is calibrated. Therefore, the calibration procedure must be conducted
	carefully to maximize the reliability of the model. In general, manual
	procedures for calibration can be extremely time-consuming and frustrating,
	and this has been a major factor inhibiting the widespread use of
	the more sophisticated and complex hydrologic models. A global optimization
	algorithm entitled shuffled complex evolution recently was developed
	that has proved to be consistent, effective, and efficient in locating
	the globally optimal model parameters of a hydrologic model. In this
	paper, the capability of the shuffled complex evolution automatic
	procedure is compared with the interactive multilevel calibration
	multistage semiautomated method developed for calibration of the
	Sacramento soil moisture accounting streamflow forecasting model
	of the U.S. National Weather Service. The results suggest that the
	state of the art in automatic calibration now can be expected to
	perform with a level of skill approaching that of a well-trained
	hydrologist. This enables the hydrologist to take advantage of the
	power of automated methods to obtain good parameter estimates that
	are consistent with the historical data and to then use personal
	judgment to refine these estimates and account for other factors
	and knowledge not incorporated easily into the automated procedure.
	The analysis also suggests that simple split-sample testing of model
	performance is not capable of reliably indicating the existence of
	model divergence and that more robust performance evaluation criteria
	are needed.},
  doi = {10.1061/(ASCE)1084-0699(1999)4:2(135)},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{gupta+al1998,
  author = {Gupta, H. and Sorooshian, S. and Yapo, P.},
  title = {Toward Improved Calibration of Hydrologic Models: Multiple and Noncommensurable
	Measures of Information},
  journal = {Water Resources Research},
  year = {1998},
  volume = {34},
  pages = {751--763},
  number = {4},
  abstract = {Several contributions to the hydrological literature have brought
	into question the continued usefulness of the classical paradigm
	for hydrologic model calibration. With the growing popularity of
	sophisticated "physicallybased" watershed models (e.g., land-surface
	hydrology and hydrochemical models) the complexity of the calibration
	problem has been multiplied many fold. We disagree with the seemingly
	widespread conviction that the model calibration problem will simply
	disappear with the availability of more and better field measurements.
	This paper suggests that the emergence of a new and more powerful
	model calibration paradigm must include recognition of the inherent
	multiobjective nature of the problem and must explicitly recognize
	the role of model error. The results of our preliminary studies are
	presented. Through an illustrative case study we show that the multiobjective
	approach is not only practical and relatively simple to implement
	but can also provide useful information about the limitations of
	a model.},
  bibkey = {RAINFALL-RUNOFF MODELS; MULTIOBJECTIVE PARAMETER-ESTIMATION; OBJECTIVE
	DECISION-MAKING; AUTOMATIC CALIBRATION; GLOBAL OPTIMIZATION; CATCHMENT
	MODELS; UNCERTAINTY; PREDICTION; IDENTIFICATION; EUTROPHICATION},
  doi = {10.1029/97WR03495},
  issn = {0043-1397},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{guptasorooshian1985,
  author = {Gupta, V. and and Sorooshian, S.},
  title = {The relationship between data and the precision of parameter estimates
	of hydrologic models},
  journal = {Journal of Hydrology},
  year = {1985},
  volume = {81},
  pages = {55--77},
  number = {1-2},
  abstract = {This paper presents a discussion of the relationship between data
	used for hydrologic model calibration and the precision of model
	parameter estimates. The analysis is conducted within the framework
	of the maximum likelihood approach to model selection. The concept
	of {``}information{''} is discussed and the relationship between
	information and parameter uncertainty is examined. This analysis
	provides some interesting insights into the role that the quantity
	and quality of the data used play in the identification procedure.
	Based on this, a method for selecting data sets suitable for model
	calibration is suggested. The ideas discussed are illustrated by
	means of simulation studies using a conceptual-type rainfall-runoff
	model},
  doi = {10.1016/0022-1694(85)90167-2},
  tags = {Calibration}
}

@ARTICLE{gutjahr1994,
  author = {Gutjahr, A. and Bullard, B. and Hatch, S. and Hughson, L.},
  title = {Joint conditional simulations and the spectral approach for flow
	modeling},
  journal = {Stochastic Hydrology and Hydraulics},
  year = {1994},
  volume = {8},
  pages = {79--108},
  number = {1},
  month = {March},
  abstract = {The use of data to condition single random fields has a well-established
	history. However, the joint use of data from several cross-correlated
	random fields is not as well developed. For example, the use of both
	transmissivity and head data in a steady state 2-d stochastic flow
	problem is essentially an inverse problem that is very important
	for both flow and transport predictions. This problem is addressed
	here by using a combination of numerical simulation and analytical
	methods and its application illustrated. The type of information
	conveyed by the different data categories is explored. The results
	presented are especially interesting in that head and transmissivity
	each give different information: Head values would appear to constrain
	the geometry of the paths while transmissivity data yields information
	about travel times. The linearized model is expanded to an iterative
	procedure and the ldquotruerdquo conditional distribution at several
	locations is compared with the iterative solution.
	
	The problem mentioned above is one with a special transfer function
	specified by the flow equation. In the second part of the paper a
	Fast Fourier Transform method for generation and conditioning of
	two or more random fields is introduced. This procedure is simple
	to implement, fast and very flexible.},
  doi = {10.1007/BF01581391},
  owner = {rojasro},
  timestamp = {2009.10.21}
}

@ARTICLE{gutjahr1989,
  author = {Gutjahr, A. and Wilson, J.},
  title = {Co--kriging for stochastic flow models},
  journal = {Transport in Porous Media},
  year = {1989},
  volume = {4},
  pages = {585--598},
  number = {6},
  abstract = {Co-kriging equations for log-transmissivity and heads are derived
	for a two-dimensional stochastic model. The behavior of the weights
	as a function of the unknown value of mean hydraulic gradient J are
	discussed and the procedure is illustrated by studying the lsquoscreeningrsquo
	effects of adjacent measurements and added head measurements. In
	addition, the bias of the estimator for head values is studied when
	J is also estimated.},
  doi = {10.1007/BF00223629},
  owner = {rojasro},
  timestamp = {2009.10.21}
}

@TECHREPORT{HAECON2004,
  author = {HAECON and {Witteveen+Bos}},
  title = {Ontwikkelen van regionale modellen ten behoeve van het {V}laams {G}rondwater
	{M}odel ({VGM}) in {GMS/MODFLOW}: {P}erceel Nr. 3 {B}rulandkrijtmodel
	({D}evelopment of regional models for the {F}lemish {G}roundwater
	{M}odel ({VGM}) in {GMS/MODFLOW}},
  institution = {AMINAL, afdeling WATER},
  year = {2004},
  address = {Brussels, Belgium},
  owner = {RRojas},
  pages = {--159 pp},
  publisher = {AMINAL, afdeling WATER},
  refid = {HAECON2004},
  timestamp = {2008.11.04}
}

@ARTICLE{haerter+al2010,
  author = {Haerter, J. and Hagemann, S. and Moseley, C. and Piani, C.},
  title = {Climate model bias correction and the role of timescales},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {1065--1079},
  number = {3},
  abstract = {It is well known that output from climate models cannot be used to
	force hydrological simulations without some form of preprocessing
	to remove the existing biases. In principle, statistical bias correction
	methodologies act on model output so the statistical properties of
	the corrected data match those of the observations. However the improvements
	to the statistical properties of the data are limited to the specific
	time scale of the fluctuations that are considered. For example,
	a statistical bias correction methodology for mean daily values might
	be detrimental to monthly statistics. Also, in applying bias corrections
	derived from present day to scenario simulations, an assumption is
	made of persistence of the bias over the largest timescales. We examine
	the effects of mixing fluctuations on different time scales and suggest
	an improved statistical methodology, referred to here as a cascade
	bias correction method, that eliminates, or greatly reduces, the
	negative effects.},
  doi = {10.5194/hess-15-1065-2011},
  owner = {rojasro},
  timestamp = {2010.12.16}
}

@ARTICLE{hagemann+al2009,
  author = {Hagemann, S. and G\"ottel, H. and Jacob, D. and Lorenz, P. and Roeckner,
	E.},
  title = {Improved regional scale processes reflected in projected hydrological
	changes over large {E}uropean catchments},
  journal = {Climate Dynamics},
  year = {2009},
  volume = {32},
  pages = {767--781},
  number = {6},
  abstract = {For the fourth assessment report of the Intergovernmental Panel on
	Climate Change (IPCC), the recent version of the coupled atmosphere/ocean
	general circulation model (GCM) of the Max Planck Institute for Meteorology
	has been used to conduct an ensemble of transient climate simulations
	These simulations comprise three control simulations for the past
	century covering the period 1860–2000, and nine simulations for the
	future climate (2001–2100) using greenhouse gas (GHG) and aerosol
	concentrations according to the three IPCC scenarios B1, A1B and
	A2. For each scenario three simulations were performed. The global
	simulations were dynamically downscaled over Europe using the regional
	climate model (RCM) REMO at 0.44° horizontal resolution (about 50
	km), whereas the physics packages of the GCM and RCM largely agree.
	The regional simulations comprise the three control simulations (1950–2000),
	the three A1B simulations and one simulation for B1 as well as for
	A2 (2001–2100). In our study we concentrate on the climate change
	signals in the hydrological cycle and the 2 m temperature by comparing
	the mean projected climate at the end of the twenty-first century
	(2071–2100) to a control period representing current climate (1961–1990).
	The robustness of the climate change signal projected by the GCM
	and RCM is analysed focussing on the large European catchments of
	Baltic Sea (land only), Danube and Rhine. In this respect, a robust
	climate change signal designates a projected change that sticks out
	of the noise of natural climate variability. Catchments and seasons
	are identified where the climate change signal in the components
	of the hydrological cycle is robust, and where this signal has a
	larger uncertainty. Notable differences in the robustness of the
	climate change signals between the GCM and RCM simulations are related
	to a stronger warming projected by the GCM in the winter over the
	Baltic Sea catchment and in the summer over the Danube and Rhine
	catchments. Our results indicate that the main explanation for these
	differences is that the finer resolution of the RCM leads to a better
	representation of local scale processes at the surface that feed
	back to the atmosphere, i.e. an improved representation of the land
	sea contrast and related moisture transport processes over the Baltic
	Sea catchment, and an improved representation of soil moisture feedbacks
	to the atmosphere over the Danube and Rhine catchments.},
  doi = {10.1007/s00382-008-0403-9},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@ARTICLE{hagemann+al2004,
  author = {Hagemann, S. and Machenhauer, B. and Jones, R. and Christensen, O.
	and D\'equ\'e, M. and Jacob, D. and Vidale, P.},
  title = {Evaluation of water and energy budgets in regional climate models
	applied over {E}urope},
  journal = {Climate Dynamics},
  year = {2004},
  volume = {23},
  pages = {547--567},
  number = {5},
  abstract = {This study presents a model intercomparison of four regional climate
	models (RCMs) and one variable resolution atmospheric general circulation
	model (AGCM) applied over Europe with special focus on the hydrological
	cycle and the surface energy budget. The models simulated the 15
	years from 1979 to 1993 by using quasi-observed boundary conditions
	derived from ECMWF re-analyses (ERA). The model intercomparison focuses
	on two large atchments representing two different climate conditions
	covering two areas of major research interest within Europe. The
	first is the Danube catchment which represents a continental climate
	dominated by advection from the surrounding land areas. It is used
	to analyse the common model error of a too dry and too warm simulation
	of the summertime climate of southeastern Europe. This summer warming
	and drying problem is seen in many RCMs, and to a less extent in
	GCMs. The second area is the Baltic Sea catchment which represents
	maritime climate dominated by advection from the ocean and from the
	Baltic Sea. This catchment is a research area of many studies within
	Europe and also covered by the BALTEX program. The observed data
	used are monthly mean surface air temperature, precipitation and
	river discharge. For all models, these are used to estimate mean
	monthly biases of all components of the hydrological cycle over land.
	In addition, the mean monthly deviations of the surface energy fluxes
	from ERA data are computed. Atmospheric moisture fluxes from ERA
	are compared with those of one model to provide an independent estimate
	of the convergence bias derived from the observed data. These help
	to add weight to some of the inferred estimates and explain some
	of the discrepancies between them. An evaluation of these biases
	and deviations suggests possible sources of error in each of the
	models. For the Danube catchment, systematic errors in the dynamics
	cause the prominent summer drying problem for three of the RCMs,
	while for the fourth RCM this is related to deficiencies in the land
	surface parametrization. The AGCM does not show this drying problem.
	For the Baltic Sea catchment, all models similarily overestimate
	the precipitation throughout the year except during the summer. This
	model deficit is probably caused by the internal model parametrizations,
	such as the large-scale condensation and the convection schemes},
  doi = {10.1007/s00382-004-0444-7},
  tags = {RCMs}
}

@ARTICLE{haijan2007,
  author = {Hajian, A.},
  title = {Efficient cosmological parameter estimation with {H}amiltonian {M}onte
	{C}arlo},
  journal = {Physical Review D - particles, fields, gravity, and cosmology},
  year = {2007},
  volume = {75},
  pages = {083525},
  abstract = {Traditional Markov Chain Monte Carlo methods suffer from low acceptance
	rate, slow mixing, and low efficiency in high dimensions. Hamiltonian
	Monte Carlo resolves this issue by avoiding the random walk. Hamiltonian
	Monte Carlo (HMC) is a Markov Chain Monte Carlo (MCMC) technique
	built upon the basic principle of Hamiltonian mechanics. Hamiltonian
	dynamics allows the chain to move along trajectories of constant
	energy, taking large jumps in the parameter space with relatively
	inexpensive computations. This new technique improves the acceptance
	rate by a factor of 4 while reducing the correlations and boosts
	up the efficiency by almost a factor of D in a D-dimensional parameter
	space. Therefore shorter chains will be needed for a reliable parameter
	estimation comparing to a traditional MCMC chain yielding the same
	performance. Besides that, the HMC is well suited for sampling from
	non-Gaussian and curved distributions which are very hard to sample
	from using the traditional MCMC methods. The method is very simple
	to code and can be easily plugged into standard parameter estimation
	codes such as CosmoMC. In this paper we demonstrate how the HMC can
	be efficiently used in cosmological parameter estimation. Also we
	discuss possible ways of getting good estimates of the derivatives
	of (the log of) posterior which is needed for HMC.},
  doi = {10.1103/PhysRevD.75.083525},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{halenka+al2009,
  author = {Halenka, T. and Miksovsky, J. and Belda, M. and Huszar, P.},
  title = {High resolution regional climate change modelling in {CECILIA} {P}roject--climate
	change signal in central and {E}astern {E}urope},
  journal = {IOP Conference Series: Earth and Environmental Science},
  year = {2009},
  volume = {6},
  pages = {022006},
  number = {2},
  abstract = {There is significant problem for decision making process arising from
	the weak link between climate change information based on global
	climate models and local impact studies necessarily based on regional
	climate change signal. Global Circulation Models (GCMs) can reproduce
	reasonably well climate features on large scales (global and continental),
	but their accuracy decreases when proceeding from continental to
	regional and local scales because of the lack of resolution. This
	is especially true for surface fields, such as precipitation, surface
	air temperature and their extremes, which are critically affected
	by topography and land use. However, in many applications, particularly
	related to the assessment of climate-change impacts, the information
	on surface climate change at regional to local scale is fundamental.
	To bridge the gap between the climate information provided by GCMs
	and that needed in impact studies, several approaches have been developed.
	The most popular approaches are (i) statistical downscaling, i.e.,
	identification of statistical relationships between large-scale fields
	and local surface climate elements, and (ii) dynamical downscaling,
	i.e., nesting of a fine scale limited area model (or Regional Climate
	Model, RCM) within the GCM. The latter approach is more correct from
	a physical point of view, but is much more demanding on computer
	resources. In the region of central and eastern Europe the need for
	high resolution studies is particularly important. This region is
	characterized by the northern flanks of the Alps, the long arc of
	the Carpathians, and smaller mountain chains and highlands in the
	Czech Republic, Slovakia, Romania and Bulgaria that significantly
	affect the local climate conditions. A resolution sufficient to capture
	the effects of these topographical and associated land-use features
	is necessary. It will be shown that neither resolutions 50 and 25
	km used in FP6 Integrated Project ENSEMBLES is not so high to capture
	necessary details in this region. That is why 10 km resolution has
	been introduced in EC FP6 project CECILIA. The main objectives of
	EC Project (STREP) CECILIA dealing with climate change impacts and
	vulnerability assessment in targeted areas of Central and Eastern
	Europe will be described. Emphasis is given to applications of regional
	climate modelling studies at a resolution of 10 km for local impact
	studies in key sectors of the region. The project contains studies
	on hydrology, water quality, and water management (focusing at medium-sized
	river catchments and the Black Sea coast), air quality issues in
	urban areas (Black Triangle – a polluted region around the common
	borders of the Czech Republic, Poland and Germany), agriculture (crop
	yield, pests and diseases, carbon cycle), and forestry (management,
	carbon cycle). The high spatial and temporal resolution of dense
	national observational networks at high temporal resolution and of
	the CECILIA regional model experiments will uniquely feed into investigations
	of climate change consequences for weather extremes in the regions
	under study. Comparison with the results based on statistical downscaling
	techniques will also be provided. Statistical downscaling methods
	for verification of the regional model results will be developed
	and applied, and assessments of their use in localization of model
	output for impact studies will be performed. The main goal of regional
	climate modelling activities in CECILIA project is to produce simulations
	on targeted domains for a past period (1961-1990) driven by ERA40
	reanalysis used for validation of the models as well as for a reference
	period (1961-1990) and scenario time slices (2021-2050 and 2071-2100)
	based on ENSEMBLES 6FP EC IP A1B GCM simulations. Two models are
	used as source of driving fields over six target areas, ALADIN-Climate
	family using stretched climate change transient run by ARPEGE/Climat
	for ENSEMBLES project, RegCM family using RegCM transient ENSEMBLES
	run for whole Europe in 25km resolution driven by transient run of
	ECHAM5. Results of model validation and climate change signal based
	on this simulations will be presented with emphasis to the regional
	details of targeted areas as well as examples of impact studies using
	these high resolution scenarios runs.},
  doi = {10.1088/1755-1307/6/2/022006},
  tags = {RCMs}
}

@ARTICLE{hanasaki2008a,
  author = {Hanasaki, N. and Kanae, S. and Oki, T. and Masuda, K. and Motoya,
	K. and Shirakawa, N. and Shen, Y. and Tanaka, K.},
  title = {An integrated model for the assessment of global water resources
	-- {P}art 1: {M}odel description and input meteorological forcing},
  journal = {Hydrology and Earth System Sciences},
  year = {2008},
  volume = {12},
  pages = {1007--1025},
  number = {4},
  abstract = {To assess global water availability and use at a subannual timescale,
	an integrated global water resources model was developed consisting
	of six modules: land surface hydrology, river routing, crop growth,
	reservoir operation, environmental flow requirement estimation, and
	anthropogenic water withdrawal. The model simulates both natural
	and anthropogenic water flow globally (excluding Antarctica) on a
	daily basis at a spatial resolution of 1°×1° (longitude and latitude).
	This first part of the two-feature report describes the six modules
	and the input meteorological forcing. The input meteorological forcing
	was provided by the second Global Soil Wetness Project (GSWP2), an
	international land surface modeling project. Several reported shortcomings
	of the forcing component were improved. The land surface hydrology
	module was developed based on a bucket type model that simulates
	energy and water balance on land surfaces. The crop growth module
	is a relatively simple model based on concepts of heat unit theory,
	potential biomass, and a harvest index. In the reservoir operation
	module, 452 major reservoirs with >1 km3 each of storage capacity
	store and release water according to their own rules of operation.
	Operating rules were determined for each reservoir by an algorithm
	that used currently available global data such as reservoir storage
	capacity, intended purposes, simulated inflow, and water demand in
	the lower reaches. The environmental flow requirement module was
	newly developed based on case studies from around the world. Simulated
	runoff was compared and validated with observation-based global runoff
	data sets and observed streamflow records at 32 major river gauging
	stations around the world. Mean annual runoff agreed well with earlier
	studies at global and continental scales, and in individual basins,
	the mean bias was less than ±20% in 14 of the 32 river basins and
	less than ±50% in 24 basins. The error in the peak was less than
	±1 mo in 19 of the 27 basins and less than ±2 mo in 25 basins. The
	performance was similar to the best available precedent studies with
	closure of energy and water. The input meteorological forcing component
	and the integrated model provide a framework with which to assess
	global water resources, with the potential application to investigate
	the subannual variability in water resources.},
  doi = {10.5194/hess-12-1007-2008},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{hanasaki2008b,
  author = {Hanasaki, N. and Kanae, S. and Oki, T. and Masuda, K. and Motoya,
	K. and Shirakawa, N. and Shen, Y. and Tanaka, K.},
  title = {An integrated model for the assessment of global water resources
	-- {P}art 2: {A}pplications and assessments},
  journal = {Hydrology and Earth System Sciences},
  year = {2008},
  volume = {12},
  pages = {1027--1037},
  number = {4},
  abstract = {To assess global water resources from the perspective of subannual
	variation in water availability and water use, an integrated water
	resources model was developed. In a companion report, we presented
	the global meteorological forcing input used to drive the model and
	six modules, namely, the land surface hydrology module, the river
	routing module, the crop growth module, the reservoir operation module,
	the environmental flow requirement module, and the anthropogenic
	withdrawal module. Here, we present the results of the model application
	and global water resources assessments. First, the timing and volume
	of simulated agriculture water use were examined because agricultural
	use composes approximately 85% of total consumptive water withdrawal
	in the world. The estimated crop calendar showed good agreement with
	earlier reports for wheat, maize, and rice in major countries of
	production. In major countries, the error in the planting date was
	±1 mo, but there were some exceptional cases. The estimated irrigation
	water withdrawal also showed fair agreement with country statistics,
	but tended to be underestimated in countries in the Asian monsoon
	region. The results indicate the validity of the model and the input
	meteorological forcing because site-specific parameter tuning was
	not used in the series of simulations. Finally, global water resources
	were assessed on a subannual basis using a newly devised index. This
	index located water-stressed regions that were undetected in earlier
	studies. These regions, which are indicated by a gap in the subannual
	distribution of water availability and water use, include the Sahel,
	the Asian monsoon region, and southern Africa. The simulation results
	show that the reservoir operations of major reservoirs (>1 km3) and
	the allocation of environmental flow requirements can alter the population
	under high water stress by approximately ?11% to +5% globally. The
	integrated model is applicable to assessments of various global environmental
	projections such as climate change.},
  doi = {10.5194/hess-12-1027-2008},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{hanel2010,
  author = {Hanel, M. and Buishand, T.},
  title = {Analysis of precipitation extremes in an ensemble of transient regional
	climate model simulations for the {R}hine basin},
  journal = {Climate Dynamics},
  year = {2010},
  volume = {36},
  pages = {1135--1153},
  number = {5--6},
  abstract = {A non-stationary index-flood model was used to analyse the 1-day summer
	and 5-day winter precipitation maxima in the Rhine basin in an ensemble
	of 15 transient regional climate model (RCM) simulations. It is assumed
	that the seasonal precipitation maxima follow a generalized extreme
	value (GEV) distribution with time varying parameters. The index-flood
	assumption implies that the dispersion coefficient (the ratio of
	the scale and the location parameters) and the shape parameter are
	constant over predefined regions, while the location parameter varies
	within these regions. A comparison with the estimates from gridded
	observations shows that these GEV parameters are too large in the
	summer season, while there is a large overestimation of the location
	parameter and underestimation of the dispersion coefficient in winter.
	However, a large part of the biases in the summer season might be
	due to the low number of stations used for gridding the observations.
	Though there is considerable variation in the changes of the extreme
	value distributions among the RCM simulations, common tendencies
	can be identified. In summer, large quantiles increase as a consequence
	of an increase of the dispersion coefficient, while there is almost
	no change of low quantiles. In winter, low quantiles increase because
	of an increase of the location parameter. This effect is, however,
	counterbalanced by a decrease of the shape parameter in most RCM
	simulations, resulting in only a slight increase of large quantiles.
	Departures from the assumed index-flood model were observed in the
	Alpine region in the south of the basin. This is due to the strong
	spatial heterogeneity in the dispersion coefficient in a number of
	RCM simulations and a significant altitude dependence of the trend
	in the location parameter in winter in five RCM simulations.},
  doi = {10.1007/s00382-010-0822-2},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{hanna1998,
  author = {Hanna, S. and Yeh, T.},
  title = {Estimation of co--conditional moments of transmissivity, hydraulic
	head and velocity fields},
  journal = {Advances in Water Resources},
  year = {1998},
  volume = {22},
  pages = {87--95},
  number = {1},
  abstract = {An iterative co-conditional Monte Carlo simulation (IMCS) approach
	is developed. This approach derives co-conditional means and variances
	of transmissivity (T), head (?), and Darcy's velocity (q), based
	on sparse measurements of T and ? in heterogeneous, confined aquifers
	under steady-state conditions. It employs the classical co-conditional
	Monte Carlo simulation technique (MCS) and a successive linear estimator
	that takes advantage of our prior knowledge of the covariances of
	T and ? and their cross-covariance. In each co-conditional simulation,
	a linear estimate of T is improved by solving the governing steady-state
	flow equation, and by updating residual covariance functions iteratively.
	These residual covariance functions consist of the covariance of
	T and ? and the cross-covariance function between T and ?. As a result,
	the non-linear relationship between T and ? is incorporated in the
	co-conditional realizations of T and ?. Once the T and ? fields are
	generated, a corresponding velocity field is also calculated. The
	average of the co-conditioned realizations of T, ?, and q yields
	the co-conditional mean fields. In turn, the co-conditional variances
	of T, ?, and q, which measure the reduction in uncertainty due to
	measurements of T and ?, are derived. Results of our numerical experiments
	show that the co-conditional means from IMCS for T and ? fields have
	smaller mean square errors (MSE) than those from a non-iterative
	Monte Carlo simulation (NIMCS). Finally, the co-conditional mean
	fields from IMCS are compared with the co-conditional effective fields
	from a direct approach developed by Yeh et al. [Water Resources Research,
	32(1), 85–92, 1996].},
  doi = {10.1016/S0309-1708(97)00033-X},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@ARTICLE{hannafordmarsh2006,
  author = {Hannaford, Jamie and Marsh, Terry},
  title = {An assessment of trends in UK runoff and low flows using a network
	of undisturbed catchments},
  journal = {International Journal of Climatology},
  year = {2006},
  volume = {26},
  pages = {1237--1253},
  number = {9},
  abstract = {In many parts of the world attempts to discern climatic-driven changes
	in river flow patterns are hindered by the combined impact of other,
	more direct anthropogenic disturbances such as abstraction and impoundments.
	This study capitalises on a newly defined ‘benchmark’ network of
	natural catchments in the UK to discern natural variability in flow
	regimes. Trend tests were applied to time series of runoff and indicators
	of low-flow magnitude and duration for two study periods to assess
	the sensitivity of trends to the frame of reference over which tests
	were conducted. Notwithstanding the volatility of the recent past,
	the results demonstrate a general stability in runoff and low flows
	since the early 1960s. The strongest signal to emerge from the study
	is a positive runoff trend for catchments in Scotland, which was
	resilient to the effect of the study periods. There was also some
	indication of increasing runoff in maritime western areas of England
	and Wales. These increases in maritime areas are likely to reflect
	the dominance of westerly airflows in the recent past, associated
	with an increase in the North Atlantic Oscillation (NAO) Index. For
	low flows, there were no compelling trends—significant positive trends
	over the 1973–2002 period are influenced by a sequence of notably
	dry years at the start of the period and were not observed over a
	40-year time-frame. There are some indications of a tendency towards
	decreasing low flows in some eastern catchments, but this is supported
	by few significant results. The results of this study provide a baseline
	against which to assess longer-term change from historical flow time
	series and to monitor future change in the benchmark network. Copyright
	© 2006 Royal Meteorological Society},
  doi = {10.1002/joc.1303},
  issn = {1097-0088},
  keywords = {trend(s), river flows, low flows, runoff, climate change, natural
	catchments, UK},
  publisher = {John Wiley \& Sons, Ltd.}
}

@ARTICLE{hao+al2004,
  author = {Hao, {F-H} and Zhang, {X-S} and Yang, {Z-F}},
  title = {A distributed non-point source pollution model: calibration and validation
	in the Yellow River Basin},
  journal = {Journal of Environmental Sciences},
  year = {2004},
  volume = {16},
  pages = {646--50},
  number = {4},
  abstract = {The applicability of a non-point source pollution model{--}SWAT(soil
	and water assessment tools) in a large river basin with high sediment
	runoff modulus(770 t/km2 in the Yellow River) was examined. The basic
	database, which includes DEM, soil and landuse map, weather data,
	and land management data, was established for the study area using
	GIS. A two-stage },
  keywords = {SWAT},
  tags = {Calibration, SWAT},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/15495973}
}

@TECHREPORT{mf2005,
  author = {Harbaugh, A.},
  title = {M{ODFLOW}--2005, the {U}.{S}. {G}eological {S}urvey modular ground-water
	model--the {G}round--{W}ater {F}low {P}rocess},
  institution = {United States Geological Survey},
  year = {2005},
  type = {Techniques and Methods 6--A16},
  address = {Reston, Virginia, USA},
  booktitle = {Techniques and Methods 6-A16},
  owner = {RRojas},
  pages = {--253 pp},
  publisher = {U.S. Geological Survey},
  refid = {HARBAUGH2005},
  timestamp = {2008.11.04}
}

@TECHREPORT{zonebudget,
  author = {Harbaugh, A.},
  title = {A computer program for calculating subregional water budgets using
	results from the U.S. Geological Survey modular three-dimensional
	ground-water flow model},
  institution = {United States Geological Survey},
  year = {1990},
  type = {Open File Report 90-392},
  address = {Reston, Virginia, USA},
  owner = {rojasro},
  timestamp = {2011.10.14}
}

@TECHREPORT{mf2k,
  author = {Harbaugh, A. and Banta, E. and Hill, M. and McDonald, M.},
  title = {M{ODFLOW}--2000 {U}.{S}. {G}eological {S}urvey modular ground--water
	model--user guide to modularization concepts and the ground--water
	flow process},
  institution = {United States Geological Survey},
  year = {2000},
  type = {Open File Report, 00-92},
  address = {Reston, Virginia, USA},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{harmelsmith2007,
  author = {Harmel, R. and Smith, P.},
  title = {Consideration of measurement uncertainty in the evaluation of goodness-of-fit
	in hydrologic and water quality modeling},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {337},
  pages = {326--336},
  abstract = {As hydrologic and water quality (H/WQ) models are increasingly used
	to guide water resource policy, management, and regulation, it is
	no longer appropriate to disregard uncertainty in model calibration,
	validation, and evaluation. In the present research, the method of
	calculating the error term in pairwise comparisons of measured and
	predicted values was modified to consider measurement uncertainty
	with the goat of facilitating enhanced evaluation of H/WQ models.
	The basis of this method was the theory that H/WQ models should not
	be evaluated against the values of measured data, which are uncertain,
	but against the inherent measurement uncertainty. Specifically, the
	deviation calculations of several goodness-of-fit indicators were
	modified based on the uncertainty boundaries (Modification 1) or
	the probability distribution of measured data (Modification 2). The
	choice between these two modifications is based on absence or presence
	of distributional information on measurement uncertainty. Modification
	1, which is appropriate in the absence of distributional information,
	minimizes the calculated deviations and thus produced substantial
	improvements in goodness-of-fit indicators for each example data
	set. Modification 2, which provides a more realistic uncertainty
	estimate but requires distributional information on uncertainty,
	resulted in smaller improvements. Modification 2 produced small goodness-of-fit
	improvement for measured data with little uncertainty but produced
	modest improvement when data with substantial uncertainty were compared
	with both poor and good model predictions. This limited improvement
	is important because poor model good ness-of-fit, especially due
	to model structure deficiencies, should not appear satisfactory simply
	by including measurement uncertainty. (C) 2007 Elsevier B.V. All
	rights reserved.},
  doi = {10.1016/j.jhydrol.2007.01.043},
  keywords = {model calibration, model validation, statistics, Nash-Sutcliffe, index
	of agreement, PARAMETER UNCERTAINTY, AGRICULTURAL WATERSHEDS, DRAINMOD
	PREDICTIONS, PHOSPHORUS, RIVER, EQUIFINALITY, NITROGEN, STORAGE},
  tags = {SWAT, Calibration, Goodness-of-Fit}
}

@ARTICLE{harrar2003,
  author = {Harrar, W. and Sonnenberg, T. and Henriksen, H.},
  title = {Capture zone, travel time, and solute transport predictions using
	inverse modelling and different geological models},
  journal = {Hydrogeology Journal},
  year = {2003},
  volume = {11},
  pages = {536--548},
  number = {5},
  abstract = {Six regional-scale flow models are compared to gain insight into how
	different representations of hydraulic-conductivity distributions
	affect model calibration and predictions. Deterministic geological
	models were used to define hydraulic-conductivity distributions in
	two steady-state flow models that were calibrated to heads and baseflow
	estimates using inverse techniques. Optimized hydraulic-conductivity
	estimates from the two models were used to calculate layer and model
	mean hydraulic-conductivity values. Despite differences in the two
	geological models, inverse calibration produced mean hydraulic-conductivity
	values for the entire model domain that are quite similar. The layer
	and model mean hydraulic-conductivity values were used to generate
	four additional flow models and forward runs were performed. All
	of the models adequately simulate the observed heads and total baseflow.
	The six flow models were used to predict the steady-state impact
	of a proposed well field, and the flow solutions were used in simulating
	particle tracking and solute transport. Results of the predictive
	simulations show that, for this example, simple models of heterogeneity
	produce capture zones similar to more complex models, but with very
	different travel times and breakthroughs. Inverse modeling combined
	with different geological models can provide a measure of capture
	zone and breakthrough reliability.},
  doi = {10.1007/s10040-003-0276-2},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{hashino2007,
  author = {Hashino, T. and Bradley, A. and Schwartz, S.},
  title = {Evaluationof bias--correection methods for ensemble streamflow volume
	forecasts},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {939--950},
  number = {2},
  abstract = {Ensemble prediction systems are used operationally to make probabilistic
	streamflow forecasts for seasonal time scales. However, hydrological
	models used for ensemble streamflow prediction often have simulation
	biases that degrade forecast quality and limit the operational usefulness
	of the forecasts. This study evaluates three bias-correction methods
	for ensemble streamflow volume forecasts. All three adjust the ensemble
	traces using a transformation derived with simulated and observed
	flows from a historical simulation. The quality of probabilistic
	forecasts issued when using the three bias-correction methods is
	evaluated using a distributions-oriented verification approach. Comparisons
	are made of retrospective forecasts of monthly flow volumes for a
	north-central United States basin (Des Moines River, Iowa), issued
	sequentially for each month over a 48-year record. The results show
	that all three bias-correction methods significantly improve forecast
	quality by eliminating unconditional biases and enhancing the potential
	skill. Still, subtle differences in the attributes of the bias-corrected
	forecasts have important implications for their use in operational
	decision-making. Diagnostic verification distinguishes these attributes
	in a context meaningful for decision-making, providing criteria to
	choose among bias-correction methods with comparable skill.},
  doi = {10.5194/hess-11-939-2007},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@ARTICLE{hassan2004a,
  author = {Hassan, A.},
  title = {A methodology for validating numerical ground water models},
  journal = {Ground Water},
  year = {2004},
  volume = {42},
  pages = {347--362},
  number = {3},
  abstract = {Ground water validation is one of the most challenging issues facing
	modelers and hydrogeologists. Increased complexity in ground water
	models has created a gap between model predictions and the ability
	to validate or build confidence in predictions. Specific procedures
	and tests that can be easily adapted and applied to determine the
	validity of site-specific ground water models do not exist. This
	is true for both deterministic and stochastic models, with stochastic
	models posing the more difficult validation problem. The objective
	of this paper is to propose a general validation approach that addresses
	important issues recognized in previous validation studies, conferences,
	and symposia. The proposed method links the processes for building,
	calibrating, evaluating, and validating models in an iterative loop.
	The approach focuses on using collected validation data to reduce
	uncertainty in the model and narrow the range of possible outcomes.
	This method is designed for stochastic numerical models utilizing
	Monte Carlo simulation approaches, but it can be easily adapted for
	deterministic models. The proposed methodology relies on the premise
	that absolute validity is not theoretically possible, nor is it a
	regulatory requirement. Rather, the proposed methodology highlights
	the importance of testing various aspects of the model and using
	diverse statistical tools for rigorous checking and confidence building
	in the model and its predictions. It is this confidence that will
	encourage regulators and the public to accept decisions based on
	the model predictions. This validation approach will be applied to
	a model, described in this paper, dealing with an underground nuclear
	test site in rural Nevada.},
  doi = {10.1111/j.1745-6584.2004.tb02683.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{hassan2004b,
  author = {Hassan, A.},
  title = {Validation of numerical ground water models used to guide decision
	making},
  journal = {Ground Water},
  year = {2004},
  volume = {42},
  pages = {277--290},
  number = {2},
  abstract = {Many sites of ground water contamination rely heavily on complex numerical
	models of flow and transport to develop closure plans. This complexity
	has created a need for tools and approaches that can build confidence
	in model predictions and provide evidence that these predictions
	are sufficient for decision making. Confidence building is a long-term,
	iterative process and the author believes that this process should
	be termed model validation. Model validation is a process, not an
	end result. That is, the process of model validation cannot ensure
	acceptable prediction or quality of the model. Rather, it provides
	an important safeguard against faulty models or inadequately developed
	and tested models. If model results become the basis for decision
	making, then the validation process provides evidence that the model
	is valid for making decisions (not necessarily a true representation
	of reality). Validation, verification, and confirmation are concepts
	associated with ground water numerical models that not only do not
	represent established and generally accepted practices, but there
	is not even widespread agreement on the meaning of the terms as applied
	to models. This paper presents a review of model validation studies
	that pertain to ground water flow and transport modeling. Definitions,
	literature debates, previously proposed validation strategies, and
	conferences and symposia that focused on subsurface model validation
	are reviewed and discussed. The review is general and focuses on
	site-specific, predictive ground water models used for making decisions
	regarding remediation activities and site closure. The aim is to
	provide a reasonable starting point for hydrogeologists facing model
	validation for ground water systems, thus saving a significant amount
	of time, effort, and cost. This review is also aimed at reviving
	the issue of model validation in the hydrogeologic community and
	stimulating the thinking of researchers and practitioners to develop
	practical and efficient tools for evaluating and refining ground
	water predictive models.},
  doi = {10.1111/j.1745-6584.2004.tb02674.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{hassan2009,
  author = {Hassan, A. and Bekhit, H. and Chapman, J.},
  title = {Using {M}arkov {C}hain {M}onte {C}arlo to quantify parameter uncertainty
	and its effect on predcitions of a groundwater flow model},
  journal = {Environmental Modelling \& Software},
  year = {2009},
  volume = {24},
  pages = {749--763},
  number = {6},
  abstract = {A statistical Bayesian framework is used to solve the inverse problem
	and develop the posterior distributions of parameters for a density-driven
	groundwater flow model. This Bayesian approach is implemented using
	a Markov Chain Monte Carlo (MCMC) sampling method. Three sets of
	data pertaining to the location of the freshwater–seawater transition
	zone exist for the site, including chemistry data, hydraulic head
	data and newly collected magnetotelluric (MT) data. A sequential
	conditioning approach is implemented where the chemistry data and
	MT-converted salinity are combined as a single data set and are used
	to first condition the parameter distributions. The head data are
	subsequently used as a second conditioning data set where the posterior
	distribution developed by the first conditioning is used as a prior
	for this second conditioning. Results of this analysis indicate that
	conditioning on the available data sets yields dramatic reduction
	of uncertainty compared to unconditioned simulations, especially
	for the recharge–conductivity ratio. This ratio controls the location
	of the transition zone, and the conditioning results in a smaller
	range of variability compared to the distribution used in previous
	modelling of the site. Using the conditioned distributions to solve
	the density-driven flow problem in a stochastic framework (i.e.,
	model parameters are randomly sampled from the posterior distributions)
	results in a range of output flow fields that is much narrower than
	the previous model. The ensemble mean of these solutions and the
	uncertainty bounds expressed by the mean ± one standard deviation
	lie within the uncertainty bounds of the original model. For the
	case study shown here, the effect of conditioning data is dominant
	over the effect of prior information.},
  doi = {10.1016/j.envsoft.2008.11.002},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{hassan2008,
  author = {Hassan, A. and Bekhit, H. and Chapman, J.},
  title = {Uncertainty assessment of a stochastic groundwater flow model using
	{GLUE} analysis},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {362},
  pages = {89--109},
  number = {1--2},
  abstract = {The use of the generalized likelihood uncertainty estimation (GLUE)
	methodology in analyzing the results of stochastic groundwater models
	is evaluated. The ability of the GLUE methodology to mitigate the
	effect of the selection of the input parameter prior distributions
	on the modeling results is investigated. This is important when no
	prior information is available or when significantly different priors
	come from different sources or experts. The different approaches
	that can be used to implement the GLUE methodology in analyzing the
	stochastic results of such models and quantifying the uncertainty
	in model prediction are evaluated. Recent debates about the GLUE
	methodology and the problem of using “less formal likelihood” functions
	are discussed in terms of the applicability of such issues to groundwater
	studies in general and a given field site specifically. These issues
	are investigated using a density-driven groundwater flow model of
	a nuclear testing site (Milrow) on Amchitka Island, Alaska. Results
	of the analysis highlight the subjectivity of the choice of the shape
	factor associated with the GLUE likelihood measures. However, the
	arbitrary choice of this factor can be tied to the level of confidence
	one can place on the available observations. While traditional GLUE
	applications focus on displaying prediction quantiles, GLUE can be
	used to develop uncertainty bounds that are qualitatively similar
	to predictive uncertainty. Interestingly, for the case study shown
	here the traditional GLUE quantiles and the uncertainty bounds are
	almost identical. Results also show that the GLUE-based ensemble
	averaging yields results that are controlled by the data more than
	by the prior distributions. The GLUE quantiles or GLUE-developed
	uncertainty bounds provide conditional predictions that are free
	from the artificial smoothing associated with ensemble averaging.},
  doi = {10.1016/j.jhydrol.2008.08.017},
  owner = {RRojas},
  timestamp = {2008.12.02}
}

@ARTICLE{hastings1970,
  author = {Hastings, W.},
  title = {Monte {C}arlo sampling methods using {M}arkov chains and their applications},
  journal = {Biometrika},
  year = {1970},
  volume = {57},
  pages = {97--109},
  number = {1},
  abstract = {A generalization of the sampling method introduced by Metropolis et
	al. (1953) is presented along with an exposition of the relevant
	theory, techniques of application and methods and difficulties of
	assessing the error in Monte Carlo estimates. Examples of the methods,
	including the generation of random orthogonal matrices and potential
	applications of the methods to numerical problems arising in statistics,
	are discussed.},
  doi = {10.1093/biomet/57.1.97},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{hay+al2002,
  author = {Hay, L. and Clark, M. and Wilby, R. and Gutowski, W. and Leavesley,
	G. and Pan, Z. and Arritt, R. and Takle, E.},
  title = {Use of regional climate model output for hydrologic simulations},
  journal = {Journal of Hydrometeorology},
  year = {2002},
  volume = {3},
  pages = {571--590},
  number = {5},
  abstract = {Daily precipitation and maximum and minimum temperature time series
	from a regional climate model (RegCM2) configured using the continental
	United States as a domain and run on a 52-km (approximately) spatial
	resolution were used as input to a distributed hydrologic model for
	one rainfall-dominated basin (Alapaha River at Statenville, Georgia)
	and three snowmelt-dominated basins (Animas River at Durango, Colorado;
	east fork of the Carson River near Gardnerville, Nevada; and Cle
	Elum River near Roslyn, Washington). For comparison purposes, spatially
	averaged daily datasets of precipitation and maximum and minimum
	temperature were developed from measured data for each basin. These
	datasets included precipitation and temperature data for all stations
	(hereafter, All-Sta) located within the area of the RegCM2 output
	used for each basin, but excluded station data used to calibrate
	the hydrologic model.},
  doi = {10.1175/1525-7541(2002)003<0571:UORCMO>2.0.CO;2},
  keywords = {UNITED-STATES, PARAMETERIZATIONS, PRECIPITATION, SCENARIOS, PROJECT},
  tags = {Climate Models, Downscaling}
}

@ARTICLE{hayhoe+al2004,
  author = {Hayhoe, K. and Cayan, D. and Field, C. and Frumhoff, P. and Maurer,
	E. and Miller, N. and Moser, S. and Schneider, S. and Cahill, K.
	and Cleland, E. and Dale, L. and Drapek, R. and Hanemann, R. and
	Kalkstein, L. and Lenihan, J. and Lunch, C. and Neilson, R. and Sheridan,
	S. and Verville, J.},
  title = {Emissions pathways, climate change, and impacts on {C}alifornia},
  journal = {Proceedings of The National Academy of Sciences of The United States
	of America},
  year = {2004},
  volume = {101},
  pages = {12422--12427},
  number = {34},
  abstract = {The magnitude of future climate change depends substantially on the
	greenhouse gas emission pathways we choose. Here we explore the implications
	of the highest and lowest Intergovernmental Panel on Climate Change
	emissions pathways for climate change and associated impacts in California.
	Based on climate projections from two state-of-the-art climate models
	with low and medium sensitivity (Parallel Climate Model and Hadley
	Centre Climate Model, version 3, respectively), we find that annual
	temperature increases nearly double from the lower B1 to the higher
	A1fi emissions scenario before 2100. Three of four simulations also
	show greater increases in summer temperatures as compared with winter.
	Extreme heat and the associated impacts on a range of temperature-sensitive
	sectors are substantially greater under the higher emissions scenario,
	with some interscenario differences apparent before midcentury. By
	the end of the century under the B1 scenario, heatwaves and extreme
	heat in Los Angeles quadruple in frequency while heat-related mortality
	increases two to three times; alpine/subalpine forests are reduced
	by 50-75%; and Sierra snowpack is reduced 30-70%. Under A1fi, heatwaves
	in Los Angeles are six to eight times more frequent, with heat-related
	excess mortality increasing five to seven times; alpine/subalpine
	forests are reduced by 75-90%; and snowpack declines 73-90%, with
	cascading impacts on runoff and streamflow that, combined with projected
	modest declines in winter precipitation, could fundamentally disrupt
	California's water rights system. Although interscenario differences
	in climate impacts and costs of adaptation emerge mainly in the second
	half of the century, they are strongly dependent on emissions from
	preceding decades.},
  doi = {10.1073/pnas.0404500101},
  keywords = {SAN-FRANCISCO ESTUARY, UNITED-STATES, POTENTIAL IMPACTS, CHANGE SCENARIOS,
	WATER-RESOURCES, RIVER-BASINS, MODEL, RESPONSES, WEATHER, MORTALITY},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{haylock+al2006,
  author = {Haylock, M. and Cawley, G. and Harpham, C. and Wilby, R. and Goodess,
	C.},
  title = {Downscaling heavy precipitation over the {U}nited {K}ingdom: a comparison
	of dynamical and statistical methods and their future scenarios},
  journal = {International Journal of Climatology},
  year = {2006},
  volume = {26},
  pages = {1397--1415},
  number = {10},
  month = {August},
  abstract = {Six statistical and two dynamical downscaling models were compared
	with regard to their ability to downscale seven seasonal indices
	of heavy precipitation for two station networks in northwest and
	southeast England. The skill among the eight downscaling models was
	high for those indices and seasons that had greater spatial coherence.
	Generally, winter showed the highest downscaling skill and summer
	the lowest. The rainfall indices that were indicative of rainfall
	occurrence were better modelled than those indicative of intensity.
	Models based on non-linear artificial neural networks were found
	to be the best at modelling the inter-annual variability of the indices;
	however, their strong negative biases implied a tendency to underestimate
	extremes. A novel approach used in one of the neural network models
	to output the rainfall probability and the gamma distribution scale
	and shape parameters for each day meant that resampling methods could
	be used to circumvent the underestimation of extremes. Six of the
	models were applied to the Hadley Centre global
	
	circulation model HadAM3P forced by emissions according to two SRES
	scenarios. This revealed that the inter-model differences between
	the future changes in the downscaled precipitation indices were at
	least as large as the differences between the emission scenarios
	for a single model. This implies caution when interpreting the output
	from a single model or a single type of model (e.g. regional climate
	models) and the advantage of including as many different types of
	downscaling models, global models and emission scenarios as possible
	when developing climate-change projections at the local scale.},
  doi = {10.1002/joc.1318},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{haylock+al2008,
  author = {Haylock, M. and Hofstra, N. and Klein, A. and Klok, E. and Jones,
	P. and New, M.},
  title = {A {E}uropean daily high--resolution gridded data set of surface temperature
	and precipitation for 1950--2006},
  journal = {Geophysical Research Letters},
  year = {2008},
  volume = {113},
  pages = {1--12},
  number = {D20119},
  abstract = {We present a European land-only daily high-resolution gridded data
	set for precipitation and minimum, maximum, and mean surface temperature
	for the period 1950–2006. This data set improves on previous products
	in its spatial resolution and extent, time period, number of contributing
	stations, and attention to finding the most appropriate method for
	spatial interpolation of daily climate observations. The gridded
	data are delivered on four spatial resolutions to match the grids
	used in previous products as well as many of the rotated pole Regional
	Climate Models (RCMs) currently in use. Each data set has been designed
	to provide the best estimate of grid box averages rather than point
	values to enable direct comparison with RCMs. We employ a three-step
	process of interpolation, by first interpolating the monthly precipitation
	totals and monthly mean temperature using three-dimensional thin-plate
	splines, then interpolating the daily anomalies using indicator and
	universal kriging for precipitation and kriging with an external
	drift for temperature, then combining the monthly and daily estimates.
	Interpolation uncertainty is quantified by the provision of daily
	standard errors for every grid square. The daily uncertainty averaged
	across the entire region is shown to be largely dependent on the
	season and number of contributing observations. We examine the effect
	that interpolation has on the magnitude of the extremes in the observations
	by calculating areal reduction factors for daily maximum temperature
	and precipitation events with return periods up to 10 years.},
  doi = {10.1029/2008JD010201},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{he+al2007,
  author = {He, B. and Takase, K. and Wang, Y.},
  title = {Regional groundwater prediction model using automatic parameter calibration
	SCE method for a coastal plain of Seto Island},
  journal = {Water Resource Management},
  year = {2007},
  volume = {21},
  pages = {947--959},
  number = {6},
  abstract = {Operational groundwater prediction models vary in complexity, but
	most of them have parameters for which values must be estimated.
	In the present study, the proposed regional groundwater prediction
	model was based on a nonlinear water balance model, which is very
	easy to be used once its parameters were determined. The traditional
	procedure of the model calibration was done manually using a trial
	and error process of parameter adjustments. In this case, the goodness-of-fit
	of the calibrated model is based on a visual judgment by comparing
	the simulated and the observed data. It requires considerable training
	or experience and is also typically laborious and time consuming.
	Thus, this paper proposed an approach, which considered the possibilities
	of using a nonlinear optimization technique -- the Shuffled Complex
	Evolution (SCE) method to calibrate the groundwater model. The applicability
	of this technique was demonstrated with a case study for a coastal
	plain in Japan. The performance of the groundwater model with SCE
	method was evaluated by comparing the measured and predicted data.},
  doi = {10.1007/s11269-006-9066-7},
  tags = {Calibration}
}

@ARTICLE{hellstromchen2003,
  author = {Hellstr\"om, C. and Chen, D.},
  title = {Statistical downscaling based on dynamically downscaled predictors:
	{A}pplication to monthly precipitation in {S}weden},
  journal = {Advances In Atmospheric Sciences},
  year = {2003},
  volume = {20},
  pages = {951--958},
  number = {6},
  abstract = {A prerequisite of a successful statistical downscaling is that large-scale
	predictors simulated by the General Circulation Model (GCM) must
	be realistic. It is assumed here that features smaller than the GCM
	resolution are important in determining the realism of the large-scale
	predictors. It is tested whether. a three-step method can improve
	conventional one-step statistical downscaling. The method uses predictors
	that are upscaled from a dynamical downscaling instead of predictors
	taken directly from a GCM simulation. The method is applied to downscaling
	of monthly precipitation in Sweden. The statistical model used is
	a multiple regression model that uses indices of large-scale atmospheric
	circulation and 850-hPa specific humidity as predictors. Data from
	two GCMs (HadCM2 and ECHAM4) and two RCM experiments of the Rossby
	Centre model (RCA1) driven by the GCMs are used. It is found that
	upscaled RCA1 predictors capture the seasonal cycle better than those
	from the GCMs, and hence increase the reliability of the downscaled
	precipitation. However, there are only slight improvements in the
	simulation of the seasonal cycle of downscaled precipitation. Due
	to the cost of the method and the limited improvements in the downscaling
	results, the three-step method is not justified to replace the one-step
	method for downscaling of Swedish precipitation.},
  doi = {10.1007/BF02915518},
  keywords = {downscaling, multiple regression, atmospheric circulation indices,
	monthly precipitation, Sweden, GENERAL-CIRCULATION MODEL, REGIONAL
	CLIMATE MODEL, TEMPERATURE TIME-SERIES, ATMOSPHERIC CIRCULATION,
	CHANGE SCENARIOS, NORTHERN EUROPE, GCM OUTPUT, SIMULATION, VALIDATION,
	PERFORMANCE},
  tags = {Downscaling}
}

@ARTICLE{hellstrom+al2001,
  author = {Hellstr\"om, C. and Chen, D. and Achberger, C. and R\"ais\"anen,
	J.},
  title = {Comparison of climate change scenarios for {S}weden based on statistical
	and dynamical downscaling of monthly precipitation},
  journal = {Climate Research},
  year = {2001},
  volume = {19},
  pages = {44--55},
  number = {1},
  abstract = {Two dynamically and statistically downscaled precipitation scenarios
	for Sweden are compared with respect to changes in the mean. The
	dynamically downscaled scenarios are generated by a 44 km version
	of the Rossby Centre regional climate model (RCM). The RCM is driven
	by data from 2 global greenhouse gas simulations sharing a 2.6Â°C
	global warming, one made by the HadCM2 and the other by the ECHAM4
	general circulation model (GCM). The statistical downscaling model
	driven by the same GCMs is regression-based and incorporates large-scale
	circulation indices of the 2 geostrophic wind components (u and v),
	total vorticity (Î¾) and large-scale humidity at 850 hPa (q850) as
	predictors. The precipitation climates of the GCMs, RCMs and statistical
	models from the control runs are compared with respect to their ability
	to reproduce the observed seasonal cycle. Great improvements in the
	simulation of the seasonal cycle by all the downscaling models compared
	to the GCMs significantly increase the credibility of the downscaling
	models. The precipitation changes produced by the statistical models
	result from changes in all predictors, but the change in Î¾ is the
	greatest contributor in southern Sweden followed by q850 and u, while
	changes in q850 have greater effects in the northern parts of the
	country. The temporal and spatial variability of precipitation changes
	are higher in the statistically downscaled scenarios than in the
	dynamically downscaled ones. Comparisons of the 4 scenarios show
	that the spread of the scenarios created by the statistical model
	is on average larger than that between the RCM scenarios. The relatively
	large average spread is mainly due to the large differences found
	in summer. The seasonally averaged difference of the dynamical and
	statistical scenarios for the ECHAM4-based downscaled scenarios is
	12%, and for the HadCM2 downscaled scenarios 21%. The differences
	in annual precipitation change are smaller, on average 4.5% among
	the HadCM2-based downscaled scenarios, and 6.9% among the ECHAM4-based
	downscaling scenarios},
  doi = {10.3354/cr019045},
  keywords = {downscaling, multiple regression, atmospheric circulation indices,
	monthly precipitation, Sweden.},
  tags = {Downscaling}
}

@ARTICLE{helton1996,
  author = {Helton, J.},
  title = {Guest editorial: {T}reatement of aleatory and epistemic uncertainty
	in performance assessment for complex systems},
  journal = {Reliability Engineering \& System Safety},
  year = {1996},
  volume = {54},
  pages = {91--94},
  number = {2--3},
  doi = {10.1016/S0951-8320(96)00066-X},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{hendricks2003,
  author = {{Hendricks Franssen}, {H-J}. and G\'omez-Hern\'andez, J. and Sahuquillo,
	A.},
  title = {Coupled inverse modelling of groundwater flow and mass transport
	and the worth of concentration data},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {281},
  pages = {281--295},
  number = {4},
  abstract = {This paper presents the extension of the self-calibrating method to
	the coupled inverse modelling of groundwater flow and mass transport.
	The method generates equally likely solutions to the inverse problem
	that display the variability as observed in the field and are not
	affected by a linearisation of the state equations. Conditioning
	to the state variables is measured by an objective function including,
	among others, the mismatch between the simulated and measured concentrations.
	Conditioning is achieved by minimising the objective function by
	gradient-based methods. The gradient contains the partial derivatives
	of the objective function with respect to: log conductivities, log
	storativities, prescribed heads at boundaries, retardation coefficients
	and mass sources. The derivatives of the objective function with
	respect to log conductivity are the most cumbersome and need the
	most CPU-time to be evaluated. For this reason, to compute this derivative
	only advective transport is considered. The gradient is calculated
	by the adjoint-state method. The method is demonstrated in a controlled,
	synthetic study, in which the worth of concentration data is analysed.
	It is shown that concentration data are essential to improve transport
	predictions and also help to improve aquifer characterisation and
	flow predictions, especially in the upstream part of the aquifer,
	even in the case that a considerable amount of other experimental
	data like conductivities and heads are available. Besides, conditioning
	to concentration data reduces the ensemble variances of estimated
	transmissivity, hydraulic head and concentration.},
  doi = {10.1016/S0022-1694(03)00191-4},
  owner = {RRojas},
  timestamp = {2009.03.13}
}

@ARTICLE{hernandez2006,
  author = {Hern\'andez, A. and Neuman, S. and Guadagnini, A. and Carrera, J.},
  title = {Inverse stochastic moment analysis of steady state flow in randomly
	heterogeneous media},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W05425},
  abstract = {Nonlocal stochastic moment equations have been used successfully to
	analyze steady state and transient flow in randomly heterogeneous
	media conditional on measured values of medium properties. We present
	a nonlinear geostatistical inverse algorithm for steady state flow
	that makes it possible to further condition such analyses on measured
	values of state variables. Our approach accounts for all scales of
	spatial variability resolvable by the computational grid. It is based
	on recursive finite element approximations of exact nonlocal first
	and second conditional moment equations. Hydraulic conductivity is
	parameterized geostatistically on the basis of measured values at
	discrete locations and unknown values at discrete “pilot points.”
	Prior estimates of pilot point values are obtained (optionally) by
	generalized kriging. Posterior estimates at pilot points and (optionally)
	at measurement points are obtained by calibrating mean flow against
	measured values of head. The parameters are projected onto a computational
	grid via kriging. Maximum likelihood calibration allows estimating
	not only hydraulic but also (optionally) unknown variogram parameters
	with or without prior information about the former. The approach
	yields covariance matrices for parameter estimation as well as head
	and flux prediction errors, the latter being obtained a posteriori.
	We implement our inverse approach on highly and mildly heterogeneous
	media under superimposed mean uniform and convergent flows in a bounded
	two-dimensional domain. Our examples illustrate how conductivity
	and head data act separately and jointly to reduce parameter estimation
	errors and to model predictive uncertainty. We also evaluate the
	functional form of the log conductivity variogram and its parameters
	using likelihood-based model discrimination criteria.},
  doi = {10.1029/2005WR004449},
  owner = {rojasro},
  timestamp = {2009.10.21}
}

@ARTICLE{herrera+al2010,
  author = {Herrera, S. and Fita, L. and Fernandez, J. and Gutierrez, J.},
  title = {Evaluation of the mean and extreme precipitation regimes from the
	{ENSEMBLES} regional climate multimodel simulations over {Spain}},
  journal = {Journal of Geophysical Research},
  year = {2010},
  volume = {115},
  pages = {1--13},
  number = {D21117},
  abstract = {A state?of?the?art ensemble of regional climate model (RCM) simulations
	provided by the European Union–funded project ENSEMBLES is used to
	test the ability of RCMs to reproduce the mean and extreme precipitation
	regimes over Spain. To this aim, ERA?40–driven simulations at 25
	km resolution are compared with the 20 km daily precipitation grid
	Spain02, considering the period 1960–2000. This gridded data set
	has been interpolated from thousands of quality?controlled stations
	capturing the spatial variability of precipitation over this RCM
	benchmark?like area with complex orography and influence of both
	Atlantic and Mediterranean climates. The results show a good representation
	of the mean regimes and the annual cycle but an overestimation of
	rainfall frequency leading to a wrong estimation of wet and dry spells.
	The amount of rainfall coming from extreme events is also deficient
	in the RCMs. The use of the multimodel ensemble improves the results
	of the individual models; moreover, discarding the worst performing
	models for the particular area and variable leads to improved results
	and reduced spread.},
  doi = {10.1029/2010JD013936},
  owner = {rojasro},
  timestamp = {2010.11.12}
}

@MISC{hewitt2005,
  author = {Hewitt, C.},
  title = {E{NSEMBLES}--providing ensemble--based predictions of climate changes
	and their impacts},
  howpublished = {Parliament Magazine, 11 July 2005, p. 57. Available online at \url{http://ensembles-eu.metoffice.com/docs/ParlMag_11Jul05.pdf}},
  year = {2005},
  tags = {Climate Models, RCMs}
}

@TECHREPORT{CIS2003,
  author = {Hiederer, R. and {de Roo}, A.},
  title = {A European flow network and catchment data set},
  institution = {Joint Research Centre, European Commission},
  year = {2003},
  owner = {rojasro},
  timestamp = {2011.05.12}
}

@INPROCEEDINGS{higashiIba2003,
  author = {Higashi, N. and Iba, H.},
  title = {Particle swarm optimization with Gaussian mutation},
  booktitle = {Swarm Intelligence Symposium, 2003. SIS {'}03. Proceedings of the
	2003 IEEE},
  year = {2003},
  pages = {72--79},
  month = {april},
  abstract = { In this paper we present particle swarm optimization with Gaussian
	mutation combining the idea of the particle swarm with concepts from
	evolutionary algorithms. This method combines the traditional velocity
	and position update rules with the ideas of Gaussian mutation. This
	model is tested and compared with the standard PSO and standard GA.
	The comparative experiments have been conducted on unimodal functions
	and multimodal functions. PSO with Gaussian mutation is able to obtain
	a result superior to GA. We also apply the PSO with Gaussian mutation
	to a gene network. Consequently, it has succeeded in acquiring better
	results than those by GA and PSO alone.},
  doi = {10.1109/SIS.2003.1202250},
  issn = { },
  keywords = { Gaussian mutation, evolutionary algorithms, gene network, multimodal
	functions, particle swarm optimization, position update rules, search
	techniques, unimodal functions, velocity update rules, Gaussian distribution,
	evolutionary computation, optimisation, search problems},
  tags = {Calibration, PSO}
}

@ARTICLE{hill2006,
  author = {Hill, M.},
  title = {The practical use of simplicity in developing ground water models},
  journal = {Ground Water},
  year = {2006},
  volume = {44},
  pages = {775--781},
  number = {6},
  abstract = {The advantages of starting with simple models and building complexity
	slowly can be significant in the development of ground water models.
	In many circumstances, simpler models are characterized by fewer
	defined parameters and shorter execution times. In this work, the
	number of parameters is used as the primary measure of simplicity
	and complexity; the advantages of shorter execution times also are
	considered. The ideas are presented in the context of constructing
	ground water models but are applicable to many fields. Simplicity
	first is put in perspective as part of the entire modeling process
	using 14 guidelines for effective model calibration. It is noted
	that neither very simple nor very complex models generally produce
	the most accurate predictions and that determining the appropriate
	level of complexity is an ill-defined process. It is suggested that
	a thorough evaluation of observation errors is essential to model
	development. Finally, specific ways are discussed to design useful
	ground water models that have fewer parameters and shorter execution
	times.},
  doi = {10.1111/j.1745-6584.2006.00227.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{hill1998,
  author = {Hill, M.},
  title = {Methods and guidelines for effective model calibration},
  institution = {United States Geological Survey},
  year = {1998},
  type = {Water Resources Investigation Report 98-4005},
  address = {Denver, Colorado, USA},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{mf2kobs,
  author = {Hill, M. and Banta, E. and Harbaugh, A. and Anderman, E.},
  title = {M{ODFLOW}--2000. The {U}.{S}. {G}eological {S}urvey modular ground-water
	model--{U}ser guide to the observation, sensitivity, and parameter--estimation
	processes and three post processing programs},
  institution = {United States Geological Survey},
  year = {2000},
  type = {Open-File Report 00-184},
  address = {Denver, Colorado, USA},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{hilletal1998,
  author = {Hill, M. and Cooley, R. and Pollock, D.},
  title = {A controlled experiment in ground water flow model calibration},
  journal = {Ground Water},
  year = {1998},
  volume = {36},
  pages = {520--535},
  number = {3},
  abstract = {Nonlinear regression was introduced to ground water modeling in the
	1970s, but has been used very little to calibrate numerical models
	of complicated ground water systems. Apparently, nonlinear regression
	is thought by many to be incapable of addressing such complex problems.
	With what we believe to be the most complicated synthetic test case
	used for such a study, this work investigates using nonlinear regression
	in ground water model calibration. Results of the study fall into
	two categories. First, the study demonstrates how systematic use
	of a well designed nonlinear regression method can indicate the importance
	of different types of data and can lead to successive improvement
	of models and their parameterizations. Our method differs from previous
	methods presented in the ground water literature in that (1) weighting
	is more closely related to expected data errors than is usually the
	case; (2) defined diagnostic statistics allow for more effective
	evaluation of the available data, the model, and their interaction;
	and (3) prior information is used more cautiously. Second, our results
	challenge some commonly held beliefs about model calibration. For
	the test case considered, we show that (1) field measured values
	of hydraulic conductivity are not as directly applicable to models
	as their use in some geostatistical methods imply; (2) a unique model
	does not necessarily need to be identified to obtain accurate predictions;
	and (3) in the absence of obvious model bias, model error was normally
	distributed. The complexity of the test case involved implies that
	the methods used and conclusions drawn are likely to be powerful
	in practice.},
  doi = {10.1111/j.1745-6584.1998.tb02824.x},
  owner = {rojasro},
  timestamp = {2010.02.09}
}

@BOOK{hilltiedeman2007,
  title = {Effective groundwater model calibration: {W}ith analysis of data,
	sensitivities, predictions and uncertainty},
  publisher = {John Wiley \& Sons, Inc.},
  year = {2007},
  author = {Hill, M. and Tiedeman, C.},
  pages = {480},
  address = {New Jersey},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.13}
}

@ARTICLE{hinnell+al2010,
  author = {Hinnell, A. and Ferr\'e, T. and Vrugt, J. and Huisman, J. and Moysey,
	S. and Rings, J. and Kowalsky, M.},
  title = {Improved extraction of hydrologic information from geophysical data
	through coupled hydrogeophysical inversion},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W00D40},
  abstract = {There is increasing interest in the use of multiple measurement types,
	including indirect (geophysical) methods, to constrain hydrologic
	interpretations. To date, most examples integrating geophysical measurements
	in hydrology have followed a three-step, uncoupled inverse approach.
	This approach begins with independent geophysical inversion to infer
	the spatial and/or temporal distribution of a geophysical property
	(e.g., electrical conductivity). The geophysical property is then
	converted to a hydrologic property (e.g., water content) through
	a petrophysical relation. The inferred hydrologic property is then
	used either independently or together with direct hydrologic observations
	to constrain a hydrologic inversion. We present an alternative approach,
	coupled inversion, which relies on direct coupling of hydrologic
	models and geophysical models during inversion. We compare the abilities
	of coupled and uncoupled inversion using a synthetic example where
	surface-based electrical conductivity surveys are used to monitor
	one-dimensional infiltration and redistribution. Through this illustrative
	example, we show that the coupled approach can provide significant
	reductions in uncertainty for hydrologic properties and associated
	predictions if the underlying model is a faithful representation
	of the hydrologic processes. However, if the hydrologic model exhibits
	structural errors, the coupled inversion may not improve the hydrologic
	interpretation. Despite this limitation, our results support the
	use of coupled hydrogeophysical inversion both for the direct benefits
	of reduced errors during inversion and because of the secondary benefits
	that accrue because of the extensive communication and sharing of
	data necessary to produce a coupled model, which will likely lead
	to more thoughtful use of geophysical data in hydrologic studies.
	},
  doi = {10.1029/2008WR007060},
  tags = {Uncertainty, Calibration}
}

@ARTICLE{hisdal+al2001,
  author = {Hisdal, Hege and Stahl, Kerstin and Tallaksen, Lena M. and Demuth,
	Siegfried},
  title = {Have streamflow droughts in Europe become more severe or frequent?},
  journal = {International Journal of Climatology},
  year = {2001},
  volume = {21},
  pages = {317--333},
  number = {3},
  abstract = {Changes in the magnitude and frequency of droughts will have extensive
	impacts on water management, agriculture and aquatic ecosystems.
	With the projected global temperature increase, scientists generally
	agree that the global hydrological cycle will intensify and suggest
	that extremes will become or have already become more common. In
	this study, a pan-European dataset of more than 600 daily streamflow
	records from the European Water Archive (EWA) was analysed to detect
	spatial and temporal changes in streamflow droughts. Four different
	time periods were analysed: 1962–1990, 1962–1995, 1930–1995 and 1911–1995.
	The focus was on hydrological droughts derived by applying the threshold
	level approach, which defines droughts as periods during which the
	streamflow is below a certain threshold. The Annual Maximum Series
	(AMS) of drought severity and the frequency of droughts in Partial
	Duration Series (PDS) were studied. Despite several reports on recent
	droughts in Europe, the non-parametric Mann–Kendall test and a resampling
	test for trend detection showed that it is not possible to conclude
	that drought conditions in general have become more severe or frequent.
	The period analysed and the selection of stations strongly influenced
	the regional pattern. For most stations, no significant changes were
	detected. However, distinct regional differences were found. Within
	the period 1962–1990 examples of increasing drought deficit volumes
	were found in Spain, the eastern part of Eastern Europe and in large
	parts of the UK, whereas decreasing drought deficit volumes occurred
	in large parts of Central Europe and in the western part of Eastern
	Europe. Trends in drought deficit volumes or durations could, to
	a large extent, be explained through changes in precipitation or
	artificial influences in the catchment. Changes in the number of
	drought events per year were determined by the combined effect of
	climate and catchment characteristics such as storage capacity. The
	importance of the time period chosen for trend analysis is illustrated
	using two very long time series. Copyright © 2001 Royal Meteorological
	Society},
  doi = {10.1002/joc.619},
  issn = {1097-0088},
  keywords = {streamflow drought, streamflow, trend test, Europe, regional study,
	European Water Archive (EWA)},
  publisher = {John Wiley \& Sons, Ltd.}
}

@ARTICLE{hoeksema1984,
  author = {Hoeksema, R. and Kitanidis, P.},
  title = {An application of the geostatistical approach to the inverse problem
	in two--dimensional groundwater modeling},
  journal = {Water Resources Research},
  year = {1984},
  volume = {20},
  pages = {1003-1020},
  number = {7},
  month = {July},
  abstract = {The geostatistical approach to the estimation of transmissivity from
	head and transmissivity measurements is developed for two-dimensional
	steady flow. The field of the logarithm of transmissivity (logtransmissivity)
	is represented as a zero-order intrinsic random field; its spatial
	structure is described in this application through a two-term covariance
	function that is linear in the parameters ?1 and ?2. Linearization
	of the discretized flow equations allows the construction of the
	joint covariance matrix of the head and log transmissivity measurements
	as a linear function of ?1 and ?2. In this particular application
	the coefficient matrices are calculated numerically in a noniterative
	fashion. Maximum likelihood estimation is employed to estimate ?1
	and ?2 as well as additional parameters from measurements. Linear
	estimation theory (cokriging) then yields point or block-averaged
	estimates of transmissivity. The approach is first applied to a test
	case with favorable results. It is shown that the application of
	the methodology gives good estimates of transmissivities. It is also
	shown that when the transmissivities are used in a numerical model
	they reproduce the head measurements quite well. Results from the
	application of the methodology to the Jordan aquifer in Iowa are
	also presented.},
  doi = {10.1029/WR020i007p01003},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@INPROCEEDINGS{hoeting2002,
  author = {Hoeting, J.},
  title = {Methodology for {B}ayesian model averaging: {A}n update},
  booktitle = {International Biometric Conference},
  year = {2002},
  pages = {231--240},
  address = {Freiburg. Germany},
  publisher = {International Biometric Society},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{hoeting1999,
  author = {Hoeting, J. and Madigan, D. and Raftery, A. and Volinsky, C.},
  title = {Bayesian model averaging: {A} tutorial},
  journal = {Statistical Science},
  year = {1999},
  volume = {14},
  pages = {382--401},
  number = {4},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts
	typically select a model from some class of models and then proceed
	as if the selected model had generated the data. This approach ignores
	the uncertainty in model selection, leading to over-confident in-
	ferences and decisions that are more risky than one thinks they are.
	Bayesian model averaging (BMA) provides a coherent mechanism for
	ac- counting for this model uncertainty. Several methods for implementing
	BMA have recently emerged. We discuss these methods and present a
	number of examples. In these examples, BMA provides improved out-of-
	sample predictive performance. We also provide a catalogue of currently
	available BMA software.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2676803}
}

@ARTICLE{hofstra+al2010,
  author = {Hofstra, N. and New, M. and {McSweeney}, C.},
  title = {The influence of interpolation and station network density on the
	distributions and trends of climate variables in gridded daily data},
  journal = {Climate Dynamics},
  year = {2010},
  volume = {35},
  pages = {841--858},
  number = {5},
  abstract = {We study the influence of station network density on the distributions
	and trends in indices of area-average daily precipitation and temperature
	in the E-OBS high resolution gridded dataset of daily climate over
	Europe, which was produced with the primary purpose of Regional Climate
	Model evaluation. Area averages can only be determined with reasonable
	accuracy from a sufficiently large number of stations within a grid-box.
	However, the station network on which E-OBS is based comprises only
	2,316 stations, spread unevenly across approximately 18,000 0.22°
	grid-boxes. Consequently, grid-box data in E-OBS are derived through
	interpolation of stations up to 500 km distant, with the distance
	of stations that contribute significantly to any grid-box value increasing
	in areas with lower station density. Since more dispersed stations
	have less shared variance, the resultant interpolated values are
	likely to be over-smoothed, and extreme daily values even more so.
	We perform an experiment over five E-OBS grid boxes for precipitation
	and temperature that have a sufficiently dense local station network
	to enable a reasonable estimate of the area-average. We then create
	a series of randomly selected station sub-networks ranging in size
	from four to all stations within the E-OBS interpolation search radii.
	For each sub-network realisation, we estimate the grid-box average
	applying the same interpolation methodology as used for E-OBS, and
	then evaluate the effect of network density on the distribution of
	daily values, as well as trends in extremes indices. The results
	show that when fewer stations have been used for the interpolation,
	both precipitation and temperature are over-smoothed, leading to
	a strong tendency for interpolated daily values to be reduced relative
	to the “true” area-average. The smoothing is greatest for higher
	percentiles, and therefore has a disproportionate effect on extremes
	and any derived extremes indices. For many regions of the E-OBS dataset,
	the station density is sufficiently low to expect this smoothing
	effect to be significant and this should be borne in mind by any
	users of the E-OBS dataset.},
  doi = {10.1007/s00382-009-0698-1},
  owner = {rojasro},
  timestamp = {2010.11.11}
}

@ARTICLE{holvoet+al2005,
  author = {Holvoet, K. and {van Griensven}, A. and Seuntjens, P. and Vanrolleghem,
	P.},
  title = {Sensitivity analysis for hydrology and pesticide supply towards the
	river in SWAT},
  journal = {Physics and Chemistry of the Earth, Parts A/B/C},
  year = {2005},
  volume = {30},
  pages = {518--526},
  number = {8--10},
  abstract = {The dynamic behaviour of pesticides in river systems strongly depends
	on varying climatological conditions and agricultural management
	practices. To describe this behaviour at the river-basin scale, integrated
	hydrological and water quality models are needed. A crucial step
	in understanding the various processes determining pesticide fate
	is to perform a sensitivity analysis. Sensitivity analysis for hydrology
	and pesticide supply in SWAT (Soil and Water Assessment Tool) will
	provide useful support for the development of a reliable hydrological
	model and will give insight in which parameters are most sensitive
	concerning pesticide supply towards rivers. The study was performed
	on the Nil catchment in Belgium. In this study we utilised an LH-OAT
	sensitivity analysis. The LH-OAT method combines the One-factor-At-a-Time
	(OAT) design and Latin Hypercube (LH) sampling by taking the Latin
	Hypercube samples as initial points for an OAT design. By means of
	the LH-OAT sensitivity analysis, the dominant hydrological parameters
	were determined and a reduction of the number of model parameters
	was performed. Dominant hydrological parameters were the curve number
	(CN2), the surface runoff lag (surlag), the recharge to deep aquifer
	(rchrg_dp) and the threshold depth of water in the shallow aquifer
	(GWQMN). Next, the selected parameters were estimated by manual calibration.
	Hereby, the Nash–Sutcliffe coefficient of efficiency improved from
	an initial value of ?22.4 to +0.53. In the second part, sensitivity
	analyses were performed to provide insight in which parameters or
	model inputs contribute most to variance in pesticide output. The
	results of this study show that for the Nil catchment, hydrologic
	parameters are dominant in controlling pesticide predictions. The
	other parameter that affects pesticide concentrations in surface
	water is ‘apfp_pest’, which meaning was changed into a parameter
	that controls direct losses to the river system (e.g., through the
	clean up of spray equipment, leaking tools, processing of spray waste
	on paved surfaces). As a consequence, it is of utmost importance
	that hydrology is well calibrated while––in this case––a correct
	estimation of the direct losses is of importance as well. Besides,
	a study of only the pesticide related parameters, i.e. application
	rate (kg/ha), application time (day), etc., reveals that the application
	time has much more impact than the application rate, which has itself
	a higher impact than errors in the daily rainfall observations.},
  doi = {10.1016/j.pce.2005.07.006},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{hong2008,
  author = {Hong, Wei-Chiang},
  title = {Rainfall forecasting by technological machine learning models},
  journal = {Applied Mathematics and Computation},
  year = {2008},
  volume = {200},
  pages = {41-57},
  number = {1},
  abstract = {Accurate forecasting of rainfall has been one of the most important
	issues in hydrological research. Due to rainfall forecasting involves
	a rather complex nonlinear data pattern; there are lots of novel
	forecasting approaches to improve the forecasting accuracy. Recurrent
	artificial neural networks (RNNS) have played a crucial role in forecasting
	rainfall data. Meanwhile, support vector machines (SVMs) have been
	successfully employed to solve nonlinear regression and time series
	problems. This investigation elucidates the feasibility of hybrid
	model of RNNs and SVMs, namely RSVR, to forecast rainfall depth values.
	Moreover, chaotic particle swarm optimization algorithm (CPSO) is
	employed to choose the parameters of a SVR model. Subsequently, example
	of rainfall values during typhoon periods from Northern Taiwan is
	used to illustrate the proposed RSVRCPSO model. The empirical results
	reveal that the proposed model yields well forecasting performance,
	RSVRCPSO model provides a promising alternative for forecasting rainfall
	values.},
  doi = {10.1016/j.amc.2007.10.046},
  keywords = {Rainfall forecasting; Support vector regression (SVR); Chaotic particle
	swarm optimization algorithm (CPSO); Recurrent SVR; Machine learning},
  tags = {PSO}
}

@ARTICLE{hornberger+al1985,
  author = {Hornberger, G. and Beven, K. and Cosby, B. and Sappington, D.},
  title = {Shenandoah Watershed Study: Calibration of a Topography-Based, Variable
	Contributing Area Hydrological Model to a Small Forested Catchment},
  journal = {Water Resources Research},
  year = {1985},
  volume = {21},
  pages = {1841--1850},
  number = {12},
  abstract = {The topography-based, variable contributing area model of catchment
	hydrology of K. J. Beven and E. F. Wood (1983) was adapted for continuous
	simulation and extended to take account of observed processes in
	White Oak Run, a small forested catchment in Shenandoah National
	Park, Virginia. Automatic calibration of the model was attempted
	using eight different objective functions. All objective functions
	were indifferent to many of the model parameters and thus parameter
	estimation could not be done reliably. On the basis of results from
	a regionalized sensitivity analysis, the original model structure
	was greatly simplified. The parameters of the simplified model, which
	produced fits to the measured data very nearly as good as did the
	more complex model, were estimated well using a sum of squared errors
	criterion.},
  doi = {10.1029/WR021i012p01841},
  tags = {conceptual model, Calibration}
}

@ARTICLE{horton+al2006,
  author = {Horton, P. and Schaefli, B. and Mezghani, A. and Beno\^it, H. and
	Musy, A.},
  title = {Assessment of climate--change impacts on alpine discharge regimes
	with climate model uncertainty},
  journal = {Hydrological Processes},
  year = {2006},
  volume = {20},
  pages = {2091--2109},
  number = {10},
  abstract = {This study analyses the uncertainty induced by the use of different
	state-of-the-art climate models on the prediction of climate-change
	impacts on the runoff regimes of 11 mountainous catchments in the
	Swiss Alps having current proportions of glacier cover between 0
	and 50%. The climate-change scenarios analysed are the result of
	19 regional climate model (RCM) runs obtained for the period 2070-2099
	based on two different greenhouse-gas emission scenarios (the A2
	and B2 scenarios defined by the Intergovernmental Panel on Climate
	Change) and on three different coupled atmosphere-ocean general circulation
	models (AOGCMs), namely HadCM3, ECHAM4/OPYC3 and ARPEGE/OPA. The
	hydrological response of the study catchments to the climate scenarios
	is simulated through a conceptual reservoir-based precipitation-runoff
	transformation model called GSM-SOCONT. For the glacierized catchments,
	the glacier surface corresponding to these future scenarios is updated
	through a conceptual glacier surface evolution model. The results
	obtained show that all climate-change scenarios induce, in all catchments,
	an earlier start of the snowmelt period, leading to a shift of the
	hydrological regimes and of the maximum monthly discharges. The mean
	annual runoff decreases significantly in most cases. For the glacierized
	catchments, the simulated regime modifications are mainly due to
	an increase of the mean temperature and the corresponding impacts
	on the snow accumulation and melting processes. The hydrological
	regime of the catchments located at lower altitudes is more strongly
	affected by the changes of the seasonal precipitation. For a given
	emission scenario, the simulated regime modifications of all catchments
	are highly variable for the different RCM runs. This variability
	is induced by the driving AOGCM, but also in large part by the inter-RCM
	variability. The differences between the different RCM runs are so
	important that the predicted climate-change impacts for the two emission
	scenarios A2 and B2 are overlapping.},
  doi = {10.1002/hyp.6197},
  tags = {Uncertainty, Climate Change}
}

@ARTICLE{houghtoncarr1999,
  author = {{Houghton-Carr}, H.},
  title = {Assessment criteria for simple conceptual daily rainfall-runoff models},
  journal = {Hydrological Sciences Journal},
  year = {1999},
  volume = {44},
  pages = {237--261},
  number = {2},
  abstract = {Four simple conceptual daily rainfall-runoff models are applied to
	a 25-basin data set. The drainage basins are all from the UK, covering
	a range of sizes, topographies, soils and climates. The quality of
	the simulation of the observed response is classically quantified
	by a minimized objective function. However, in this instance, model
	performance is judged by a range of quantitative and qualitative
	measures of fit, applied to both the calibration and validation periods.
	These include efficiency, mean annual runoff, baseflow index, the
	synthetic monthly and daily flow regimes, and the flow duration curve.
	The main conclusion is that the quantitative criteria used alone
	are rarely sufficient to determine the quality of the model performance.
	It is usually necessary to include some qualitative indication of
	goodness-of-fit, such as the quality of the synthetic daily flow
	hydrograph. However, assessment of the quality of daily flow regimes
	can be highly subjective. },
  doi = {10.1080/02626669909492220},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{houston2006,
  author = {Houston, J.},
  title = {The great {A}tacama flood of 2001 and its implications for Andean
	hydrology},
  journal = {Hydrologcial Processes},
  year = {2006},
  volume = {20},
  pages = {591--610},
  number = {3},
  abstract = {In February 2001, widespread flooding occurred throughout the Atacama
	Desert of northern Chile and southern Peru. It was particularly severe
	in the Río Loa basin, where roads and bridges were disrupted and
	the town of Calama inundated. The instantaneous peak flow in the
	Río Salado, a tributary of the Río Loa, reached 310 m3 s-1, an order
	of magnitude higher than any previously recorded event. The flood
	is estimated to have a return period of 100-200 years and is shown
	to have been caused by intense, long-duration rainfall in the western
	Cordillera associated with La Niña. The surface water response is
	typical of arid areas and highly dependent on antecedent conditions,
	but is quite different in perennial and ephemeral catchments. Ephemeral
	flood flows suffer high transmission losses, recharging phreatic
	aquifers. Perennial rivers have lower runoff coefficients, but baseflow
	levels remained high after the event for several months due to bank
	storage rebound and interflow. Extremely high energies of 3000 W
	m-2 were generated by the floods in the Cordillera, becoming less
	in the Precordillera and downstream. Erosion and sediment transport
	were consequently highest in the upper and middle reaches of the
	rivers, with mixed erosion-deposition in the lowest reach. The new
	insights gained from the interpretation and quantification of this
	event have important implications for palaeoenvironmental analysis,
	hazard management, water resource evaluation and the palaeohydrological
	evolution of the Andes.},
  doi = {10.1002/hyp.5926},
  owner = {RRojas},
  timestamp = {2009.04.08}
}

@ARTICLE{houston2006a,
  author = {Houston, J.},
  title = {Evaporation in the {Atacama Desert}: {A}n empirical study of spatio--temporal
	variations and their causes},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {330},
  pages = {402--412},
  number = {3--4},
  abstract = {The Atacama Desert is hyper-arid, and areas where adequate moisture
	exists for evaporation are spatially highly restricted. Nevertheless,
	water resources exist and their evaluation requires knowledge of
	this elusive but important component of the hydrological cycle. Evaporation
	may occur in four typical areas: rivers and associated riparian zones,
	localized springs, large playas and extensive areas of bare soil
	after infrequent precipitation events. Transpiration is locally possible
	where moisture is sufficiently close to the surface to allow phreatophytes
	or scarce grass cover to grow, but virtually no information is available
	for quantification. Pan evaporation data from 11 stations for the
	period 1977–1991 is analyzed and complemented by analysis of an evaporation
	study conducted in the Salar de Atacama during 1987/1988. The results
	show that pan evaporation, and hence maximum potential evaporation
	may be considered largely a function of maximum temperature and elevation
	as well as density of the evaporating fluid. Actual evaporation is
	limited by available moisture and diminishes rapidly as the level
	of soil moisture saturation drops below the soil surface, extinguishing
	at ca. 2 m depth. Evaporation is greatest during the summer, but
	at higher elevations convective cloudiness develops during January
	and February reducing evaporating rates at a time when significant
	precipitation may occur. Inter-annual variations in pan evaporation
	are considerable and weakly correlated with ENSO, but variations
	in actual evaporation are damped by comparison. Regression equations
	are developed which have widespread applicability and may be used
	to estimate evaporation in areas where no site-specific data exists.},
  doi = {10.1016/j.jhydrol.2006.03.036},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{houston2006b,
  author = {Houston, J.},
  title = {Recharge to groundwater in the {Turi} {Basin}, northern {Chile}:
	{A}n evaluation based on tritium and cholride mass balance techniques},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {334},
  pages = {534--544},
  number = {3--4},
  abstract = {The Turi Basin contains an unconfined phreatic aquifer within the
	Toconce Formation volcano-sedimentary sequence that passes eastwards
	under the Quaternary volcanic chain and outcrops in the Altiplano.
	The phreatic aquifer is floored by impermeable welded Miocene ignimbrites.
	Precipitation captured by the volcanic edifices infiltrates through
	high permeability soils to recharge the phreatic aquifer. The hydrochemistry
	of the groundwater in the phreatic aquifer shows little evidence
	of mixing with geothermal water and therefore approximates to a closed
	system. Tritium values demonstrate that recharge is occurring under
	current climatic conditions and takes around 40 years to pass 10+
	km from recharge areas on the slopes of the volcanoes and the Altiplano
	to the center of the basin. The chloride mass balance technique has
	been used to quantify precipitation recharge at 15,500 m3 d?1. This
	is significantly less than the flow through the phreatic aquifer
	(58,000–86,000 m3 d?1) calculated from its physical properties and
	the difference is interpreted to be due to subsurface inflow through
	the Toconce Formation from the Altiplano. The results of this study
	have wider application throughout the Andean hydrological environment
	of northern Chile.},
  doi = {10.1016/j.jhydrol.2006.10.030},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{houston2002,
  author = {Houston, J.},
  title = {Groundwater recharge through an alluvial fan in the {A}tacama {D}esert,
	northern {C}hile: {M}echanisms, magnitudes and causes},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {3019--3035},
  number = {15},
  abstract = {The Chacarilla fan in the Atacama Desert is one of several formed
	in the Late Miocene at the foot of the Pre-Andean Cordillera overlying
	the large, complex, Pampa Tamarugal aquifer contained in the continental
	clastic sediments of the fore-arc basin. The Pampa Tamarugal aquifer
	is a strategic source of water for northern Chile but there is continuing
	doubt over the resource magnitude and recharge. During January 2000
	a 1 in 4 year storm in the Andes delivered a 34 million m3 flash
	flood to the fan apex where c. 70% percolated to the underlying aquifers.
	Groundwater recharge through the fan is calculated to be a minimum
	of 200 l/s or 6% of the long-term catchment rainfall. These figures
	are supported by hydrochemical data that suggest that recharge may
	be 9% of long-term rainfall. Isotopic data suggest groundwater less
	than 50 years old is transmitted westward through the permeable sheetflood
	sediments of the fan overlying the main aquifer. Analysis of this
	and other events shows that the hydrological system is non-linear
	with positive feedback. The magnitude of groundwater recharge is
	dependent on climatic variations, antecedent soil moisture storage
	and changes in channel characteristics. Long-term declines in groundwater
	level may partly result from climatic fluctuations and the causes
	of such fluctuations are discussed.},
  doi = {10.1002/hyp.1086},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{hsu2009,
  author = {Hsu, K. and Moradkhani, H. and Sorooshian, S.},
  title = {A sequential {B}ayesian approach for hydrologic model selection and
	prediction},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W00B12},
  abstract = {When a single model is used for hydrologic prediction, it must be
	capable of estimating system behavior accurately at all times. Multiple-model
	approaches integrate several model behaviors and, when effective,
	they can provide better estimates than that of any single model alone.
	This paper discusses a sequential model fusion strategy that uses
	the Bayes rule. This approach calculates each model's transient posterior
	distribution at each time when a new observation is available and
	merges all model estimates on the basis of each model's posterior
	probability. This paper demonstrates the feasibility of this approach
	through case studies that fuse three hydrologic models, auto regressive
	with exogenous inputs, Sacramento soil moisture accounting, and artificial
	neural network models, to predict daily watershed streamflow.},
  doi = {10.1029/2008WR006824},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{hu2000,
  author = {Hu, L.},
  title = {Gradual deformation and iterative calibration of {G}aussian--related
	stochastic models},
  journal = {Mathematical Geology},
  year = {2000},
  volume = {32},
  pages = {87--108},
  number = {1},
  month = {January},
  abstract = {This paper describes a new method for gradually deforming realizations
	of Gaussian-related stochastic models while preserving their spatial
	variability. This method consists in building a stochastic process
	whose state space is the ensemble of the realizations of a spatial
	stochastic model. In particular, a stochastic process, built by combining
	independent Gaussian random functions, is proposed to perform the
	gradual deformation of realizations. Then, the gradual deformation
	algorithm is coupled with an optimization algorithm to calibrate
	realizations of stochastic models to nonlinear data. The method is
	applied to calibrate a continuous and a discrete synthetic permeability
	fields to well-test pressure data. The examples illustrate the efficiency
	of the proposed method. Furthermore, we present some extensions of
	this method (multidimensional gradual deformation, gradual deformation
	with respect to structural parameters, and local gradual deformation)
	that are useful in practice. Although the method described in this
	paper is operational only in the Gaussian framework (e.g., lognormal
	model, truncated Gaussian model, etc.), the idea of gradually deforming
	realizations through a stochastic process remains general and therefore
	promising even for calibrating non-Gaussian models.},
  doi = {10.1023/A:1007506918588},
  owner = {rojasro},
  timestamp = {2009.10.20}
}

@ARTICLE{huangmohan2005,
  author = {Huang, T. and Mohan, A.S.},
  title = {A Hybrid Boundary Condition for Robust Particle Swarm Optimization},
  journal = {Antennas and Wireless Propagation Letters},
  year = {2005},
  volume = {4},
  pages = {112-117},
  number = {1},
  abstract = {The particle swarm optimization (PSO) technique is a powerful stochastic
	evolutionary algorithm that can be used to find the global optimum
	solution in a complex search space. However, it has been observed
	that there is a great variation in its performance due to the dimensionality
	of the problem and the location of the global optimum with respect
	to the boundaries of the search space. The present paper attempts
	to resolve this problem by proposing a simple hybrid "damping" boundary
	condition that combines the characteristics offered by the existing
	"absorbing" and "reflecting" boundaries. Simulation results on microwave
	image reconstruction have shown that with the proposed "damping"
	boundary condition, a much robust and consistent optimization performance
	can be obtained for PSO regardless of the dimensionality and location
	of the global optimum solution.},
  doi = {10.1109/LAWP.2005.846166},
  keywords = {damping boundary condition},
  tags = {PSO}
}

@ARTICLE{hughes+al2006,
  author = {Hughes, D. and Andersson, L. and Wilk, J. and Savenije, H.},
  title = {Regional calibration of the Pitman model for the Okavango River},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {331},
  pages = {30--42},
  number = {1-2},
  abstract = {This paper reports on the application of a monthly rainfall-runoff
	model for the Okavango River Basin. Streamflow is mainly generated
	in Angola where the Cuito and Cubango rivers arise. They then join
	and cross the Namibia/Angola border, flowing into the Okavango wetland
	in Botswana. The model is a modified version of the Pitman model,
	including more explicit ground and surface water interactions. Significant
	limitations in access to climatological data, and lack of sufficiently
	long records of observed flow for the eastern sub-basins represent
	great challenges to model calibration. The majority of the runoff
	is generated in the wetter headwater tributaries, while the lower
	sub-basins are dominated by channel loss processes with very little
	incremental flow contributions, even during wet years. The western
	tributaries show significantly higher seasonal variation in flow,
	compared to the baseflow dominated eastern tributaries: observations
	that are consistent with their geological differences. The basin
	was sub-divided into 24 sub-basins, of which 18 have gauging stations
	at their outlet. Satisfactory simulations were achieved with sub-basin
	parameter value differences that correspond to the spatial variability
	in basin physiographic characteristics. The limited length of historical
	rainfall and river discharge data over Angola precluded the use of
	a split sample calibration/validation test. However, satellite generated
	rainfall data, revised to reflect the same frequency characteristics
	as the historical rainfall data, were used to validate the model
	against the available downstream flow data during the 1990s. The
	overall conclusion is that the model, in spite of the limited data
	access, adequately represents the hydrological response of the basin
	and that it can be used to assess the impact of future development
	scenarios. (c) 2006 Elsevier B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2006.04.047},
  keywords = {Okavango basin, rainfall-runoff models, regional calibration},
  optannote = {"The overall conclusion is that the monthly rainfall-runoff model,
	in spite of the limited data access, adequately represents the hydrological
	response of the basin and that it can be used to assess the impact
	of future development scenarios."abstract},
  tags = {Calibration}
}

@ARTICLE{hulme+al1999,
  author = {Hulme, M. and Mitchell, J. and Ingram, W. and Lowe, J. and Johns,
	T. and New, M. and Viner, D.},
  title = {Climate change scenarios for global impacts studies},
  journal = {Global Environmental Change},
  year = {1999},
  volume = {9},
  pages = {S3--S19},
  number = {1},
  abstract = {We describe a set of global climate change scenarios that have been
	used in a series of studies investigating the global impacts of climate
	change on several environmental systems and resources â€” ecosystems,
	food security, water resources, malaria and coastal flooding. These
	scenarios derive from modelling experiments completed by the Hadley
	Centre over the last four years using successive versions of their
	coupled ocean--atmosphere global climate model. The scenarios benefit
	from ensemble simulations (made using HadCM2) and from an un-flux-corrected
	experiment (made using HadCM3), but consider only the effects of
	increasing greenhouse gas concentrations. The effects of associated
	changes in sulphate aerosol concentrations are not considered. The
	scenarios are presented for three future time periods â€” 30-year
	means centred on the 2020s, the 2050s and the 2080s â€” and are expressed
	with respect to the mean 1961--1990 climate. A global land observed
	climatology at 0.5Â° latitude/longitude resolution is used to describe
	current climate. Other scenario variables â€” atmospheric CO2 concentrations,
	global-mean sea-level rise and non-climatic assumptions relating
	to population and economy â€” are also provided. We discuss the limitations
	of the created scenarios and in particular draw attention to sources
	of uncertainty that we have not fully sampled},
  doi = {10.1016/S0959-3780(99)00015-1},
  keywords = {Climate change scenarios, HadCM2, HadCM3, Sea-level rise},
  tags = {Scenarios}
}

@ARTICLE{hunt2007,
  author = {Hunt, R. and Doherty, J. and Tonkin, M.},
  title = {Are models too simple? {A}rguments for increased parameterization},
  journal = {Ground Water},
  year = {2007},
  volume = {45},
  pages = {254--262},
  number = {3},
  abstract = {The idea that models should be as simple as possible is often accepted
	without question. However, too much simplification and parsimony
	may degrade a model's utility. Models are often constructed to make
	predictions; yet, they are commonly parameterized with a focus on
	calibration, regardless of whether (1) the calibration data can constrain
	simulated predictions or (2) the number and type of calibration parameters
	are commensurate with the hydraulic property details on which key
	predictions may depend. Parameterization estimated through the calibration
	process is commonly limited by the necessity that the number of calibration
	parameters be smaller than the number of observations. This limitation
	largely stems from historical restrictions in calibration and computing
	capability; we argue here that better methods and computing capabilities
	are now available and should become more widely used. To make this
	case, two approaches to model calibration are contrasted: (1) a traditional
	approach based on a small number of homogeneous parameter zones defined
	by the modeler a priori and (2) regularized inversion, which includes
	many more parameters than the traditional approach. We discuss some
	advantages of regularized inversion, focusing on the increased insight
	that can be gained from calibration data. We present these issues
	using reasoning that we believe has a common sense appeal to modelers;
	knowledge of mathematics is not required to follow our arguments.
	We present equations in an Appendix, however, to illustrate the fundamental
	differences between traditional model calibration and a regularized
	inversion approach.},
  doi = {10.1111/j.1745-6584.2007.00316.x},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{huntington2006,
  author = {Huntington, T.},
  title = {Evidence for intensification of the global water cycle: {R}eview
	and synthesis},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {319},
  pages = {83--95},
  number = {1--4},
  abstract = {One of the more important questions in hydrology is: if the climate
	warms in the future, will there be an intensification of the water
	cycle and, if so, the nature of that intensification? There is considerable
	interest in this question because an intensification of the water
	cycle may lead to changes in water-resource availability, an increase
	in the frequency and intensity of tropical storms, floods, and droughts,
	and an amplification of warming through the water vapor feedback.
	Empirical evidence for ongoing intensification of the water cycle
	would provide additional support for the theoretical framework that
	links intensification with warming. This paper briefly reviews the
	current state of science regarding historical trends in hydrologic
	variables, including precipitation, runoff, tropospheric water vapor,
	soil moisture, glacier mass balance, evaporation, evapotranspiration,
	and growing season length. Data are often incomplete in spatial and
	temporal domains and regional analyses are variable and sometimes
	contradictory; however, the weight of evidence indicates an ongoing
	intensification of the water cycle. In contrast to these trends,
	the empirical evidence to date does not consistently support an increase
	in the frequency or intensity of tropical storms and floods.},
  doi = {10.1016/j.jhydrol.2005.07.003},
  tags = {Global Cycle}
}

@ARTICLE{hurkmans+al2010,
  author = {Hurkmans, R. and Terink, W. and Uijlenhoet, R. and Torfs, P. and
	Jacob, D. and Troch, P.},
  title = {Changes in streamflow dynamics in the {R}hine basin under three high--resolution
	regional climate scenarios},
  journal = {Journal of Climate},
  year = {2010},
  volume = {23},
  pages = {679--699},
  number = {3},
  abstract = {Because of global warming, the hydrologic behavior of the Rhine basin
	is expected to shift from a combined snowmelt- and rainfall-driven
	regime to a more rainfall-dominated regime. Previous impact assessments
	have indicated that this leads, on average, to increasing streamflow
	by 30% in winter and spring and decreasing streamflow by a similar
	value in summer. In this study, high-resolution (0.088°) regional
	climate scenarios conducted with the regional climate model REMO
	(REgional MOdel) for the Rhine basin are used to force a macroscale
	hydrological model. These climate scenarios are based on model output
	from the ECHAM5–Max Planck Institute Ocean Model (MPI-OM) global
	climate model, which is in turn forced by three Special Report on
	Emissions Scenarios (SRES) emission scenarios: A2, A1B, and B1. The
	Variable Infiltration Capacity model (VIC; version 4.0.5) is used
	to examine changes in streamflow at various locations throughout
	the Rhine basin. Average streamflow, peak flows, low flows, and several
	water balance terms are evaluated for both the first and second half
	of the twenty-first century. The results reveal a distinct contrast
	between those periods. The first half is dominated by increased precipitation,
	causing increased streamflow throughout the year. During the second
	half of the century, a streamflow increase in winter/spring and a
	decrease in summer is found, similar to previous studies. This is
	caused by 1) temperature and evapotranspiration, which are considerably
	higher during the second half of the century; 2) decreased precipitation
	in summer; and 3) an earlier start of the snowmelt season. Magnitudes
	of peak flows increase during both periods, and the magnitudes of
	streamflow droughts increase only during the second half of the century.},
  doi = {10.1175/2009JCLI3066.1},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@ARTICLE{hurvich1989,
  author = {Hurvich, C. and Tsai, C.},
  title = {Regression and time series model selection in small sample},
  journal = {Biometrika},
  year = {1989},
  volume = {76},
  pages = {297--307},
  number = {2},
  abstract = {A bias correction to the Akaike information criterion, AIC, is derived
	for regression and autoregressive time series models. The correction
	is of particular use when the sample size is small, or when the number
	of fitted parameters is a moderate to large fraction of the sample
	size. The corrected method, called AICC, is asymptotically efficient
	if the true model is infinite dimensional. Furthermore, when the
	true model is of finite dimension, AICC is found to provide better
	model order choices than any other asymptotically efficient method.
	Applications to nonstationary autoregressive and mixed autoregressive
	moving average time series models are also discussed.},
  doi = {10.1093/biomet/76.2.297},
  owner = {RRojas},
  refid = {HURVICH1989},
  timestamp = {2008.11.04}
}

@ARTICLE{hojberg2005,
  author = {H{\o}jberg, A. and Refsgaard, J.},
  title = {Model uncertainty--parameter uncertainty versus conceptual models},
  journal = {Water Science \& Technology},
  year = {2005},
  volume = {52},
  pages = {177--186},
  number = {6},
  abstract = {Uncertainties in model structures have been recognised often to be
	the main source of uncertainty in predictive model simulations. Despite
	this knowledge, uncertainty studies are traditionally limited to
	a single deterministic model and the uncertainty addressed by a parameter
	uncertainty study. The extent to which a parameter uncertainty study
	may encompass model structure errors in a groundwater model is studied
	in a case study. Three groundwater models were constructed on the
	basis of three different hydrogeological interpretations. Each of
	the models was calibrated inversely against groundwater heads and
	streamflows. A parameter uncertainty analysis was carried out for
	each of the three conceptual models by Monte Carlo simulations. A
	comparison of the predictive uncertainties for the three conceptual
	models showed large differences between the uncertainty intervals.
	Most discrepancies were observed for data types not used in the model
	calibration. Thus uncertainties in the conceptual models become of
	increasing importance when predictive simulations consider data types
	that are extrapolates from the data types used for calibration.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.iwaponline.com/wst/05206/wst052060177.htm}
}

@ARTICLE{ihakagentleman1996,
  author = {Ihaka, R. and Gentleman, R.},
  title = {{R}: A Language for Data Analysis and Graphics},
  journal = {Journal of Computational and Graphical Statistics},
  year = {1996},
  volume = {5},
  pages = {299-314},
  number = {3},
  tags = {R}
}

@ARTICLE{iizumi+al2011,
  author = {Iizumi, T. and Nishimori, M. and Dairaku, K. and Adachi, S. and Yokozawa,
	M.},
  title = {{Evaluation and intercomparison of downscaled daily precipitation
	indices over Japan in present-day climate: Strengths and weaknesses
	of dynamical and bias correction-type statistical downscaling methods}},
  journal = {Journal of Geophysical Research},
  year = {2011},
  volume = {116},
  pages = {1--20},
  number = {D01111},
  abstract = {In this study, we evaluate the accuracy of four regional climate models
	(NHRCM, NRAMS, TRAMS, and TWRF) and one bias correction-type statistical
	model (CDFDM) for daily precipitation indices under the present-day
	climate (1985–2004) over Japan on a 20 km grid interval. The evaluated
	indices are (1) mean precipitation, (2) number of days with precipitation
	?1 mm/d (corresponds to number of wet days), (3) mean amount per
	wet day, (4) 90th percentile of daily precipitation, and (5) number
	of days with precipitation ?90th percentile of daily precipitation.
	The boundary conditions of the dynamical models and the predictors
	of the statistical model are given from the single reanalysis data,
	i.e., JRA25. Both types of models successfully improved the accuracy
	of the indices relative to the reanalysis data in terms of bias,
	seasonal cycle, geographical pattern, cumulative distribution function
	of wet-day amount, and interannual variation pattern. In most aspects,
	NHRCM is the best model of all indices. Through the intercomparison
	between the dynamical and statistical models, respective strengths
	and weaknesses emerged. Briefly, (1) many dynamical models simulate
	too many wet days with a small amount of precipitation in humid climate
	zones, such as summer in Japan, relative to the statistical model,
	unless the cumulus convection scheme improved for such a condition
	is incorporated; (2) a few dynamical models can derive a better high-order
	percentile of daily precipitation (e.g., 90th percentile) than the
	statistical model; (3) both the dynamical and statistical models
	are still insufficient in the representation of the interannual variation
	pattern of the number of days with precipitation ?90th percentile
	of daily precipitation; (4) the statistical model is comparable to
	the dynamical models in the long-term mean geographical pattern of
	the indices even on a 20 km grid interval if a dense observation
	network is applicable; (5) the statistical model is less accurate
	than the dynamical models in the temporal variation pattern due to
	the strong dependence of the predictand on the relatively less accurate
	predictor (daily reanalysis precipitation); and (6) the simple statistical
	model is less plausible in the physical sense because of the oversimplification
	of underlying physical processes compared to the dynamical models
	and more sophisticated statistical models.},
  doi = {10.1029/2010JD014513},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@ARTICLE{ijiri2009,
  author = {Ijiri, Y. and Saegusa, H. and Sawada, A. and Ono, M. and Watanabe,
	K. and Karasaki, K. and Doughty, C. and Shimo, M. and Fumimura, K.},
  title = {Evaluation of uncertainties originating from the different modeling
	approaches applied to analyze regional groundwater flow in the {Tono}
	area of {Japan}},
  journal = {Journal of Contaminant Hydrology},
  year = {2009},
  volume = {103},
  pages = {168--181},
  number = {3--4},
  abstract = {Qualitative evaluation of the effects of uncertainties originating
	from scenario development, modeling approaches, and parameter values
	is an important subject in the area of safety assessment for high-level
	nuclear waste disposal sites. In this study, regional-scale groundwater
	flow analyses for the Tono area, Japan were conducted using three
	continuous models designed to handle heterogeneous porous media.
	We evaluated the simulation results to quantitatively analyze uncertainties
	originating from modeling approaches. We found that porous media
	heterogeneity is the main factor which causes uncertainties. We also
	found that uncertainties originating from modeling approaches greatly
	depend on the types of hydrological structures and heterogeneity
	of hydraulic conductivity values in the domain assigned by modelers.
	Uncertainties originating from modeling approaches decrease as the
	amount of labor and time spent increase, and iterations between investigation
	and analyses increases.},
  doi = {10.1016/j.jconhyd.2008.10.010},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{Im2007,
  author = {Im, S. and Brannan, K. and Mostaghimi, S. and Kim, S.},
  title = {Comparison of {HSPF} and {SWAT} models performance for runoff and
	sediment yield prediction},
  journal = {Journal of Environmental Science And Health Part A-Toxic/Hazardous
	Substances \& Environmental Engineering},
  year = {2007},
  volume = {42},
  pages = {1561--1570},
  number = {11},
  abstract = {A watershed model can be used to better understand the relationship
	between land use activities and hydrologic/water quality processes
	that occur within a watershed. The physically based, distributed
	parameter model ( SWAT) and a conceptual, lumped parameter model
	(HSPF), were selected and their performance were compared in simulating
	runoff and sediment yields from the Polecat Creek watershed in Virginia,
	which is 12,048 ha in size. A monitoring project was conducted in
	Polecat Creek watershed during the period of October 1994 to June
	2000. The observed data ( stream flow and sediment yield) from the
	monitoring project was used in the calibration/validations of the
	models. The period of September 1996 to June 2000 was used for the
	calibration and October 1994 to December 1995 was used for the validation
	of the models. The outputs from the models were compared to the observed
	data at several sub-watershed outlets and at the watershed outlet
	of the Polecat Creek watershed. The results indicated that both models
	were generally able to simulate stream flow and sediment yields well
	during both the calibration/ validation periods. For annual and monthly
	loads, HSPF simulated hydrologic and sediment yield more accurately
	than SWAT at all monitoring sites within the watershed. The results
	of this study indicate that both the SWAT and HSPF watershed models
	performed sufficiently well in the simulation of stream flow and
	sediment yield with HSPF performing moderately better than SWAT for
	simulation time-steps greater than a month.},
  doi = {10.1080/10934520701513456},
  keywords = {watershed model, HSPF, SWAT, runoff, sediment yield, POLLUTION},
  pmid = {17849297},
  tags = {SWAT}
}

@ARTICLE{ines2006,
  author = {Ines, A. and Hansen, J.},
  title = {Bias correction of daily {GCM} rainfall for crop simulation studies},
  journal = {Agricultural and Forest Meteorology},
  year = {2006},
  volume = {138},
  pages = {44--53},
  number = {1--4},
  abstract = {General circulation models (GCMs), used to predict rainfall at a seasonal
	lead-time, tend to simulate too many rainfall events of too low intensity
	relative to individual stations within a GCM grid cell. Even if bias
	in total rainfall is corrected relative to a target location, this
	distortion of frequency and intensity is expected to adversely affect
	simulations of crop growth and yield. We present a procedure that
	calibrates both the frequency and the intensity distribution of daily
	GCM rainfall relative to a target station, and demonstrate its application
	to maize yield simulation at a location in semi-arid Kenya. If GCM
	rainfall frequency is greater than observed frequency for a given
	month, averaged across years, GCM rainfall frequency is corrected
	by discarding rainfall events below a calibrated threshold. To correct
	the intensity distribution, each GCM rainfall amount above the calibrated
	threshold is mapped from the GCM intensity distribution onto the
	observed distribution. We used a gamma distribution for observed
	rainfall intensity, and considered both gamma and empirical distributions
	for GCM rainfall intensity. At the study location, the proposed correction
	procedure corrected both the mean and variance of monthly and seasonal
	GCM rainfall total, frequency and mean intensity. The empirical (GCM)-gamma
	(observed) transformation overestimated mean intensity slightly.
	A simple multiplicative shift did a better job of correcting monthly
	and seasonal rainfall totals, but left substantial frequency and
	intensity bias. All of the bias correction procedures improved maize
	yield simulations, but resulted in substantial negative mean bias.
	This bias appears to be associated with a tendency for the GCM rainfall
	to be more strongly autocorrelated than observed rainfall, resulting
	in unrealistically long dry spells during the growing season. Nonlinearity
	of crop response to the variability of water availability across
	GCM realizations may also contribute. Averaging simulated yields
	each year across multiple GCM realizations improved yield predictions.
	The proposed correction procedure provides an option for using the
	daily output of dynamic climate prediction models for impact studies
	in a manner that preserves any useful predictive information about
	the timing of rainfall within the season. However, its practical
	utility for yield forecasting at a long lead-time may be limited
	by the ability of GCMs to simulate rainfall with a realistic time
	structure.},
  doi = {10.1016/j.agrformet.2006.03.009},
  owner = {rojasro},
  timestamp = {2010.06.25}
}

@MISC{IPCC-DDC2010,
  author = {{IPCC DDC}},
  title = {Climate model output: period--averages},
  howpublished = {\url{http://www.ipcc-data.org/ddc_climscen.html}},
  year = {2010},
  note = {[Online; last accessed Feb-2010]},
  tags = {Climate Models}
}

@INCOLLECTION{IPCC2007,
  author = {{IPCC}},
  title = {Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution
	of Working Group II to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {Parry, M.L. and Canziani, O.F. and Palutikof, J.P. and {van der Linden},
	P.J. and Hanson, C.E.},
  address = {Cambridge},
  owner = {rojasro},
  timestamp = {2012.06.26}
}

@INCOLLECTION{IPCC1996,
  author = {{IPCC}},
  title = {Climate Change 1995: Impacts, Adaptions and Mitigation of Climate
	Change: Scientific-Technical Analyses},
  booktitle = {Contribution of Working Group I to the Second Assessment Report of
	the {I}ntergovernmental {P}anel on {C}limate {C}hange},
  publisher = {Cambridge University Press, Cambridge},
  year = {1996},
  tags = {Downscaling, IPCC}
}

@BOOK{isaaks1989,
  title = {An introduction to {A}pplied {G}eostatistics},
  publisher = {Oxford University Press},
  year = {1989},
  author = {Isaaks, E. and Srivastava, R.},
  pages = {592},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.27}
}

@ARTICLE{jacob+al2007,
  author = {Jacob, D. and B\"arring, L. and Christensen, O. and Christensen,
	J. and {de Castro}, M. and D\'equ\'e, M. and Giorgi, F. and Hagemann,
	S. and Hirschi, M. and Jones, R. and Kjellstr\"om, E. and Lenderink,
	G. and Rockel, B. and S\'anchez, E. and Sch\"ar, C. and Seneviratne,
	S. and Somot, S. and Ulden, A. and Hurk, B.},
  title = {An inter--comparison of regional climate models for {E}urope: {M}odel
	performance in present--day climate},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {31--52},
  number = {1},
  abstract = {The analysis of possible regional climate changes over Europe as simulated
	by 10 regional climate models within the context of PRUDENCE requires
	a careful investigation of possible systematic biases in the models.
	The purpose of this paper is to identify how the main model systematic
	biases vary across the different models. Two fundamental aspects
	of model validation are addressed here: the ability to simulate (1)
	the long-term (30 or 40 years) mean climate and (2) the inter-annual
	variability. The analysis concentrates on near-surface air temperature
	and precipitation over land and focuses mainly on winter and summer.
	In general, there is a warm bias with respect to the CRU data set
	in these extreme seasons and a tendency to cold biases in the transition
	seasons. In winter the typical spread (standard deviation) between
	the models is 1 K. During summer there is generally a better agreement
	between observed and simulated values of inter-annual variability
	although there is a relatively clear signal that the modeled temperature
	variability is larger than suggested by observations, while precipitation
	variability is closer to observations. The areas with warm (cold)
	bias in winter generally exhibit wet (dry) biases, whereas the relationship
	is the reverse during summer (though much less clear, coupling warm
	(cold) biases with dry (wet) ones). When comparing the RCMs with
	their driving GCM, they generally reproduce the large-scale circulation
	of the GCM though in some cases there are substantial differences
	between regional biases in surface temperature and precipitation.},
  doi = {10.1007/s10584-006-9213-4},
  keywords = {PRUCDENCE, bias},
  tags = {Multimodel - Ensambles}
}

@INCOLLECTION{jakeman+al1993,
  author = {Jakeman, A. and Chen, T. and Post, D. and Hornberger, G. and Littlewood,
	I.},
  title = {Assessing uncertainties in hydrological response to climate at large
	scales},
  booktitle = {Macroscale modelling of the hydrosphere},
  publisher = {IAHS Press},
  year = {1993},
  editor = {W. B. Wilkinson},
  pages = {37--47},
  address = {Wallingford, UK},
  organisation = {IAHS Publication No. 214},
  tags = {Uncertainty}
}

@ARTICLE{jakemanhornberger1993,
  author = {Jakeman, A. and Hornberger, G.},
  title = {How Much Complexity Is Warranted In A Rainfall-Runoff Model},
  journal = {Water Resources Research},
  year = {1993},
  volume = {29},
  pages = {2637--2649},
  number = {8},
  abstract = {Development of mathematical models relating the precipitation incident
	upon a catchment to the streamflow emanating from the catchment has
	been a major focus of surface water hydrology for decades. Generally,
	values for parameters in such models must be selected so that runoff
	calculated from the model ''matches'' recorded runoff from some historical
	period. Despite the fact that the physics governing the path of a
	drop of water through a catchment to the stream involves complex
	relationships, evidence indicates that the information content in
	a rainfall-runoff record is sufficient to support models of only
	very limited complexity. This begs the question of what limits the
	observed data place on the allowable complexity of rainfall-runoff
	models. Time series techniques are applied for estimating transfer
	functions to determine how many parameters are appropriate to describe
	the relationship between precipitation and streamflow in the case
	where data on only precipitation, air temperature, and streamflow
	are available. Statistics from an ''information matrix'' provide
	the clues necessary for determining allowable model complexity. Time
	series models are developed for seven catchments with widely varying
	physical characteristics in different temperate climatic regimes
	to demonstrate the method. It is found that after modulating the
	measured rainfall using a nonlinear loss function, the rainfall-runoff
	response of all catchments is well represented using a linear model.
	Also, for all catchments a two-component linear model with four parameters
	is the model of choice. The two components can be interpreted as
	defining a ''quick flow'' and ''slow flow'' response of the given
	catchment. The method therefore provides a statistically rigorous
	way to separate hydrographs and parameterize their response behavior.
	The ability to construct reliable transfer function models for describing
	the rainfall-runoff process offers a new approach to investigate
	empirically the controls of physical catchment descriptors, land
	use change, climate change, etc., on the dynamic response of catchments
	through the extensive analysis of historical data sets.},
  doi = {10.1029/93WR00877},
  keywords = {SMALL UPLAND CATCHMENTS, CALIBRATION, IDENTIFICATION},
  tags = {conceptual model, Philosophical}
}

@ARTICLE{jakeman+al2006,
  author = {Jakeman, A. and Letcher, R. and Norton, J.},
  title = {Ten iterative steps in development and evaluation of environmental
	models},
  journal = {Environmental Modelling \& Software},
  year = {2006},
  volume = {21},
  pages = {602--614},
  number = {5},
  doi = {10.1016/j.envsoft.2006.01.004},
  owner = {rojasro},
  timestamp = {2012.03.22}
}

@ARTICLE{james1997,
  author = {James, A. and Oldenburg, C.},
  title = {Linear and {M}onte {C}arlo analysis for subsurface contaminant transport
	simulation},
  journal = {Water Resources Research},
  year = {1997},
  volume = {33},
  pages = {2495--2508},
  number = {11},
  abstract = {Transport simulation can be used to predict subsurface contaminant
	distributions and the effectiveness of site remediation technologies.
	Because of the uncertainty inherent in subsurface transport prediction,
	an integral part of predictive modeling is uncertainty analysis.
	We have investigated the uncertainty of simulated trichloroethylene
	(TCE) concentrations due to parameter uncertainty and variation in
	conceptual model. The transport simulation is performed with T2VOC,
	a three-dimensional integral finite difference code for three-phase
	(gas, aqueous, non-aqueous phase liquid), three-component (air, water,
	volatile organic compound), nonisothermal subsurface flow. Our modeling
	is based on an actual site and considers a three-dimensional system
	with a thick vadose zone (25 m) into which 35,000 kg of TCE was disposed
	of in surface trenches over 10 years. Subsurface transport involves
	TCE moving in the vadose and saturated zones from the source trench
	toward a nearby residence and includes the processes of advection,
	diffusion, and adsorption over extended distances and timescales.
	Uncertainties in the calculated concentrations due to variance in
	the major transport parameters are quantified using the inverse modeling
	code ITOUGH2. ITOUGH2 calculates uncertainty in the T2VOC simulation
	by both first-order, second-moment (FOSM) and Monte Carlo analyses.
	Significant uncertainty in simulated TCE concentrations at a site
	of potential human exposure is observed owing to uncertainty in permeability,
	porosity, diffusivity, chemical solubility, and adsorption within
	a single conceptual model. For the case study considered the linear
	FOSM analysis generally captures the uncertainty range calculated
	by the more accurate Monte Carlo method. Calculations also show that
	significant output uncertainty is introduced by conceptual model
	variation. Because human exposure and health risk depend strongly
	on contaminant concentrations, risk assessment and remediation selection
	based on transport simulation is meaningful only if the analysis
	includes quantitative estimates of transport simulation uncertainty.},
  doi = {10.1029/97WR01925},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{jansonmiddendorf2005,
  author = {Janson, S. and Middendorf, M.},
  title = {A Hierarchical Particle Swarm Optimizer and Its Adaptive Variant},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics},
  year = {2005},
  volume = {35},
  pages = {1272--1282},
  number = {6},
  abstract = {A hierarchical version of the particle swarm optimization (PSO) metaheuristic
	is introduced in this paper. In the new method called H-PSO, the
	particles are arranged in a dynamic hierarchy that is used to define
	a neighborhood structure. Depending on the quality of their so-far
	best-found solution, the particles move up or down the hierarchy.
	This gives good particles that move up in the hierarchy a larger
	influence on the swarm. We introduce a variant of H-PSO, in which
	the shape of the hierarchy is dynamically adapted during the execution
	of the algorithm. Another variant is to assign different behavior
	to the individual particles with respect to their level in the hierarchy.
	H-PSO and its variants are tested on a commonly used set of optimization
	functions and are compared to PSO using different standard neighborhood
	schemes.},
  doi = {10.1109/TSMCB.2005.850530},
  keywords = {H-PSO method, hierarchical particle swarm optimization, metaheuristic,
	heuristic programming, particle swarm optimisation, Algorithms, Artificial
	Intelligence, Computer Simulation, Models, Theoretical},
  tags = {PSO, Calibration}
}

@ARTICLE{janssenheuberger1995,
  author = {Janssen, P. and Heuberger, P.},
  title = {Calibration of process-oriented models},
  journal = {Ecological Modelling},
  year = {1995},
  volume = {83},
  pages = {55--66},
  number = {1-2},
  abstract = {Model calibration is a critical phase in the modelling process, and
	the need for a well-established calibration strategy is obvious.
	Therefore a systematic approach for model calibration is proposed
	which is guided by the intended model use, and which is supported
	by adequate techniques, prior knowledge and expert judgement. The
	success of calibration will be primarily limited by the nature, amount
	and quality of the available data, in relation to the complexity
	of the model; additional limitations are the effectiveness of the
	applied techniques and the availability of time, man- and computer
	power, adequate expertise and financial resources. These limitations
	will often preclude a unique calibrated model. As a consequence,
	calibration studies should provide information on the non-uniqueness
	and/or uncertainty which will be left in the model (parameters) after
	calibration, and this uncertainty should be adequately accounted
	for in subsequent model applications.},
  doi = {10.1016/0304-3800(95)00084-9},
  issn = {0304-3800},
  keywords = {Calibration, Uncertainty analysis, Validation},
  mzb_note = {Modelling Water, Carbon and Nutrient Cycles in Forests},
  tags = {Calibration}
}

@PHDTHESIS{jensen2003,
  author = {Jensen, J.},
  title = {Parameter and uncertainty estimation in groundwater modelling},
  school = {Department of Civil Engineering. Aalborg University},
  year = {2003},
  address = {Denmark},
  month = {September},
  note = {Series Paper Nr. 23},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{jeremiah+al2011,
  author = {Jeremiah, E. and Sisson, S. and Marshall, L. and Mehrotra, R. and
	Sharma, A.},
  title = {{Bayesian calibration and uncertainty analysis of hydrological models:
	A comparison of adaptive Metropolis and sequential Monte Carlo samplers}},
  journal = {Water Resources Research},
  year = {2011},
  volume = {47},
  number = {W07547},
  abstract = {Bayesian statistical inference implemented by stochastic algorithms
	such as Markov chain Monte Carlo (MCMC) provides a flexible probabilistic
	framework for model calibration that accounts for both model and
	parameter uncertainties. The effectiveness of such Monte Carlo algorithms
	depends strongly on the user-specified proposal or sampling distribution.
	In this article, a sequential Monte Carlo (SMC) approach is used
	to obtain posterior parameter estimates of a conceptual hydrologic
	model using data from selected catchments in eastern Australia. The
	results are evaluated against the popular adaptive Metropolis MCMC
	sampling approach. Both methods display robustness and convergence,
	but the SMC displays greater efficiency in exploring the parameter
	space in catchments where the optimal solutions lie in the tails
	of the prescribed prior distribution. The SMC method is also able
	to identify a different set of parameters with an overall improvement
	in likelihood and Nash-Sutcliffe efficiency for selected catchments.
	As a result of its population-based sampling mechanism, the SMC method
	is shown to offer improved efficiency in identifying parameter optimization
	and to provide sampling robustness, in particular in identifying
	global posterior modes.},
  doi = {10.1029/2010WR010217},
  owner = {rojasro},
  timestamp = {2012.09.13}
}

@ARTICLE{jian+al2007a,
  author = {Jian, T. and Chen, Y. and Xu, {C.-Y.} and Chen, X. and Chen, X. and
	Singh, V.},
  title = {Comparison of hydrological impacts of climate change simulated by
	six hydrological models in the {D}ongjiang {B}asin, {S}outh {C}hina},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {336},
  pages = {316--333},
  number = {3--4},
  abstract = {Large differences in future climatic scenarios found when different
	global circulation models (GCMs) are employed have been extensively
	discussed in the scientific literature. However, differences in hydrological
	responses to the climatic scenarios resulting from the use of different
	hydrological models have received much less attention. Therefore,
	comparing and quantifying such differences are of particular importance
	for the water resources management of a catchment, a region, a continent,
	or even the globe. This study investigates potential impacts of human-induced
	climate change on the water availability in the Dongjiang basin,
	South China, using six monthly water balance models, namely the Thornthwaite--Mather
	(TM), Vrije Universitet Brussel (VUB), Xinanjiang (XAJ), Guo (GM),
	WatBal (WM), and Schaake (SM) models. The study utilizes 29-year
	long records of monthly streamflow and climate in the Dongjiang basin.
	The capability of the six models in simulating the present climate
	water balance components is first evaluated and the results of the
	models in simulating the impact of the postulated climate change
	are then analyzed and compared. The results of analysis reveal that
	(1) all six conceptual models have similar capabilities in reproducing
	historical water balance components; (2) greater differences in the
	model results occur when the models are used to simulate the hydrological
	impact of the postulated climate changes; and (3) a model without
	a threshold in soil moisture simulation results in greater changes
	in model-predicted soil moisture with respect to alternative climates
	than the models with a threshold soil moisture. The study provides
	insights into the plausible changes in basin hydrology due to climate
	change, that is, it shows that there can be significant implications
	for the investigation of response strategies for water supply and
	flood control due to climate change},
  doi = {10.1016/j.jhydrol.2007.01.010},
  keywords = {Climate change, Water balance models, Model comparison, Hydrological
	impacts},
  tags = {Impacts, Uncertainty}
}

@ARTICLE{jian+al2007b,
  author = {Jiang, Y. and Hu, T. and Huang, C. and Wu, X.},
  title = {An improved particle swarm optimization algorithm},
  journal = {Applied Mathematics and Computation},
  year = {2007},
  volume = {193},
  pages = {231--239},
  number = {1},
  note = {IPSO},
  abstract = {An improved particle swarm optimization (IPSO) is proposed in this
	paper. In the new algorithm, a population of points sampled randomly
	from the feasible space. Then the population is partitioned into
	several sub-swarms, each of which is made to evolve based on particle
	swarm optimization (PSO) algorithm. At periodic stages in the evolution,
	the entire population is shuffled, and then points are reassigned
	to sub-swarms to ensure information sharing. This method greatly
	elevates the ability of exploration and exploitation. Simulations
	for three benchmark test functions show that IPSO possesses better
	ability to find the global optimum than that of the standard PSO
	algorithm. Compared with PSO, IPSO is also applied to identify the
	hydrologic model. The results show that IPSO remarkably improves
	the calculation accuracy and is an effective global optimization
	to calibrate hydrologic},
  doi = {10.1016/j.amc.2007.03.047},
  keywords = {Particle swarm optimization, Improved particle swarm optimization,
	Global optimization, Hydrologic model, Parameters calibration},
  tags = {Calibration, PSO}
}

@ARTICLE{jiang+al2010,
  author = {Jiang, Y. and Liu, C. and Huang, C. and Wu, X.},
  title = {Improved particle swarm algorithm for hydrological parameter optimization},
  journal = {Applied Mathematics and Computation},
  year = {2010},
  volume = {217},
  pages = {3207--3215},
  number = {7},
  abstract = {In this paper, a new method named MSSE-PSO (master–slave swarms shuffling
	evolution algorithm based on particle swarm optimization) is proposed.
	Firstly, a population of points is sampled randomly from the feasible
	space, and then partitioned into several sub-swarms (one master swarm
	and other slave swarms). Each slave swarm independently executes
	PSO or its variants, including the update of particles’ position
	and velocity. For the master swarm, the particles enhance themselves
	based on the social knowledge of master swarm and that of slave swarms.
	At periodic stage in the evolution, the master swarm and the whole
	slave swarms are forced to mix, and points are then reassigned to
	several sub-swarms to ensure the share of information. The process
	is repeated until a user-defined stopping criterion is reached. The
	tests of numerical simulation and the case study on hydrological
	model show that MSSE-PSO remarkably improves the accuracy of calibration,
	reduces the time of computation and enhances the performance of stability.
	Therefore, it is an effective and efficient global optimization method.},
  doi = {10.1016/j.amc.2010.08.053},
  owner = {rojasro},
  timestamp = {2011.11.18}
}

@ARTICLE{jiang2006,
  author = {Jiang, Y. and Woodbury, A.},
  title = {A full--{B}ayesian approach to the inverse problem for steady--state
	groundwater flow and heat transport},
  journal = {Geophysical Journal International},
  year = {2006},
  volume = {167},
  pages = {1501--1512},
  number = {3},
  abstract = {The full (hierarchal) Bayesian approach proposed by Woodbury & Ulrych
	and Jiang et al. is extended to the inverse problem for 2-D steady-state
	groundwater flow and heat transport. A stochastic conceptual framework
	for the heat flow and groundwater flow is adopted. A perturbation
	of both the groundwater flow and the advection-conduction heat transport
	equations leads to a linear formulation between heads, temperature
	and logarithm transmissivity [denoted as ln (T)]. A Bayesian updating
	procedure similar to that of Woodbury & Ulrych can then be performed.
	This new algorithm is examined against a generic example through
	simulations. The prior mean, variance and integral scales of ln (T)
	(hyperparameters) are treated as random variables and their pdfs
	are determined from maximum entropy considerations. It is also assumed
	that the statistical properties of the noise in the hydraulic head
	and temperature measurements are also uncertain. Uncertainties in
	all pertinent hyperparameters are removed by marginalization. It
	is found that the use of temperature measurements is showed to further
	improve the ln (T) estimates for the test case in comparison to the
	updated ln (T) field conditioned on ln (T) and head data; the addition
	of temperature data without hydraulic head data to the update also
	aids refinement of the ln (T) field compared to simply interpolating
	ln (T) data alone these results suggest that temperature measurements
	are a promising data source for site characterization for heterogeneous
	aquifer, which can be accomplished through the full-Bayesian methodology.},
  doi = {10.1111/j.1365-246X.2006.03145.x},
  owner = {rojasro},
  timestamp = {2009.10.21}
}

@ARTICLE{jin+al2010,
  author = {Xiaoli Jin and Chong-Yu Xu and Qi Zhang and V.P. Singh},
  title = {Parameter and modeling uncertainty simulated by GLUE and a formal
	Bayesian method for a conceptual hydrological model},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {383},
  pages = {147--155},
  number = {3–-4},
  abstract = {Summary Quantification of uncertainty of hydrological models has attracted
	much attention in hydrologic research in recent years. Many methods
	for quantification of uncertainty have been reported in the literature,
	of which GLUE and formal Bayesian method are the two most popular
	methods. There have been many discussions in the literature concerning
	differences between these two methods in theory (mathematics) and
	results, and this paper focuses on the computational efficiency and
	differences in their results, but not on philosophies and mathematical
	rigor that both methods rely on. By assessing parameter and modeling
	uncertainty of a simple conceptual water balance model (WASMOD) with
	the use of GLUE and formal Bayesian method, the paper evaluates differences
	in the results of the two methods and discusses the reasons for these
	differences. The main findings of the study are that: (1) the parameter
	posterior distributions generated by the Bayesian method are slightly
	less scattered than those by the GLUE method; (2) using a higher
	threshold value (&gt;0.8) GLUE results in very similar estimates
	of parameter and model uncertainty as does the Bayesian method; and
	(3) GLUE is sensitive to the threshold value used to select behavioral
	parameter sets and lower threshold values resulting in a wider uncertainty
	interval of the posterior distribution of parameters, and a wider
	confidence interval of model uncertainty. More study is needed to
	generalize the findings of the present study.},
  doi = {10.1016/j.jhydrol.2009.12.028},
  issn = {0022-1694},
  keywords = {Conceptual hydrological model}
}

@ARTICLE{johnstonpilgrim1976,
  author = {Johnston, P. and Pilgrim, D.},
  title = {Parameter Optimization for Watershed Models},
  journal = {Water Resources Research},
  year = {1976},
  volume = {12},
  pages = {477--486},
  number = {3},
  abstract = {A detailed search for the optimum values of the parameters of the
	Boughton model is described. The Simplex and Davidon optimization
	methods were used. Rapid initial reductions in the objective function
	were readily achieved, but the solutions approached several widely
	different apparent optima. Alternate use of different optimization
	methods and numerical and algebraic studies enabled considerable
	further progress to be made in the search. Much information was obtained
	on various aspects of parameter optimization. These include interdependence
	and indifference of parameters, the form of the response surface
	and the occurrence of discontinuities, the required length of the
	{`}warm-up{'} period for different types of stores, and the effects
	of using different types of objective functions. As typical stores
	were analyzed and the only basic assumption involved was that the
	data contained errors, the findings should apply to most watershed
	models},
  doi = {10.1029/WR012i003p00477},
  tags = {Calibration}
}

@ARTICLE{jones2000,
  author = {Jones, R.},
  title = {Managing uncertainty in climate change projections--{I}ssues for
	impact assessment--{A}n editorial comment},
  journal = {Climatic Change},
  year = {2000},
  volume = {45},
  pages = {403--419},
  number = {3--4},
  abstract = {Climate change projection is the term the IPCC Second Assessment Report
	(SAR) uses for model estimates of future climate. In that report,
	projections are presented in two forms: as single model scenarios
	and as projected ranges of uncertainty. In climate studies, scenarios
	are commonly regarded as being plausible, but have no further probability
	attached. Projected ranges of uncertainty can have probabilities
	attached to the range and within the range, so are more likely to
	occur than individual scenarios. However, as there is significant
	remaining uncertainty beyond the projected range, such projections
	cannot be regarded as forecasts. An appropriate terminology is required
	to communicate this distinction. The sources of uncertainty in projected
	ranges of global temperature to 2100 are analysed by Visser et al.
	(2000), who recommend that ail major sources of uncertainty be incorporated
	into global warming projections. This will expand its projected range
	beyond that of the IPCC SAR. Further sources of uncertainties are
	contained within projections of regional climate. Several strategies
	that aim to manage that uncertainty are described. Uncertainty can
	also be managed where it is unquantifiable. An example is rapid climate
	change, where discarding the term climate 'surprises' in favour of
	more precise terminology to aid in identifying possible adaptation
	strategies, is recommended.},
  doi = {10.1023/A:1005551626280},
  tags = {Uncertainty}
}

@ARTICLE{jongman+al2012,
  author = {Brenden Jongman and Philip J. Ward and Jeroen C.J.H. Aerts},
  title = {Global exposure to river and coastal flooding: Long term trends and
	changes},
  journal = {Global Environmental Change},
  year = {2012},
  volume = {22},
  pages = {823--835},
  number = {4},
  abstract = {Flood damage modelling has traditionally been limited to the local,
	regional or national scale. Recent flood events, population growth
	and climate change concerns have increased the need for global methods
	with both spatial and temporal dynamics. This paper presents a first
	estimation of global economic exposure to both river and coastal
	flooding for the period 1970–2050, using two different methods for
	damage assessment. One method is based on population and the second
	is based on land-use within areas subject to 1/100 year flood events.
	On the basis of population density and GDP per capita, we estimate
	a total global exposure to river and coastal flooding of 46 trillion
	USD in 2010. By 2050, these numbers are projected to increase to
	158 trillion USD. Using a land-use based assessment, we estimated
	a total flood exposure of 27 trillion USD in 2010. For 2050 we simulate
	a total exposure of 80 trillion USD. The largest absolute exposure
	changes between 1970 and 2050 are simulated in North America and
	Asia. In relative terms we project the largest increases in North
	Africa and Sub-Saharan Africa. The models also show systematically
	larger growth in the population living within hazard zones compared
	to total population growth. While the methods unveil similar overall
	trends in flood exposure, there are significant differences in the
	estimates and geographical distribution. These differences result
	from inherent model characteristics and the varying relationship
	between population density and the total urban area in the regions
	of analysis. We propose further research on the modelling of inundation
	characteristics and flood protection standards, which can complement
	the methodologies presented in this paper to enable the development
	of a global flood risk framework.},
  doi = {10.1016/j.gloenvcha.2012.07.004},
  issn = {0959-3780},
  keywords = {Flood risk}
}

@ARTICLE{kalf2005,
  author = {Kalf, F. and Woolley, D.},
  title = {Applicability and methodology of determining sustainable yield in
	groundwater systems},
  journal = {Hydrogeology Journal},
  year = {2005},
  volume = {13},
  pages = {295--312},
  number = {1},
  abstract = {There is currently a need for a review of the definition and methodology
	of determining sustainable yield. The reasons are: (1) current definitions
	and concepts are ambiguous and non-physically based so cannot be
	used for quantitative application, (2) there is a need to eliminate
	varying interpretations and misinterpretations and provide a sound
	basis for application, (3) the notion that all groundwater systems
	either are or can be made to be sustainable is invalid, (4) often
	there are an excessive number of factors bound up in the definition
	that are not easily quantifiable, (5) there is often confusion between
	production facility optimal yield and basin sustainable yield, (6)
	in many semi-arid and arid environments groundwater systems cannot
	be sensibly developed using a sustained yield policy particularly
	where ecological constraints are applied. Derivation of sustainable
	yield using conservation of mass principles leads to expressions
	for basin sustainable, partial (non-sustainable) mining and total
	(non-sustainable) mining yields that can be readily determined using
	numerical modelling methods and selected on the basis of applied
	constraints. For some cases there has to be recognition that the
	groundwater resource is not renewable and its use cannot therefore
	be sustainable. In these cases, its destiny should be the best equitable
	use.},
  doi = {10.1007/s10040-004-0401-x},
  owner = {RRojas},
  timestamp = {2008.12.04}
}

@ARTICLE{kannan+al2008,
  author = {Kannan, N. and Santhi, C. and Arnold, J.},
  title = {Development of an automated procedure for estimation of the spatial
	variation of runoff in large river basins},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {359},
  pages = {1--15},
  abstract = {The use of distributed parameter models to address water resource
	management problems has increased in recent years. Calibration is
	necessary to reduce the uncertainties associated with model input
	parameters. Manual calibration of a distributed parameter model,
	is a very time consuming effort. Therefore, more attention is given
	to automated calibration procedures. This paper describes the development
	and demonstration of such an automated procedure developed for a
	national/continental scale assessment study called Conservation Effects
	Assessment Project (CEAP). The automated procedure is developed to
	calibrate spatial variation of annual average runoff components for
	each USGS eight-digit watershed of the United States. It uses nine
	parameters to calibrate water yield, surface runoff and sub-surface
	flow respectively. If necessary, the procedure uses a linear interpolation
	method to arrive at a better value of a model. parameter. When tested
	for the Upper Mississippi river basin of the United States, the automated
	calibration procedure gave satisfactory results. Other test results
	from the procedure are very encouraging and show potential for its
	use in very large-scale hydrologic modeling studies. (C) 2008 Elsevier
	B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2008.06.001},
  keywords = {SWAT, calibration, regional modeling, Upper Mississippi, parameterization,
	runoff, CONTERMINOUS UNITED-STATES, SENSITIVITY-ANALYSIS, GLOBAL
	OPTIMIZATION, HYDROLOGIC-MODELS, WATERSHED MODELS, BASE-FLOW, CALIBRATION,
	EVAPOTRANSPIRATION, PRECIPITATION, SWAT},
  tags = {Applications}
}

@ARTICLE{kannan+al2007,
  author = {Kannan, N. and White, S. and Worrall, F. and Whelan, M.},
  title = {Sensitivity analysis and identification of the best evapotranspiration
	and runoff options for hydrological modelling in {SWAT}-2000},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {332},
  pages = {456--466},
  number = {3-4},
  abstract = {Distributed models used in hydrological modelling, have many parameters.
	To get useful results from the model, every parameter is required
	to have a sensible value. Usually a calibration is undertaken to
	reduce the uncertainties associated with the estimation of model
	parameters. To ensure efficient calibration, a sensitivity analysis
	is conducted to identify the most sensitive parameters. This paper
	describes simple and efficient approaches for sensitivity analysis,
	calibration and identification of the best methodology within a modelling
	framework. For this study, the SWAT-2000 model was used on a small
	catchment of 141.5 ha in the Unilever Colworth estate, in Bedfordshire,
	England. Acceptable performance in hydrological modelling, and correct
	simulation of the processes driving the water balance were essential
	requirements for subsequent pesticide modelling. SWAT gives various
	options for both evapotranspiration and runoff modelling. Identification
	of the best modelling option for these processes is a pre-requisite
	to achieve these requirements. As a first step, a sensitivity analysis
	was conducted to identify the sensitive parameters affecting stream
	flow for subsequent application in stream flow calibration. Hydrological
	modelling has been carried out for the catchment for the period September
	1999 to May 2002 inclusive using both daily and sub-daily rainfall
	data. The Hargreaves and Penman-Montieth methods of evapotranspiration
	estimation and the NRCS curve number (CN) and Green and Ampt infiltration
	methods for runoff estimation techniques were used, in four different
	combinations, to identify the combination of methodologies that best
	reproduced the observed data. In addition, as the initial calibration
	period, starting in September 1999, was substantially wetter than
	the following corresponding validation period, the calibration and
	validation periods are interchanged to test the impact of calibration
	using wet or dry periods. (c) 2006 Elsevier B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2006.08.001},
  keywords = {SWAT, hydrological modelling, Colworth, curve number (CN), sensitivity,
	stream flow, GREEN-AMPT, CALIBRATION, PARAMETERS, SIMULATION, SWAT},
  tags = {Sensitivity Analysis, SWAT}
}

@ARTICLE{kashyap1982,
  author = {Kashyap, R.},
  title = {Optimal choice of {AR} and {MA} parts in autoregressive moving average
	models},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {1982},
  volume = {PAMI-4},
  pages = {99--104},
  number = {2},
  abstract = {This paper deals with the Bayesian method of choosing the best model
	for a given one-dimensional series among a finite number of candidates
	belonging to autoregressive (AR), moving average (MA), ARMA, and
	other families. The series could be either a sequence of observations
	in time as in speech applications, or a sequence of pixel intensities
	of a two-dimensional image. The observation set is not restricted
	to be Gaussian. We first derive an optimum decision rule for assigning
	the given observation set to one of the candidate models so as to
	minimize the average probability of error in the decision. We also
	derive an optimal decision rule so as to minimize the average value
	of the loss function. Then we simplify the decision rule when the
	candidate models are different Gaussian ARMA models of different
	orders. We discuss the consistency of the optimal decision rule and
	compare it with the other decision rules in the literature for comparing
	dynamical models.},
  file = {:E\:\\rojasro\\My Documents\\articles\\Optimal choice of AR and MA parts in autoregressive moving average models (Kashyap, R. 1984).pdf:PDF},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{kass1998,
  author = {Kass, R. and Carlin, B. and Gelman, A. and Neal, R.},
  title = {Markov {C}hain {M}onte {C}arlo in practice: {A} roundtable discussion},
  journal = {The American Statistician},
  year = {1998},
  volume = {52},
  pages = {93--100},
  number = {2},
  abstract = {Markov chain Monte Carlo (MCMC) methods make possi- ble the use of
	flexible Bayesian models that would other- wise be computationally
	infeasible. In recent years, a great variety of such applications
	have been described in the lit- erature. Applied statisticians who
	are new to these methods may have several questions and concerns,
	however: How much effort and expertise are needed to design and use
	a Markov chain sampler? How much confidence can one have in the answers
	that MCMC produces? How does the use of MCMC affect the rest of the
	model-building process? At the Joint Statistical Meetings in August,
	1996, a panel of experienced MCMC users discussed these and other
	issues, as well as various "tricks of the trade." This article is
	an edited recreation of that discussion. Its purpose is to offer
	advice and guidance to novice users of MCMC-and to not- so-novice
	users as well. Topics include building confidence in simulation results,
	methods for speeding and assessing convergence, estimating standard
	errors, identification of models for which good MCMC algorithms exist,
	and the current state of software development.},
  owner = {RRojas},
  refid = {KASS1998},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2685466}
}

@ARTICLE{kassraftery1995,
  author = {Kass, R. and Raftery, A.},
  title = {Bayes factors},
  journal = {Journal of the American Statistical Association},
  year = {1995},
  volume = {90},
  pages = {773--795},
  number = {430},
  abstract = {In a 1935 paper and in his book Theory of Probability, Jeffreys developed
	a methodology for quantifying the evidence in favor of a scientific
	theory. The centerpiece was a number, now called the Bayes factor,
	which is the posterior odds of the null hypothesis when the prior
	probability on the null is one-half. Although there has been much
	discussion of Bayesian hypothesis testing in the context of criticism
	of P-values, less attention has been given to the Bayes factor as
	a practical tool of applied statistics. In this article we review
	and discuss the uses of Bayes factors in the context of five scientific
	applications in genetics, sports, ecology, sociology, and psychology.
	We emphasize the following points: * From Jeffreys' Bayesian viewpoint,
	the purpose of hypothesis testing is to evaluate the evidence in
	favor of a scientific theory. * Bayes factors offer a way of evaluating
	evidence in favor of a null hypothesis. * Bayes factors provide a
	way of incorporating external information into the evaluation of
	evidence about a hypothesis. * Bayes factors are very general and
	do not require alternative models to be nested. * Several techniques
	are available for computing Bayes factors, including asymptotic approximations
	that are easy to compute using the output from standard packages
	that maximize likelihoods. * In "nonstandard" statistical models
	that do not satisfy common regularity conditions, it can be technically
	simpler to calculate Bayes factors than to derive non-Bayesian significance
	tests. * The Schwarz criterion (or BIC) gives a rough approximation
	to the logarithm of the Bayes factor, which is easy to use and does
	not require evaluation of prior distributions. * When one is interested
	in estimation or prediction, Bayes factors may be converted to weights
	to be attached to various models so that a composite estimate or
	prediction may be obtained that takes account of structural or model
	uncertainty. * Algorithms have been proposed that allow model uncertainty
	to be taken into account when the class of models initially considered
	is very large. * Bayes factors are useful for guiding an evolutionary
	model-building process. * It is important, and feasible, to assess
	the sensitivity of conclusions to the prior distributions used.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2291091}
}

@ARTICLE{kasswasserman1996,
  author = {Kass, R. and Wasserman, L.},
  title = {The selection of prior distributions by formal rules},
  journal = {Journal of the American Statistical Association},
  year = {1996},
  volume = {91},
  pages = {1343--1370},
  number = {435},
  abstract = {Subjectivism has become the dominant philosophical foundation for
	Bayesian inference. Yet in practice, most Bayesian analyses are performed
	with so-called "noninformative" priors, that is, priors constructed
	by some formal rule. We review the plethora of techniques for constructing
	such priors and discuss some of the practical and philosophical issues
	that arise when they are used. We give special emphasis to Jeffreys's
	rules and discuss the evolution of his viewpoint about the interpretation
	of priors, away from unique representation of ignorance toward the
	notion that they should be chosen by convention. We conclude that
	the problems raised by the research on priors chosen by formal rules
	are serious and may not be dismissed lightly: When sample sizes are
	small (relative to the number of parameters being estimated), it
	is dangerous to put faith in any "default" solution; but when asymptotics
	take over, Jeffreys's rules and their variants remain reasonable
	choices. We also provide an annotated bibliography.},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2291752}
}

@ARTICLE{katz2010,
  author = {Katz, R.},
  title = {Statistics of extremes in climate change},
  journal = {Climatic Change},
  year = {2010},
  volume = {100},
  pages = {71--76},
  number = {1},
  abstract = {This editorial essay concerns the use (or lack thereof) of the statistics
	of extremes in climate change research. So far, the statistical theory
	of extreme values has been primarily applied to climate under the
	assumption of stationarity. How this theory can be applied in the
	context of climate change, including implications for the analysis
	of the economic impacts of extremes, is described. Future research
	challenges include the statistical modeling of complex extreme events,
	such as heat waves, and taking into account spatial dependence in
	the statistical modeling of extremes for fields of climate observations
	or of numerical model output. Addressing these challenges will require
	increased collaboration between climate scientists and statisticians.},
  doi = {10.1007/s10584-010-9834-5},
  owner = {rojasro},
  timestamp = {2011.07.12}
}

@ARTICLE{katz+al2002,
  author = {Katz, R. and Parlange, M. and Naveau, P.},
  title = {Statistics of extremes in hydrology},
  journal = {Advances in Water Resources},
  year = {2002},
  volume = {25},
  pages = {1287--1304},
  number = {8--12},
  abstract = {The statistics of extremes have played an important role in engineering
	practice for water resources design and management. How recent developments
	in the statistical theory of extreme values can be applied to improve
	the rigor of hydrologic applications and to make such analyses more
	physically meaningful is the central theme of this paper. Such methodological
	developments primarily relate to maximum likelihood estimation in
	the presence of covariates, in combination with either the block
	maxima or peaks over threshold approaches. Topics that are treated
	include trends in hydrologic extremes, with the anticipated intensification
	of the hydrologic cycle as part of global climate change. In an attempt
	to link downscaling (i.e., relating large-scale atmosphere–ocean
	circulation to smaller-scale hydrologic variables) with the statistics
	of extremes, statistical downscaling of hydrologic extremes is considered.
	Future challenges are reviewed, such as the development of more rigorous
	statistical methodology for regional analysis of extremes, as well
	as the extension of Bayesian methods to more fully quantify uncertainty
	in extremal estimation. Examples include precipitation and streamflow
	extremes, as well as economic damage associated with such extreme
	events, with consideration of trends and dependence on patterns in
	atmosphere–ocean circulation (e.g., El Niño phenomenon).},
  doi = {10.1016/S0309-1708(02)00056-8},
  owner = {rojasro},
  timestamp = {2010.11.22}
}

@ARTICLE{kavetski2007,
  author = {Kavetski, D. and Kuczera, G.},
  title = {Model smoothing strategies to remove microscale discontinuities and
	spurious secondary optima in objective functions in hydrological
	calibration},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W03411},
  abstract = {Environmental processes frequently exhibit threshold-type behavior,
	e.g., the initiation of fluxes such as snowmelt, recharge, and quick
	flow. Incorporating such thresholds into hydrological models introduces
	discontinuities into the objective functions used in model calibration,
	making parameter estimation unnecessarily more difficult. Moreover,
	this study shows that model thresholds can produce spurious multimodality
	in least squares objective functions even if the underlying model
	is near linear in its parameters. In contrast, smoothing the model
	with respect to its parameters and inputs yields differentiable objective
	functions and, in some cases, can also improve its macroscale characteristics
	by removing spurious secondary optima. This simplifies model calibration
	and sensitivity analysis by reducing the complexity of objective
	functions and permitting the use of powerful derivative-based analysis
	methods such as Newton-type optimization and Hessian-based uncertainty
	assessment. This paper details smoothing strategies for several classes
	of thresholds and discontinuities commonly found in hydrological
	models, including step and angle discontinuities in the constitutive
	functions and flux constraints arising from conservation laws in
	the governing differential equations. The smoothing algorithms and
	their parameters are selected to ensure infinite differentiability
	of the model and its objective functions while preserving the macroscale
	behavior of the original governing equations. The improvements in
	the structure of the model and its objective functions are illustrated
	empirically for a degree-day-based snow model. The smoothing techniques
	are general and can be applied to other models with thresholds.},
  doi = {10.1029/2006WR005195},
  owner = {rojasro},
  timestamp = {2010.02.18}
}

@ARTICLE{kavetski+al2006a,
  author = {Kavetski, D. and Kuczera, G. and Franks, S.},
  title = {{Bayesian analysis of input uncertainty in hydrological modeling:
	1. Theory}},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  number = {W03407},
  abstract = {Parameter estimation in rainfall-runoff models is affected by uncertainties
	in the measured input/output data (typically, rainfall and runoff,
	respectively), as well as model error. Despite advances in data collection
	and model construction, we expect input uncertainty to be particularly
	significant (because of the high spatial and temporal variability
	of precipitation) and to remain considerable in the foreseeable future.
	Ignoring this uncertainty compromises hydrological modeling, potentially
	yielding biased and misleading results. This paper develops a Bayesian
	total error analysis methodology for hydrological models that allows
	(indeed, requires) the modeler to directly and transparently incorporate,
	test, and refine existing understanding of all sources of data uncertainty
	in a specific application, including both rainfall and runoff uncertainties.
	The methodology employs additional (latent) variables to filter out
	the input corruption given the model hypothesis and the observed
	data. In this study, the input uncertainty is assumed to be multiplicative
	Gaussian and independent for each storm, but the general framework
	allows alternative uncertainty models. Several ways of incorporating
	vague prior knowledge of input corruption are discussed, contrasting
	Gaussian and inverse gamma assumptions; the latter method avoids
	degeneracies in the objective function. Although the general methodology
	is computationally intensive because of the additional latent variables,
	a range of modern numerical methods, particularly Monte Carlo analysis
	combined with fast Newton-type optimization methods and Hessian-based
	covariance analysis, can be employed to obtain practical solutions.},
  doi = {10.1029/2005WR004368},
  owner = {rojasro},
  timestamp = {2012.09.13}
}

@ARTICLE{kavetski+al2006b,
  author = {Kavetski, D. and Kuczera, G. and Franks, S.},
  title = {{Bayesian analysis of input uncertainty in hydrological modeling:
	2}},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  number = {W03408},
  abstract = {The Bayesian total error analysis (BATEA) methodology directly addresses
	both input and output errors in hydrological modeling, requiring
	the modeler to make explicit, rather than implicit, assumptions about
	the likely extent of data uncertainty. This study considers a BATEA
	assessment of two North American catchments: (1) French Broad River
	and (2) Potomac basins. It assesses the performance of the conceptual
	Variable Infiltration Capacity (VIC) model with and without accounting
	for input (precipitation) uncertainty. The results show the considerable
	effects of precipitation errors on the predicted hydrographs (especially
	the prediction limits) and on the calibrated parameters. In addition,
	the performance of BATEA in the presence of severe model errors is
	analyzed. While BATEA allows a very direct treatment of input uncertainty
	and yields some limited insight into model errors, it requires the
	specification of valid error models, which are currently poorly understood
	and require further work. Moreover, it leads to computationally challenging
	highly dimensional problems. For some types of models, including
	the VIC implemented using robust numerical methods, the computational
	cost of BATEA can be reduced using Newton-type methods.},
  doi = {1029/2005WR004376},
  owner = {rojasro},
  timestamp = {2012.09.13}
}

@ARTICLE{kavetski+al2006c,
  author = {Kavetski, D. and Kuczera, G. and Franks, S.},
  title = {Calibration of conceptual hydrological models revisited: 1. Overcoming
	numerical artefacts},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {320},
  pages = {173--186},
  number = {1-2},
  abstract = {Conceptual hydrological modelling has traditionally been plagued by
	calibration difficulties due to the roughness and complex shape of
	objective functions. These problems led to the abandonment of powerful
	classical analysis methods (Newton-type optimisation, derivative-based
	uncertainty analysis) and have motivated extensive research into
	nonsmooth optimisation and even new parameter estimation philosophies
	(e.g. GLUE). This paper shows that some of these complexities are
	not inherent features of hydrological models, but are numerical artefacts
	due to model thresholds and poorly selected time stepping schemes.
	We present a numerically robust methodology for implementing conceptual
	models, including rainfall-runoff and snow models, that ensures micro-scale
	smoothness of objective functions and guarantees macro-scale model
	stability. The methodology employs robust and unconditionally stable
	time integration of the models, complemented by careful threshold
	smoothing. A case study demonstrates the benefits of these techniques.},
  doi = {10.1016/j.jhydrol.2005.07.012},
  keywords = {Rainfall-runoff models, SPM, Degree-day snow model, Parameter estimation,
	Numerical artefacts, Model smoothing, Model thresholds, Model stability,
	Implicit time stepping},
  tags = {Calibration}
}

@UNPUBLISHED{kay+al2006b,
  author = {Kay, A. and Bell, V. and Davies, H.},
  title = {Model Quality and Uncertainty for Climate Change Impact. Centre for
	Ecology and Hydrology},
  note = {NERC/Centre for Ecology and Hydrology, Wallingford},
  year = {2006},
  tags = {Uncertainty}
}

@ARTICLE{kay+al2009,
  author = {Kay, A. and Davies, H. and Bell, V. and Jones, R.},
  title = {Comparison of uncertainty sources for climate change impacts: {F}lood
	frequency in {E}ngland},
  journal = {Climatic Change},
  year = {2009},
  volume = {92},
  pages = {41--63},
  number = {1--2},
  abstract = {This paper investigates the uncertainty in the impact of climate change
	on flood frequency in England, through the use of continuous simulation
	of river flows. Six different sources of uncertainty are discussed:
	future greenhouse gas emissions; Global Climate Model (GCM) structure;
	downscaling from GCMs (including Regional Climate Model structure);
	hydrological model structure; hydrological model parameters and the
	internal variability of the climate system (sampled by applying different
	GCM initial conditions). These sources of uncertainty are demonstrated
	(separately) for two example catchments in England, by propagation
	through to flood frequency impact. The results suggest that uncertainty
	from GCM structure is by far the largest source of uncertainty. However,
	this is due to the extremely large increases in winter rainfall predicted
	by one of the five GCMs used. Other sources of uncertainty become
	more significant if the results from this GCM are omitted, although
	uncertainty from sources relating to modelling of the future climate
	is generally still larger than that relating to emissions or hydrological
	modelling. It is also shown that understanding current and future
	natural variability is critical in assessing the importance of climate
	change impacts on hydrology},
  doi = {10.1007/s10584-008-9471-4},
  tags = {Uncertainty}
}

@ARTICLE{kay+al2006a,
  author = {Kay, A. and Jones, R. and Reynard, N.},
  title = {R{CM rainfall for UK flood frequency estimation. II. Climate change
	results}},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {318},
  pages = {163--172},
  number = {1--4},
  abstract = {The first part of this two-part paper demonstrated the feasibility
	of the direct use of data from a high resolution (25 km) Regional
	Climate Model (RCM) to provide inputs for a rainfall-runoff model,
	in order to obtain estimates of flood frequency. This paper uses
	data from a climate change experiment with the same RCM (HadRM3H)
	to provide estimates of change in flood frequency between the 1970s
	and 2080s, for 15 catchments across Great Britain. This experiment
	is a rerun, at twice the horizontal resolution, of one of those used
	by the UK Climate Impacts Programme (UKCIP) in the construction of
	its UKCIP02 climate change scenarios for the UK. It thus allows an
	exploration of the implication of these scenarios using a resolution
	more suited to the detailed hydrological application addressed in
	this paper. Despite decreases in annual average rainfall in all but
	one catchment, eight show an increase in flood frequency at most
	return periods whereas two show substantial decreases. As part I
	of this paper showed a distinct positive correlation between errors
	in annual rainfall and errors in flood frequency, the fact that flood
	frequency can increase despite an overall decrease in rainfall implies
	a marked change in the distribution of rainfall, either in terms
	of the probability of rainfall events and/or its seasonal cycle.
	Decreases in flood peaks are shown for a number of the catchments
	in the south and east of England, despite an increase in winter mean
	and extreme rainfall. Increased summer and autumn soil moisture deficits
	are thought to be the reason for this. Other catchments, further
	north or west, show an increase in flood peaks, in some cases of
	over 50% at the 50-year return period. Care needs to be taken when
	interpreting these results, as they are based on a single RCM experiment
	(using driving data from one GCM under a single emissions scenario).
	Other RCM experiments may give quite different results, and ensemble
	runs would ideally be required to limit sample error. (c) 2005 Elsevier
	Ltd All rights reserved.},
  doi = {10.1016/j.jhydrol.2005.06.013},
  keywords = {regional climate models, flood frequency, rainfall-runoff model, spatial
	generalisation, CHANGE IMPACTS, RIVERS, BASIN},
  tags = {Impacts}
}

@ARTICLE{kendon+al2010,
  author = {Kendon, E. and Jones, R. and Kjellstr\"om, E. and Murphy, J.},
  title = {{Using and designing GCM–RCM ensemble regional climate projections}},
  journal = {Journal of Climate},
  year = {2010},
  volume = {23},
  pages = {6485--6503},
  number = {24},
  abstract = {Multimodel ensembles, whereby different global climate models (GCMs)
	and regional climate models (RCMs) are combined, have been widely
	used to explore uncertainties in regional climate projections. In
	this study, the extent to which information can be enhanced from
	sparsely filled GCM–RCM ensemble matrices and the way in which simulations
	should be prioritized to sample uncertainties most effectively are
	examined. A simple scaling technique, whereby the local climate response
	in an RCM is predicted from the large-scale change in the GCM, is
	found to often show skill in estimating local changes for missing
	GCM–RCM combinations. In particular, scaling shows skill for precipitation
	indices (including mean, variance, and extremes) across Europe in
	winter and mean and extreme temperature in summer and winter, except
	for hot extremes over central/northern Europe in summer. However,
	internal variability significantly impacts the ability to determine
	scaling skill for precipitation indices, with a three-member ensemble
	found to be insufficient for identifying robust local scaling relationships
	in many cases. This study suggests that, given limited computer resources,
	ensembles should be designed to prioritize the sampling of GCM uncertainty,
	using a reduced set of RCMs. Exceptions are found over the Alps and
	northeastern Europe in winter and central Europe in summer, where
	sampling multiple RCMs may be equally or more important for capturing
	uncertainty in local temperature or precipitation change. This reflects
	the significant role of local processes in these regions. Also, to
	determine the ensemble strategy in some cases, notably precipitation
	extremes in summer, better sampling of internal variability is needed.},
  doi = {10.1175/2010JCLI3502.1},
  owner = {rojasro},
  timestamp = {2011.04.29}
}

@INCOLLECTION{kennedy2006,
  author = {Kennedy, J.},
  title = {Swarm Intelligence},
  booktitle = {Handbook of Nature-Inspired and Innovative Computing},
  publisher = {Springer US},
  year = {2006},
  editor = {Albert Zomaya},
  pages = {187--219},
  doi = {10.1007/0-387-27705-6\_6},
  isbn = {978-0-387-27705-9},
  tags = {PSO}
}

@INPROCEEDINGS{kennedy1999,
  author = {Kennedy, J.},
  title = {Small worlds and mega-minds: effects of neighborhood topology onparticle
	swarm performance},
  booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation, 1999},
  year = {1999},
  volume = {3},
  pages = {3 vol. (xxxvii+2348)},
  doi = {10.1109/CEC.1999.785509},
  owner = {rojasro},
  timestamp = {2011.10.20}
}

@INPROCEEDINGS{kennedyeberhart1995,
  author = {Kennedy, J. and Eberhart, R.},
  title = {Particle swarm optimization},
  booktitle = {Proceedings IEEE International Conference on Neural Networks, 1995},
  year = {1995},
  volume = {4},
  pages = {1942--1948},
  month = {nov/dec},
  abstract = {A concept for the optimization of nonlinear functions using particle
	swarm methodology is introduced. The evolution of several paradigms
	is outlined, and an implementation of one of the paradigms is discussed.
	Benchmark testing of the paradigm is described, and applications,
	including nonlinear function optimization and neural network training,
	are proposed. The relationships between particle swarm optimization
	and both artificial life and genetic algorithms are described},
  doi = {10.1109/ICNN.1995.488968},
  keywords = {artificial life, evolution, genetic algorithms, multidimensional search,
	neural network, nonlinear functions, optimization, particle swarm,
	simulation, social metaphor, artificial intelligence, genetic algorithms,
	neural nets, search problems, simulation},
  tags = {Calibration, PSO}
}

@INPROCEEDINGS{kennedymendes2003,
  author = {Kennedy, J. and Mendes, R.},
  title = {Neighborhood topologies in fully-informed and best-of-neighborhood
	particle swarms},
  booktitle = {Proceedings of the 2003 IEEE International Workshop on Soft Computing
	in Industrial Applications, 2003. SMCia/03.},
  year = {2003},
  pages = {45-50},
  abstract = {We vary the way an individual in the particle swarm interacts with
	its neighbors. Performance depends on population topology as well
	as algorithm version.},
  doi = {10.1109/SMCIA.2003.1231342},
  keywords = {FIPS, topology, von Neumann},
  tags = {PSO}
}

@INPROCEEDINGS{kennedymendes2002,
  author = {Kennedy, J. and Mendes, R.},
  title = {Population structure and particle swarm performance},
  booktitle = {Proceedings of the 2002 Congress on Evolutionary Computation, CEC
	'02},
  year = {2002},
  pages = {1671--1676},
  month = {May},
  abstract = {The effects of various population topologies on the particle swarm
	algorithm were systematically investigated. Random graphs were generated
	to specifications, and their performance on several criteria was
	compared. What makes a good population structure? We discovered that
	previous assumptions may not have been correct.},
  doi = {10.1109/CEC.2002.1004493},
  owner = {rojasro},
  timestamp = {2011.10.14}
}

@ARTICLE{kerr2002,
  author = {Kerr, R.},
  title = {Climate change--{R}educing uncertainties of global warming},
  journal = {Science},
  year = {2002},
  volume = {295},
  pages = {29--+},
  number = {5552},
  abstract = {Climate researchers have so far been unable to say how bad things
	could get as the world continues to warm. Now in this issue of Science
	(p. 113), a group of researchers report plugging different combinations
	of values for fundamental properties of the climate system--such
	as its sensitivity to the nudge that humans are giving it--into a
	computer model and looking to see how well the model's output matched
	long-term observations. The results are mixed.},
  doi = {10.1126/science.295.5552.29a},
  tags = {Uncertainty}
}

@ARTICLE{khu+al2008,
  author = {Khu,{S-T} and Madsden, H. and {di Pierro}, F.},
  title = {Incorporating multiple observations for distributed hydrologic model
	calibration: {A}n approach using a multi-objective evolutionary algorithm
	and clustering},
  journal = {Advances in Water Resources},
  year = {2008},
  volume = {31},
  pages = {1387--1398},
  number = {10},
  abstract = {The use of distributed data for model calibration is becoming more
	popular in the advent of the availability of spatially distributed
	observations. Hydrological model calibration has traditionally been
	carried out using single objective optimisation and only recently
	has been extended to a multi-objective optimisation domain. By formulating
	the calibration problem with several objectives, each objective relating
	to a set of observations, the parameter sets can be constrained more
	effectively. However, many previous multi-objective calibration studies
	do not consider individual observations or catchment responses separately,
	but instead utilises some form of aggregation of objectives. This
	paper proposes a multi-objective calibration approach that can efficiently
	handle many objectives using both clustering and preference ordered
	ranking. The algorithm is applied to calibrate the MIKE SHE distributed
	hydrologic model and tested on the Karup catchment in Denmark. The
	results indicate that the preferred solutions selected using the
	proposed algorithm are good compromise solutions and the parameter
	values are well defined. Clustering with Kohonen mapping was able
	to reduce the number of objective functions from 18 to 5. Calibration
	using the standard deviation of groundwater level residuals enabled
	us to identify a group of wells that may not be simulated properly,
	thus highlighting potential problems with the model parameterisation.},
  doi = {10.1016/j.advwatres.2008.07.011},
  keywords = {Calibration, Distributed modelling, Multi-objective, Self-organising
	map (SOM), Multiple observations, Evolutionary algorithms, Groundwater},
  tags = {Calibration}
}

@ARTICLE{kilsby+al2007,
  author = {Kilsby, C. and Tellier, S. and Fowler, H. and Howels, T.},
  title = {Hydrological impacts of climate change on the {Tejo} and {Guadiana}
	{Rivers}},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1175--1189},
  number = {3},
  abstract = {A distributed daily rainfall–runoff model is applied to the Tejo and
	Guadiana river basins in Spain and Portugal to simulate the effects
	of climate change on runoff production, river flows and water resource
	availability with results aggregated to the monthly level. The model
	is calibrated, validated and then used for a series of climate change
	impact assessments for the period 2070–2100. Future scenarios are
	derived from the HadRM3H regional climate model (RCM) using two techniques:
	firstly a bias-corrected RCM output, with monthly mean correction
	factors calculated from observed rainfall records; and, secondly,
	a circulation-pattern-based stochastic rainfall model. Major reductions
	in rainfall and streamflow are projected throughout the year; these
	results differ from those for previous studies where winter increases
	are projected. Despite uncertainties in the representation of heavily
	managed river systems, the projected impacts are serious and pose
	major threats to the maintenance of bipartite water treaties between
	Spain and Portugal and the supply of water to urban and rural regions
	of Portugal.},
  doi = {10.5194/hess-11-1175-2007},
  owner = {rojasro},
  timestamp = {2010.07.30}
}

@ARTICLE{kim+al2007,
  author = {Kim, {S-M} and Benham, B. and Brannan, K. and Zeckoski, R. and Doherty,
	J.},
  title = {Comparison of hydrologic calibration of {HSPF} using automatic and
	manual methods},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W01402},
  abstract = {The automatic calibration software Parameter Estimation (PEST) was
	used in the hydrologic calibration of Hydrological Simulation Program{--}Fortran
	(HSPF), and the results were compared with a manual calibration assisted
	by the Expert System for the Calibration of HSPF (HSPEXP). In this
	study, multiobjective functions based on the HSPEXP model performance
	criteria were developed for use in PEST, which allowed for the comparison
	of the calibration results of the two methods. The calibrated results
	of both methods were compared in terms of HSPEXP model performance
	criteria, goodness-of-fit measures (R 2, E, and RMSE), and base flow
	index. The automatic calibration results satisfied most of the HSPEXP
	model performance criteria and performed better with respect to R
	2, E, RMSE, and base flow index than manual calibration results.
	The results of the comparison with the manual calibration suggest
	that the automatic method using PEST may be a suitable alternative
	to manual method assisted by HSPEXP for calibration of hydrologic
	parameters for HSPF. However, further research of the weights used
	in the objective functions is necessary to provide guidance when
	applying PEST to surface water modeling.},
  doi = {10.1029/2006WR004883},
  tags = {Calibration}
}

@ARTICLE{king1994,
  author = {King, D. and Daroussin, J. and Tavernier, R.},
  title = {{Development of a soil geographic database from the Soil Map of the
	European Communities}},
  journal = {Catena},
  year = {1994},
  volume = {21},
  pages = {37--56},
  number = {1},
  abstract = {Questions on land use and soil conservation require increasingly accurate
	information on soil properties and their geographical location. Soil
	maps have helped to answer them thus helping in decision making.
	Information presented on soil maps are now managed by computer. This
	is the case for the Soil Map of European Communities (EC) at a scale
	of 1 : 1 000 000. Computerization of soil maps is often limited to
	soil boundaries and to the few descriptive items on the paper themselves.
	Much of the original survey is lost either during mapping or because
	it is published separately in explanatory notes or legends. This
	was also the case for the EC Soil Map. Many scientific publications
	and draft documents were used to make the original paper map, but
	were greatly condensed and simplified. The first version of the EC
	soil database is an exact copy of the paper map, thus having the
	same deficiencies. Using Geographical Information System technology,
	an efficient data structure has to be developed to take into account
	efficiently the internal organization of the soil cover. Such a structure
	should match conceptually the soil scientist review of spatial soil
	organization at a given scale within a computerized model. As a first
	step towards such a “Soil Spatial Organization Model” the material
	available for the compilation of the EC Soil Map is analysed. A logical
	data structure to receive a posteriori these informations is proposed
	and the database's improvement in terms of quantity as well as quality
	is demonstrated.},
  doi = {10.1016/0341-8162(94)90030-2},
  owner = {rojasro},
  timestamp = {2011.05.12}
}

@ARTICLE{kirchner2006,
  author = {Kirchner, J.},
  title = {Getting the right answers for the right reasons: {L}inking measurements,
	analyses, and models to advance the science of hydrology},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W03S04},
  abstract = {The science of hydrology is on the threshold of major advances, driven
	by new hydrologic measurements, new methods for analyzing hydrologic
	data, and new approaches to modeling hydrologic systems. Here I suggest
	several promising directions forward, including (1) designing new
	data networks, field observations, and field experiments, with explicit
	recognition of the spatial and temporal heterogeneity of hydrologic
	processes, (2) replacing linear, additive “black box” models with
	“gray box” approaches that better capture the nonlinear and non-additive
	character of hydrologic systems, (3) developing physically based
	governing equations for hydrologic behavior at the catchment or hillslope
	scale, recognizing that they may look different from the equations
	that describe the small-scale physics, (4) developing models that
	are minimally parameterized and therefore stand some chance of failing
	the tests that they are subjected to, and (5) developing ways to
	test models more comprehensively and incisively. I argue that scientific
	progress will mostly be achieved through the collision of theory
	and data, rather than through increasingly elaborate and parameter-rich
	models that may succeed as mathematical marionettes, dancing to match
	the calibration data even if their underlying premises are unrealistic.
	Thus advancing the science of hydrology will require not only developing
	theories that get the right answers but also testing whether they
	get the right answers for the right reasons.},
  doi = {10.1029/2005WR004362},
  owner = {rojasro},
  timestamp = {2010.02.18}
}

@ARTICLE{kirckpatrick+al1983,
  author = {Kirckpatrick, S. and Gelatt, C. and Vecchi, M.},
  title = {Optimization by simulated annealing},
  journal = {Science},
  year = {1983},
  volume = {220},
  pages = {671--680},
  number = {4598},
  abstract = {There is a deep and useful connection between statistical mechanics
	(the behavior of systems with many degrees of freedom in thermal
	equilibrium at a finite temperature) and multivariate or combinatorial
	optimization (finding the minimum of a given function depending on
	many parameters). A detailed analogy with annealing in solids provides
	a framework for optimization of the properties of very large and
	complex systems. This connection to statistical mechanics exposes
	new information and provides an unfamiliar perspective on traditional
	optimization problems and methods.},
  doi = {10.1126/science.220.4598.671},
  owner = {rojasro},
  timestamp = {2011.10.11}
}

@INCOLLECTION{kitanidis1988,
  author = {Kitanidis, P.},
  title = {The concept of predictive probability and a simple test for geostatistical
	model validation},
  booktitle = {Consequences of spatial variability in aquifer properties and data
	limitations for groundwater modelling practice},
  publisher = {International Association of Hydrological Sciences},
  year = {1988},
  editor = {Peck, A. and Gorelick, S. and {de Marsily}, G. and Foster, S. and
	Kovalevsky, V.},
  owner = {rojasro},
  timestamp = {2009.09.14}
}

@BOOK{kitanidis1997,
  title = {Introduction to geostatistics--{A}pplications in hydrogeology},
  publisher = {Cambridge University Press},
  year = {1997},
  author = {Kitanidis, P.},
  pages = {272},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{kitanidis1995,
  author = {Kitanidis, P.},
  title = {Quasi--linear geostatistical theory for inversing},
  journal = {Water Resources Research},
  year = {1995},
  volume = {31},
  pages = {2411--2419},
  number = {10},
  abstract = {A quasi-linear theory is presented for the geostatistical solution
	to the inverse problem. The archetypal problem is to estimate the
	log transmissivity function from observations of head and log transmissivity
	at selected locations. The unknown is parameterized as a realization
	of a random field, and the estimation problem is solved in two phases:
	structural analysis, where the random field is characterized, followed
	by estimation of the log transmissivity conditional on all observations.
	The proposed method generalizes the linear approach of Kitanidis
	and Vomvoris (1983). The generalized method is superior to the linear
	method in cases of large contrast in formation properties but informative
	measurements, i.e., there are enough observations that the variance
	of estimation error of the log transmissivity is small. The methodology
	deals rigorously with unknown drift coefficients and yields estimates
	of covariance parameters that are unbiased and grid independent.
	The applicability of the methodology is demonstrated through an example
	that includes structural analysis, determination of best estimates,
	and conditional simulations.},
  doi = {10.1029/95WR01945},
  owner = {rojasro},
  timestamp = {2009.09.14}
}

@ARTICLE{kitanidisbras1980a,
  author = {Kitanidis, P. and Bras, R.},
  title = {Real-Time Forecasting with a Conceptual Hydrologic Model 1. Analysis
	of Uncertainty},
  journal = {Water Resources Research},
  year = {1980},
  volume = {16},
  pages = {1025--1033},
  number = {6},
  abstract = {The optimal control of watershed systems requires accurate real-time
	short-term forecasts of river flows. For the first time, this paper
	formulates a large, nonlinear conceptual model (the National Weather
	Service catchment model) in a mode amenable to analysis of uncertainty
	and the utilization of real-time information (measurements, forecasts,
	guesses) to update system states and improve streamflow predictions.
	The proposed methodology is based on the state space formulation
	of the equations describing the hydrologic model and the assumption
	of sources of uncertainty in the data and in the model structure.
	The first two moments of random variables are estimated in a computationally
	efficient way using on-line linear estimation techniques. Linearization
	of functional relationships is performed with the uncommon but powerful
	multiple-input describing function technique for the most strongly
	nonlinear responses and Taylor expansion for the rest. The linear
	feedback rule developed is based on the Kaiman filter},
  doi = {10.1029/WR016i006p01025},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{klitanidisbras1980,
  author = {Kitanidis, P. and Bras, R.},
  title = {Real-Time Forecasting with a Conceptual Hydrologic Model 2. Applications
	and Results},
  journal = {Water Resources Research},
  year = {1980},
  volume = {16},
  pages = {1034--1044},
  number = {6},
  abstract = {The results from an application of a conceptual hydrologic model,
	combined with filtering and statistical estimation methods, to real-time
	forecasting of river discharges are very encouraging. The use of
	feedback significantly improves the overall forecasting capability
	of the model even when the model and input error statistics are not
	perfectly known. Identification of these statistics through adaptive
	filtering techniques is practical and further improves the performance
	of the model. Comparison with a simple linear adaptive {`}black box{'}
	model is very favorable for the conceptual hydrologic model, especially
	for forecast lead times comparably to the response time of the catchment.
	The results emphasize the importance of using a realistic model of
	uncertainty accounting for the nonstationarity in the rainfall-runoff
	process.},
  bibkey = {persistence index, coefficient of persistence, conceptual hydrologic
	modelling},
  doi = {10.1029/WR016i004p00740},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{kitanidis1983,
  author = {Kitanidis, P. and Vomvoris, E.},
  title = {A geostatistical approach to the inverse problem in groundwater modeling
	(steady state) and one--dimensional simulations},
  journal = {Water Resources Research},
  year = {1983},
  volume = {19},
  pages = {677-690},
  number = {3},
  abstract = {The problem of estimating Hydrogeologic parameters, in particular,
	permeability, from input-output measurements is reexamined in a geostatistical
	framework. The field of the unknown parameters is represented as
	a ‘random field’ and the estimation procedure consists of two main
	steps. First, the structure of the parameter field is identified,
	i.e., mathematical representations of the variogram and the trend
	are selected and their parameters are established by using all available
	information, including measurements of hydraulic head and permeability.
	Second, linear estimation theory is applied to provide minimum variance
	and unbiased point estimates of hydrogeologic parameters (‘kriging’).
	Structure identification is achieved iteratively in three substeps
	: structure selection, maximum likelihood estimation, and model validation
	and diagnostic checking. The methodology was extensively tested through
	simulations on a simple one-dimensional case. The results are remarkably
	stable and well behaved. The estimated field is smooth, while small-scale
	variability is statistically described. As the quality of measurements
	improves, the procedure reproduces more features of the original
	field. The results are also shown to be rather insensitive to deviations
	from assumptions about the geostatistical structure of the field.},
  doi = {10.1029/WR019i003p00677},
  owner = {RRojas},
  timestamp = {2009.03.13}
}

@ARTICLE{kjellstrom+al2007,
  author = {Kjellstr\"om, E. and B\"arring, L. and Jacob, D. and Jones, R. and
	Lenderink, G. and Sch\"ar, C.},
  title = {{Modelling daily temperature extremes: recent climate and future
	changes over Europe}},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {249--265},
  number = {0},
  abstract = {Probability distributions of daily maximum and minimum temperatures
	in a suite of ten RCMs are investigated for (1) biases compared to
	observations in the present day climate and (2) climate change signals
	compared to the simulated present day climate. The simulated inter-model
	differences and climate changes are also compared to the observed
	natural variability as reflected in some very long instrumental records.
	All models have been forced with driving conditions from the same
	global model and run for both a control period and a future scenario
	period following the A2 emission scenario from IPCC. We find that
	the bias in the fifth percentile of daily minimum temperatures in
	winter and at the 95th percentile of daily maximum temperature during
	summer is smaller than 3 (±5°C) when averaged over most (all) European
	sub-regions. The simulated changes in extreme temperatures both in
	summer and winter are larger than changes in the median for large
	areas. Differences between models are larger for the extremes than
	for mean temperatures. A comparison with historical data shows that
	the spread in model predicted changes in extreme temperatures is
	larger than the natural variability during the last centuries.},
  doi = {10.1007/s10584-006-9220-5},
  owner = {rojasro},
  timestamp = {2011.02.07}
}

@ARTICLE{kjellstrom+al2010,
  author = {Kjellstr\"om, E. and Boberg, F. and Castro, M. and Chirstensen, J.
	and Nikulin, G. and S{\'a}nchez, E.},
  title = {Daily and monthly temperature and precipitation statistics as performance
	indicators for regional climate models},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {135--150},
  number = {2--3},
  abstract = {We evaluated daily and monthly statistics of maximum and minimum temperatures
	and precipitation in an ensemble of 16 regional climate models (RCMs)
	forced by boundary conditions from reanalysis data for 1961–1990.
	A high-resolution gridded observational data set for land areas in
	Europe was used. Skill scores were calculated based on the match
	of simulated and observed empirical probability density functions.
	The evaluation for different variables, seasons and regions showed
	that some models were better/worse than others in an overall sense.
	It also showed that no model that was best/worst in all variables,
	seasons or regions. Biases in daily precipitation were most pronounced
	in the wettest part of the probability distribution where the RCMs
	tended to overestimate precipitation compared to observations. We
	also applied the skill scores as weights used to calculate weighted
	ensemble means of the variables. We found that weighted ensemble
	means were slightly better in comparison to observations than corresponding
	unweighted ensemble means for most seasons, regions and variables.
	A number of sensitivity tests showed that the weights were highly
	sensitive to the choice of skill score metric and data sets involved
	in the comparison.},
  doi = {10.3354/cr00932},
  owner = {rojasro},
  timestamp = {2011.01.12}
}

@ARTICLE{kjellstromgiorgi2010,
  author = {Kjellstr\"om, E. and Giorgi, F.},
  title = {Introduction},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {117--119},
  number = {2--3},
  abstract = {An ensemble of regional climate models downscaling reanalysis data
	has been evaluated against observations for the time period 1961–2000.
	Various aspects of model performance including both their representation
	of large-scale features and their ability to add value on smaller
	spatial scales have been considered. A set of metrics has been derived
	and combined into a performance-based weigthing system that is used
	in the production of probabilistic climate change projections. Strengths
	and weaknesses of weighting techniques for RCM ensembles are discussed.},
  doi = {10.3354/cr00976},
  owner = {rojasro},
  timestamp = {2011.05.02}
}

@ARTICLE{kjellstrom+al2011,
  author = {Kjellstr\"om, E. and Nikulin, G. and Hansson, U. and Strandberg,
	G. and Ullerstig, A.},
  title = {{21st century changes in the European climate: uncertainties derived
	from an ensemble of regional climate model simulations}},
  journal = {Tellus {A}},
  year = {2011},
  volume = {63},
  pages = {24--40},
  abstract = {Seasonal mean temperature, precipitation and wind speed over Europe
	are analysed in an ensemble of 16 regional climate model (RCM) simulations
	for 1961–2100. The RCM takes boundary conditions from seven global
	climate models (GCMs) under four emission scenarios. One GCM was
	run three times under one emission scenario differing only in initial
	conditions. The ensemble is used to; (i) evaluate the simulated climate
	for 1961–1990, (ii) assess future climate change and (iii) illustrate
	uncertainties in future climate change related to natural variability,
	boundary conditions and emissions. Biases in the 1961–1990 period
	are strongly related to errors in the large-scale circulation in
	the GCMs. Significant temperature increases are seen for all of Europe
	already in the next decades. Precipitation
	
	increases in northern and decreases in southern Europe with a zone
	in between where the sign of change is uncertain. Wind speed decreases
	in many areas with exceptions in the northern seas and in parts of
	the Mediterranean in summer. Uncertainty largely depends on choice
	of GCM and their representation of changes in the large-scale circulation.
	The uncertainty related to forcing is most important by the end of
	the century while natural variability sometimes dominates the uncertainty
	in the nearest few decades.},
  doi = {10.1111/j.1600-0870.2010.00475.x},
  owner = {rojasro},
  timestamp = {2011.04.29}
}

@ARTICLE{kjellstrom2007,
  author = {Kjellstr\"om, E. and Ruosteenoja, K.},
  title = {{Present-day and future precipitation in the Baltica Sea region as
	simulated in a suite of reginal climate models}},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {97--122},
  number = {1},
  abstract = {Here we investigate simulated changes in the precipitation climate
	over the Baltic Sea and surrounding land areas for the period 2071–2100
	as compared to 1961–1990. We analyze precipitation in 10 regional
	climate models taking part in the European PRUDENCE project. Forced
	by the same global driving climate model, the mean of the regional
	climate model simulations captures the observed climatological precipitation
	over the Baltic Sea runoff land area to within 15% in each month,
	while single regional models have errors up to 25%. In the future
	climate, the precipitation is projected to increase in the Baltic
	Sea area, especially during winter. During summer increased precipitation
	in the north is contrasted with a decrease in the south of this region.
	Over the Baltic Sea itself the future change in the seasonal cycle
	of precipitation is markedly different in the regional climate model
	simulations. We show that the sea surface temperatures have a profound
	impact on the simulated hydrological cycle over the Baltic Sea. The
	driving global climate model used in the common experiment projects
	a very strong regional increase in summertime sea surface temperature,
	leading to a significant increase in precipitation. In addition to
	the common experiment some regional models have been forced by either
	a different set of Baltic Sea surface temperatures, lateral boundary
	conditions from another global climate model, a different emission
	scenario, or different initial conditions. We make use of the large
	number of experiments in the PRUDENCE project, providing an ensemble
	consisting of more than 25 realizations of climate change, to illustrate
	sources of uncertainties in climate change projections.},
  doi = {10.1007/s10584-006-9219-y},
  owner = {rojasro},
  timestamp = {2011.08.01}
}

@ARTICLE{kleintank2003,
  author = {{Klein Tank}, A. and K{\¨o}nnen, G.},
  title = {Trend in indices of daily temperature and precipitation extremes
	in Europe; 1946-99},
  journal = {Journal of Climate},
  year = {2003},
  volume = {16},
  pages = {3665--3680},
  number = {22},
  abstract = {Trends in indices of climate extremes are studied on the basis of
	daily series of temperature and precipitation observations from more
	than 100 meteorological stations in Europe. The period is 1946–99,
	a warming episode. Averaged over all stations, the indices of temperature
	extremes indicate ‘‘symmetric’’ warming of the cold and warm tails
	of the distributions of daily minimum and maximum temperature in
	this period. However, "asymmetry" is found for the trends if the
	period is split into two subperiods. For the 1946–75 subperiod, an
	episode of slight cooling, the annual number of warm extremes decreases,
	but the annual number of cold extremes does not increase. This implies
	a reduction in temperature variability. For the 1976–99 subperiod,
	an episode of pronounced warming, the annual number of warm extremes
	increases 2 times faster than expected from the corresponding decrease
	in the number of cold extremes. This implies an increase in temperature
	variability, which is mainly due to stagnation in the warming of
	the cold extremes. For precipitation, all Europe-average indices
	of wet extremes increase in the 1946–99 period, although the spatial
	coherence of the trends is low. At stations where the annual amount
	increases, the index that represents the fraction of the annual amount
	due to very wet days gives a signal of disproportionate large changes
	in the extremes. At stations with a decreasing annual amount, there
	is no such amplified response of the extremes. The indices of temperature
	and precipitation extremes in this study were selected from the list
	of climate
	
	change indices recommended by the World Meteorological Organization–Commission
	for Climatology (WMO–CCL) and the Research Programme on Climate Variability
	and Predictability (CLIVAR). The selected indices are expressions
	of events with return periods of 5–60 days. This means that the annual
	number of events is sufficiently large to allow for meaningful trend
	analysis in ;50 yr time series. Although the selected indices refer
	to events that may be called ‘‘soft’’ climate extremes, these indices
	have clear impact relevance.},
  doi = {10.1175/1520-0442(2003)016<3665:TIIODT>2.0.CO;2},
  owner = {rojasro},
  timestamp = {2010.11.12}
}

@ARTICLE{kleinn+al2005,
  author = {Kleinn, J. and Frei, C. and Gurtz, J. and L\"uthi, D. and Vidale,
	P. and Sch\"ar, C.},
  title = {Hydrologic simulations in the {R}hine basin driven by a regional
	climate model},
  journal = {Journal of Geophysical Research},
  year = {2005},
  volume = {110},
  pages = {1--18},
  number = {D04102},
  abstract = {We describe and evaluate a model chain for studying streamflow responses
	to climate variations and anthropogenic climate change. The model
	chain was developed for the Rhine basin upstream of Cologne, a 145,000
	km2 river basin in Central Europe north of the Alps. It encompasses
	a regional climate model (RCM) at grid spacings of 56 and 14 km,
	and a distributed hydrological model with a grid spacing of 1 km.
	The hydrological model is one-way nested into the RCM through a downscaling
	interface, which introduces fine-scale structures in the forcing
	data (i.e. temperature, precipitation, total net surface radiation,
	10-m wind speed, and relative humidity). Biases in precipitation
	and temperature are accounted for by catchment-dependent but seasonally
	constant correction factors. Apart from these bias corrections, the
	hydrological model is forced by hourly RCM data. In the evaluation
	we compare a 5-year integration driven by observed lateral boundary
	conditions (ECMWF reanalysis) against daily analysis of high-density
	rain-gauge data and streamflow data. The regional climate model is
	found to qualitatively reproduce the main mesoscale precipitation
	patterns and their seasonal evolution. Systematic biases are, however,
	found in the distribution of precipitation with topographic height
	in the Alpine region and at adjacent hill ranges. The RCM also reproduces
	intercatchment variations in the frequency distribution of daily
	precipitation. Simulated runoff resembles closely the mean annual
	cycle, and daily runoff agrees well with observations in timing and
	amplitude of runoff events for lowland gauges. Larger model errors
	are found for high-altitude Alpine catchments. The 14-km RCM provides
	much finer and more realistic precipitation fields compared to the
	56-km RCM, but these improvements did not have a significant impact
	on the skill of the hydrological model to simulate streamflow. The
	model chain was found to reproduce observed month-to-month variations
	of basin-mean winter precipitation and streamflow with correlations
	between 0.85 and 0.95. This result provides confidence that the model
	chain is able to represent key processes related to streamflow variations
	in response to climate variations and climate change.},
  doi = {10.1029/2004JD005143},
  owner = {rojasro},
  timestamp = {2010.07.30}
}

@ARTICLE{klemes1986,
  author = {Kleme\v{s}, V.},
  title = {Operational testing of hydrological simulation models},
  journal = {Hydrological Sciences Journal},
  year = {1986},
  volume = {31},
  pages = {13--24},
  number = {1},
  abstract = {A hierarchical scheme for the systematic testing of hydrological simulation
	models is proposed which ties the nature of the test to the difficulty
	of the modelling task. The testing is referred to as operational,
	since its aim is merely to assess the performance of a model in situations
	as close as possible to those in which it is supposed to be used
	in practice; in other words, to assess its operational adequacy.
	The measure of the quality of performance is the degree of agreement
	of the simulation result with observation. Hence, the power of the
	tests being proposed is rather modest, and even a fully successful
	result can be seen only as a necessary, rather than a sufficient,
	condition for model adequacy vis-agrave-vis the specific modelling
	objective. The scheme contains no new and original ideas; it is merely
	an attempt to present an organized methodology based on standard
	techniques, a methodology that can be viewed as a generalization
	of the routine split-sample test. Its main aim is to accommodate
	the possibility of testing model transposability, both of the simple
	geographical kind and of more complex kinds, such as transposability
	between different types of land use, climate, and other types of
	environmental changes},
  doi = {10.1080/02626668609491024},
  tags = {Calibration}
}

@ARTICLE{koch2009,
  author = {Koch, H. and V\"ogele, S.},
  title = {Dynamic modelling of water demand, water availability and adaption
	strategies for power plants to global change},
  journal = {Ecological Economics},
  year = {2009},
  volume = {68},
  pages = {2031--2039},
  number = {7},
  month = {May},
  abstract = {According to the latest IPCC reports, the frequency of hot and dry
	periods will increase in many regions of the world in the future.
	For power plant operators, the increasing possibility of water shortages
	is an important challenge that they have to face. Shortages of electricity
	due to water shortages could have an influence on industries as well
	as on private households. Climate change impact analyses must analyse
	the climate effects on power plants and possible adaptation strategies
	for the power generation sector. Power plants have lifetimes of several
	decades. Their water demand changes with climate parameters in the
	short- and medium-term. In the long-term, the water demand will change
	as old units are phased out and new generating units appear in their
	place. In this paper, we describe the integration of functions for
	the calculation of the water demand of power plants into a water
	resources management model. Also included are both short-term reactive
	and long-term planned adaptation. This integration allows us to simulate
	the interconnection between the water demand of power plants and
	water resources management, i.e. water availability. Economic evaluation
	functions for water shortages are also integrated into the water
	resources management model. This coupled model enables us to analyse
	scenarios of socio-economic and climate change, as well as the effects
	of water management actions.},
  doi = {10.1016/j.ecolecon.2009.02.015},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{koivumaki+al2010,
  author = {Koivum\"aki, L. and Alho, P. and Lotsari, E. and K\"ayhk\"o, J. and
	Saari, A. and Hyypp\"a, H.},
  title = {Uncertainties in flood risk mapping: case study on estimating building
	damages for a river flood in {Finland}},
  journal = {Journal of Flood Risk Management},
  year = {2010},
  volume = {3},
  pages = {166--183},
  number = {2},
  abstract = {In our review of recent flood risk mapping approaches in Europe, we
	noted that the sources of uncertainty were rarely questioned. We
	demonstrated potential sources of uncertainty in flood risk mapping
	of buildings using a case study of a spring flood in 2005, in Kittilä,
	Finland. One- and two-dimensional hydraulic models of the flood corresponded
	well with the actual inundation. The initial modelling result of
	the inundated buildings differed considerably from reality, but this
	could be improved through modelling performed with more diverse building
	elevation data. The accuracy of the digital terrain model is a key
	determinant in the accuracy of flood hazard modelling. An exposure
	analysis of buildings is often utilized by an overlay analysis of
	map layers representing both the flood and the buildings. However,
	we indicated that the analysis may be partly hindered by the characteristics
	and inaccuracies of the building datasets used and the modelled flood.
	In flood damage modelling, the average damages calculated from the
	database were used, as empirical damage data were too general for
	a detailed flood damage assessment. Damage modelling with empirical
	and synthetic damage data could be made more reliable through better
	archiving of actual flood damages and by performing more diverse
	damage estimates of standard buildings.},
  doi = {10.1111/j.1753-318X.2010.01064.x},
  owner = {rojasro},
  timestamp = {2010.08.06}
}

@ARTICLE{kovumaki+al2010,
  author = {Koivumäki, L. and Alho, P. and Lotsari, E. and Käyhkö, J. and Saari,
	A. and Hyyppä, H.},
  title = {Uncertainties in flood risk mapping: a case study on estimating building
	damages for a river flood in Finland},
  journal = {Journal of Flood Risk Management},
  year = {2010},
  volume = {3},
  pages = {166--183},
  number = {2},
  abstract = {In our review of recent flood risk mapping approaches in Europe, we
	noted that the sources of uncertainty were rarely questioned. We
	demonstrated potential sources of uncertainty in flood risk mapping
	of buildings using a case study of a spring flood in 2005, in Kittilä,
	Finland. One- and two-dimensional hydraulic models of the flood corresponded
	well with the actual inundation. The initial modelling result of
	the inundated buildings differed considerably from reality, but this
	could be improved through modelling performed with more diverse building
	elevation data. The accuracy of the digital terrain model is a key
	determinant in the accuracy of flood hazard modelling. An exposure
	analysis of buildings is often utilized by an overlay analysis of
	map layers representing both the flood and the buildings. However,
	we indicated that the analysis may be partly hindered by the characteristics
	and inaccuracies of the building datasets used and the modelled flood.
	In flood damage modelling, the average damages calculated from the
	database were used, as empirical damage data were too general for
	a detailed flood damage assessment. Damage modelling with empirical
	and synthetic damage data could be made more reliable through better
	archiving of actual flood damages and by performing more diverse
	damage estimates of standard buildings.},
  doi = {10.1111/j.1753-318X.2010.01064.x},
  issn = {1753-318X},
  keywords = {Flood risk mapping, flood damages of buildings, modelling, the EU's
	Flood Directive, uncertainty},
  publisher = {Blackwell Publishing Ltd}
}

@ARTICLE{konikow1992,
  author = {Konikow, L. and Bredehoeft, J.},
  title = {Ground--water models cannot be validated},
  journal = {Advances in Water Resources},
  year = {1992},
  volume = {15},
  pages = {75--83},
  number = {1},
  abstract = {Ground-water models are embodiments of scientific hypotheses. As such,
	the models cannot be proven or validated, but only tested and invalidated.
	However, model testing and the evaluation of predictive errors lead
	to improved models and a better understanding of the problem at hand.
	In applying ground-water models to field problems, errors arise from
	conceptual deficiencies, numerical errors, and inadequate parameter
	estimation. Case histories of model applications to the Dakota Aquifer,
	South Dakota, to bedded salts in New Mexico, and to the upper Coachella
	Valley, California, illustrate that calibration produces a nonunique
	solution and that validation, per se, is a futile objective. Although
	models are definitely valuable tools for analyzing ground-water systems,
	their predictive accuracy is limited. The terms validation and verification
	are misleading and their use in ground-water science should be abandoned
	in favor of more meaningful model-assessment descriptors.},
  doi = {10.1016/0309-1708(92)90033-X},
  owner = {RRojas},
  timestamp = {2009.03.25}
}

@INPROCEEDINGS{konikow2007,
  author = {Konikow, L. and Reilly, T. and Barlow, P. and Voss, C.},
  title = {Groundwater modeling},
  booktitle = {The handbook of groundwater engineering, chapter 23.},
  year = {2007},
  editor = {Delleur, J.},
  pages = {1--50},
  address = {Boca Raton, Florida},
  publisher = {CRC Press},
  chapter = {23},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.06.02}
}

@ARTICLE{krause+al2005,
  author = {Krause, P. and Boyle, D. and B{\"a}se, F.},
  title = {Comparison of different efficiency criteria for hydrological model
	assessment},
  journal = {Advances in Geosciences},
  year = {2005},
  volume = {5},
  pages = {89--97},
  number = {5},
  abstract = {The evaluation of hydrologic model behaviour and performance is commonly
	made and reported through comparisons of simulated and observed variables.
	Frequently, comparisons are made between simulated and measured streamflow
	at the catchment outlet. In distributed hydrological modelling approaches,
	additional comparisons of simulated and observed measurements for
	multi-response validation may be integrated into the evaluation procedure
	to assess overall modelling performance. In both approaches, single
	and multi-response, efficiency criteria are commonly used by hydrologists
	to provide an objective assessment of the "closeness" of the simulated
	behaviour to the observed measurements. While there are a few efficiency
	criteria such as the Nash-Sutcliffe efficiency, coefficient of determination,
	and index of agreement that are frequently used in hydrologic modeling
	studies and reported in the literature, there are a large number
	of other efficiency criteria to choose from. The selection and use
	of specific efficiency criteria and the interpretation of the results
	can be a challenge for even the most experienced hydrologist since
	each criterion may place different emphasis on different types of
	simulated and observed behaviours. In this paper, the utility of
	several efficiency criteria is investigated in three examples using
	a simple observed streamflow hydrograph.},
  doi = {10.5194/adgeo-5-89-2005},
  keywords = {bR2},
  tags = {calibration, Goodness-of-Fit}
}

@ARTICLE{kraussecullmann2011a,
  author = {Krau{\ss}e, T. and Cullmann, J.},
  title = {Identification of hydrological model parameters for flood forecasting
	using data depth measures},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2011},
  volume = {8},
  pages = {2423--2476},
  number = {2},
  abstract = {The development of methods for estimating the parameters of hydrological
	models considering uncertainties has been of high interest in hydrological
	research over the last years. Besides the very popular Markov Chain
	Monte Carlo (MCMC) methods which estimate the uncertainty of model
	parameters in the settings of a Bayesian framework, the development
	of depth based sampling methods, also entitled robust parameter estimation
	(ROPE), have attracted an increasing research interest. These methods
	understand the estimation of model parameters as a geometric search
	of a set of robust performing parameter vectors by application of
	the concept of data depth. Recent studies showed that the parameter
	vectors estimated by depth based sampling perform more robust in
	validation. One major advantage of this kind of approach over the
	MCMC methods is that the formulation of a likelihood function within
	a Bayesian uncertainty framework gets obsolete and arbitrary purpose-oriented
	performance criteria defined by the user can be integrated without
	any further complications. In this paper we present an advanced ROPE
	method entitled the Advanced Robust Parameter Estimation by Monte
	Carlo algorithm (AROPEMC). The AROPEMC algorithm is a modified version
	of the original robust parameter estimation algorithm ROPEMC developed
	by B{\'a}rdossy and Singh (2008). AROPEMC performs by merging iterative
	Monte Carlo simulations, identifying well performing parameter vectors,
	the sampling of robust parameter vectors according to the principle
	of data depth and the application of a well-founded stopping criterion
	applied in supervised machine learning. The principals of the algorithm
	are illustrated by means of the Rosenbrock's and Rastrigin's function,
	two well known performance benchmarks for optimisation algorithms.
	Two case studies demonstrate the advantage of AROPEMC compared to
	state of the art global optimisation algorithms. A distributed process-oriented
	hydrological model is calibrated and validated for flood forecasting
	in a small catchment characterised by extreme process dynamics.},
  doi = {10.5194/hessd-8-2423-2011},
  tags = {Calibration, Floods}
}

@ARTICLE{kraussecullmann2011b,
  author = {Krau{\ss}e, T. and Cullmann, J.},
  title = {Towards a more representative parametrisation of hydrological models
	via synthesizing the strengths of particle swarm optimisation and
	robust parameter estimation},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2011},
  volume = {8},
  pages = {2373--2422},
  number = {2},
  abstract = {The development of methods for estimating the parameters of hydrological
	models considering uncertainties has been of high interest in hydrological
	research over the last years. In particular methods which understand
	the estimation of hydrological model parameters as a geometric search
	of a set of robust performing parameter vectors by application of
	the concept of data depth found growing research interest. B{\'a}rdossy
	and Singh (2008) presented a first proposal and applied it for the
	calibration of a conceptual rainfall-runoff model with daily time
	step. Krau{{\ss}}e and Cullmann (2011) further developed this method
	and applied it in a case study to calibrate a process oriented hydrological
	model with hourly time step focussing on flood events in a fast responding
	catchment. The results of both studies showed the potential of the
	application of the principle of data depth. However, also the weak
	point of the presented approach got obvious. The algorithm identifies
	a set of model parameter vectors with high model performance and
	subsequently generates a set of parameter vectors with high data
	depth with respect to the first set. These both steps are repeated
	iteratively until a stopping criterion is met. In the first step
	the estimation of the good parameter vectors is based on the Monte
	Carlo method. The major shortcoming of this method is that it is
	strongly dependent on a high number of samples exponentially growing
	with the dimensionality of the problem. In this paper we present
	another robust parameter estimation strategy which applies an approved
	search strategy for high-dimensional parameter spaces, the particle
	swarm optimisation in order to identify a set of good parameter vectors
	with given uncertainty bounds. The generation of deep parameters
	is according to Krau{{\ss}}e and Cullmann (2011). The method was
	compared to the Monte Carlo based robust parameter estimation algorithm
	on the example of a case study in Krau{{\ss}}e and Cullmann (2011)
	to calibrate the process-oriented distributed hydrological model
	focussing for flood forecasting in a small catchment characterised
	by extreme process dynamics. In a second case study the comparison
	is repeated on a problem with higher dimensionality considering further
	parameters of the soil module},
  doi = {10.5194/hessd-8-2373-2011},
  tags = {Calibration, PSO}
}

@ARTICLE{kreibich+al2009,
  author = {Kreibich, H. and Piroth, K. and Seifert, I. and Maiwald, H. and Kunert,
	U. and Schwarz, J. and Merz, B. and Thieken, A. H.},
  title = {Is flow velocity a significant parameter in flood damage modelling?},
  journal = {Natural Hazards and Earth System Science},
  year = {2009},
  volume = {9},
  pages = {1679--1692},
  number = {5},
  doi = {10.5194/nhess-9-1679-2009}
}

@ARTICLE{krueger+al2010,
  author = {Krueger, T. and Freer, J. and Quinton, J. and Macleod, C. and Bilotta,
	G. and Brazier, R. and Butler, P. and Haygarth, P.},
  title = {Ensemble evaluation of hydrological model hypotheses},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {1--17},
  doi = {10.1029/2009WR007845},
  owner = {rojasro},
  timestamp = {2010.07.30}
}

@ARTICLE{krysanova+al2005,
  author = {Krysanova, V. and Hattermann, F. and Habeck, A.},
  title = {Expected changes in water resources availability and water quality
	with respect to climate change in the {E}lbe {R}iver basin ({G}ermany)},
  journal = {Nordic Hydrology},
  year = {2005},
  volume = {36},
  pages = {321--333},
  number = {4--5},
  abstract = {Reliable modelling of climate–water interactions at the river basin
	and regional scale requires development of advanced modelling approaches
	at scales relevant for assessing the potential effects of climate
	change on the hydrological cycle. These approaches should represent
	the atmospheric, surface and subsurface hydrological processes and
	take into account their characteristic temporal and spatial scales
	of occurrence. The paper presents a climate change impact assessment
	performed for the Elbe River basin in Germany (about 100?000?km2).
	The method used for the study combines: (a) a statistical downscaling
	method driven by GCM-predicted temperature trend for producing climate
	scenarios, and (b) a simulation technique based on an ecohydrological
	semi-distributed river basin model, which was thoroughly validated
	in advance. The overall result of the climate impact study for the
	basin is that the mean water discharge and the mean groundwater recharge
	in the Elbe basin will be most likely decreased under the expected
	climate change and diffuse source pollution will be diminished. Our
	study confirms that the uncertainty in hydrological and water quality
	responses to changing climate is generally higher than the uncertainty
	in climate input. The method is transferable to other basins in the
	temperate zone.},
  owner = {rojasro},
  timestamp = {2010.08.04},
  url = {http://www.iwaponline.com/nh/036/nh0360321.htm}
}

@ARTICLE{krysanova+al2007,
  author = {Krysanova, V. and Hattermann, F. and Wechsung, F.},
  title = {Implications of complexity and uncertainty for integrated modelling
	and impact assessment in river basins},
  journal = {Environmental Modelling \& Software},
  year = {2007},
  volume = {22},
  pages = {701--709},
  number = {5},
  abstract = {The paper focuses on implications of complexity and uncertainty in
	climate change impact assessment at the river basin and regional
	scales. The study was performed using the process-based ecohydrological
	spatially semi distributed model SWIM (Soil and Water Integrated
	Model). The model integrates hydrological processes, vegetation/crop
	growth, erosion and nutrient dynamics in river basins. It was developed
	from the SWAT and MATSALU models for climate and land use change
	impact assessment. The study area is the German part of the Elbe
	River basin (about 100,000 km2). It is representative for semi-humid
	landscapes in Europe, where water availability during the summer
	season is the limiting factor for plant growth and crop yield. The
	validation method followed the multi-scale, multi-site and multi-criteria
	approach and enabled to reproduce (a) water discharge and nutrient
	load at the river outlet along with (b) local ecohydrological processes
	like water table dynamics in subbasins, nutrient fluxes and vegetation
	growth dynamics at multiple scales and sites. The uncertainty of
	climate impacts was evaluated using comprehensive Monte Carlo simulation
	experiments},
  doi = {10.1016/j.envsoft.2005.12.029},
  keywords = {SWAT, Integrated modelling, Ecohydrological model, Climate impact,
	Uncertainty, Elbe River},
  tags = {Uncertainty, Climate Change, SWAT}
}

@ARTICLE{krzysztofowi2010,
  author = {Krzysztofowicz, R.},
  title = {Decision criteria, data fusion and prediction calibration: a {B}ayesian
	approach},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {1033--1050},
  number = {6},
  abstract = {The novel research paradigm, dubbed the {``}court of miracles of hydrological
	modelling{''}, focuses on achieving scientific progress in circumstances
	which traditionally would be called model failures. Many of the associated
	modelling issues can be addressed systematically and coherently within
	the mathematical framework of Bayesian forecast-decision theory.
	Five of them are addressed herein: (a) choosing a criterion function
	for making rational decisions under uncertainty (a meta-decision
	problem); (b) modelling stochastic dependence between variates to
	quantify uncertainty and predict realizations; (c) fusing data from
	asymmetric samples to cope with unrepresentativeness of small samples
	and corruptive effects of outliers; (d) calibrating probabilistic
	predictions against a prior distribution; and (e) ordering predictors,
	or models, in terms of their informativeness (equivalently, in terms
	of their economic value to a decider). It is suggested that communication
	between hydrologists and deciders (planners, engineers, operators
	of hydrosystems) would benefit if hydrologists adopted, at least
	on some issues, the perspective of deciders, who must act in a timely
	and rational manner, and for whom hydrological estimates and predictions
	have economic consequences. },
  doi = {10.1080/02626667.2010.505894},
  keywords = {decision making, decision criteria, expected utility, estimation,
	uncertainty, distribution function, data fusion, prediction calibration,
	informativeness, Bayesian approach },
  tags = {Calibration, Uncertainty}
}

@ARTICLE{kristofowicz1999,
  author = {Krzysztofowicz, R.},
  title = {Bayesian theory of probabilistic forecasting via deterministic hydrologic
	model},
  journal = {Water Resources Research},
  year = {1999},
  volume = {35},
  pages = {2739--2750},
  number = {9},
  abstract = {Rational decision making (for flood warning, navigation, or reservoir
	systems) requires that the total uncertainty about a hydrologic predictand
	(such as river stage, discharge, or runoff volume) be quantified
	in terms of a probability distribution, conditional on all available
	information and knowledge. Hydrologic knowledge is typically embodied
	in a deterministic catchment model. Fundamentals are presented of
	a Bayesian forecasting system (BFS) for producing a probabilistic
	forecast of a hydrologic predictand via any deterministic catchment
	model. The BFS decomposes the total uncertainty into input uncertainty
	and hydrologic uncertainty, which are quantified independently and
	then integrated into a predictive (Bayes) distribution. This distribution
	results from a revision of a prior (climatic) distribution, is well
	calibrated, and has a nonnegative ex ante economic value. The BFS
	is compared with Monte Carlo simulation and “ensemble forecasting”
	technique, none of which can alone produce a probabilistic forecast
	that meets requirements of rational decision making, but each can
	serve as a component of the BFS.},
  doi = {10.1029/1999WR900099},
  owner = {RRojas},
  timestamp = {2008.11.28}
}

@ARTICLE{kuczera1990,
  author = {Kuczera, G.},
  title = {Assessing hydrologic model nonlinearity using response surface plots},
  journal = {Journal of Hydrology},
  year = {1990},
  volume = {118},
  pages = {143},
  number = {1-4},
  doi = {10.1016/0022-1694(90)90255-V},
  tags = {Calibration}
}

@ARTICLE{kuczera1983a,
  author = {Kuczera, G.},
  title = {Improved Parameter Inference in Catchment Models 1. Evaluating Parameter
	Uncertainty},
  journal = {Water Resources Research},
  year = {1983},
  volume = {19},
  pages = {1151--1162},
  number = {5},
  abstract = {A Bayesian methodology is developed to evaluate parameter uncertainty
	in catchment models fitted to a hydrologic response such as runoff,
	the goal being to improve the chance of successful regionalization.
	The catchment model is posed as a nonlinear regression model with
	stochastic errors possibly being both autocorrelated and heteroscedastic.
	The end result of this methodology, which may use Box-Cox power transformations
	and ARMA error models, is the posterior distribution, which summarizes
	what is known about the catchment model parameters. This can be simplified
	to a multivariate normal provided a linearization in parameter space
	is acceptable; means of checking and improving this assumption are
	discussed. The posterior standard deviations give a direct measure
	of parameter uncertainty, and study of the posterior correlation
	matrix can indicate what kinds of data are required to improve the
	precision of poorly determined parameters. Finally, a case study
	involving a nine-parameter catchment model fitted to monthly runoff
	and soil moisture data is presented. It is shown that use of ordinary
	least squares when its underlying error assumptions are violated
	gives an erroneous description of parameter uncertainty.},
  doi = {10.1029/WR019i005p01151},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{kuczera1983b,
  author = {Kuczera, G.},
  title = {Improved Parameter Inference in Catchment Models 2. Combining Different
	Kinds of Hydrologic Data and Testing Their Compatibility },
  journal = {Water Resources Research},
  year = {1983},
  volume = {19},
  pages = {1163--1172},
  number = {5},
  doi = {10.1029/WR019i005p01163},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{kuczera+al2006,
  author = {Kuczera, G. and Kavetski, D. and Franks, S. and Thyer, M.},
  title = {Towards a Bayesian total error analysis of conceptual rainfall-runoff
	models: Characterising model error using storm-dependent parameters},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {331},
  pages = {161},
  number = {1-2},
  abstract = {Calibration and prediction in conceptual rainfall-runoff (CRR) modelling
	is affected by the uncertainty in the observed forcing/response data
	and the structural error in the model. This study works towards the
	goal of developing a robust framework for dealing with these sources
	of error and focuses on model error. The characterisation of model
	error in CRR modelling has been thwarted by the convenient but indefensible
	treatment of CRR models as deterministic descriptions of catchment
	dynamics. This paper argues that the fluxes in CRR models should
	be treated as stochastic quantities because their estimation involves
	spatial and temporal averaging. Acceptance that CRR models are intrinsically
	stochastic paves the way for a more rational characterisation of
	model error. The hypothesis advanced in this paper is that CRR model
	error can be characterised by storm-dependent random variation of
	one or more CRR model parameters. A simple sensitivity analysis is
	used to identify the parameters most likely to behave stochastically,
	with variation in these parameters yielding the largest changes in
	model predictions as measured by the Nash{--}Sutcliffe criterion.
	A Bayesian hierarchical model is then formulated to explicitly differentiate
	between forcing, response and model error. It provides a very general
	framework for calibration and prediction, as well as for testing
	hypotheses regarding model structure and data uncertainty. A case
	study calibrating a six-parameter CRR model to daily data from the
	Abercrombie catchment (Australia) demonstrates the considerable potential
	of this approach. Allowing storm-dependent variation in just two
	model parameters (with one of the parameters characterising model
	error and the other reflecting input uncertainty) yields a substantially
	improved model fit raising the Nash{--}Sutcliffe statistic from 0.74
	to 0.94. Of particular significance is the use of posterior diagnostics
	to test the key assumptions about the data and model errors. The
	assumption that the storm-dependent parameters are log-normally distributed
	is only partially supported by the data, which suggests that the
	parameter hyper-distributions have thicker tails. The results also
	indicate that in this case study the uncertainty in the rainfall
	data dominates model uncertainty.},
  doi = {10.1016/j.jhydrol.2006.05.010},
  keywords = {Conceptual rainfall-runoff modelling, Parameter calibration, Model
	error, Input uncertainty, Bayesian parameter estimation, Parameter
	variation, Model determinism},
  tags = {Calibration}
}

@ARTICLE{kuczera+al2010,
  author = {Kuczera, G. and Kavetski, D. and Renard, B. and Thyer, M.},
  title = {A limited-memory acceleration strategy for {MCMC} sampling in hierarchical
	{B}ayesian calibration of hydrological models},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W07602},
  abstract = {Hydrological calibration and prediction using conceptual models is
	affected by forcing/response data uncertainty and structural model
	error. The Bayesian Total Error Analysis methodology uses a hierarchical
	representation of individual sources of uncertainty. However, it
	is shown that standard multiblock {``}Metropolis-within-Gibbs{''}
	Markov chain Monte Carlo (MCMC) samplers commonly used in Bayesian
	hierarchical inference are exceedingly computationally expensive
	when applied to hydrologic models, which use recursive numerical
	solutions of coupled nonlinear differential equations to describe
	the evolution of catchment states such as soil and groundwater storages.
	This note develops a {``}limited-memory{''} algorithm for accelerating
	multiblock MCMC sampling from the posterior distributions of such
	models using low-dimensional jump distributions. The new algorithm
	exploits the decaying memory of hydrological systems to provide accurate
	tolerance-based approximations of traditional {``}full-memory{''}
	MCMC methods and is orders of magnitude more efficient than the latter.
	},
  doi = {10.1029/2009WR008985},
  tags = {Calibration}
}

@ARTICLE{kuczera+al2010b,
  author = {Kuczera, G. and Renard, B. and Thyer, M. and Kavetski, D.},
  title = {There are no hydrological monsters, just models and observations
	with large uncertainties!},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {980--991},
  number = {6},
  abstract = {Catchments that do not behave in the way the hydrologist expects,
	expose the frailties of hydrological science, particularly its unduly
	simplistic treatment of input and model uncertainty. A conceptual
	rainfall-runoff model represents a highly simplified hypothesis of
	the transformation of rainfall into runoff. Sub-grid variability
	and mis-specification of processes introduce an irreducible model
	error, about which little is currently known. In addition, hydrological
	observation systems are far from perfect, with the principal catchment
	forcing (rainfall) often subject to large sampling errors. When ignored
	or treated simplistically, these errors develop into monsters that
	destroy our ability to model certain catchments. In this paper, these
	monsters are tackled using Bayesian Total Error Analysis, a framework
	that accounts for user-specified sources of error and yields quantitative
	insights into how prior knowledge of these uncertainties affects
	our ability to infer models and use them for predictive purposes.
	A case study involving a catchment with an apparent water balance
	anomaly (a hydrological monstrosity!) illustrates these concepts.
	It is found that, in the absence of additional information, the rainfall-runoff
	record is insufficient to explain this anomaly - it could be due
	to a large export of groundwater, systematic overestimation of catchment
	rainfall of the order of 40\%, or a conspiracy of these factors.
	There is {``}no free lunch{''} in hydrology. The rainfall-runoff
	record on its own is insufficient to decompose the different sources
	of uncertainty affecting calibration, testing and prediction, and
	hydrological monstrosities will persist until additional independent
	knowledge of uncertainties is obtained. },
  doi = {10.1080/02626667.2010.504677},
  keywords = {Bayesian total error analysis, model structural error, data errors,
	rainfall-runoff models, ill-posedness },
  tags = {Calibration, Philosophical, Uncertainty}
}

@ARTICLE{kuczerawilliams1992,
  author = {G. Kuczera and B. J. Williams},
  title = {Effect of Rainfall Errors on Accuracy of Design Flood Estimates},
  journal = {Water Resources Research},
  year = {1992},
  volume = {28},
  pages = {1145--1153},
  number = {4},
  abstract = {A procedure is presented to explicitly evaluate the effect of estimation
	errors in the temporal and spatial distribution of rainfall on the
	uncertainty of calibrated rainfall-runoff model parameters. The effect
	of this uncertainty on the reliability of design flood predictions
	is considered. A case study of the Hacking catchment, south of Sydney,
	Australia, is presented to illustrate the procedure. For a major
	storm event a simple stochastic transformed rainfall model is calibrated
	and validated using kriging and then used to infer the mean and covariance
	of subareal rainfall. The RORB model, a distributed nonlinear rainfall-runoff
	model, is calibrated to this storm event. When allowance is made
	for uncertainty in the calibration event rainfall, the results indicate
	that the uncertainty in the calibrated parameters increases, especially
	in the rainfall excess parameters, and the 90\% prediction interval
	on the 100-year design flood increases by about 100\%},
  doi = {10.1029/91WR03002},
  tags = {Rainfall}
}

@ARTICLE{kundzewicz2011,
  author = {Kundzewicz, Z.},
  title = {{Nonstationarity in water resources--central European perspective}},
  journal = {Journal of the American Water Resources Association},
  year = {2011},
  volume = {47},
  pages = {550--562},
  number = {3},
  abstract = {Nonstationarity in variables describing water quantity and water quality
	characteristics is reviewed, and an attempt to interpret nonstationary
	behavior is made with particular reference to the Central European
	region. Nonstationarity in water-related variables results from several
	nonclimatic and climatic factors. Albeit evidence of climate change
	in Central Europe is clear, anthropogenic nonclimatic change, such
	as land-use or land-cover changes, water engineering measures, and
	in-catchment water management play important roles. Systemic socioeconomic
	and political changes are the main factors responsible for the observed
	change in water quality in the region. The observed climate change
	in the Central European region has not been dramatic enough to persuade
	the water management community that changes of standards, criteria,
	and evaluation procedures should be made. Projections for the future
	largely differ between models and scenarios, hence information obtained
	from climate models is found too vague to be used. However, the water
	management community shows interest in climate change observations,
	projections, and impact assessments. Numerous hydrological research
	projects to tackle nonstationarity have been undertaken in the region.
	Also important acts of legislation, such as the European Union’s
	Water Framework Directive and Floods Directive can be regarded in
	the context of nonstationarity of water-related variables.},
  doi = {10.1111/j.1752-1688.2011.00549.x},
  owner = {rojasro},
  timestamp = {2011.08.01}
}

@ARTICLE{kundzewicz+al2010,
  author = {Kundzewicz, Z. and Hirabayaschi, Y. and Kanae, S.},
  title = {River floods in the changing climate--observations and projections},
  journal = {Water Resources Management},
  year = {2010},
  volume = {24},
  pages = {2633--2646},
  number = {11},
  abstract = {River flood damages, worldwide, have increased dynamically in the
	last few decades, so that it is necessary to interpret this change.
	River flooding is a complex phenomenon which can be affected by changes
	coupled to terrestrial, socio-economic and climate systems. The climate
	track in the observed changes is likely, even if human encroaching
	into the harm’s way and increase in the damage potential in floodplains
	can be the dominating factors in many river basins. Increase in intense
	precipitation has already been observed, with consequences to increasing
	risk of rain-induced flooding. Projections for the future, based
	on climate model simulations, indicate increase of flood risks in
	many areas, globally. Over large areas, a 100-year flood in the control
	period is projected to become much more frequent in the future time
	horizon. Despite the fact that the degree of uncertainty in model-based
	projections is considerable and difficult to quantify, the change
	in design flood frequency has obvious relevance to flood risk management
	practice. The number of flood-affected people is projected to increase
	with the amount of warming. For a 4°C warming the number of flood-affected
	people is over 2.5 times higher than for a 2°C warming. The present
	contribution addresses the climate track in an integrated way, tackling
	issues related to multiple factors, change detection, projections,
	and adaptation to floods.},
  doi = {10.1007/s11269-009-9571-6},
  owner = {rojasro},
  timestamp = {2011.08.01}
}

@ARTICLE{kundedzewicz+al2010,
  author = {Kundzewicz, Z. and Lugeri, N. and Dankers, R. and Hirabayaschi, Y.
	and D\"oll, P. Pi\'nskar, I. and Dysarz, T. and Hochrainer, S. and
	Matczak, P.},
  title = {Assessing river flood risk and adaptation in {E}urope---review of
	projections for the future},
  journal = {Mitigation and Adaptation Strategies for Global Change},
  year = {2010},
  volume = {15},
  pages = {641--656},
  number = {7},
  abstract = {Flood damages have exhibited a rapid upward trend, both globally and
	in Europe, faster than population and economic growth. Hence, vigorous
	attempts of attribution of changes have been made. Flood risk and
	vulnerability tend to change over many areas, due to a range of climatic
	and nonclimatic impacts whose relative importance is site-specific.
	Flooding is a complex phenomenon and there are several generating
	mechanisms, among others intense and/or long-lasting precipitation,
	snowmelt, ice jam. Projected climate-driven changes in future flood
	frequency are complex, depending on the generating mechanism, e.g.,
	increasing flood magnitudes where floods result of heavy rainfall
	and possibly decreasing magnitudes where floods are generated by
	spring snowmelt. Climate change is likely to cause an increase of
	the risk of riverine flooding across much of Europe. Projections
	of flood hazard in Europe based on climatic and hydrological models,
	reviewed in this paper, illustrate possible changes of recurrence
	of a 100-year flood (with probability of exceedance being 1-in-100
	years) in Europe. What used to be a 100-year flood in the control
	period is projected to become either more frequent or less frequent
	in the future time horizon of concern. For a large part of the continent,
	large flooding is projected to become more commonplace in future,
	warmer climate. Due to the large uncertainty of climate projections,
	it is currently not possible to devise a scientifically-sound procedure
	for redefining design floods (e.g. 100-year flood) in order to adjust
	flood defenses. For the time being, we recommend to adjust design
	floods using a “climate change factor” approach.},
  doi = {10.1007/s11027-010-9213-6},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{kundzewicz+al2008,
  author = {Kundzewicz, Z. and Mata, L. and Arnell, N. and D\"oll, P. and Jimenez,
	B. and Miller, K. and Oki, T. and Sen, Z. and Shiklomanov, I.},
  title = {The implications of projected climate change for freshwater resources
	and their management},
  journal = {Hydrological Sciences Journal},
  year = {2008},
  volume = {1},
  pages = {3--10},
  number = {1},
  month = {February},
  doi = {10.1623/hysj.53.1.11},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@INCOLLECTION{kundzewicz+al2007,
  author = {Kundzewicz, Z. and Mata, L. and Arnell, N. and D\"oll, P. and Kabat,
	P. and Jim\'enez, B. and Miller, K. and Oki, T. and Sen, Z. and Shiklomanov,
	I.},
  title = {Freshwater resources and their management},
  booktitle = {Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution
	of Working Group II to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge {U}niversity {P}ress},
  year = {2007},
  editor = {M. L. Parry and O. F. Canziani and J. P. Palutikof and P. J. {van
	der Linden} and C.E. Hanson},
  address = {Cambridge, {UK}, 173-210},
  tags = {IPCC}
}

@ARTICLE{kundzewicz+al2006,
  author = {Kundzewicz, Z. and Radziejewski, M. and Pi\"nskwar, I.},
  title = {{Precipitation extremes in the changing climate of Europe}},
  journal = {Climate Research},
  year = {2006},
  volume = {31},
  pages = {51--58},
  number = {1},
  abstract = {Several episodes of extreme precipitation or extreme lack of precipitation
	(and high temperature) leading to dramatic and high-impact floods
	and droughts have occurred in Europe in recent years. Climate scenarios
	suggest that problems of too little or too much water may become
	more severe in the future. Using data from the Hadley Centre’s HadRM3
	model, this paper analyzes future changes in the characteristics
	of intense precipitation (mean daily precipitation amounts and number
	of days with intense precipitation in a year) and the duration of
	dry (also dry and hot) spells over the European continent, comparing
	the time periods of 1961–1990 and 2070–2099. The potential for intense
	precipitation is likely to increase in the warmer climate of the
	future, contributing to the growth of flood hazard in areas where
	inundations are typically triggered by heavy rain. The projected
	number of days with intense precipitation and the maximum daily precipitation
	are likely to increase over much of Europe, especially in the central
	and northern parts. According to projections, ‘dry and hot’ extremes
	may become more severe for most of Europe. The areas already affected
	by water stress in the present climate (e.g. southern Europe) are
	expected to experience even more severe conditions.},
  doi = {10.3354/cr031051},
  owner = {rojasro},
  timestamp = {2011.07.08}
}

@ARTICLE{kundzewicz+al2005,
  author = {Kundzewicz, Z. and Ulbrich, U. and Br\"ucher, T. and Graczyk, D.
	and Kr\"uger, A. and Leckebusch, G. and Menzel, L. and Pi\'nskwar,
	I. and Radziejewski, M. and Szwed, M.},
  title = {{Summer floods in Central Europe - Climate change track?}},
  journal = {Natural Hazards},
  year = {2005},
  volume = {36},
  pages = {165--189},
  number = {1},
  abstract = {In Central Europe, river flooding has been recently recognized as
	a major hazard, in particular after the 1997 Odra /Oder flood, the
	2001 Vistula flood, and the most destructive 2002 deluge on the Labe/Elbe.
	Major recent floods in central Europe are put in perspective and
	their common elements are identified. Having observed that flood
	risk and vulnerability are likely to have grown in many areas, one
	is curious to understand the reasons for growth. These can be sought
	in socio-economic domain (humans encroaching into floodplain areas),
	terrestrial systems (land-cover changes – urbanization, deforestation,
	reduction of wetlands, river regulation), and climate system. The
	atmospheric capacity to absorb moisture, its potential water content,
	and thus potential for intense precipitation, are likely to increase
	in a warmer climate. The changes in intense precipitation and high
	flows are examined, based on observations and projections. Study
	of projected changes in intense precipitation, using climate models,
	for several areas of central Europe, and in particular, for drainage
	basins of the upper Labe/Elbe, Odra/Oder, and Vistula is reported.
	Significant changes have been identified between future projections
	and the reference period, of relevance to flood hazard in areas,
	which have experienced severe recent floodings.},
  doi = {10.1007/s11069-004-4547-6},
  owner = {rojasro},
  timestamp = {2011.07.08}
}

@ARTICLE{kunstmann2002,
  author = {Kunstmann, H. and Kinzelbach, W. and Siegfried, T.},
  title = {Conditional first-order second--moment method and its applications
	to the quantification of uncertainty in groundwater modeling},
  journal = {Water Resources Research},
  year = {2002},
  volume = {38},
  pages = {1035--1048},
  number = {4},
  abstract = {Decision making in water resources management usually requires the
	quantification of uncertainties. Monte Carlo techniques are suited
	for this analysis but imply a huge computational effort. An alternative
	and computationally efficient approach is the first-order second-moment
	(FOSM) method which directly propagates parameter uncertainty into
	the result. We apply the FOSM method to both the groundwater flow
	and solute transport equations. It is shown how conditioning on the
	basis of measured heads and/or concentrations yields the “principle
	of interdependent uncertainty” that correlates the uncertainties
	of feasible hydraulic conductivities and recharge rates. The method
	is used to compute the uncertainty of steady state heads and of steady
	state solute concentrations. It is illustrated by an application
	to the Palla Road Aquifer in semiarid Botswana, for which the quantification
	of the uncertainty range of groundwater recharge is of prime interest.
	The uncertainty bounds obtained by the FOSM method correspond well
	with the results obtained by the Monte Carlo method. The FOSM method,
	however, is much more advantageous with respect to computational
	efficiency. It is shown that at the planned abstraction rate the
	probability of exceeding the natural replenishment of the Palla Road
	Aquifer by overpumping is 30%.},
  doi = {10.1029/2000WR000022},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{kunstmann+al2006,
  author = {Kunstmann, H. and Krause, J. and Mayr, S.},
  title = {Inverse distributed hydrological modelling of {A}lpine catchments},
  journal = {Hydrology and Earth System Sciences},
  year = {2006},
  volume = {10},
  pages = {395--412},
  abstract = {Even in physically based distributed hydrological models, various
	remaining parameters must be estimated for each sub-catchment. This
	can involve tremendous effort, especially when the number of sub-catchments
	is large and the applied hydrological model is computationally expensive.
	Automatic parameter estimation tools can significantly facilitate
	the calibration process. Hence, we combined the nonlinear parameter
	estimation tool PEST with the distributed hydrological model WaSiM.
	PEST is based on the Gauss-Marquardt-Levenberg method, a gradient-based
	nonlinear parameter estimation algorithm. WaSiM is a fully distributed
	hydrological model using physically based algorithms for most of
	the process descriptions.},
  doi = {10.5194/hess-10-395-2006},
  keywords = {GLOBAL OPTIMIZATION, WATERSHED MODEL, PARAMETERS, PEST},
  tags = {Calibration}
}

@ARTICLE{kwadijk1994,
  author = {Kwadijk, J. and Middelkoop, H.},
  title = {Estimation of impact of climate change on the peak discharge probability
	of the river {Rhine}},
  journal = {Climatic Change},
  year = {1994},
  volume = {27},
  pages = {199--224},
  number = {2},
  abstract = {RHINEFLOW is a GIS based water balance model that has been developed
	to study the changes in the water balance compartments of the river
	Rhine basin on a monthly time basis. The model has been designed
	to study the sensitivity of the Rhine discharge to a climate change.
	The calculated discharge has been calibrated and validated on the
	period 1956 to 1980. For this period the model efficiency of RHINEFLOW
	is between 0.74 and 0.81 both for the entire Rhine and for its tributaries.
	Also calculated values for variations in other compartments, e.g.
	snow storage and actual evapotranspiration, were in good agreement
	with the measured values. Since a high correlation between monthly
	discharge and peak discharge was found for the period 1900–1980 The
	RHINEFLOW model is used to assess the probability of exceedence for
	discharge peaks under possible future climate conditions. The probabilities
	of exceedence were calculated from the conditional probabilities
	of peak discharges for a series of 15 classes of monthly discharges.
	Comparison of a calculated frequency distribution of high discharge
	peaks with observed peaks in a test series showed that the method
	performs well. Scenarios for temperature changes between 0 °C and
	plus 4 °C and precipitation changes between plus 20% and minus 20%
	have been applied. Within this range flood frequencies are more sensitive
	for a precipitation change than for a temperature change. The present
	two-year return period peak flow (6500–7000 m3/s) decreases by about
	6% due to a temperature rise of 4 °C; a precipitation decrease of
	20% leads to 30% lower two-year peaks whilst 20% precipitation increase
	raises them by approximately 30%. Application of a lsquoBusiness
	As Usualrsquo (BAU) and an lsquoAccelerated Policyrsquo (AP) climate
	scenario resulted in a significant increase in probability of peak
	flows for the BAU scenario, while for the AP scenario no significant
	change could be found. Due to sampling errors, accurate estimations
	of recurrence times of discharge peaksges7000 m3/s require a longer
	sampling time series than 90 years. For management purposes the method
	can be applied to estimate changes of probabilities of events with
	a relatively long recurrence time.},
  doi = {10.1007/BF01093591},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{kwadijk1995,
  author = {Kwadijk, J. and Rotmans, J.},
  title = {The impact of climate change on the river {Rhine}: {A} scenario study},
  journal = {Climatic Change},
  year = {1995},
  volume = {30},
  pages = {397--425},
  number = {4},
  abstract = {This paper concerns the impact of human-induced global climate change
	on the River Rhine discharge. For this purpose a model for climate
	assessment, named ESCAPE, is coupled to a water balance model, named
	RHINEFLOW. From climate scenarios, changes in regional annual water
	availability and seasonal discharge in the River Rhine Basin are
	estimated. The climate scenarios are based on greenhouse gases emissions
	scenarios. An assessment is made for lsquobest guessrsquo seasonal
	discharge changes and for changes in frequencies of low and high
	discharges in the downstream reaches of the river. In addition, a
	quantitative estimation of the uncertainties associated with this
	guess is arrived at. The results show that the extent and range of
	uncertainty is large with respect to the lsquobest guessrsquo changes.
	The uncertainty range is 2–3 times larger for the Business-as-Usual
	than for the Accelerated Policies scenarios. This large range stems
	from the doubtful precipitation simulations from the present General
	Circulation Models. This scenario study showed the precipitation
	scenarios to be the key-elements within the present range of reliable
	climate change scenarios. For the River Rhine lsquobest guessrsquo
	changes for annual water availability are small according to both
	scenarios. The river changes from a present combined snow-melt-rain
	fed river to an almost entirely rain fed river. The difference between
	present-day large average discharge in winter and the small average
	discharge in autumn should increase for all scenarios. This trend
	is largest in the Alpine part of the basin. Here, winter discharges
	should increase even for scenarios forecasting annual precipitation
	decreases. Summer discharge should decrease. lsquoBest guessrsquo
	scenarios should lead to increased frequencies of both low and high
	flow events in the downstream (Dutch) part of the river. The results
	indicate changes could be larger than presently assumed in lsquoworst
	case scenariosrsquo used by the Dutch water management authorities.},
  doi = {10.1007/BF01093854},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{kyselyberanova2008,
  author = {Kysely, J. and Beranova, R.},
  title = {Climate--change effects on extreme precipitation in central {E}urope:
	uncertainties of scenarios based on regional climate models},
  journal = {Theoretical and Applied Climatology},
  year = {2009},
  volume = {95},
  pages = {361--374},
  number = {3--4},
  abstract = {The RCM outputs examined were obtained from the PRUDENCE project database.
	The study was supported by the Czech Science Foundation under project
	205/06/1535, the 6th EC Framework Programme (CECILIA project, GOCE
	037005), and the Grant Agency of AS CR under project B300420601.
	Thanks are due to I. Nemesova, Institute of Atmospheric Physics,
	Prague, for useful comments on a previous version of the manuscript,
	and an anonymous reviewer for suggestions that helped to improve
	the paper.},
  doi = {10.1007/s00704-008-0014-8},
  keywords = {INTERANNUAL VARIABILITY, CHANGE SIMULATIONS, FUTURE CHANGES, RAINFALL,
	EVENTS, PROJECTIONS, STATISTICS, ENSEMBLE, TRENDS, SERIES},
  tags = {Scenarios, Uncertainty}
}

@ARTICLE{lopezmoreno2008,
  author = {{L\'opez-Moreno}, J. and Goyette, S. and Beniston, M.},
  title = {Climate change prediction over complex areas: spatial variability
	of uncertainties and predictions over the {P}yrenees from a set of
	regional climate models},
  journal = {International Journal of Climatology},
  year = {2008},
  volume = {28},
  pages = {1535},
  number = {11},
  abstract = {We used a set of six regional climate models (RCMs) from PRUDENCE
	project to analyse the uncertainties and direction and magnitude
	of the expected changes on precipitation and temperature (B2 and
	A2 scenarios) for the end of the 21st century in the Pyrenees, south
	of Europe. There have been few studies of climate change effects
	on this mountain range, though there can be noticeable impacts on
	the economy and ecology of the region and the surrounding lowlands.
	The analysis of the accuracy of the RCMs and the impacts of climate
	change over the region are addressed considering the mean values
	for the whole region, their spatial distribution patterns and the
	inter-model variability. Previously, the creation of distributed
	layers of temperature and precipitation from data provided by weather
	observatories was necessary to assess the ability of RCMs to reproduce
	the observed climate. Results show that mean biases between observed
	climate and control runs (1960-1990) are around 20% for precipitation
	and 1 °C for temperature. At annual basis, a mean decrease of 10.7
	and 14.8% in precipitation, and an increase of 2.8 and 4 °C is expected
	in the next century in the area for A2 and B2 scenarios respectively.
	Effects of climate change will be more pronounced in the southern
	slopes of the range (Spanish Pyrenees), and lower in the coastland
	areas. Moreover, results on accuracy and expected changes are subject
	to a large spatial and seasonal variability as well, as the six RCMs
	present noticeable differences on accuracy and sensitivity to climate
	change forcings.},
  doi = {10.1002/joc.1645},
  tags = {Impacts, Uncertainty}
}

@ARTICLE{lopez-moreno2010,
  author = {{L\'opez-Moreno}, J. and {Vicente-Serrano}, S. and {Moran-Tejeda},
	E. and Zabalza, J. and {Lorenzo-Lacruz}, J. and {Garc\'ia-Riuz},
	J.},
  title = {Impact of climate evolution and land use changes on water yield in
	the {E}bro basin},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2010},
  volume = {7},
  pages = {2651--2681},
  number = {2},
  abstract = {In this study the climatic and hydrological trends across 88 sub-basins
	of the Ebro River basin were analyzed for the period 1950–2006. A
	new database of climate information and river flows for the entire
	basin facilitated a spatially distributed assessment of climate-runoff
	relationships. It constitutes the first assessment of water yield
	evolution across the whole Ebro basin, a very representative example
	of large Mediterranean rivers. The results revealed a marked decrease
	in river discharges in most of the sub-basins. Moreover, a number
	of changes in the seasonality of the river regime was found, resulting
	from dam regulation and a decrease in snowpack in the headwaters.
	Significant and positive trends in temperature were observed across
	most of the basin, whereas most of the precipitation series showed
	negative coefficients, although the decrease in magnitude was low.
	The time evolution of the residuals from empirical models that relate
	climate and runoff in each sub-basin provided evidence that climate
	alone does not explain the observed decrease in river discharge.
	Thus, changes in water yield are associated with an increase in evapotranspiration
	rates in natural vegetation, growth of which has expanded as a consequence
	of land abandonment in areas where agricultural activities and livestock
	pressure have decreased. In the lowlands of the basin the decrease
	in water yield has been exacerbated by increased water consumption
	for domestic, industrial and agricultural uses. Climate projections
	for the end of the 21st century suggest a reduced capacity for runoff
	generation because of increasing temperature and less precipitation.
	Thus, the maintenance of water supply under conditions of increasing
	demand presents a challenging issue requiring appropriate coordination
	amongst politicians and managers.},
  doi = {10.5194/hessd-7-2651-2010},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{laga2001,
  author = {Laga, P. and Louwye, S. and Geets, S.},
  title = {Paloegene and {N}eogene lithostratigraphie units ({B}elgium)},
  journal = {Geologica Belgica},
  year = {2001},
  volume = {4},
  pages = {135--152},
  number = {1--2},
  abstract = {The presented lithostratigraphy of the Paleogene and Neogene deals
	only with the formal lithostrati­graphic units of formation rank
	or higher (groups). The names of the lower rank units (members and
	beds) are just mentioned without description or other information.
	This lithostratigraphy corresponds largely with the legend of the
	new series of geological maps of Belgium - Flanders Region on scale
	1:50.000, edited since 1993.},
  owner = {RRojas},
  refid = {LAGA2001},
  timestamp = {2008.11.04},
  url = {http://popups.ulg.ac.be/Geol/document.php?id=1954}
}

@ARTICLE{lamb1998,
  author = {Lamb, R. and Beven, K. and Myrab{\o}, S.},
  title = {Use of spatially distributed water table observations to constraint
	uncertainty in a rainfall--runoff model},
  journal = {Advances in Water Resources},
  year = {1998},
  volume = {22},
  pages = {305--317},
  number = {4},
  abstract = {The Generalised Likelihood Uncertainty Estimation (GLUE) methodology
	is used to investigate how distributed water table observations modify
	simulation and parameter uncertainty for the hydrological model TOPMODEL,
	applied to the Sæternbekken Minifelt catchment in Norway. Errors
	in simulating observed flows, continuously-logged borehole water
	levels and more extensive, spatially distributed water table depths
	are combined using Bayes' equation within a ‘likelihood measure'
	L. It is shown how the distributions of L for the TOPMODEL parameters
	change as the different types of observed data are considered. These
	distributions are also used to construct corresponding simulation
	uncertainty bounds for flows, borehole water levels, and water table
	depths within the spatially-extensive piezometer network. Qualitatively
	wide uncertainty bounds for water table simulations are thought to
	be consistent with the simplified nature of the distributed model.},
  doi = {10.1016/S0309-1708(98)00020-7},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{lambertboer2001,
  author = {Lambert, S. and Boer, G.},
  title = {C{MIP1} evaluation and intercomparison of coupled climate models},
  journal = {Climate Dynamics},
  year = {2001},
  volume = {17},
  pages = {83--106},
  number = {2--3},
  abstract = {The climates simulated by 15 coupled atmosphere/ocean climate models
	participating in the first phase of the Coupled Model Intercomparison
	Project (CMIP1) are intercompared and evaluated. Results for global
	means, zonal averages, and geographical distributions of basic climate
	variables are assembled and compared with observations. The current
	generation of climate models reproduce the major features of the
	observed distribution of the basic climate parameters, but there
	is, nevertheless, a considerable scatter among model results and
	between simulated and observed values. This is particularly true
	for oceanic variables. Flux adjusted models generally produce simulated
	climates which are in better accord with observations than do non-flux
	adjusted models; however, some non-flux adjusted model results are
	closer to observations than some flux adjusted model results. Other
	model differences, such as resolution, do not appear to provide a
	clear distinction among model results in this generation of models.
	Many of the systematic differences (those differences common to most
	models), evident in previous intercomparison studies are exhibited
	also by the CMIP1 group of models although often with reduced magnitudes.
	As is characteristic of intercomparison results, different climate
	variables are simulated with different levels of success by different
	models and no one model is ``best'' for all variables. There is some
	evidence that the ``mean model'' result, obtained by averaging over
	the ensemble of models, provides an overall best comparison to observations
	for climatological mean fields. The model deficiencies identified
	here do not suggest immediate remedies and the overall success of
	the models in simulating the behaviour of the complex non-linear
	climate system apparently depends on the slow improvement in the
	balance of approximations that characterize a coupled climate model.
	Of course, the results of this and similar studies provide only an
	indication, at a particular time, of the current state and the moderate
	but steady evolution and improvement of coupled climate models.},
  doi = {10.1007/PL00013736},
  tags = {Climate Models}
}

@ARTICLE{lavenue1995,
  author = {{LaVenue}, A. and {RamaRao}, B. and {de Marsily}, G. and Marietta,
	M.},
  title = {Pilot points methodology for automated calibration of an ensemble
	of conditionally simulated transmissivity fields 2. {A}pplication},
  journal = {Water Resources Research},
  year = {1995},
  volume = {31},
  pages = {495--516},
  number = {3},
  abstract = {This paper, the second in a two-part series, presents the application
	of a methodology to assess spatial variability of the transmissivities
	within a regional aquifer in the vicinity of the Waste Isolation
	Pilot Plant (WIPP), the Culebra dolomite. An innovative aspect of
	this methodology is the generation of an ensemble of conditional
	simulations of the transmissivity field which preserve the statistical
	moments and spatial correlation structure of the measured transmissivity
	field and honors the measured transmissivity values at their locations.
	Each simulation is then calibrated, using an iterative procedure,
	to match an exhaustive set of steady state and transient pressure
	data. A fully automated inverse algorithm using pilot points as parameters
	of calibration was employed. The application of this new methodology
	to the Culebra dolomite flow system produced 70 conditional simulations
	which were consistent with all the measured transmissivity and head
	data at the site. Based on an analysis of the calibrated transmissivity
	fields, the spatial variability of the transmissivity fields was
	increased as a result of the calibration process. This increase is
	in part due to the addition of a high-transmissivity feature to each
	of the transmissivity fields which is needed to match both steady
	state and transient state head data.},
  doi = {10.1029/94WR02259},
  owner = {rojasro},
  timestamp = {2009.10.20}
}

@BOOK{leamer1978,
  title = {Specification searches: {A}d hoc inference with nonexperimental data},
  publisher = {John Wiley \& Sons},
  year = {1978},
  author = {Leamer, E.},
  pages = {384},
  address = {New York},
  edition = {First},
  booktitle = {Series in probability and mathematical statistics},
  owner = {RRojas},
  refid = {LEAMER1978},
  timestamp = {2008.11.04}
}

@ARTICLE{leander2007,
  author = {Leander, R. and Buishand, T.},
  title = {Resampling of regional climate model output for the simulation of
	extreme river flows},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {332},
  pages = {487--496},
  number = {3--4},
  abstract = {The objective of this paper is to investigate whether resampling of
	the output from a regional climate model (RCM) can provide realistic
	long-duration sequences of precipitation and temperature for the
	simulation of extreme river flows. This is important to assess the
	impact of climate change on river flooding. Daily streamflows of
	the river Meuse in western Europe are considered. Resampling is performed
	with a nearest-neighbour technique that was already successfully
	applied to the observed daily precipitation and temperature in the
	river basin. Streamflows are simulated with the semi-distributed
	HBV rainfall–runoff model. Two simulations of the KNMI regional climate
	model RACMO are considered. One of these simulations is driven by
	the global atmospheric model HadAM3H of the UK Meteorological Office
	for the period 1961–1990 and the other by ERA40 re-analysis data.
	Much attention is given to the bias correction of RACMO precipitation.
	It was found that a relatively simple nonlinear correction adjusting
	both the biases in the mean and variability led to a better reproduction
	of observed extreme daily and multi-day precipitation amounts than
	the commonly used linear scaling correction. This also resulted in
	more realistic discharge extremes, suggesting that a correct representation
	of the variability of precipitation is important for the simulation
	of extreme flood quantiles. For the Meuse basin it is further shown
	that it is advantageous to correct for the variability of the 10-day
	precipitation amounts rather than that of the daily amounts. Despite
	the remaining biases in the RCM data, the simulated extreme flood
	quantiles correspond quite well with those obtained using observed
	precipitation and temperature.},
  doi = {10.1016/j.jhydrol.2006.08.006},
  owner = {rojasro},
  timestamp = {2010.06.23}
}

@ARTICLE{leander2008,
  author = {Leander, R. and Buishand, T. and {van den Hurk}, B. and {de Wit},
	M.},
  title = {Estimated changes in flood quantiles of the river {M}euse from resampling
	of regional climate model output},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {351},
  pages = {331--343},
  number = {3--4},
  abstract = {Precipitation and temperature data from three regional climate model
	(RCM) experiments were used to assess the effect of climatic change
	on the flood quantiles of the French–Belgian river Meuse. In two
	of these experiments the RCM was driven by the global atmospheric
	model HadAM3H of the Hadley Centre (HC), and in the other experiment
	the RCM was driven by the global coupled atmosphere–ocean model ECHAM4/OPYC3
	of the Max-Planck Institute for Meteorology (MPI). RCM simulations
	for the control climate (1961–1990) and the SRES-scenario A2 (2071–2100)
	were available. The HBV rainfall–runoff model was used to simulate
	river discharges. Long synthetic sequences of precipitation and temperature
	were resampled from the RCM output using a nearest-neighbour technique
	to obtain the flood quantiles for long return periods. The maxima
	of 10-day precipitation and discharge for the winter half-year (flooding
	season) were analysed. It was found that the changes in the extreme
	quantiles of 10-day precipitation and discharge were highly sensitive
	to the driving GCM. In the runs driven by HC, there was little change
	in the most extreme quantiles, whereas the MPI-driven run projected
	a remarkable increase. It is shown that this difference between the
	HC- and MPI-driven runs is strongly related to the change in the
	coefficient of variation of the 10-day precipitation amounts, which
	decreases in the former and hardly changes in the latter. The relevance
	of bias correction of RCM output with regard to the estimated changes
	of flood quantiles is demonstrated.},
  doi = {10.1016/j.jhydrol.2007.12.020},
  owner = {rojasro},
  timestamp = {2010.06.23}
}

@ARTICLE{leavesley1994,
  author = {Leavesley, G.},
  title = {Modeling the effects of climate change on water resources--{A} review},
  journal = {Climatic Change},
  year = {1994},
  volume = {28},
  pages = {159--177},
  number = {1--2},
  abstract = {Hydrologic models provide a framework in which to conceptualize and
	investigate the relationships between climate and water resources.
	A review of current studies that assess the impacts of climate change
	using hydrologic models indicates a number of problem areas common
	to the variety of models applied. These problem areas include parameter
	estimation, scale, model validation, climate scenario generation,
	and data. Research needs to address these problems include development
	of (1) a more physically based understanding of hydrologic processes
	and their interactions; (2) parameter measurement and estimation
	techniques for application over a range of spatial and temporal scales;
	(3) quantitative measures of uncertainty in model parameters and
	model results; (4) improved methodologies of climate scenario generation;
	(5) detailed data sets in a variety of climatic and physiographic
	regions; and (6) modular modeling tools to provide a framework to
	facilitate interdisciplinary research. Solutions to these problems
	would significantly improve the capability of models to assess the
	effects of climate change.},
  doi = {10.1007/BF01094105},
  keywords = {RAINFALL-RUNOFF MODELS, SYSTEME-HYDROLOGIQUE-EUROPEEN, ATMOSPHERIC
	CARBON-DIOXIDE, EPIC MODEL, BALANCE, SENSITIVITY, SCALE, CO2, STREAMFLOW,
	BASIN},
  tags = {Impacts}
}

@ARTICLE{lee+al2006,
  author = {Lee, G. and Tachikawa, Y. and Takara, K.},
  title = {Analysis of Hydrologic Model Parameter Characteristics Using Automatic
	Global Optimization Method},
  journal = {Annuals of Disaster Prevention Research},
  year = {2006},
  volume = {49B},
  pages = {67--80},
  tags = {Calibration}
}

@ARTICLE{lee+al2005,
  author = {Lee, H. and Mcintyre, N. and Wheater, H. and Young, A.},
  title = {Selection of conceptual models for regionalisation of the rainfall-runoff
	relationship},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {312},
  pages = {125--147},
  abstract = {This study attempts to identify relationships between suitable conceptual
	rainfall-runoff model structures and catchment types, and demonstrates
	an objective procedure for selection of model structures for use
	in model regionalisation studies. 12 conceptual model structures
	have been tested on 28 UK catchments. The model structures are combinations
	of three soil moisture accounting modules (the catchment wetness
	index model structure, a modified Penman model structure, and a probability
	distributed soil moisture model), and four routing models (two linear
	reservoirs in parallel, an adaptation that represents macropore flow,
	three linear reservoirs in parallel, and a model that includes leakage
	from the groundwater store). The 28 catchments were selected to cover
	a range of hydrological types based on known catchment characteristics
	(designed combinations of catchment size, baseflow index, and annual
	rainfall are used). The important points in objectively selecting
	model structures for regionalisation are proposed as: performance
	in calibration and validation, trade-off between high and low flow
	performance, and model complexity (here represented as the number
	of parameters). Although, the results in this study provide no evidence
	of relationships between catchment type and preferred model structure,
	the study indicates that out of the 12 trial model structures, the
	following four may be the most suitable for regionalisation across
	UK catchments: the modified Penman model with two parallel linear
	routing reservoirs, and the probability distributed soil moisture
	model with two parallel linear routing reservoirs, three parallel
	linear routing reservoirs, or the macropore adaptation. (c) 2005
	Elsevier B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2005.02.016},
  keywords = {rainfall-runoff model, regionalisation, model selection, UNGAUGED
	CATCHMENTS, RESPONSE CHARACTERISTICS, HYDROLOGICAL MODELS, PARAMETERS,
	UNCERTAINTY, CALIBRATION, STREAMFLOW, IMPACT, FLOW, SIMULATION},
  tags = {conceptual model}
}

@ARTICLE{legates1999,
  author = {Legates, D. and {McCabe Jr.}, G.},
  title = {Evaluating the use of ``goodness-of-fit'' measures in hydrologic
	and hydroclimatic model validation},
  journal = {Water Resources Research},
  year = {1999},
  volume = {35},
  pages = {233--241},
  number = {1},
  abstract = {Correlation and correlation-based measures (e.g., the coefficient
	of determination) have been widely used to evaluate the “goodness-of-fit”
	of hydrologic and hydroclimatic models. These measures are oversensitive
	to extreme values (outliers) and are insensitive to additive and
	proportional differences between model predictions and observations.
	Because of these limitations, correlation-based measures can indicate
	that a model is a good predictor, even when it is not. In this paper,
	useful alternative goodness-of-fit or relative error measures (including
	the coefficient of efficiency and the index of agreement) that overcome
	many of the limitations of correlation-based measures are discussed.
	Modifications to these statistics to aid in interpretation are presented.
	It is concluded that correlation and correlation-based measures should
	not be used to assess the goodness-of-fit of a hydrologic or hydroclimatic
	model and that additional evaluation measures (such as summary statistics
	and absolute error measures) should supplement model evaluation tools.},
  doi = {10.1029/1998WR900018},
  keywords = {Calibration, gooness-of-fit},
  tags = {calibration, Goodness-of-Fit}
}

@ARTICLE{lehner+al2006,
  author = {Lehner, B. and D\"oll, P. and Alcamo, J. and Henrichs, T. and Kaspar,
	F.},
  title = {Estimating the impact of global change on flood and drought risks
	in {E}urope: {A} continental integrated analysis},
  journal = {Climatic Change},
  year = {2006},
  volume = {75},
  pages = {273--299},
  number = {3},
  abstract = {Most studies on the impact of climate change on regional water resources
	focus on long-term average flows or mean water availability, and
	they rarely take the effects of altered human water use into account.
	When analyzing extreme events such as floods and droughts, the assessments
	are typically confined to smaller areas and case studies. At the
	same time it is acknowledged that climate change may severely alter
	the risk of hydrological extremes over large regional scales, and
	that human water use will put additional pressure on future water
	resources. In an attempt to bridge these various aspects, this paper
	presents a first-time continental, integrated analysis of possible
	impacts of global change (here defined as climate and water use change)
	on future flood and drought frequencies for the selected study area
	of Europe. The global integrated water model WaterGAP is evaluated
	regarding its capability to simulate high and low-flow regimes and
	is then applied to calculate relative changes in flood and drought
	frequencies. The results indicate large ‘critical regions’ for which
	significant changes in flood or drought risks are expected under
	the proposed global change scenarios. The regions most prone to a
	rise in flood frequencies are northern to northeastern Europe, while
	southern and southeastern Europe show significant increases in drought
	frequencies. In the critical regions, events with an intensity of
	today's 100-year floods and droughts may recur every 10–50 years
	by the 2070s. Though interim and preliminary, and despite the inherent
	uncertainties in the presented approach, the results underpin the
	importance of developing mitigation and adaptation strategies for
	global change impacts on a continental scale.},
  doi = {10.1007/s10584-006-6338-4},
  owner = {rojasro},
  timestamp = {2010.08.09}
}

@TECHREPORT{lehner2001,
  author = {Lehner, B. and Henrichs, T. and D\"oll, P. and Alcamo, J.},
  title = {{EuroWasser--Model-based assessment of European water resources and
	hydrology in the face of global change. Kassel World Water Series
	5}},
  institution = {Center for Environmental Systems Research},
  year = {2001},
  address = {{Kurt-WOlters-Strasse 3, 34109 Kassel, Germany}},
  owner = {rojasro},
  timestamp = {2009.08.10},
  url = {http://www.usf.uni-kassel.de/cesr/index.php?option=com_content&task=view&id=134&Itemid=72}
}

@ARTICLE{lenderink2010,
  author = {Lenderink, G.},
  title = {Exploring metrics of extreme daily precipitation in a large ensemble
	of regional climate model simulations},
  journal = {Climate Research},
  year = {2010},
  volume = {44},
  pages = {151--166},
  number = {2--3},
  abstract = {The ability of a large ensemble of 15 state-of-the-art regional climate
	models (RCMs) to simulate precipitation extremes was investigated.
	The 99th, 99.9th and 99.99th percentiles of daily precipitation in
	the models were compared with those in the recently released E-OBS
	observational database for winter, spring, summer and autumn. The
	majority of the models overestimated the values of the precipitation
	extremes compared with E-OBS, on average by approximately 38%, but
	some models exceeded 50%. To measure model performance, a simple
	metric is proposed that averages a nonlinear function of the seasonal
	biases over the European area. The sensitivity of the metric to different
	assumptions in the construction and the quality of the observational
	data was explored. Generally, low sensitivities of the metric to
	spatial and seasonal averaging were found. However, large sensitivities
	to potential biases in the observational database were found. An
	alternative metric that measures the spatial pattern of the extremes
	(which is not sensitive to a potential constant offset in the observational
	data) was further explored. With this metric, the ranking between
	the models changed substantially. However, the 2 models with the
	worst score in the standard metric also displayed the worst scores
	with this alternative metric. Finally, the regional climate models
	displayed the largest biases compared with E-OBS in areas where the
	underlying station density used in E-OBS is low, thus suggesting
	that data quality is indeed an important issue. In summary, the results
	show that: (1) there is no metric that guarantees an objective and
	precise ranking or weighting of the models, (2) by exploring different
	metrics it nevertheless appears possible to indentify models that
	perform consistently worse than other models, and (3) the observational
	data quality should be considered when designing and interpreting
	metrics.},
  doi = {10.3354/cr00946},
  owner = {rojasro},
  timestamp = {2011.01.12}
}

@ARTICLE{lenderink+al2007,
  author = {Lenderink, G. and Buishand, A. and {Van Deursen}, W.},
  title = {Estimates of future discharges of the river {R}hine using two scenario
	methodologies: {D}irect versus delta approach},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1145--1159},
  number = {3},
  abstract = {Simulations with a hydrological model for the river Rhine for the
	present (1960-1989) and a projected future (2070-2099) climate are
	discussed. The hydrological model (RhineFlow) is driven by meteorological
	data from a 90-years (ensemble of three 30-years) simulation with
	the HadRM3H regional climate model for both present-day and future
	climate (A2 emission scenario). Simulation of present-day discharges
	is realistic provided that (1) the HadRM3H temperature and precipitation
	are corrected for biases, and (2) the potential evapotranspiration
	is derived from temperature only. Different methods are used to simulate
	discharges for the future climate: one is based on the direct model
	output of the future climate run (direct approach), while the other
	is based on perturbation of the present-day HadRM3H time series (delta
	approach). Both methods predict a similar response in the mean annual
	discharge, an increase of 30% in winter and a decrease of 40% in
	summer. However, predictions of extreme flows differ significantly,
	with increases of 10% in flows with a return period of 100 years
	in the direct approach and approximately 30% in the delta approach.
	A bootstrap method is used to estimate the uncertainties related
	to the sample size (number of years simulated) in predicting changes
	in extreme flows.},
  doi = {10.5194/hess-11-1145-2007},
  keywords = {climate change impact, regional climate model, extreme river flows,
	REGIONAL CLIMATE MODEL, EUROPE, SIMULATIONS, TRENDS, WATER},
  tags = {Scenarios, Impacts}
}

@ARTICLE{lenhart+al2002,
  author = {Lenhart, T. and Eckhardt, K. and Fohrer, N. and Freede, H.},
  title = {Comparison of two different approaches of sensitivity analysis},
  journal = {Physics and Chemistry of the Earth},
  year = {2002},
  volume = {27},
  pages = {645--654},
  abstract = {Due to spatial variability, budget constraints or access difficulties
	model input parameters always are uncertain to some extent. Therefore
	the knowledge of sensitive input parameters is beneficial for model
	development and application. It can lead to a better understanding
	and to better estimated values and thus reduced uncertainty.},
  doi = {S1474-7065(02)00049-9},
  keywords = {sensitivity analysis, uncertainty, model calibration, water quality,
	distributed models, SYSTEMS},
  tags = {Sensitivity Analysis, SWAT}
}

@ARTICLE{li+al2011,
  author = {Li, F. and Collins, W. and Wehener, M. and Williamson, D. and Olson,
	J. and Algieri, C.},
  title = {{Impact of horizontal resolution on simulation of precipitation extremes
	in an aqua-planet version of Community Atmospheric Model (CAM3)}},
  journal = {Tellus {A}},
  year = {2011},
  volume = {63},
  pages = {884--892},
  number = {5},
  abstract = {One key question regarding current climate models is whether the projection
	of climate extremes converges to a realistic representation as the
	spatial and temporal resolutions of the model are increased. Ideally
	the model extreme statistics should approach a fixed distribution
	once the resolutions are commensurate with the characteristic length
	and time scales of the processes governing the formation of the extreme
	phenomena of interest. In this study, a series of AGCM runs with
	idealized ‘aquaplanet-steady-state’ boundary conditions have been
	performed with the Community Atmosphere Model CAM3 to investigate
	the effect of horizontal resolution on climate extreme simulations.
	The use of the aquaplanet framework highlights the roles of model
	physics and dynamics and removes any apparent convergence in extreme
	statistics due to better resolution of surface boundary conditions
	and other external inputs. Assessed at a same large spatial scale,
	the results show that the horizontal resolution and time step have
	strong effects on the simulations of precipitation extremes. The
	horizontal resolution has a much stronger impact on precipitation
	extremes than on mean precipitation. Updrafts are strongly correlated
	with extreme precipitation at tropics at all the resolutions, while
	positive low-tropospheric temperature anomalies are associated with
	extreme precipitation at mid-latitudes.},
  doi = {10.1111/j.1600-0870.2011.00544.x},
  owner = {rojasro},
  timestamp = {2011.12.01}
}

@ARTICLE{li+al2010,
  author = {Lu Li and Jun Xia and Chong-Yu Xu and V.P. Singh},
  title = {Evaluation of the subjective factors of the GLUE method and comparison
	with the formal Bayesian method in uncertainty assessment of hydrological
	models},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {390},
  pages = {210--221},
  number = {3–-4},
  abstract = {Summary Quantification of uncertainty of hydrological models has attracted
	much attention in the recent hydrological literature. Different results
	and conclusions have been reported which result from the use of different
	methods with different assumptions. In particular, the disagreement
	between the Generalized Likelihood Uncertainty Estimation (GLUE)
	and the Bayesian methods for assessing the uncertainty in conceptual
	watershed modelling has been widely discussed. What has been mostly
	criticized is the subjective choice as regards the influence of threshold
	value, number of sample simulations, and likelihood function in the
	GLUE method. In this study the impact of threshold values and number
	of sample simulations on the uncertainty assessment of GLUE is systematically
	evaluated, and a comprehensive evaluation about the posterior distribution,
	parameter and total uncertainty estimated by GLUE and a formal Bayesian
	approach using the Metropolis Hasting (MH) algorithm are performed
	for two well-tested conceptual hydrological models (WASMOD and DTVGM)
	in an arid basin from North China. The results show that in the GLUE
	method, the posterior distribution of parameters and the 95% confidence
	interval of the simulated discharge are sensitive to the choice of
	the threshold value as measured by the acceptable samples rate (ASR).
	However, when the threshold value in the GLUE method is high enough
	(i.e., when the ASR value is smaller than 0.1%), the posterior distribution
	of parameters, the 95% confidence interval of simulated discharge
	and the percent of observations bracketed by the 95% confidence interval
	(P-95CI) for the GLUE method approach those values estimated by the
	Bayesian method for both hydrological models. Second, in the GLUE
	method, the insufficiency of number of sample simulations will influence
	the maximum Nash–Sutcliffe (MNS) efficiency value when ASR is fixed.
	However, as soon as the number of sample simulations increases to
	2&#xa0;×&#xa0;104 for WASMOD and to 8&#xa0;×&#xa0;104 for the DTVGM
	model the influence of number of sample simulations on the model
	simulation results becomes of minor importance. Third, the uncertainty
	in simulated discharges resulting from parameter uncertainty is much
	smaller than that resulting from the model structure uncertainty
	for both hydrological models. Fourth, the goodness of model fit as
	measured by the maximum Nash–Sutcliffe efficiency value is nearly
	the same for the GLUE and the Bayesian methods for both hydrological
	models. Thus this study provides useful information on the uncertainty
	assessment of hydrological models.},
  doi = {10.1016/j.jhydrol.2010.06.044},
  issn = {0022-1694},
  keywords = {Uncertainty assessment}
}

@ARTICLE{li2009,
  author = {Li, X. and Tsai, F.},
  title = {Bayesian model averaging for groundwater head prediction and uncertainty
	analysis using multimodel and multimethod},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W09403},
  abstract = {This study introduces a Bayesian model averaging (BMA) method that
	incorporates multiple groundwater models and multiple hydraulic conductivity
	estimation methods to predict groundwater heads and evaluate prediction
	uncertainty. BMA is able to distinguish prediction uncertainty arising
	from individual models, between models, and between methods. Moreover,
	BMA is able to identify unfavorable models even though they may present
	small prediction uncertainty. Uncertainty propagation, from model
	parameter uncertainty to model prediction uncertainty, can also be
	studied through BMA. This study adopts a variance window to obtain
	reasonable BMA weights for the best models, which are usually exaggerated
	by Occam's window. Results from a synthetic case study show that
	BMA with the variance window can provide better head prediction than
	individual models, or at least can obtain better predictions close
	to the best model. The BMA was applied to predicting groundwater
	heads in the “1500-foot” sand of the Baton Rouge area in Louisiana.
	Head prediction uncertainty was assessed by the BMA prediction variance.
	BMA confirms that large head prediction uncertainty occurs at areas
	lacking head observations and hydraulic conductivity measurements.
	Further study in these areas is necessary to reduce head prediction
	uncertainty.},
  doi = {10.1029/2008WR007488},
  owner = {rojasro},
  timestamp = {2009.09.18}
}

@ARTICLE{li+al2009,
  author = {Li, Z. and Xu, Z. and Shao, Q. and Yang, J.},
  title = {Parameter estimation and uncertainty analysis of {SWAT} model in
	upper reaches of the {H}eihe river basin},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {23},
  pages = {2744--2753},
  number = {19},
  abstract = {Heihe river basin, the second largest inland river basin in China,
	has attracted more attention in China due to the ever increasing
	water resources and eco-environmental problems. In this article,
	SWAT (Soil and Water Assessment Tool; http://www.brc.tamus.edu/swat/)
	model was applied to upper reaches of the basin for better understanding
	of the hydrological process over the watershed. Parameter uncertainty
	and its contribution on model simulation are the main foci. In model
	calibration, the aggregate parameters instead of the original parameters
	in SWAT model were used to reduce the computing effort. The Bayesian
	approach was employed for parameter estimation and uncertainty analysis
	because its posterior distribution provides not only parameter estimation
	but also uncertainty analysis without normality assumption. The results
	indicated that: (1) SWAT model performs satisfactorily in this watershed
	as a whole, although some low and high flows were under- or overestimated,
	particularly in dry (e.g. 1991) and wet (e.g. 1996) years; (2) all
	calibrated parameters were not normally distributed (essentially
	positively or negatively skewed) and the parameter uncertainties
	were relatively small; and (3) the contributions of parameter uncertainty
	on model simulation uncertainty were relatively small},
  doi = {10.1002/hyp.7371},
  keywords = {SWAT, parameter uncertainty, simulation uncertainty, aggregate parameter,
	Bayesian method},
  tags = {SWAT, Uncertainty, Thesis}
}

@ARTICLE{liang2001,
  author = {Liang, F. and Truong, Y. and Wong, W.},
  title = {Automatic {B}ayesian model averaging for linear regression and application
	in {B}ayesian curve fitting},
  journal = {Statistica Sinica},
  year = {2001},
  volume = {11},
  pages = {1005--1029},
  number = {4},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{liang+al2010,
  author = {Liang, S. and Ge, S. and Wan, L. and Zhang, J.},
  title = {Can climate change cause the {Y}ellow {R}iver to dry up?},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W02505},
  number = {2},
  abstract = {The source region of the Yellow River, China, has experienced eco-environmental
	deterioration in recent decades. The Yellow River discharge has reduced
	to zero numerous times since 1960, which has left a devastating impact
	on water resources and ecosystems downstream. Seeking to understand
	the causes of drying up, we analyze the records of discharge, temperature,
	and precipitation and explore the linkages among them. Using wavelet
	analysis, we show that the Yellow River discharge represents a comprehensive
	response to climatic variations in temperature and precipitation.
	We suggest that the discharge is best explained by variations in
	temperature over a 35--50 year trend and precipitation over periods
	of less than 10 years. Zero discharge events can be quantitatively
	correlated to periods of warming and reduced precipitation. Findings
	of this study may have a broad implication in assessing how water
	resources respond to climate changes.},
  doi = {10.1029/2009WR007971},
  tags = {Climate Change}
}

@ARTICLE{limbrick+al2000,
  author = {Limbrick, K.},
  title = {Assessing the potential impacts of various climate change scenarios
	on the hydrological regime of the {R}iver {K}ennet at {T}heale, {B}erkshire,
	south-central {E}ngland, {UK}: an application and evaluation of the
	new semi--distributed model, {INCA}},
  journal = {The Science of The Total Environment},
  year = {2000},
  volume = {251-252},
  pages = {539--555},
  number = {1--3},
  abstract = {A new semi-distributed integrated nitrogen in catchments (INCA) model
	was used to attempt to assess the potential impacts of several recent
	Hadley Centre climate change scenarios on the hydrological flow regime
	of the entire River Kennet catchment to Theale, south-central England,
	UK. The climatically and hydrologically anomalous period 1985--1995
	was used for baseline data in an attempt to: (1) represent any possible
	future climatic or hydrological variability not available from scenario
	use alone; and (2) attain maximum possible model calibration validity
	under future climates by simulating extremes of within-year hydrological
	variability. Substantial reductions in total annual runoff occurred,
	with an average reduction of 18.97%. Summer and late autumn soil
	moisture deficits (SMDs) increased in intensity, and were also found
	to persist for longer periods into autumn and (occasionally) winter.
	A generally enhanced hydrological regime of the River Kennet was
	simulated, with increased seasonality overall. A greater percentage
	of flow was observed to occur in spring and (occasionally) winter.
	Month-to-month variability of flow was discovered to be greater than
	annual changes. An average reduction in minimum annual flows of 46.03%
	occurred. Implications for catchment ecology and water resource requirements
	are briefly discussed. An evaluation of the new INCA modelâ€™s performance
	as a tool for climate change impacts assessment is made},
  doi = {10.1016/S0048-9697(00)00394-6},
  keywords = { River Kennet, Catchment hydrology, Hydrological regime, INCA, Hydrologically
	effective rainfall (HER), Soil moisture deficit (SMD), Runoff, Meteorological
	Office rainfall and evaporation calculation system (MORECS), Groundwater
	recharge, Chalk aquifer, Water resources},
  tags = {Impacts}
}

@ARTICLE{linradcliffe2006,
  author = {Lin, Z. and Radcliffe, D.},
  title = {Automatic calibration and predictive uncertainty analysis of a semidistributed
	watershed model},
  journal = {Vadose Zone Journal},
  year = {2006},
  volume = {5},
  pages = {248--260},
  abstract = {Semidistributed models are commonly calibrated manually, but software
	for automatic calibration is now available. We present a two-stage
	routine for automatic calibration of the semidistributed watershed
	model Soil and Water Assessment Tool ( SWAT) that finds the best
	values for the model parameters, preserves spatial variability in
	essential parameters, and leads to a measure of the model prediction
	uncertainty. In the first stage, a modified global Shuffled Complex
	Evolution (SCE-UA) method was employed to find the "best'' values
	for the lumped model parameters. In the second stage, the spatial
	variability of the original model parameters was restored and a local
	search method ( a variant of Levenberg - Marquart method) was used
	to find a more distributed set of parameters using the results of
	the previous stage as starting values. A method called "regularization''
	was adopted to prevent the parameters from taking extreme values.
	In addition, we applied a nonlinear calibration-constrained method
	to develop confidence intervals for annual and 7-d average flow predictions.
	We calibrated stream flow in the Etowah River measured at Canton,
	GA ( a watershed area of 1580 km(2)) for the years 1983 to 1992 and
	used the years 1993 to 2001 for validation. The Parameter Estimator
	( PEST) software was used to conduct the two-stage automatic calibration
	and prediction uncertainty analysis. Calibration for daily and monthly
	flow produced a very good fit to the measured data. Nash-Sutcliffe
	coefficients for daily and monthly flow over the calibration period
	were 0.60 and 0.86, respectively. They were 0.61 and 0.87, respectively,
	for the validation period. The nonlinear prediction uncertainty analysis
	worked well for long-term ( annual) flow in that our prediction confidence
	intervals included or were very near to the observed flow for most
	years. It did not work well for short-term (7-d average) flows in
	that the prediction confidence intervals did not include the observed
	flow, especially for low and high flow conditions.},
  doi = {10.2136/vzj2005.0025},
  keywords = {RAINFALL-RUNOFF MODELS, GLOBAL OPTIMIZATION, HYDROLOGIC-MODELS, PARAMETER-ESTIMATION,
	ALGORITHM, MULTIPLE, SCHEME},
  tags = {SWAT, Calibration}
}

@ARTICLE{telinde+al2011,
  author = {te Linde, A. H. and Bubeck, P. and Dekkers, J. E. C. and de Moel,
	H. and Aerts, J. C. J. H.},
  title = {Future flood risk estimates along the river Rhine},
  journal = {Natural Hazards and Earth System Science},
  year = {2011},
  volume = {11},
  pages = {459--473},
  number = {2},
  doi = {10.5194/nhess-11-459-2011}
}

@BOOK{lindley2006,
  title = {Understanding uncertainty},
  publisher = {John Wiley \& Sons},
  year = {2006},
  author = {Lindley, D.},
  pages = {272},
  address = {New Jersey},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{liu+al2005,
  author = {Liu, B. and Wang, L. and Jin, {Y-H} and Tang, F. and Huang, {D-X}},
  title = {Improved particle swarm optimization combined with chaos},
  journal = {Chaos, Solitons \& Fractals},
  year = {2005},
  volume = {25},
  pages = {1261},
  number = {5},
  doi = {10.1016/j.chaos.2004.11.095},
  tags = {Calibration, PSO}
}

@ARTICLE{liu2006,
  author = {Liu, G. and Zhang, D. and Lu, Z.},
  title = {Stochastic uncertainty analysis for unconfined flow layers},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W09412},
  abstract = {A new stochastic approach proposed by Zhang and Lu (2004), called
	the Karhunen-Loeve decomposition-based moment equation (KLME), has
	been extended to solving nonlinear, unconfined flow problems in randomly
	heterogeneous aquifers. This approach is on the basis of an innovative
	combination of Karhunen-Loeve decomposition, polynomial expansion,
	and perturbation methods. The random log-transformed hydraulic conductivity
	field (lnK S ) is first expanded into a series in terms of orthogonal
	Gaussian standard random variables with their coefficients obtained
	as the eigenvalues and eigenfunctions of the covariance function
	of lnK S . Next, head h is decomposed as a perturbation expansion
	series ?h (m), where h (m) represents the mth-order head term with
	respect to the standard deviation of lnK S . Then h (m) is further
	expanded into a polynomial series of m products of orthogonal Gaussian
	standard random variables whose coefficients h i 1,i 2,…,i m (m)
	are deterministic and solved sequentially from low to high expansion
	orders using MODFLOW-2000. Finally, the statistics of head and flux
	are computed using simple algebraic operations on h i 1,i 2,…,i m
	(m). A series of numerical test results in 2-D and 3-D unconfined
	flow systems indicated that the KLME approach is effective in estimating
	the mean and (co)variance of both heads and fluxes and requires much
	less computational effort as compared to the traditional Monte Carlo
	simulation technique.},
  doi = {10.1029/2005WR004766},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{liuhan2010,
  author = {Liu, J. and Han, D.},
  title = {Indices for calibration data selection of the rainfall-runoff model},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W04512},
  number = {4},
  abstract = {The identification of rainfall-runoff models requires selection of
	appropriate data for model calibration. Traditionally, hydrologists
	use rules of thumb to select a certain period of hydrological data
	to calibrate the models (i.e., 6 year data). There are no numerical
	indices to help hydrologists to quantitatively select the calibration
	data. There are two questions: how long should the calibration data
	be (e.g., 6 months), and from which period should the data be selected
	(e.g., which 6 month data should be selected)? In this study, some
	indices for the selection of calibration data with adequate lengths
	and appropriate durations are proposed by examining the spectral
	properties of data sequences before the calibration work. With the
	validation data determined beforehand, we assume that the more similarity
	the calibration data set bears to the validation set, the better
	should the performance of the rainfall-runoff model be after calibration.
	Three approaches are applied to reveal the similarity between the
	validation and calibration data sets: flow-duration curve, Fourier
	transform, and wavelet analysis. Data sets used for calibration are
	generated by designing three scenario groups with fixed lengths of
	6, 12, and 24 months, respectively, from 8 year continuous observations
	in the Brue catchment of the United Kingdom. Scenarios in each group
	have different starting times and thus various durations with specific
	hydrological characteristics. With a predetermined 18 month validation
	set and the rainfall-runoff model chosen to be the probability distributed
	model, useful indices are produced for certain scenario groups by
	all three approaches. The information cost function, an entropy-like
	function based on the decomposition results of the discrete wavelet
	transform, is found to be the most effective index for the calibration
	data selection. The study demonstrates that the information content
	of the calibration data is more important than the data length; thus
	6 month data may provide more useful information than longer data
	series. This is important for hydrological modelers since shorter
	and more useful data help hydrologists to build models more efficiently
	and effectively. The idea presented in this paper has also shown
	potential in enhancing the efficiency of calibration data utilization,
	especially for data-limited catchments},
  doi = {10.1029/2009WR008668},
  keywords = {FDC, DWT, Fourier, Validation, Calibration length, Calibration period},
  tags = {Calibration, FDC, Goodness-of-Fit}
}

@ARTICLE{liu+al2009a,
  author = {Liu, X. and Wang, Q. and Liu, H. and Li, L.},
  title = {Particle Swarm Optimization with Dynamic Inertia Weight and Mutation},
  journal = {Genetic and Evolutionary Computing, International Conference on},
  year = {2009},
  volume = {0},
  pages = {620--623},
  number = {0},
  abstract = {The Particle Swarm Optimization (PSO) plunges into the local minimum
	easily. In order to overcome this shortcoming, we propose an improved
	PSO algorithm with the features of linearly decreasing of inertia
	weight and the re-initialization of the particle when it gets stagnated.
	The improved PSO is a local PSO and its topology is wheels. From
	the experimental results of three non-linear testing functions and
	a problem with non-convex solution space, it is obvious that the
	improved PSO algorithm greatly enhances the rate of global convergence.},
  address = {Los Alamitos, CA, USA},
  doi = {http://doi.ieeecomputersociety.org/10.1109/WGEC.2009.99},
  isbn = {978-0-7695-3899-0},
  keywords = {particle swarm optimization, inertia weight, convergence rate, constrained
	layout optimization},
  publisher = {IEEE Computer Society},
  tags = {Calibration, PSO}
}

@ARTICLE{liu+al2009b,
  author = {Liu, Y. and Freer, J. and Beven, K. and Matgen, P.},
  title = {Towards a limits of acceptability approach to the calibration of
	hydrological models: {E}xtending observation error},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {367},
  pages = {93--103},
  number = {1-2},
  abstract = {Within the GLUE methodology, there are a number of advantages of taking
	a limits of acceptability approach to model evaluation for non-ideal
	applications where the strong assumptions of statistical identification
	might be difficult to justify. However, there is a question of how
	the limits of acceptability might be specified in a way that reflects
	the different sources of uncertainty in the modeling process. Here,
	a novel method for identifying behavioural models in an extended
	GLUE methodology is developed and applied to an application of Dynamic
	TOPMODEL to the Attert catchment in Luxemburg with semi-distributed
	inputs to nested sub-catchments. The results raise some important
	issues about testing model structures as hypotheses of catchment
	responses.},
  doi = {10.1016/j.jhydrol.2009.01.016},
  keywords = {GLUE, Equifinality, Dynamic TOPMODEL, Limits of acceptability, Hypothesis
	testing},
  tags = {Uncertainty}
}

@ARTICLE{liu2007,
  author = {Liu, Y. and Gupta, H.},
  title = {Uncertainty in hydrologic modeling: {T}oward an integrated data assimilation
	framework},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W07401},
  abstract = {Despite significant recent developments in computational power and
	distributed hydrologic modeling, the issue of how to adequately address
	the uncertainty associated with hydrological predictions remains
	a critical and challenging one. This issue needs to be properly addressed
	for hydrological modeling to realize its maximum practical potential
	in environmental decision-making processes. Arguably, the key to
	properly addressing hydrologic uncertainty is to understand, quantify,
	and reduce uncertainty involved in hydrologic modeling in a cohesive,
	systematic manner. Although general principles and techniques on
	addressing hydrologic uncertainty are emerging in the literature,
	there exist no well-accepted guidelines about how to actually implement
	these principles and techniques in various hydrologic settings in
	an integrated manner. This paper reviews, in relevant detail, the
	common data assimilation methods that have been used in hydrologic
	modeling to address problems of state estimation, parameter estimation,
	and system identification. In particular, the paper discusses concepts,
	methods, and issues involved in hydrologic data assimilation from
	a systems perspective. An integrated hierarchical framework is proposed
	for pursuing hydrologic data assimilation in several progressive
	steps to maximally reduce uncertainty in hydrologic predictions.},
  doi = {10.1029/2006WR005756},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{liu2005,
  author = {Liu, Z. and Martina, M. and Todini, E.},
  title = {Flood forecasting using fully distributed model: {A}pplication of
	the {TOPKAPI} model to the upper {X}ixian catchment},
  journal = {Hydrology and Earth System Sciences},
  year = {2005},
  volume = {9},
  pages = {347--364},
  number = {4},
  abstract = {TOPKAPI is a physically-based, fully distributed hydrological model
	with a simple and parsimonious parameterisation. The original TOPKAPI
	is structured around five modules that represent evapotranspiration,
	snowmelt, soil water, surface water and channel water, respectively.
	Percolation to deep soil layers was ignored in the old version of
	the TOPKAPI model since it was not important in the basins to which
	the model was originally applied. Based on published literature,
	this study developed a new version of the TOPKAPI model, in which
	the new modules of interception, infiltration, percolation, groundwater
	flow and lake/reservoir routing are included. This paper presents
	an application study that makes a first attempt to derive information
	from public domains through the internet on the topography, soil
	and land use types for a case study Chinese catchment - the Upper
	Xixian catchment in Huaihe River with an area of about 10000 km2,
	and apply a new version of TOPKAPI to the catchment for flood simulation.
	A model parameter value adjustment was performed using six months
	of the 1998 dataset. Calibration did not use a curve fitting process,
	but was chiefly based upon moderate variations of parameter values
	from those estimated on physical grounds, as is common in traditional
	calibration. The hydrometeorological dataset of 2002 was then used
	to validate the model, both against the outlet discharge as well
	as at an internal gauging station. Finally, to complete the model
	performance analysis, parameter uncertainty and its effects on predictive
	uncertainty were also assessed by estimating a posterior parameter
	probability density via Bayesian inference.},
  doi = {10.5194/hess-9-347-2005},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{llopisalbert2009,
  author = {{Llopis-Albert}, C. and Capilla, J.},
  title = {Gradual conditioning of non--{G}aussian transmissivity fields to
	flow and mass transport data: 2. {D}emonstration on a synthetic aquifer},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {371},
  pages = {53--65},
  number = {1--4},
  abstract = {In the first paper of this series a methodology for the generation
	of non-Gaussian transmissivity fields conditional to flow, mass transport
	and secondary data was presented. This methodology, referred to as
	the gradual conditioning (GC) method, constitutes a new and advanced
	powerful approach in the field of stochastic inverse modelling. It
	is based on gradually changing an initial transmissivity (T) field,
	conditioned only to T and secondary data, to honour flow and transport
	measured data. The process is based on combining the initial T field
	with other seed T fields in successive iterations maintaining the
	stochastic structure of T, previously inferred from data. The iterative
	procedure involves the minimization of a penalty function which depends
	on one parameter, and is made up by the weighted summation of the
	square deviations among flow and/or transport variables, and the
	corresponding known measurements. The GC method leads gradually to
	a final simulated field, uniformly converging to a better reproduction
	of conditioning data as more iterations are performed. The methodology
	is now demonstrated on a synthetic aquifer in a non-multi-Gaussian
	stochastic framework. First, an initial T field is simulated, and
	retained as reference T field. With prescribed head boundary conditions,
	transient flow created by an abstraction well and a mass solute plume
	migrating through the formation, a long-term and large scale hypothetical
	tracer experiment is run in this reference synthetic aquifer. Then
	T, piezometric head (h), solute concentration (c) and travel time
	(?) are sampled at a limited number of points, and for different
	time steps where applicable. Using this limited amount of information
	the GC method is applied, conditioning to different sets of these
	sampled data and model results are compared to those from the reference
	synthetic aquifer. Results demonstrate the ability and robustness
	of the GC method to include different types of data without adopting
	any Gaussian assumptions, and its high potential to be used together
	with the Monte Carlo method for uncertainty analysis of flow and
	mass transport model results. Moreover, the simplicity of the formulation
	of the method, based on forward flow and mass transport solvers,
	the flexibility of the stochastic random definition required, and
	the simple form of the minimization problems solved during the iterative
	procedure, make this a very valuable tool and robust alternative
	to other methods for real applications.},
  doi = {10.1016/j.jhydrol.2009.03.014},
  owner = {rojasro},
  timestamp = {2009.10.20}
}

@ARTICLE{lo+al2010,
  author = {Lo, {M-H} and Famiglietti, J. and Yeh, P. and Syed, T.},
  title = {Improving parameter estimation and water table depth simulation in
	a land surface model using {GRACE} water storage and estimated base
	flow data},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W05517},
  number = {5},
  abstract = {Several previous studies have shown the significance of representing
	shallow groundwater in land surface model (LSM) simulations. However,
	optimal methods for parameter estimation in order to realistically
	simulate water table depth have received little attention. The recent
	availability of Gravity Recovery and Climate Experiment (GRACE) water
	storage data provides a unique opportunity to constrain LSM simulations
	of terrestrial hydrology. In this study, we incorporate both GRACE
	(storage) and estimated base flow (flux) data in the calibration
	of LSM parameters, and demonstrate the advantages gained from this
	approach using a Monte Carlo simulation framework. This approach
	improves parameter estimation and reduces the uncertainty of water
	table simulations in the LSM. Using the optimal parameter set identified
	from the multiobjective calibration, water table simulation can be
	improved due to close dependence of both base flow and total subsurface
	water storage on the water table depth. Moreover, it is shown that
	parameters calibrated from short-term (2003{--}2005) GRACE and base
	flow data can be validated using simulations for the periods of 1984{--}1998
	and 2006{--}2007, which implies that the proposed multiobjective
	calibration strategy is robust. More important, this study has demonstrated
	the potential for the joint use of routinely available GRACE water
	storage data and streamflow records to constrain LSM simulations
	at the global scale},
  doi = {10.1029/2009WR007855},
  tags = {Calibration}
}

@ARTICLE{loaiciga1987,
  author = {Loaiciga, H. and Mari\~no, M.},
  title = {Parameter estimation in grondwater; {C}lassical, {B}ayesian, and
	deterministic assumptions and their impact on management policies},
  journal = {Water Resources Research},
  year = {1987},
  volume = {23},
  pages = {1027--1035},
  number = {6},
  abstract = {This work deals with a theoretical analysis of parameter uncertainty
	in groundwater management models. The importance of adopting classical,
	Bayesian, or deterministic distribution assumptions on parameters
	is examined from a mathematical standpoint. In the classical case,
	the parameters (e.g., hydraulic conductivities or storativities)
	are assumed fixed (i.e., nonrandom) but unknown. The Bayesian assumption
	considers the parameters as random entities with some probability
	distribution. The deterministic case, also called certainty equivalence,
	assumes that the parameters are fixed and known. Previous work on
	the inverse problem has emphasized the numerical solution for parameter
	estimates with the subsequent aim to use them in the simulation of
	field variables. In this paper, the role of parameter uncertainty
	(measured by their statistical variability) in groundwater management
	decisions is investigated. It is shown that the classical, Bayesian,
	and deterministic assumptions lead to analytically different management
	solutions. Numerically, the difference between such solutions depends
	upon the covariance of the parameter estimates. The theoretical analyses
	of this work show the importance of specifying the proper distributional
	assumption on groundwater parameters, as well as the need for using
	efficient and statistically consistent methods to solve the inverse
	problem. The distributional assumptions on groundwater parameters
	and the covariance of their sample estimators are shown to be the
	dominant parameter uncertainty factors affecting groundwater management
	solutions. An example illustrates the conceptual findings of this
	work.},
  doi = {10.1029/WR023i006p01027},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{lorenz2010,
  author = {Lorenz, P. and Jacob, D.},
  title = {{Validation of temperature trends in the ENSEMBLES regional climate
	model runs driven by ERA40}},
  journal = {Climatic Change},
  year = {2010},
  volume = {44},
  pages = {167--177},
  number = {2--3},
  abstract = {In the EU-Project ENSEMBLES, a major objective was the development
	of a weighting system for regional climate models (RCMs) based on
	the ability of the RCMs to reproduce observed characteristics of
	important atmospheric variables. Within this context, a suite of
	state-of-the-art RCMs was employed for the creation of probabilistic
	future regional climate change scenarios for Europe. Among others,
	one measure to determine the individual RCM-weights is their ability
	to reproduce the observed trends in 2 m temperature in the reanalysis
	(ERA40)-driven experiments for the period 1960–2000. As the reference,
	we used the new ENSEMBLES observational gridded dataset for Europe
	(E-OBS). As additional datasets for comparisons, we also used the
	near-surface temperature datasets from the CRU observations and from
	the ERA40 and NCEP/NCAR reanalysis. The analysis was performed for
	the land fraction of 8 different European regions, the so-called
	PRUDENCE regions defined within the PRUDENCE project (http://prudence.dmi.dk).
	Annual and seasonal linear trends in near-surface temperature were
	computed for each ENSEMBLES RCM, the E-OBS dataset, and for the additional
	datasets mentioned above. In all regions, the computed linear temperature
	trends based on annual mean temperatures showed smaller values for
	the RCMs and the NCEP/NCAR reanalysis than for both observational
	datasets, and in most regions also smaller values than for the ERA40
	reanalysis dataset. Depending on the magnitude of the difference
	in linear trends between the individual RCMs and the E-OBS dataset,
	skill scores were assigned to each RCM. The resulting skill scores
	were of similar magnitude (0.7–0.9) for the different models and
	regions (except for Scandinavia, which had lower skill scores around
	0.6–0.8). Spatially aggregated for all of Europe, and combined from
	annual and seasonal into one value for each RCM, these skill scores
	were included in the general ENSEMBLES RCM weighting system},
  doi = {10.3354/cr00973},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@ARTICLE{madadgar2012,
  author = {Madadgar, S. and Moradkhani, H.},
  title = {Drought analysis under climate change using copula},
  journal = {Journal of Hydrologic Engineering},
  year = {2012},
  volume = {in press},
  doi = {10.1061/(ASCE)HE.1943-5584.0000532},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{madiganraftery1994,
  author = {Madigan, D. and Raftery, A.},
  title = {Model selection and accounting for model uncertainty in graphical
	models using {O}ccam's window},
  journal = {Journal of the American Statistical Association},
  year = {1994},
  volume = {89},
  pages = {1535--1546},
  number = {428},
  abstract = {We consider the problem of model selection and accounting for model
	uncertainty in high-dimensional contingency tables, motivated by
	expert system applications. The approach most used currently is a
	stepwise strategy guided by tests based on approximate asymptotic
	P values leading to the selection of a single model; inference is
	then conditional on the selected model. The sampling properties of
	such a strategy are complex, and the failure to take account of model
	uncertainty leads to underestimation of uncertainty about quantities
	of interest. In principle, a panacea is provided by the standard
	Bayesian formalism that averages the posterior distributions of the
	quantity of interest under each of the models, weighted by their
	posterior model probabilities. Furthermore, this approach is optimal
	in the sense of maximizing predictive ability. But this has not been
	used in practice, because computing the posterior model probabilities
	is hard and the number of models is very large (often greater than
	101 ). We argue that the standard Bayesian formalism is unsatisfactory
	and propose an alternative Bayesian approach that, we contend, takes
	full account of the true model uncertainty by averaging over a much
	smaller set of models. An efficient search algorithm is developed
	for finding these models. We consider two classes of graphical models
	that arise in expert systems: the recursive causal models and the
	decomposable log-linear models. For each of these, we develop efficient
	ways of computing exact Bayes factors and hence posterior model probabilities.
	For the decomposable log-linear models, this is based on properties
	of chordal graphs and hyper-Markov prior distributions and the resultant
	calculations can be carried out locally. The end product is an overall
	strategy for model selection and accounting for model uncertainty
	that searches efficiently through the very large classes of models
	involved. Three examples are given. The first two concern data sets
	that have been analyzed by several authors in the context of model
	selection. The third addresses a urological diagnostic problem. In
	each example, our model averaging approach provides better out- of-sample
	predictive performance than any single model that might reasonably
	have been selected},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://www.jstor.org/stable/2291017}
}

@ARTICLE{madsen2003,
  author = {Madsen, H.},
  title = {Parameter estimation in distributed hydrological catchment modelling
	using automatic calibration with multiple objectives},
  journal = {Advances in Water Resources},
  year = {2003},
  volume = {26},
  pages = {205--216},
  number = {2},
  doi = {DOI: 10.1016/S0309-1708(02)00092-1},
  issn = {0309-1708},
  keywords = {Hydrological modelling, Distributed model, MIKE SHE, Parameter estimation,
	Automatic calibration, Multiple objectives},
  tags = {Calibration},
  url = {http://www.sciencedirect.com/science/article/B6VCF-47DT88V-6/2/0167372144b8e9b4a2464afb17129985}
}

@ARTICLE{madsen2000,
  author = {Madsen, H.},
  title = {Automatic calibration of a conceptual rainfall-runoff model using
	multiple objectives},
  journal = {Journal of Hydrology},
  year = {2000},
  volume = {235},
  pages = {276--288},
  abstract = {Formulation of an automatic calibration strategy for the MIKE 11/NAM
	rainfall-runoff model is outlined. The calibration scheme includes
	optimisation of multiple objectives that measure different aspects
	of the hydrograph: (1) overall water balance, (2) overall shape of
	the hydrograph, (3) peak flows, and (4) low flows. An automatic optimisation
	procedure based on the shuffled complex evolution algorithm is introduced
	for solving the multi-objective calibration problem. A test example
	is presented that illustrates the principles and implications of
	using multiple objectives in model calibration. Significant trade-offs
	between the different objectives are observed in this case and no
	single unique set of parameter values is able to optimise all objectives
	simultaneously. Instead, the solution to the calibration problem
	is given as a set of Pareto optimal solutions, which from a multi-objective
	viewpoint are equivalent. A large variability is observed in the
	Pareto optimal parameter sets, resulting in a large range of "equally
	good" simulated hydrographs. From the set of Pareto optimal solutions,
	one can draw a single solution according to priorities of the different
	objectives for the specific model application being considered. A
	balanced aggregated objective function is proposed, which provides
	a compromise solution that puts equal weights to the different objectives.
	(C) 2000 Elsevier Science B.V. All rights reserved.},
  doi = {10.1016/S0022-1694(00)00279-1},
  keywords = {rainfall-runoff models, calibration, parameter estimation, optimisation,
	multiple objectives, HBV HYDROLOGICAL MODEL, GLOBAL OPTIMIZATION,
	UNCERTAINTY, ALGORITHMS, SCHEME},
  tags = {SWAT, Calibration, Goodness-of-Fit}
}

@ARTICLE{madsen+al2002,
  author = {Madsen, H. and Wilson, G. and Ammentrop, H.},
  title = {Comparison of different automated strategies for calibration of rainfall-runoff
	models},
  journal = {Journal of Hydrology},
  year = {2002},
  volume = {261},
  pages = {48--59},
  number = {1-4},
  abstract = {Three different automated methods for calibration of rainfall-runoff
	models are presented and compared. The methods represent various
	calibration strategies that utilise multiple objectives and allow
	user intervention on different levels and different stages in the
	calibration process. The methods have been applied for calibration
	of a test catchment and compared on validation data with respect
	to overall performance measures in terms of water balance error and
	general hydrograph shape, and simulation of high and low flow, events,
	The results illustrate the problem of non-uniqueness in model calibration
	since none of the methods are superior with respect to all performance
	measures considered, In general, the different methods put emphasis
	on different response modes of the hydrograph. Calibration based
	on the use of generic search routines in combination with user-specified
	calibration priorities is seen to compare favourably with an expert
	system that is designed for the specific model being considered and
	requires user intervention during the entire calibration process.
	(C) 2002 Elsevier Science B.V. All rights reserved.},
  doi = {10.1016/S0022-1694(01)00619-9},
  keywords = {rainfall-runoff models, calibration, parameter estimation, optimisation
	routines, expert system, HBV HYDROLOGICAL MODEL, GLOBAL OPTIMIZATION,
	ALGORITHMS, MULTIPLE, SCHEME},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{magaritz1990,
  author = {Magaritz, M. and Aravena, R. and Pe\~na, H. and Suzuki, O. and Grilli,
	A.},
  title = {Source of groundwaters in the deserts of northern Chile: {E}vidence
	for deep circulation of groundwaters from the Andes},
  journal = {Ground Water},
  year = {1990},
  volume = {28},
  pages = {513--517},
  number = {4},
  abstract = {Ground water in the Pampa del Tamarugal basin part of the desert of
	Northern Chile is the main water resource for cities and agricultural
	activities in the region. As the area received virtually no precipitation,
	the source of the ground water is rains in the high Andes. Aquifer
	recharge has been linked to a drainage system associated with this
	part of the Andes. Ground-water flow is from northeast to southwest,
	and a flow component from east to west is also observed in some parts
	of the basin. In general, the water becomes very saline towards the
	western part of the basin, giving rise to salt lakes (salares) in
	ground-water discharge areas. However, isotopic, chemical, and geological
	evidence plus ground-water temperatures distribution, suggest the
	existence of a different recharge mechanism linked to a regional
	ground-water flow system. It is suggested that low-salinity water
	emerges from ground water recharged in the high Andes through a basement
	fault system underneath the basin. This model has implications for
	the future exploration for new fresh-water resources.},
  doi = {10.1111/j.1745-6584.1990.tb01706.x},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{makowski2002,
  author = {Makowski, D. and Wallach, D. and Tremblay, M.},
  title = {Using a {B}ayesian approach to parameter estimation; comparison of
	the {GLUE} and {MCMC} methods},
  journal = {Agronomie},
  year = {2002},
  volume = {22},
  pages = {191--203},
  number = {2},
  abstract = {The Bayesian approach allows one to estimate model parameters from
	prior expert knowledge about parameter values and from experimental
	data. The purpose of this paper is to compare the performances of
	two Bayesian methods, namely the Metropolis-Hastings algorithm and
	the Generalized Likelihood Uncertainty Estimation method (GLUE).
	These two methods are applied to a non-linear model that includes
	22 parameters. This model has the main features of an agronomic model.
	The two Bayesian methods give similar results. The parameter estimates
	obtained with the two methods have similar properties. Both methods
	improve strongly the accuracy of model predictions even when only
	few data samples are available for estimating the parameters. However,
	the values of mean squared error of prediction of the model are slightly
	higher with the GLUE method than with the Metropolis-Hastings algorithm.
	The performances of the methods are sensitive to the prior assumptions
	made about parameter values.},
  doi = {10.1051/agro:2002007},
  owner = {RRojas},
  timestamp = {2008.07.11}
}

@ARTICLE{manning2009,
  author = {Manning, L. and Hall, J. and Fowler, H. and Kilsby, C.},
  title = {Using probabilistic climate change information from a multimodel
	ensemble for water resources assessment},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W11411},
  abstract = {Increasing availability of ensemble outputs from general circulation
	models (GCMs) and regional climate models (RCMs) permits fuller examination
	of the implications of climate uncertainties in hydrological systems.
	A Bayesian statistical framework is used to combine projections by
	weighting and to generate probability distributions of local climate
	change from an ensemble of RCM outputs. A stochastic weather generator
	produces corresponding daily series of rainfall and potential evapotranspiration,
	which are input into a catchment rainfall-runoff model to estimate
	future water abstraction availability. The method is applied to the
	Thames catchment in the United Kingdom, where comparison with previous
	studies shows that different downscaling methods produce significantly
	different flow predictions and that this is partly attributable to
	potential evapotranspiration predictions. An extended sensitivity
	test exploring the effect of the weights and assumptions associated
	with combining climate model projections illustrates that under all
	plausible assumptions the ensemble implies a significant reduction
	in catchment water resource availability},
  doi = {10.1029/2007WR006674},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@ARTICLE{mantovantodini2006,
  author = {Mantovan, P. and Todini, E.},
  title = {Hydrological forecasting uncertainty assessment: {I}ncoherence of
	the {GLUE} methodology},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {330},
  pages = {368--381},
  number = {1--2},
  abstract = {The aim of the paper is to demonstrate the incoherence, in terms of
	Bayesian inference, of the generalized likelihood uncertainty estimation
	(GLUE) approach, introduced by Beven and Binley in 1992. This results
	into a reduced capacity of the technique to extract information,
	in other words to “learn”, from observations. The paper also discusses
	the implications of this reduced learning capacity for parameter
	estimation and hydrological forecasting uncertainty assessment, which
	has led to the definition of the “equifinality” principle. The notions
	of coherence for learning and prediction processes as well as the
	value of a statistical experiment are introduced. These concepts
	are useful in showing that the GLUE methodology defines a statistical
	inference process, which is inconsistent and incoherent.},
  doi = {10.1016/j.jhydrol.2006.04.046},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{mantovantodini2007,
  author = {Mantovan, P. and Todini, E. and Martina, M.},
  title = {Reply to comment by {K}. {B}even, {P}. {S}mith and {J}. {F}reer on
	``{H}ydrological forecasting uncertainty assessment: {I}ncoherence
	of the {GLUE} methodology''},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {338},
  pages = {319--324},
  number = {3--4},
  doi = {10.1016/j.jhydrol.2007.02.029},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{mapfumo+al2004,
  author = {Mapfumo, E. and Chanasyk, D. and Willms, W.},
  title = {Simulating daily soil water under foothills fescue grazing with the
	soil and water assessment tool model ({A}lberta, {C}anada)},
  journal = {Hydrological Processes},
  year = {2004},
  volume = {18},
  pages = {2787--2800},
  number = {15},
  abstract = {Grazing is common in the foothills fescue grasslands and may influence
	the seasonal soil-water patterns, which in turn determine range productivity.
	Hydrological modelling using the soil and water assessment tool (SWAT)
	is becoming widely adopted throughout North America especially for
	simulation of stream flow and runoff in small and large basins. Although
	applications of the SWAT model have been wide, little attention has
	been paid to the model's ability to simulate soil-water patterns
	in small watersheds. Thus a daily profile of soil water was simulated
	with SWAT using data collected from the Stavely Range Sub-station
	in the foothills of south-western Alberta, Canada. Three small watersheds
	were established using a combination of natural and artificial barriers
	in 1996-97. The watersheds were subjected to no grazing (control),
	heavy grazing (2{$\cdot$}4 animal unit months (AUM) per hectare)
	or very heavy grazing (4{$\cdot$}8 AUM ha-1). Soil-water measurements
	were conducted at four slope positions within each watershed (upper,
	middle, lower and 5 m close to the collector drain), every 2 weeks
	annually from 1998 to 2000 using a downhole CPN 503 neutron moisture
	meter. Calibration of the model was conducted using 1998 soil-water
	data and resulted in Nash-Sutcliffe coefficient (EF or R2) and regression
	coefficient of determination (r2) values of 0{$\cdot$}77 and 0{$\cdot$}85,
	respectively. Model graphical and statistical evaluation was conducted
	using the soil-water data collected in 1999 and 2000. During the
	evaluation period, soil water was simulated reasonably with an overall
	EF of 0{$\cdot$}70, r2 of 0{$\cdot$}72 and a root mean square error
	(RMSE) of 18{$\cdot$}01. The model had a general tendency to overpredict
	soil water under relatively dry soil conditions, but to underpredict
	soil water under wet conditions. Sensitivity analysis indicated that
	absolute relative sensitivity indices of input parameters in soil-water
	simulation were in the following order; available water capacity
	> bulk density > runoff curve number > fraction of field capacity
	(FFCB) > saturated hydraulic conductivity. Thus these data were critical
	inputs to ensure reasonable simulation of soil-water patterns. Overall,
	the model performed satisfactorily in simulating soil-water patterns
	in all three watersheds with a daily time-step and indicates a great
	potential for monitoring soil-water resources in small watersheds},
  doi = {10.1002/hyp.1493},
  keywords = {hydrology, model sensitivity, soil moisture, statistical evaluation,
	SWAT},
  tags = {SWAT, Applications}
}

@ARTICLE{marshall+al2007,
  author = {Marshall, L. and Nott, D. and Sharma, A.},
  title = {Towards dynamic catchment modelling: a {B}ayesian hierarchical mixtures
	of experts framework},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {847--861},
  number = {7},
  abstract = {Despite the abundance of existing hydrological models, there is no
	single model that has been identified as performing consistently
	over the range of possible catchment types and catchment conditions.
	An attractive alternative to selecting a single model is to combine
	the results from several different hydrological models, thereby providing
	a more appropriate representation of model uncertainty than is the
	case otherwise. Methods based on Bayesian statistical techniques
	provide an ideal means to compare and combine competing models, as
	they explicitly account for model uncertainty. Bayesian model averaging
	is one such alternative that combines individual models by weighting
	models proportional to their respective posterior probability of
	selection. However, the necessity of having fixed weights for each
	model over the entire length of the simulation period means that
	the relative usefulness of different models at different times is
	not considered. The hierarchical mixtures of experts (HME) framework
	is an appealing extension of the model averaging framework that allows
	the individual model weights to be estimated dynamically. Consequently,
	a model more capable at simulating low flow characteristics attains
	a higher weight (probability) when such conditions are likely, switching
	over to a lower weight when catchment storage increases. In this
	way, different models apply in different hydrological states, with
	the probability of selecting each model being allowed to depend on
	relevant antecedent condition characteristics. HME models provide
	additional flexibility compared with simple combinations of models,
	by allowing the way that model predictions are combined to depend
	on predictor variables. Thus, for hydrological models, the switch
	from one model to another can depend on the existing catchment condition.
	This new modelling framework is applied using a simple conceptual
	model to 10 selected Australian catchments. The study regions are
	chosen to vary considerably in terms of size, yield and location.
	Results from this application are compared with the alternative where
	a single fixed model structure is applied. Comparison of the model
	simulations using the maximum log-likelihood and the Nash-Sutcliffe
	coefficient of efficiency show that more variance in streamflow was
	explained by the HME model, compared with the conceptual model alone
	for each of the catchments investigate},
  doi = {10.1002/hyp.6294},
  keywords = {Bayesian, rainfall-runoff model, model averaging},
  tags = {conceptual model, Calibration}
}

@ARTICLE{martinecRango1989,
  author = {Martinec, J. and Rango, A.},
  title = {Merits of statistical criteria for the performance of hydrological
	models},
  journal = {Journal of the American Water Resources Association},
  year = {1989},
  volume = {25},
  pages = {421--432},
  number = {2},
  abstract = {The performance of a hydrological model is usually assessed first
	by visual inspection of the measured and computed hydrographs. Numerous
	statistical criteria are available for numerical evaluations of model
	accuracy in each single year, in a particular season of the year,
	or in a sequence of years or seasons. In the last case, the problem
	of computing the overall result has to be considered. If too many
	criteria are used and the criteria are switched frequently, an assessment
	of a model's performance becomes difficult for a potential user.
	Therefore, this paper concentrates on just three criteria and their
	combined evaluation: The Nash-Sutcliffe coefficient, which compares
	the model computed discharge with the average measured discharge;
	the {``}coefficient of gain from daily means{''} in which a uniform
	average discharge is replaced by daily average discharges; and the
	volumetric difference between the total measured and computed runoff.
	The three criteria are combined in a three dimensional representation
	that allows intercomparisons of model performance in a single diagram.},
  doi = {10.1111/j.1752-1688.1989.tb03079.x},
  keywords = { * model performance, * accuracy criteria, * statistical analysis,
	* snowmelt runoff, * hydrologic modeling},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{martinezgupta2010,
  author = {Martinez, G. and Gupta, H.},
  title = {Toward improved identification of hydrological models: {A} diagnostic
	evaluation of the "abcd" monthly water balance model for the conterminous
	{U}nited {S}tates},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W08507},
  number = {8},
  abstract = {Continental-scale water balance (WB) assessments are important for
	characterizing hydrologic systems and understanding regional-scale
	dynamics and for identifying hydroclimatic trends and systematic
	data biases. However, it is not clear whether existing models can
	reproduce the catchment dynamics observed in nature. Nor has our
	ability to evaluate model results kept pace with computational and
	data processing abilities. Consequently, methods for diagnostic model
	evaluation and improvement remain weak. There is a need for well-conceived,
	systematic strategies to guide model selection, establish data requirements,
	estimate parameters, and evaluate and track model performance. We
	examine these challenges in the context of monthly WB modeling for
	the conterminous United States by applying the {``}abcd{''} model
	to 764 catchments selected for their comprehensive coverage of hydrogeological
	conditions. By examining diagnostically relevant components of model
	error, we evaluate the details of its spatial variability across
	the continental United States. Model performance, parameters, and
	structures are found to be correlated with hydroclimatic variables.
	However, our results indicate a need for the conventional identification
	approach to be improved. Because they do not constrain models to
	reproduce important hydrological behaviors, reported values of NSE
	or r2 performance can be misleading. Further, we must establish suitable
	model hypotheses with appropriate spatiotemporal scale for each hydroclimatic
	region. Until these issues are resolved, such models cannot reliably
	be used to infer the spatiotemporal dynamics of continental-scale
	water balance or to regionalize model structures and parameters to
	ungaged locations. },
  doi = {10.1029/2009WR008294},
  tags = {Calibration, conceptual model}
}

@ARTICLE{mathevetgarcon2010,
  author = {Mathevet, T. and Garcon, R.},
  title = {Tall tales from the hydrological crypt: are models monsters?},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {857--871},
  number = {6},
  abstract = {"Bizarre", "monstrous": in society as well as in science, this is
	the way we are used to describing objects that deviate from an expected
	standard. Hydrology is no exception. The bizarre or the monstrous
	describes every object that has a low probability of occurring, or
	that our models fail to represent. Actually, the bizarre or the monstrous
	is often a demonstration of the limits of our models rather than
	an intrinsic characteristic of the objects we study. This article
	provides a reflection on the definition of bizarre and monstrous
	in the context of hydrology. We base our reflection on 60 years of
	experience in hydrometeorological operational management and applied
	research at the French national electricity company (EDF-DTG). First,
	we describe several classical a priori models or conceptions trusted
	by hydrologists, sometimes erroneously. These include classical rainfall
	or streamflow measurement issues, certain limits of the watershed
	concept or problems in the spatialization of local measurements.
	Then we attempt to show how the misuse of statistical models can
	generate bizarre or monstrous results. We give examples related to
	outliers and to the homogeneity and stationarity hypotheses. We show
	how difficult it may be for operational forecasters to anticipate
	and believe that extreme (monstrous) events will occur in the near
	future. Finally, we wish to show that the bizarre or the monstrous
	should not be rejected in hydrology, but instead is something to
	study in greater depth. We believe that this type of analysis offers
	new opportunities to improve the explanatory and predictive capacity
	of our models. },
  doi = {10.1080/02626667.2010.503934},
  tags = {Calibration, Philosophical}
}

@MANUAL{ostrich2005,
  title = {{Ostrich: An optimization software tool, Documentation and User’s
	Guide, Version 1.6}},
  author = {Matott, L.},
  organization = {Department of Civil, Structural and Environmental Engineering, University
	at Buffalo},
  address = {Buffalo, NY},
  year = {2005},
  owner = {rojasro},
  timestamp = {2011.10.11},
  url = {http:\\www.groundwater.buffalo.edu}
}

@ARTICLE{matott2009,
  author = {Matott, L. and Babendreier, J. and Purucker, S.},
  title = {Evaluating uncertainty in integrated environmental models: {A} review
	of concepts and tools},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W06421},
  number = {45},
  abstract = {This paper reviews concepts for evaluating integrated environmental
	models and discusses a list of relevant software-based tools. A simplified
	taxonomy for sources of uncertainty and a glossary of key terms with
	“standard” definitions are provided in the context of integrated
	approaches to environmental assessment. These constructs provide
	a reference point for cataloging 65 different model evaluation tools.
	Each tool is described briefly (in the auxiliary material) and is
	categorized for applicability across seven thematic model evaluation
	methods. Ratings for citation count and software availability are
	also provided, and a companion Web site containing download links
	for tool software is introduced. The paper concludes by reviewing
	strategies for tool interoperability and offers guidance for both
	practitioners and tool developers.},
  doi = {10.1029/2008WR007301},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{matott2008,
  author = {Matott, L. and Rabideau, A.},
  title = {Calibration of complex subsurface reaction models using a surrogate-model
	approach},
  journal = {Advances in Water Resources},
  year = {2008},
  volume = {31},
  pages = {1697--1707},
  number = {12},
  abstract = {Automatic calibration of complex subsurface reaction models involves
	numerous difficulties, including the existence of multiple plausible
	models, parameter non-uniqueness, and excessive computational burden.
	To overcome these difficulties, this study investigated a novel procedure
	for performing simultaneous calibration of multiple models (SCMM).
	By combining a hybrid global-plus-polishing search heuristic with
	a biased-but-random adaptive model evaluation step, the new SCMM
	method calibrates multiple models via efficient exploration of the
	multi-model calibration space. Central algorithm components are an
	adaptive assignment of model preference weights, mapping functions
	relating the uncertain parameters of the alternative models, and
	a shuffling step that efficiently exploits pseudo-optimal configurations
	of the alternative models. The SCMM approach was applied to two nitrate
	contamination problems involving batch reactions and one-dimensional
	reactive transport. For the chosen problems, the new method produced
	improved model fits (i.e. up to 35% reduction in objective function)
	at significantly reduced computational expense (i.e. 40–90% reduction
	in model evaluations), relative to previously established benchmarks.
	Although the method was effective for the test cases, SCMM relies
	on a relatively ad-hoc approach to assigning intermediate preference
	weights and parameter mapping functions. Despite these limitations,
	the results of the numerical experiments are empirically promising
	and the reasoning and structure of the approach provide a strong
	foundation for further development.},
  doi = {10.1016/j.advwatres.2008.08.006},
  owner = {rojasro},
  timestamp = {2011.10.11}
}

@ARTICLE{maurer2007,
  author = {Maurer, E.},
  title = {Uncertainty in hydrologic impacts of climate change in the {S}ierra
	{N}evada, {C}alifornia, under two emissions scenarios},
  journal = {Climatic Change},
  year = {2007},
  volume = {82},
  pages = {309--325},
  number = {3--4},
  abstract = {A hydrologic model was driven by the climate projected by 11 GCMs
	under two emissions scenarios (the higher emission SRES A2 and the
	lower emission SRES B1) to investigate whether the projected hydrologic
	changes by 2071--2100 have a high statistical confidence, and to
	determine the confidence level that the A2 and B1 emissions scenarios
	produce differing impacts. There are highly significant average temperature
	increases by 2071--2100 of 3.7Â°C under A2 and 2.4Â°C under B1; July
	increases are 5Â°C for A2 and 3Â°C for B1. Two high confidence hydrologic
	impacts are increasing winter streamflow and decreasing late spring
	and summer flow. Less snow at the end of winter is a confident projection,
	as is earlier arrival of the annual flow volume, which has important
	implications on California water management. The two emissions pathways
	show some differing impacts with high confidence: the degree of warming
	expected, the amount of decline in summer low flows, the shift to
	earlier streamflow timing, and the decline in end-of-winter snow
	pack, with more extreme impacts under higher emissions in all cases.
	This indicates that future emissions scenarios play a significant
	role in the degree of impacts to water resources in California.},
  doi = {10.1007/s10584-006-9180-9},
  tags = {Uncertainty}
}

@ARTICLE{mckay1979,
  author = {McKay, D. and Beckman, R. and Conover, W.},
  title = {A comparison of three methods for selecting values of input variables
	in the analysis of output from a computer code},
  journal = {Technometrics},
  year = {1979},
  volume = {21},
  pages = {239--245},
  number = {2},
  abstract = {Two types of sampling plans are examined as alternatives to simple
	random sampling in Monte Carlo studies. These plans are shown to
	be improvements over simple random sampling with respect to variance
	for a class of estimators which includes the sample mean and the
	empirical distribution function.},
  owner = {RRojas},
  timestamp = {2008.11.18},
  url = {http://www.jstor.org/stable/1268522}
}

@ARTICLE{mclaughlin1988,
  author = {{McLaughlin}, D. and Wood, E.},
  title = {A distributed parameter approach for evaluating the accuracy of groundwater
	model predictions: 2. {A}pplication to groundwater flow},
  journal = {Water Resources Research},
  year = {1988},
  volume = {24},
  pages = {1048--1060},
  number = {7},
  abstract = {The distributed parameter theory presented by McLaughlin and Wood
	(this issue) is used to evaluate the accuracy of a groundwater flow
	model. The special case investigated assumes that the model's prediction
	errors are due primarily to data limitations. In this case, approximate
	prediction error moments (mean and covariance) are obtained by solving
	two sets of coupled partial differential equations which have the
	same basic structure as the original flow equation. Solutions can
	be obtained with spectral, Green's function, or numerical techniques,
	depending on the assumptions made. Two examples are solved using
	a finite element approach. The first (two-dimensional) example confirms
	steady state infinite domain results obtained with spectral methods.
	The second (three-dimensional) example investigates the influence
	of spatial variability, sampling strategy, and suboptimal estimation
	on model prediction accuracy.},
  doi = {10.1029/WR024i007p01048},
  owner = {rojasro},
  timestamp = {2010.02.17}
}

@ARTICLE{mcmillan2009,
  author = {{McMillan}, H. and Clark, M.},
  title = {{Rainfall-runoff model calibration using informal likelihood measures
	within a Markov Chain Monte Carlo sampling scheme}},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W04418},
  abstract = {This paper considers the calibration of a distributed rainfall-runoff
	model in a catchment where heterogeneous geology leads to a difficult
	and high-dimensional calibration problem and where the response surface
	has multiple optima and strong parameter interactions. These characteristics
	render the problem unsuitable for solution by uniform Monte Carlo
	sampling and require a more targeted sampling strategy. MCMC methods,
	using the SCEM-UA algorithm, are trialed using both formal and informal
	likelihood measures. Each method is assessed in its success at predicting
	the catchment flow response and capturing the total uncertainty associated
	with this prediction. The comparison is made at both the catchment
	outlet and at internal catchment locations with distinct geological
	characteristics. Informal likelihoods are found to provide a more
	complete exploration of the behavioral regions of the response space
	and hence more accurate estimation of total uncertainty. Last, we
	demonstrate how information gained from the investigation of the
	response space, in conjunction with qualitative knowledge of system
	behavior, can be used to constrain the Markov chain trajectory.},
  doi = {10.1029/2008WR007288},
  owner = {rojasro},
  timestamp = {2012.10.10}
}

@INCOLLECTION{mearns+al2001,
  author = {Mearns, L. and Hulme, M. and Carter, T. and Whetton, P.},
  title = {Climate scenario development},
  booktitle = {Climate Change 2001: The Scientific Basis, Chapter 13, Contribution
	of Working Group I to the Third Assessment Report of the Intergovernmental
	Pannel on Climate Change (IPCC)},
  publisher = {Cambridge University Press},
  year = {2001},
  editor = {J. T. Houghton and K. Maskell and X. Dai and P. J. {van der Linden}
	and C. A. Johnson and Y. Ding and D. J. Griggs and M. Noguer},
  pages = {739--768},
  address = {Cambridge, UK},
  keywords = {climate scenario},
  tags = {Scenarios}
}

@INCOLLECTION{meehl+al2007,
  author = {Meehl, G. and Stocker, T. and Collins, W. and Friedlingstein, P.
	and Gaye, A. and Gregory, J. and Kitoh, A. and Knutti, R. and Murphy,
	J. and Noda, A. and Raper, S. and Watterson, I. and Weaver, A. and
	Zhao, {Z.-C.}},
  title = {Global climate projections},
  booktitle = {Climate Change 2007: The Physical Science Basis. Contribution of
	Working Group I to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {S. Solomon and D. Qin and M. Manning and Z. Chen and M. Marquis and
	K.B. Averyt and M. Tignor and H.L. Miller},
  address = {Cambridge, United Kingdom and New York, NY, USA.},
  tags = {IPCC}
}

@PHDTHESIS{mendes2004,
  author = {Mendes, R.},
  title = {Population topologies and their influence in particle swarm performance},
  school = {Departamento de Inform\'atica. Escola de Engenharia. Universidade
	do Minho},
  year = {2004},
  address = {Minho, Portugal.},
  owner = {rojasro},
  timestamp = {2011.11.17}
}

@ARTICLE{mendes+al2004,
  author = {Mendes, R. and Kennedy, J. and Neves, J.},
  title = {The {F}ully {I}nformed {P}article Swarm: Simpler, Maybe Better},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2004},
  volume = {8},
  pages = {204--210},
  number = {3},
  abstract = {The canonical particle swarm algorithm is a new approach to optimization,
	drawing inspiration from group behavior and the establishment of
	social norms. It is gaining popularity, especially because of the
	speed of convergence and the fact that it is easy to use. However,
	we feel that each individual is not simply influenced by the best
	performer among his neighbors. We, thus, decided to make the individuals
	"fully informed." The results are very promising, as informed individuals
	seem to find better solutions in all the benchmark functions.},
  doi = {10.1109/TEVC.2004.826074},
  tags = {Calibration, PSO}
}

@ARTICLE{mertens2004,
  author = {Mertens, J. and Madsen, H. and Feyen, L. and Jacques, D. and Feyen,
	J.},
  title = {Including prior information in the estimation of effective soil parameters
	in unsaturated zone modelling},
  journal = {Journal of Hydrology},
  year = {2004},
  volume = {294},
  pages = {251--269},
  number = {4},
  abstract = {In this paper we propose a methodology to include prior information
	in the estimation of effective soil parameters for modelling the
	soil moisture content in the unsaturated zone. Laboratory measurements
	on undisturbed soil cores were used to estimate the moisture retention
	curve and hydraulic conductivity curve parameters. The soil moisture
	content was measured at 25 locations along three transects and at
	three different depths (surface, 30 and 60 cm) on an 80×20 m hillslope
	for the year 2001. Soil cores were collected in 84 locations situated
	in three profile pits along the hillslope. For the estimation of
	the effective soil hydraulic parameters the joint probability distribution
	of measured parameter values was used as prior information. A two-horizon
	single column 1D MIKE SHE model based on Richards' equation was set-up
	for nine soil moisture measurement locations along the middle transect
	of the hillslope. The goal of the model is to simulate the soil moisture
	profile at each location. The shuffled complex evolution (SCE) algorithm
	has been applied to estimate effective model parameters using either
	wide parameter ranges, referred to as the ‘no-prior’ case, or the
	joint probability distribution of measured parameter values as prior
	information (‘prior’ case). When the prior information is incorporated
	in the SCE optimisation the goodness-of-fit of the model predictions
	is only slightly worse compared to when no-prior information is incorporated.
	However, the effective parameter estimates are more realistic when
	the prior information is incorporated. For both the no-prior and
	prior case the generalised likelihood uncertainty estimation procedure
	(GLUE) was subsequently used to estimate the uncertainty bounds (UB)
	on the model predictions. When incorporating the prior information
	more parameter sets were accepted for the estimation of the predictive
	uncertainty and the parameter values were more realistic. Moreover,
	UB better enclosed the observations. Thus, incorporating prior information
	in GLUE reduces the amount of model evaluations needed to obtain
	sufficient behavioural parameter sets. The results indicate the importance
	of prior information in the SCE and GLUE parameter estimation strategies.},
  doi = {10.1016/j.jhydrol.2004.02.011},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{merz+al2010b,
  author = {Merz, B. and Hall, J. and Disse, M. and Schumann, A.},
  title = {Fluvial flood risk management in a changing world},
  journal = {Natural Hazards and Earth System Science},
  year = {2010},
  volume = {10},
  pages = {509--527},
  number = {3},
  doi = {10.5194/nhess-10-509-2010}
}

@ARTICLE{merz+al2008,
  author = {Merz, B. and Kreibich, H. and Apel, H.},
  title = {Flood risk analysis: uncertainties and validation},
  journal = {Österreichische Wasser- und Abfallwirtschaft},
  year = {2008},
  volume = {60},
  pages = {89--94},
  abstract = {Zur Quantifizierung des Hochwasserrisikos werden Risikoanalysen durchgeführt.
	Solche Analysen beschäftigen sich typischerweise mit extremen Ereignissen
	und Versagensszenarios, die zum Zeitpunkt der Analyse kaum oder überhaupt
	nicht beobachtet wurden. Aus diesem Grund sind Risikoanalysen mit
	beträchtlicher Unsicherheit verbunden. Analysen zur Unsicherheit
	von Hochwasser-Risikoaussagen werden heute immer noch selten durchgeführt.
	Dieser Beitrag argumentiert für die Durchführung von Unsicherheitsanalysen
	und zeigt, dass diese (1) Risikoanalysen verbessern, (2) die Validierung
	bzw. Plausibilisierung von Risikoanalysen unterstützen, und (3) eine
	zusätzliche Information für die Entscheidungsfindung sind.},
  affiliation = {GeoForschungsZentrum Potsdam Potsdam Germany},
  doi = {10.1007/s00506-008-0001-4},
  issn = {0945-358X},
  issue = {5},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Wien}
}

@ARTICLE{merz+al2010,
  author = {Merz, B. and Kreibich, H. and Schwarze, R. and Thieken, A.},
  title = {Review article "Assessment of economic flood damage"},
  journal = {Natural Hazards and Earth System Science},
  year = {2010},
  volume = {10},
  pages = {1697--1724},
  number = {8},
  doi = {10.5194/nhess-10-1697-2010}
}

@ARTICLE{merz+al2001,
  author = {Merz, B. and Kreibich, H. and Thieken, A. and Schmidtke, R.},
  title = {Estimation uncertainty of direct monetary flood damage to buildings},
  journal = {Natural Hazards and Earth System Science},
  year = {2004},
  volume = {4},
  pages = {153--163},
  number = {1},
  doi = {10.5194/nhess-4-153-2004}
}

@ARTICLE{merz+al2004,
  author = {Merz, B. and Kreibich, H. and Thieken, A. and Schmidtke, R.},
  title = {Estimation uncertainty of direct monetary flood damage to buildings},
  journal = {Natural Hazards and Earth System Science},
  year = {2004},
  volume = {4},
  pages = {153--163},
  number = {1},
  doi = {10.5194/nhess-4-153-2004}
}

@ARTICLE{messager+al2006,
  author = {Messager, C. and Gall\'ee, H. and Brasseur, O. and Cappelaere, B.
	and Peugeot, C. and S\'eguis, L. and Vauclin, M. and Ramel, R. and
	Grasseau, G. and L\'eger, L. and Girou, D.},
  title = {Influence of observed and {RCM}-simulated precipitation on the water
	discharge over the Sirba basin, {B}urkina {F}aso/{N}iger},
  journal = {Climate Dynamics},
  year = {2006},
  volume = {27},
  pages = {199--214},
  number = {2--3},
  abstract = {The forcing of a hydrologic model (ABC) by both observed and simulated
	precipitation from a regional climate model (MAR) has been performed
	over the Sirba watershed (39,000 km2) located in the Sahelian region.
	Two aspects have been more specifically examined: the spatial and
	temporal representations of precipitation. The comparison between
	simulated and observed dischargesâ€”using observed rainfall datasets
	as forcing of the hydrologic modelâ€”has shown that the representation
	of daily precipitation (which is mainly convective in the Sahelian
	region) was not sufficiently accurate to correctly simulate the hydrologic
	response of the watershed. Since this response drives the soil water
	budget and consequently the amount of evaporation in forthcoming
	coupling experiments, it is thus necessary to develop more realistic
	infra-daily precipitation associated with convective events. A new
	temporal disaggregation scheme has been then developed. Considering
	observed as well as simulated precipitation fields, this method has
	significantly improved the simulated discharge at the catchment outlet.
	The major role played by the temporal component compared to spatial
	component of the precipitation has been then underlined. In addition,
	the present study shows the unsuitability of the simulated precipitation
	from the RCM to directly force a hydrologic model at infra daily
	timescale even if the cumulative amount and the main features of
	the precipitation seasonal cycle are well simulated.},
  doi = {10.1007/s00382-006-0131-y},
  tags = {Climate Models, Impacts, RCMs}
}

@ARTICLE{metropolis1953,
  author = {Metropolis, N. and Rosenbluth, A. and Rosenbluth, M. and Teller,
	A. and Teller, E.},
  title = {Equation of state calculations by fast computing machines},
  journal = {The Journal of Chemical Physics},
  year = {1953},
  volume = {21},
  pages = {1087--1092},
  number = {6},
  abstract = {A general method, suitable for fast computing machines, for investigating
	such properties as equations of state for substances consisting of
	interacting individual molecules is described. The method consists
	of a modified Monte Carlo integration over configuration space. Results
	for the two?dimensional rigid?sphere system have been obtained on
	the Los Alamos MANIAC and are presented here. These results are compared
	to the free volume equation of state and to a four?term virial coefficient
	expansion.},
  doi = {10.1063/1.1699114},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@TECHREPORT{meyeretal2004,
  author = {Meyer, P. and Ye, M. and Neuman, S. and Cantrell, K.},
  title = {Combined estimation of hydrogeologic conceptual model and parameter
	uncertainty},
  institution = {US Nuclear Regulatory Commission},
  year = {2004},
  type = {Report NUREG/CR-6843 PNNL-14534},
  address = {Washington, US},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{meyeretal2007,
  author = {Meyer, P. and Ye, M. and Rockhold, M. and Neuman, S. and Cantrell,
	K.},
  title = {Combined estimation of hydrogeologic conceptual model parameter and
	scenario uncertainty with application to uranium transport at the
	{H}anford {S}ite 300 area},
  institution = {US Nuclear Regulatory Commission},
  year = {2007},
  type = {Report NUREG/CR-6940 PNNL-16396},
  address = {Washington, US},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{meyus2000,
  author = {Meyus, Y. and Batelaan, O. and {De Smedt}, F.},
  title = {Concept {V}laams {G}rondwater {M}odel ({VGM}): {T}echnicsh concept
	van het {VGM}; deelraport 1: Hydrogeologische codering van de ondergrond
	van {V}laanderen ({HCOV}) ({T}echnical concept of the {F}lemish groundwtaer
	model: {R}eport 1: {H}ydrogeological coding of the subsoil of {F}landers)
	[{I}n {D}utch]},
  institution = {AMINAL, afedeling WATER},
  year = {2000},
  address = {Belgium},
  owner = {RRojas},
  pages = {--},
  publisher = {AMINAL, afdeling Water},
  refid = {MEYUS2000},
  timestamp = {2008.11.04}
}

@ARTICLE{middelkoop+al2001,
  author = {Middelkoop, H. and Daamen, K. and Gellens, D. and Grabs, W. and Kwadijk,
	J. and Lang, H. and Parmet, B. and Schadler, B. and Schulla, J. and
	Wilke, K.},
  title = {Impact of climate change on hydrological regimes and water resources
	management in the {R}hine basin},
  journal = {Climatic Change},
  year = {2001},
  volume = {49},
  pages = {105--128},
  number = {1--2},
  abstract = {The International Commission for the Hydrology of the Rhine basin
	(CHR) has carried out a research project to assess the impact of
	climate change on the river flow conditions in the Rhine basin. Along
	a bottom-up line, different detailed hydrological models with hourly
	and daily time steps have been developed for representative sub-catchments
	of the Rhine basin. Along a top-down line, a water balance model
	for the entire Rhine basin has been developed, which calculates monthly
	discharges and which was tested on the scale of the major tributaries
	of the Rhine. Using this set of models, the effects of climate change
	on the discharge regime in different parts of the Rhine basin were
	calculated using the results of UKHI and XCCC GCM-experiments. All
	models indicate the same trends in the changes: higher winter discharge
	as a result of intensified snow-melt and increased winter precipitation,
	and lower summer discharge due to the reduced winter snow storage
	and an increase of evapotranspiration. When the results are considered
	in more detail, however, several differences show up. These can firstly
	be attributed to different physical characteristics of the studied
	areas, but different spatial and temporal scales used in the modelling
	and different representations of several hydrological processes (e.g.,
	evapotranspiration, snow melt) are responsible for the differences
	found as well. Climate change can affect various socio-economic sectors.
	Higher temperatures may threaten winter tourism in the lower winter
	sport areas. The hydrological changes will increase flood risk during
	winter, whilst low flows during summer will adversely affect inland
	navigation, and reduce water availability for agriculture and industry.
	Balancing the required actions against economic cost and the existing
	uncertainties in the climate change scenarios, a policy of 'no-regret
	and flexibility' in water management planning and design is recommended,
	where anticipatory adaptive measures in response to climate change
	impacts are undertaken in combination with ongoing activities.},
  doi = {10.1023/A:1010784727448},
  keywords = {RIVER RHINE, CATCHMENTS, SCENARIOS, DISCHARGE, EUROPEEN, MODELS, CO2},
  tags = {Impacts}
}

@ARTICLE{mimikou+al2000,
  author = {Mimikou, M. and Baltas, E. and Varanou, E. and Pantazis, K.},
  title = {Regional impacts of climate change on water resources quantity and
	quality indicators},
  journal = {Journal of Hydrology},
  year = {2000},
  volume = {234},
  pages = {95--109},
  number = {1--2},
  abstract = {The aim of this paper is to assess the impacts of climate change on
	water resources (surface runoff) and on water quality. Two GCM-based
	climate change scenarios are considered: transient (HadCM2) and equilibrium
	(UKHI). A conceptual, physically based hydrological model (WBUDG)
	is applied on a catchment in central Greece, simulating the effect
	of the two climate scenarios on average monthly runoff. A newly developed
	in the stream model (R-Qual) is applied in order to simulate water
	quality downstream of a point source under current and climatically
	changed conditions. Simulated parameters include monthly concentrations
	of BOD, DO and NH4+. Both scenarios suggest increase of temperature
	and decrease of precipitation in the study region. Those changes
	result in a significant decrease of mean monthly runoff for almost
	all months with a considerable negative impact on summer drought.
	Moreover, quality simulations under future climatic conditions entail
	significant water quality impairments because of decreased stream
	flows},
  doi = {10.1016/S0022-1694(00)00244-4},
  tags = {Impacts}
}

@ARTICLE{min+al2007,
  author = {Min, {S.-K.} and Hense, A.},
  title = {Hierarchical evaluation of {IPCC} {AR4} coupled climate models with
	systematic consideration of model uncertainties},
  journal = {Climate Dynamics},
  year = {2007},
  volume = {29},
  pages = {853--868},
  number = {7--8},
  abstract = {The capability of reproducing observed surface air temperature (SAT)
	changes for the twentieth century is assessed using 22 multi-models
	which contribute to the Intergovernmental Panel on Climate Change
	Fourth Assessment Report. A Bayesian method is utilized for model
	evaluation by which model uncertainties are considered systematically.
	We provide a hierarchical analysis for global to sub-continental
	regions with two settings. First, regions of different size are evaluated
	separately at global, hemispheric, continental, and sub-continental
	scales. Second, the global SAT trend patterns are evaluated with
	gradual refinement of horizontal scales (higher dimensional analysis).
	Results show that models with natural plus anthropogenic forcing
	(MME_ALL) generally exhibit better skill than models with anthropogenic
	only forcing (MME_ANTH) at all spatial scales for different trend
	periods (entire twentieth century and its first and second halves).
	This confirms previous studies that suggest the important role of
	natural forcing. For the second half of the century, we found that
	MME_ANTH performs well compared to MME_ALL except for a few models
	with overestimated warming. This indicates not only major contributions
	of anthropogenic forcing over that period but also the applicability
	of both MMEs to observationally-constrained future predictions of
	climate changes. In addition, the skill-weighted averages with the
	Bayes factors [Bayesian model averaging (BMA)] show a general superiority
	over other error-based weighted averaging methods, suggesting a potential
	advantage of BMA for climate change predictions.},
  doi = {10.1007/s00382-007-0269-2},
  tags = {Uncertainty, Thesis, Climate Change}
}

@ARTICLE{mishra+al2009,
  author = {Mishra, A. and Singh, V. and Desai, V.},
  title = {Drought characterization: a probabilistic approach},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2009},
  volume = {23},
  pages = {41--55},
  abstract = {Using the alternative renewable process and run theory, this study
	investigates the distribution of drought interval time, mean drought
	interarrival time, joint probability density function and transition
	probabilities of drought events in the Kansabati River basin in India.
	The standardized precipitation index series is employed in the investigation.
	The time interval of SPI is found to have a significant effect of
	the probabilistic characteristics of drought.},
  affiliation = {McMaster University Department of Civil Engineering 1280 Main Street
	West Hamilton ON Canada L8S4L7},
  doi = {10.1007/s00477-007-0194-2},
  issn = {1436-3240},
  issue = {1},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{mishrasingh2011,
  author = {Ashok K. Mishra and Vijay P. Singh},
  title = {Drought modeling – A review},
  journal = {Journal of Hydrology},
  year = {2011},
  volume = {403},
  pages = {157--175},
  number = {1--2},
  abstract = {Summary In recent years droughts have been occurring frequently, and
	their impacts are being aggravated by the rise in water demand and
	the variability in hydro-meteorological variables due to climate
	change. As a result, drought hydrology has been receiving much attention.
	A variety of concepts have been applied to modeling droughts, ranging
	from simplistic approaches to more complex models. It is important
	to understand different modeling approaches as well as their advantages
	and limitations. This paper, supplementing the previous paper (Mishra
	and Singh, 2010) where different concepts of droughts were highlighted,
	reviews different methodologies used for drought modeling, which
	include drought forecasting, probability based modeling, spatio-temporal
	analysis, use of Global Climate Models (GCMs) for drought scenarios,
	land data assimilation systems for drought modeling, and drought
	planning. It is found that there have been significant improvements
	in modeling droughts over the past three decades. Hybrid models,
	incorporating large scale climate indices, seem to be promising for
	long lead-time drought forecasting. Further research is needed to
	understand the spatio-temporal complexity of droughts under climate
	change due to changes in spatio-temporal variability of precipitation.
	Applications of copula based models for multivariate drought characterization
	seem to be promising for better drought characterization. Research
	on decision support systems should be advanced for issuing warnings,
	assessing risk, and taking precautionary measures, and the effective
	ways for the flow of information from decision makers to users need
	to be developed. Finally, some remarks are made regarding the future
	outlook for drought research.},
  doi = {10.1016/j.jhydrol.2011.03.049},
  issn = {0022-1694},
  keywords = {Drought forecasting}
}

@ARTICLE{mishrasingh2010,
  author = {Ashok K. Mishra and Vijay P. Singh},
  title = {A review of drought concepts},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {391},
  pages = {202--216},
  number = {1--2},
  abstract = {Summary Owing to the rise in water demand and looming climate change,
	recent years have witnessed much focus on global drought scenarios.
	As a natural hazard, drought is best characterized by multiple climatological
	and hydrological parameters. An understanding of the relationships
	between these two sets of parameters is necessary to develop measures
	for mitigating the impacts of droughts. Beginning with a discussion
	of drought definitions, this paper attempts to provide a review of
	fundamental concepts of drought, classification of droughts, drought
	indices, historical droughts using paleoclimatic studies, and the
	relation between droughts and large scale climate indices. Conclusions
	are drawn where gaps exist and more research needs to be focussed.},
  doi = {10.1016/j.jhydrol.2010.07.012},
  issn = {0022-1694},
  keywords = {Definitions}
}

@ARTICLE{mishra+al2012,
  author = {Mishra, V. and Dominguez, F. and Lettenmaier, D.},
  title = {{Urban precipitation extremes: How relieable are regional climate
	models?}},
  journal = {Geophysical Research Letters},
  year = {2012},
  volume = {39},
  number = {L03407},
  abstract = {We evaluate the ability of regional climate models (RCMs) that participated
	in the North American Regional Climate Change Assessment Program
	(NARCCAP) to reproduce the historical season of occurrence, mean,
	and variability of 3 and 24-hour precipitation extremes for 100 urban
	areas across the United States. We show that RCMs with both reanalysis
	and global climate model (GCM) boundary conditions behave similarly
	and underestimate 3-hour precipitation maxima across almost the entire
	U.S. RCMs with both boundary conditions broadly capture the season
	of occurrence of precipitation maxima except in the interior of the
	western U.S. and the southeastern U.S. On the other hand, the RCMs
	do much better in identifying the season of 24-hour precipitation
	maxima. For mean annual precipitation maxima, regardless of the boundary
	condition, RCMs consistently show high (low) bias for locations in
	the western (eastern) U.S. Our results indicate that RCM-simulated
	3-hour precipitation maxima at 100-year return period could be considered
	acceptable for stormwater infrastructure design at less than 12%
	of the 100 urban areas (regardless of boundary conditions). RCM performance
	for 24-hour precipitation maxima was slightly better, with performance
	acceptable for stormwater infrastructure design judged adequate at
	about 25% of the urban areas.},
  doi = {10.1029/2011GL050658},
  owner = {rojasro},
  timestamp = {2012.05.11}
}

@TECHREPORT{mitchell+al2004,
  author = {Mitchell, T. and Carter, T. and Jones, P. and Hulme, M. and New,
	M.},
  title = {{A comprehensive set of high-resolution grids of monthly climate
	for Europe and the globe: the observed record {(1901–2000) and 16
	scenarios (2001–2100)}}},
  institution = {Tyndall Centre for Climate Research},
  year = {2004},
  address = {Norwich, UK},
  owner = {rojasro},
  timestamp = {2011.01.12}
}

@ARTICLE{demoel+al2011,
  author = {de Moel, H. and Aerts, J.},
  title = {Effect of uncertainty in land use, damage models and inundation depth
	on flood damage estimates},
  journal = {Natural Hazards},
  year = {2011},
  volume = {58},
  pages = {407--425},
  abstract = {With the recent transition to a more risk-based approach in flood
	management, flood risk models—being a key component in flood risk
	management—are becoming increasingly important. Such models combine
	information from four components: (1) the flood hazard (mostly inundation
	depth), (2) the exposure (e.g. land use), (3) the value of elements
	at risk and (4) the susceptibility of the elements at risk to hydrologic
	conditions (e.g. depth–damage curves). All these components contain,
	however, a certain degree of uncertainty which propagates through
	the calculation and accumulates in the final damage estimate. In
	this study, an effort has been made to assess the influence of uncertainty
	in these four components on the final damage estimate. Different
	land-use data sets and damage models have been used to represent
	the uncertainties in the exposure, value and susceptibility components.
	For the flood hazard component, inundation depth has been varied
	systematically to estimate the sensitivity of flood damage estimations
	to this component. The results indicate that, assuming the uncertainty
	in inundation depth is about 25 cm (about 15% of the mean inundation
	depth), the total uncertainty surrounding the final damage estimate
	in the case study area can amount to a factor 5–6. The value of elements
	at risk and depth–damage curves are the most important sources of
	uncertainty in flood damage estimates and can both introduce about
	a factor 2 of uncertainty in the final damage estimates. Very large
	uncertainties in inundation depth would be necessary to have a similar
	effect on the uncertainty of the final damage estimate, which seem
	highly unrealistic. Hence, in order to reduce the uncertainties surrounding
	potential flood damage estimates, these components deserve prioritisation
	in future flood damage research. While absolute estimates of flood
	damage exhibit considerable uncertainty (the above-mentioned factor
	5–6), estimates for proportional changes in flood damages (defined
	as the change in flood damages as a percentage of a base situation)
	are much more robust.},
  affiliation = {Institute for Environmental Studies, VU University Amsterdam, De Boelelaan
	1085, 1081 HV Amsterdam, The Netherlands},
  doi = {10.1007/s11069-010-9675-6},
  issn = {0921-030X},
  issue = {1},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Netherlands}
}

@ARTICLE{Moel2012,
  author = {de Moel, H. and Asselman, N. E. M. and Aerts, J. C. J. H.},
  title = {Uncertainty and sensitivity analysis of coastal flood damage estimates
	in the west of the Netherlands},
  journal = {Natural Hazards and Earth System Science},
  year = {2012},
  volume = {12},
  pages = {1045--1058},
  number = {4},
  doi = {10.5194/nhess-12-1045-2012}
}

@ARTICLE{montanari2007,
  author = {Montanari, A},
  title = {What do we mean by "uncertainty"? The need for a consistent wording
	about uncertainty assessment in hydrology},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {841--845},
  number = {6},
  month = {March},
  doi = {10.1002/hyp.6623},
  owner = {rojasro},
  timestamp = {2009.09.22}
}

@ARTICLE{montanari2005,
  author = {Montanari, A.},
  title = {Large sample behaviors of the generalized likelihood uncertainty
	estimation ({GLUE}) in assessing the uncertainty of rainfall--runoff
	simulations},
  journal = {Water Resources Research},
  year = {2005},
  volume = {41},
  pages = {W08406},
  abstract = {Several methods have been recently proposed for quantifying the uncertainty
	of hydrological models. These techniques are based upon different
	hypotheses, are diverse in nature, and produce outputs that can significantly
	differ in some cases. One of the favored methods for uncertainty
	assessment in rainfall-runoff modeling is the generalized likelihood
	uncertainty estimation (GLUE). However, some fundamental questions
	related to its application remain unresolved. One such question is
	that GLUE relies on some explicit and implicit assumptions, and it
	is not fully clear how these may affect the uncertainty estimation
	when referring to large samples of data. The purpose of this study
	is to address this issue by assessing how GLUE performs in detecting
	uncertainty in the simulation of long series of synthetic river flows.
	The study aims to (1) discuss the hypotheses underlying GLUE and
	derive indications about their effects on the uncertainty estimation,
	and (2) compare the GLUE prediction limits with a large sample of
	data that is to be simulated in the presence of known sources of
	uncertainty. The analysis shows that the prediction limits provided
	by GLUE do not necessarily include a percentage close to their confidence
	level of the observed data. In fact, in all the experiments, GLUE
	underestimates the total uncertainty of the simulation provided by
	the hydrological model},
  doi = {10.1029/2004WR003826},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{montanari2012,
  author = {Montanari, A. and Koutsoyiannis, D.},
  title = {A blueprint for process-based modeling of uncertaint hydrological
	systems},
  journal = {Water Resources Research},
  year = {2012},
  volume = {48},
  pages = {W09555},
  abstract = {We present a probability based theoretical scheme for building process-based
	models of uncertain hydrological systems, thereby unifying hydrological
	modeling and uncertainty assessment. Uncertainty for the model output
	is assessed by estimating the related probability distribution via
	simulation, thus shifting from one to many applications of the selected
	hydrological model. Each simulation is performed after stochastically
	perturbing input data, parameters and model output, this latter by
	adding random outcomes from the population of the model error, whose
	probability distribution is conditioned on input data and model parameters.
	Within this view randomness, and therefore uncertainty, is treated
	as an inherent property of hydrological systems. We discuss the related
	assumptions as well as the open research questions. The theoretical
	framework is illustrated by presenting real-world and synthetic applications.
	The relevant contribution of this study is related to proposing a
	statistically consistent simulation framework for uncertainty estimation
	which does not require model likelihood computation and simplification
	of the model structure. The results show that uncertainty is satisfactorily
	estimated although the impact of the assumptions could be significant
	in conditions of data scarcity.},
  doi = {10.1029/2011WR011412},
  owner = {rojasro},
  timestamp = {2012.10.11}
}

@ARTICLE{montgomery2003,
  author = {Montgomery, E. and Rosko, M. and Castro, S. and Keller, B. and Bevacqua,
	P.},
  title = {Interbasin underflow between closed altiplano basins in {Chile}},
  journal = {Ground Water},
  year = {2003},
  volume = {41},
  pages = {523--531},
  number = {4},
  abstract = {Interbasin ground water movement of 200 to 240 L/sec occurs as underflow
	beneath a mountainous surface water divide separating the topographically
	higher Salar de Michincha from the topographically lower Salar de
	Coposa internally drained basins in the Altiplano of northern Chile.
	Salt-encrusted flats (salars) and saline lakes occur on the lowest
	parts of the basin floors and comprise the principal evaporative
	discharge areas for the basins. Because a surface water divide separates
	the basins, surface water drainage boundaries do not coincide with
	ground water drainage boundaries. In the region, interbasin ground
	water movement is usually not recognized, but occurs for selected
	basins, and at places is an important component of ground water budgets.
	With increasing development of water for mining industry and potential
	exportation of ground water from the Altiplano for use at coastal
	cities, demonstration and quantification of interbasin movement is
	important for assessment of sustainable ground water development
	in a region of extreme aridity. Recognition and quantification of
	interbasin ground water underflow will assist in management of ground
	water resources in the arid Chilean Altiplano environment.},
  doi = {10.1111/j.1745-6584.2003.tb02386.x},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{moore2006,
  author = {Moore, C. and Doherty, J.},
  title = {The cost of uniqueness in groundwater model calibration},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {605--623},
  number = {4},
  abstract = {Calibration of a groundwater model requires that hydraulic properties
	be estimated throughout a model domain. This generally constitutes
	an underdetermined inverse problem, for which a solution can only
	be found when some kind of regularization device is included in the
	inversion process. Inclusion of regularization in the calibration
	process can be implicit, for example through the use of zones of
	constant parameter value, or explicit, for example through solution
	of a constrained minimization problem in which parameters are made
	to respect preferred values, or preferred relationships, to the degree
	necessary for a unique solution to be obtained. The “cost of uniqueness”
	is this: no matter which regularization methodology is employed,
	the inevitable consequence of its use is a loss of detail in the
	calibrated field. This, in turn, can lead to erroneous predictions
	made by a model that is ostensibly “well calibrated”. Information
	made available as a by-product of the regularized inversion process
	allows the reasons for this loss of detail to be better understood.
	In particular, it is easily demonstrated that the estimated value
	for an hydraulic property at any point within a model domain is,
	in fact, a weighted average of the true hydraulic property over a
	much larger area. This averaging process causes loss of resolution
	in the estimated field. Where hydraulic conductivity is the hydraulic
	property being estimated, high averaging weights exist in areas that
	are strategically disposed with respect to measurement wells, while
	other areas may contribute very little to the estimated hydraulic
	conductivity at any point within the model domain, this possibly
	making the detection of hydraulic conductivity anomalies in these
	latter areas almost impossible. A study of the post-calibration parameter
	field covariance matrix allows further insights into the loss of
	system detail incurred through the calibration process to be gained.
	A comparison of pre- and post-calibration parameter covariance matrices
	shows that the latter often possess a much smaller spectral bandwidth
	than the former. It is also demonstrated that, as an inevitable consequence
	of the fact that a calibrated model cannot replicate every detail
	of the true system, model-to-measurement residuals can show a high
	degree of spatial correlation, a fact which must be taken into account
	when assessing these residuals either qualitatively, or quantitatively
	in the exploration of model predictive uncertainty. These principles
	are demonstrated using a synthetic case in which spatial parameter
	definition is based on pilot points, and calibration is implemented
	using both zones of piecewise constancy and constrained minimization
	regularization.},
  doi = {10.1016/j.advwatres.2005.07.003},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{moore2005,
  author = {Moore, C. and Doherty, J.},
  title = {Role of the calibration process in reducing model predictive error},
  journal = {Water Resources Research},
  year = {2005},
  volume = {41},
  pages = {W05020},
  abstract = {An equation is derived through which the variance of predictive error
	of a calibrated model can be calculated. This equation has two terms.
	The first term represents the contribution to predictive error variance
	that results from an inability of the calibration process to capture
	all of the parameterization detail necessary for the making of an
	accurate prediction. If a model is “uncalibrated,” with parameter
	values being supplied solely through “outside information,” this
	is the only term required. The second term represents the contribution
	to predictive error variance arising from measurement noise. In an
	overdetermined system, such as that which may be obtained through
	“parameter lumping” (e.g., through the introduction of a spatial
	zonation scheme), this is the only term required. It is shown, however,
	that parameter lumping is a form of “implicit regularization” and
	that ignoring the implied first term of the predictive error variance
	equation can potentially lead to underestimation of predictive error
	variance. A model's role as a predictor of environmental behavior
	can be enhanced if it is calibrated in such a way as to reduce the
	variance of those predictions which it is required to make. It is
	shown that in some circumstances this can be accomplished through
	“overfitting” against historical field data. It can also be accomplished
	by giving greater weight to those measurements which carry the greatest
	information content with respect to a required prediction. This suggests
	that a departure may be necessary from the custom of using a single
	“calibrated model” for the making of many different predictions.
	Instead, model calibration may need to be repeated many times so
	that in each case the calibration process is optimized for the making
	of a specific model prediction.},
  doi = {10.1029/2004WR003501},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{moradkhanil2008,
  author = {Moradkhani, H. and Hsu, K. and Sorooshian, S.},
  title = {How to improve the hydrologic predictability: {F}rom {B}ayesian model
	averaging to sequential {B}ayesian model combination},
  journal = {Geophysical Research Abstracts},
  year = {2008},
  volume = {10},
  pages = {1--2},
  number = {EGU2008-A-11553},
  owner = {RRojas},
  timestamp = {2009.02.19},
  url = {http://meetings.copernicus.org/www.cosis.net/abstracts/EGU2008/11553/EGU2008-A-11553.pdf}
}

@ARTICLE{mori2012,
  author = {Koichiro Mori and Charles Perrings},
  title = {Optimal management of the flood risks of floodplain development},
  journal = {Science of The Total Environment},
  year = {2012},
  volume = {431},
  pages = {109--121},
  number = {0},
  abstract = {This paper presents a model of the problem on floodplain development,
	exploring the conditions that are both necessary and sufficient for
	development to be optimal. The model is calibrated for a particular
	catchment, the Ouse catchment in the United Kingdom, and is used
	both to estimate the expected impact of floodplain development and
	to explore the impact of alternative policy instruments. We find
	that the use of price-based instruments that signal the expected
	flood damage cost of floodplain development has the potential to
	lead to outcomes close to the social optimum. The finding is robust
	to two types of uncertainty: model error about the relation between
	precipitation and flood-risk and measurement error about the benefits
	of developed floodplains.},
  doi = {10.1016/j.scitotenv.2012.04.076},
  issn = {0048-9697},
  keywords = {Floodplain development}
}

@ARTICLE{moriasi+al2007,
  author = {Moriasi, D. and Arnold, J. and {Van Liew}, M. and Binger, R. and
	Harmel, R. and Veith, T.},
  title = {Model evaluation guidelines for systematic quantification of accuracy
	in watershed simulations },
  journal = {Transactions of the ASABE},
  year = {2007},
  volume = {50},
  pages = {885--900},
  number = {3},
  abstract = {Watershed models are powerful tools for simulating the effect of watershed
	processes and management on soil and water resources. However, no
	comprehensive guidance is available to facilitate model evaluation
	in terms of the accuracy of simulated data compared to measured flow
	and constituent values. Thus, the objectives of this research were
	to: (1) determine recommended model evaluation techniques (statistical
	and graphical), (2) review reported ranges of values and corresponding
	performance ratings for the recommended statistics, and (3) establish
	guidelines for model evaluation based on the review results and project-specific
	considerations; all of these objectives focus on simulation of streamflow
	and transport of sediment and nutrients. These objectives were achieved
	with a thorough review of relevant literature on model application
	and recommended model evaluation methods. Based on this analysis,
	we recommend that three quantitative statistics, Nash-Sutcliffe efficiency
	(NSE), percent bias (PBIAS), and ratio of the root mean square error
	to the standard deviation of measured data (RSR), in addition to
	the graphical techniques, be used in model evaluation. The following
	model evaluation performance ratings were established for each recommended
	statistic. In general, model simulation can be judged as satisfactory
	if NSE > 0.50 and RSR < 0.70, and if PBIAS + 25\% for streamflow,
	PBIAS + 55\% for sediment, and PBIAS + 70\% for N and P. For PBIAS,
	constituent-specific performance ratings were determined based on
	uncertainty of measured data. Additional considerations related to
	model evaluation guidelines are also discussed. These considerations
	include: single-event simulation, quality and quantity of measured
	data, model calibration procedure, evaluation time step, and project
	scope and magnitude. A case study illustrating the application of
	the model evaluation guidelines is also provided. },
  bibkey = {SWAT; Nash-Sutcliffe; NSE},
  tags = {Calibration, Goodness-of-Fit},
  url = {http://asae.frymulti.com/abstract.asp?aid=23153&t=1}
}

@ARTICLE{morris1991,
  author = {Morris, M.},
  title = {Factorial sampling plans for preliminary computational experiments},
  journal = {Technometrics},
  year = {1991},
  volume = {33},
  pages = {161--174},
  number = {2},
  abstract = {A computational model is a representation of some physical or other
	system of interest, first expressed mathematically and then implemented
	in the form of a computer program; it may be viewed as a function
	of inputs that, when evaluated, produces outputs. Motivation for
	this article comes from computational models that are deterministic,
	complicated enough to make classical mathematical analysis impractical
	and that have a moderate-to-large number of inputs. The problem of
	designing computational experiments to determine which inputs have
	important effects on an output is considered. The proposed experimental
	plans are composed of individually randomized one-factor-at-a-time
	designs, and data analysis is based on the resulting random sample
	of observed elementary effects, those changes in an output due solely
	to changes in a particular input. Advantages of this approach include
	a lack of reliance on assumptions of relative sparsity of important
	inputs, monotonicity of outputs with respect to inputs, or adequacy
	of a low-order polynomial as an approximation to the computational
	model.},
  owner = {rojasro},
  timestamp = {2011.10.10},
  url = {http://www.jstor.org/stable/1269043}
}

@ARTICLE{morseetal2003,
  author = {Morse, B. and Pohll, G. and Huntington, J. and Rodriguez, R.},
  title = {Stochastic capture zone analysis of an arsenic--contaminated well
	using the generalized likelihood uncertainty estimator ({GLUE}) methodology},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1151--1163},
  number = {6},
  abstract = {In 1992, Mexican researchers discovered concentrations of arsenic
	in excess of World Heath Organization (WHO) standards in several
	municipal wells in the Zimapan Valley of Mexico. This study describes
	a method to delineate a capture zone for one of the most highly contaminated
	wells to aid in future well siting. A stochastic approach was used
	to model the capture zone because of the high level of uncertainty
	in several input parameters. Two stochastic techniques were performed
	and compared: “standard” Monte Carlo analysis and the generalized
	likelihood uncertainty estimator (GLUE) methodology. The GLUE procedure
	differs from standard Monte Carlo analysis in that it incorporates
	a goodness of fit (termed a likelihood measure) in evaluating the
	model. This allows for more information (in this case, head data)
	to be used in the uncertainty analysis, resulting in smaller prediction
	uncertainty. Two likelihood measures are tested in this study to
	determine which are in better agreement with the observed heads.
	While the standard Monte Carlo approach does not aid in parameter
	estimation, the GLUE methodology indicates best fit models when hydraulic
	conductivity is approximately 10?6.5 m/s, with vertically isotropic
	conditions and large quantities of interbasin flow entering the basin.
	Probabilistic isochrones (capture zone boundaries) are then presented,
	and as predicted, the GLUE-derived capture zones are significantly
	smaller in area than those from the standard Monte Carlo approach.},
  doi = {10.1029/2002WR001470},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{moss+al2010,
  author = {Moss, R. and Edmonds, J. and Hibbard, K. and Manning, M. and Rose,
	S. and {van Vuuren}, D. and Carter, T. and Emori, S. and Kainuma,
	M. and Kram, T. and Meehl, G. and Mitchell, J. and Nakicenovic, N.
	and Riahi, K. and Smith, S. and Stouffer, R. and Thomson, A. and
	Weyant, J. and Wilbanks, T.},
  title = {The next generation of scenarios for climate change research and
	assessment},
  journal = {Nature},
  year = {2010},
  volume = {463},
  pages = {747--756},
  number = {7282},
  abstract = {Advances in the science and observation of climate change are providing
	a clearer understanding of the inherent variability of Earthâ€™s
	climate system and its likely response to human and natural influences.
	The implications of climate change for the environment and society
	will depend not only on the response of the Earth system to changes
	in radiative forcings, but also on how humankind responds through
	changes in technology, economies, lifestyle and policy. Extensive
	uncertainties exist in future forcings of and responses to climate
	change, necessitating the use of scenarios of the future to explore
	the potential consequences of different response options. To date,
	such scenarios have not adequately examined crucial possibilities,
	such as climate change mitigation and adaptation, and have relied
	on research processes that slowed the exchange of information among
	physical, biological and social scientists. Here we describe a new
	process for creating plausible scenarios to investigate some of the
	most challenging and important questions about climate change confronting
	the global community.},
  doi = {10.1038/nature08823},
  pmid = {20148028},
  tags = {Scenarios}
}

@ARTICLE{moussa2010,
  author = {Moussa, R.},
  title = {When monstrosity can be beautiful while normality can be ugly: assessing
	the performance of event-based flood models},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {1074--1084},
  number = {6},
  abstract = {The significance of the Nash-Sutcliffe efficiency (NSE) is analysed
	and a new method to assess the performance of flood event models
	proposed. The focus is on the specific cases of events that are difficult
	to model and characterized by low NSE values, which we call {``}monsters{''}.
	The properties of the NSE were analysed as a function of the calculated
	hydrograph shape and of the benchmark reference model. The results
	show that a {``}monster{''} can be due solely to a simple lag translation
	or a homothetic ratio which reproduces the dynamic of the hydrograph,
	with acceptable errors on other criteria. In the opposite case, good
	simulations characterized by NSE close to 1 can become {``}monsters{''}
	if the average observed discharge used as a benchmark reference model
	in the NSE is modified. Finally, a multi-criteria analysis method
	to assess the model performance on each event is proposed and applied
	on the Gardon d'Anduze catchment, in France. },
  doi = {10.1080/02626667.2010.505893},
  keywords = {flood event model, performance criteria function, Nash-Sutcliffe efficiency,
	multi-criteria performance analysis },
  tags = {Goodness-of-Fit, Calibration}
}

@ARTICLE{moussachahinian2009,
  author = {Moussa, R. and Chahinian, N.},
  title = {Comparison of different multi-objective calibration criteria using
	a conceptual rainfall-runoff model of flood events},
  journal = {Hydrology and Earth System Sciences},
  year = {2009},
  volume = {13},
  pages = {519--535},
  abstract = {A conceptual lumped rainfall-runoff flood event model was developed
	and applied on the Gardon catchment located in Southern France and
	various single-objective and multi-objective functions were used
	for its calibration. The model was calibrated on 15 events and validated
	on 14 others. The results of both the calibration and validation
	phases are compared on the basis of their performance with regards
	to six criteria, three global criteria and three relative criteria
	representing volume, peakflow, and the root mean square error. The
	first type of criteria gives more weight to large events whereas
	the second considers all events to be of equal weight. The results
	show that the calibrated parameter values are dependent on the type
	of criteria used. Significant trade-offs are observed between the
	different objectives: no unique set of parameters is able to satisfy
	all objectives simultaneously. Instead, the solution to the calibration
	problem is given by a set of Pareto optimal solutions. From this
	set of optimal solutions, a balanced aggregated objective function
	is proposed, as a compromise between up to three objective functions.
	The single-objective and multi-objective calibration strategies are
	compared both in terms of parameter variation bounds and simulation
	quality. The results of this study indicate that two well chosen
	and non-redundant objective functions are sufficient to calibrate
	the model and that the use of three objective functions does not
	necessarily yield different results. The problems of non-uniqueness
	in model calibration, and the choice of the adequate objective functions
	for flood event models, emphasise the importance of the modeller's
	intervention. The recent advances in automatic optimisation techniques
	do not minimise the user's responsibility, who has to choose multiple
	criteria based on the aims of the study, his appreciation on the
	errors induced by data and model structure and his knowledge of the
	catchment's hydrology.},
  doi = {10.5194/hess-13-519-2009},
  keywords = {MULTICRITERIA OPTIMIZATION, AUTOMATIC CALIBRATION, HYDROLOGIC-MODELS,
	MULTIPLE OBJECTIVES, GLOBAL OPTIMIZATION, CATCHMENT, UNCERTAINTY,
	ALGORITHMS, EFFICIENT, VALIDATION},
  tags = {Calibration}
}

@ARTICLE{moussa+al2007,
  author = {Moussa, R. and Chahinianb, N. and Bocquillon, C.},
  title = {Distributed hydrological modelling of a {M}editerranean mountainous
	catchment - Model construction and multi-site validation},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {337},
  pages = {35--51},
  number = {1-2},
  abstract = {A multi-site validation approach is necessary to further constrain
	distributed hydrological models. Such an approach has been tested
	on the Gardon catchment located in the mountainous Mediterranean
	zone of southern France using data gathered over a 10 year period
	on nine internal subcatchments. A spatially distributed hydrological
	model linked to a Geographical Information System, was developed
	on the basis of simplified physical process representations (infiltration,
	evapotranspiration, base flow, interflow, overland flow, channel
	routing), using conventional hydro-meteorological data and readily
	accessible geographical maps. The model parameters were estimated
	from a Digital Elevation Model, soil and land-use maps; and only
	five parameters were calibrated for the whole catchment. Three procedures
	with different levels of calibration and validation were conducted
	at a daily time step, and the results of both calibration and validation
	were compared on the basis of their performance with regards to objective
	criteria representing the water balance, the Nash and Sutcliffe efficiency
	and the correlation coefficient. The first application corresponds
	to the case of an ungauged catchment i.e. a simple application of
	the model without calibration. In the second application, the model
	was calibrated using discharge values measured at the outlet on the
	first five year period and validated using data from intermediate
	gauging stations and on the remaining period at the outlet. In the
	third application, a multi-site calibration and validation was conducted
	simultaneously for all available stations using the first five year
	period and validated on the second five year period for all stations.
	Calibration against the outlet station and internal validation against
	eight additional stations revealed some short-comings for a few upstream
	tributaries. Further calibration against additional discharge stations
	improved the model{'}s performance at the subcatchment level. These
	different calibration and validation tests challenge the predictive
	capability of the model both at the catchment and subcatchment level
	and hence illustrate the model{'}s possible improvements (structure,
	data and parameterisation strategy) for predictions on ungauged catchments},
  doi = {10.1016/j.jhydrol.2007.01.028},
  keywords = {Spatially distributed hydrological model, Mediterranean mountainous
	catchment, Model calibration and validation, Multi-site approach,
	Parameterisation},
  tags = {Calibration}
}

@ARTICLE{mroczkowski+al1997,
  author = {Mroczkowski,M. and Raper, G. and Kuczera, G.},
  title = {The quest for more powerful validation of conceptual catchment models},
  journal = {Water Resources Research},
  year = {1997},
  volume = {33},
  pages = {2325--2335},
  number = {10},
  abstract = {The power of a validation strategy (that is, its ability to discriminate
	between good and bad model hypotheses) depends on what kind of data
	are available and how the data are used to challenge the hypothesis.
	Several validation strategies are examined from the perspective of
	power and practical applicability. It is argued that validation using
	multiresponse data in a catchment experiencing a shift in hydrologie
	regime due to disturbance or extreme climatic inputs is a considerably
	more powerful strategy than traditional split-sample testing using
	streamflow data alone in undisturbed catchments. A case study testing
	two model hypotheses is presented using paired catchments for which
	multiple-response data in the form of streamflow, stream chloride,
	and groundwater levels were available. The first catchment, Salmon,
	was maintained as an established forest, while the second, Wights,
	was clear-felled and converted to pasture about 3 years after monitoring
	started. The hypotheses consider the same lumped hydrosalinity model
	with the first (H1) excluding a groundwater discharge zone and the
	second (H2) including it. It was found that even with three concurrent
	responses from the undisturbed Salmon catchment, H1 could not be
	rejected, leaving an important part of the model conceptualization
	unidentified. Moreover, a streamflow split-sample test for the disturbed
	Wights catchment failed to conclusively reject H1; parameters could
	be found which accurately tracked the streamflow changes following
	forest clearing yet produced erroneous simulations of responses such
	as stream chloride and groundwater storage. It was only when H1 was
	subjected to the scrutiny of three catchment responses from the disturbed
	Wights catchment that it could be rejected. This highlights the importance
	of challenging model hypotheses under the most demanding of tests,
	which, in this study, coincided with multiple-response validation
	in a disturbed catchment},
  doi = {10.1029/97WR01922},
  tags = {conceptual model, Calibration}
}

@ARTICLE{mudelsee+al2003,
  author = {Mudelsee, M. and B\"orngen, M. and Tetzlaff, G. and Gr\"unewald,
	U.},
  title = {No upward trends in the occurence of extreme floods in central Europe},
  journal = {Nature},
  year = {2003},
  volume = {425},
  pages = {166--169},
  number = {6954},
  doi = {10.1038/nature01928},
  owner = {rojasro},
  timestamp = {2011.08.01}
}

@ARTICLE{mugunthan2006,
  author = {Mugunthan, P. and Shoemaker, C.},
  title = {Assessing the impacts of parameter uncertainty for computationally
	expensive groundwater models},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W10427},
  number = {10},
  abstract = {The velocity reversal hypothesis is commonly cited as a mechanism
	for the maintenance of pool-riffle morphology. Although this hypothesis
	is based on the magnitude of mean flow parameters, recent studies
	have suggested that mean parameters are not sufficient to explain
	the dominant processes in many pool-riffle sequences. In this study,
	two- and three-dimensional models are applied to simulate flow in
	the pool-riffle sequence on Dry Creek, California, where the velocity
	reversal hypothesis was first proposed. These simulations provide
	an opportunity to evaluate the hydrodynamics underlying the observed
	reversals in near-bed and section-averaged velocity and are used
	to investigate the influence of secondary currents, the advection
	of momentum, and cross-stream flow variability. The simulation results
	support the occurrence of a reversal in mean velocity and mean shear
	stress with increasing discharge. However, the results indicate that
	the effects of flow convergence due to an upstream constriction and
	the routing of flow through the system are more significant in influencing
	pool-riffle morphology than the occurrence of a mean velocity reversal.
	The hypothesis of flow convergence routing is introduced as a more
	meaningful explanation of the mechanisms acting to maintain pool-riffle
	morphology.},
  doi = {10.1029/2005WR004391},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{muletanicklow2005,
  author = {Muleta, M. and Nicklow, J.},
  title = {Sensitivity and uncertainty analysis coupled with automatic calibration
	for a distributed watershed model},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {306},
  pages = {127--145},
  number = {1-4},
  abstract = {Distributed watershed models should pass through a careful calibration
	procedure before they are utilized as a decision making aid in the
	planning and management of water resources. Although manual approaches
	are still frequently used for calibration, the), are tedious, time
	consuming, and require experienced personnel. This paper describes
	an automatic approach for calibrating daily streamflow and daily
	sediment concentration values estimated by the US Department of Agriculture's
	distributed watershed simulation model, Soil and Water Assessment
	Tool (SWAT). The automatic calibration methodology applies a hierarchy
	of three techniques, namely screening, parameterization, and parameter
	sensitivity analysis, at the parameter identification stage of model
	calibration. The global parameter sensitivity analysis is conducted
	using a stepwise regression analysis on rank-transformed input-output
	data pairs. Latin hypercube sampling is used to generate input data
	from the assigned distributions and ranges, and parameter estimation
	is performed using genetic algorithm. The Generalized Likelihood
	Uncertainty Estimation methodology is subsequently implemented to
	investigate uncertainty of model estimates, accounting for errors
	due to model structure, input data and model parameters. To demonstrate
	their effectiveness, the parameter identification, parameter estimation,
	model verification, and uncertainty analysis techniques are applied
	to a watershed located in southern Illinois. (c) 2004 Elsevier B.V.
	All rights reserved.},
  doi = {10.1016/j.jhydrol.2004.09.005},
  keywords = {SWAT, sensitivity analysis, automatic calibration, uncertainty analysis,
	model verification, genetic algorithms, distributed watershed model,
	HYDROLOGICAL MODELS, GLUE METHODOLOGY, VALIDATION, EQUIFINALITY,
	PREDICTION, RUNOFF, OUTPUT, POINT},
  tags = {SWAT, Sensitivity Analysis, Calibration, Uncertainty}
}

@ARTICLE{murphy2000,
  author = {Murphy, J.},
  title = {Predictions of climate change over {E}urope using statistical and
	dynamical downscaling techniques},
  journal = {Journal of Climatology},
  year = {2000},
  volume = {20},
  pages = {489--501},
  number = {5},
  abstract = {Statistical and dynamical downscaling predictions of changes in surface
	temperature and precipitation for 2080-2100, relative to pre-industrial
	conditions, are compared at 976 European observing sites, for January
	and July. Two dynamical downscaling methods are considered, involving
	the use of surface temperature or precipitation simulated at the
	nearest grid point in a coupled ocean-atmosphere general circulation
	model (GCM) of resolution 300 km and a 50 km regional climate model
	(RCM) nested inside the GCM. The statistical method (STAT) is based
	on observed linear regression relationships between surface temperature
	or precipitation and a range of atmospheric predictor variables.
	The three methods are equally plausible a priori, in the sense that
	they estimate present-day natural variations with equal skill. For
	temperature, differences between the RCM and GCM predictions are
	quite small. Larger differences occur between STAT and the dynamical
	predictions. For precipitation, there is a wide spread between all
	three methods. Differences between the RCM and GCM are increased
	by the meso-scale detail present in the RCM. Uncertainties in the
	downscaling predictions are investigated by using the STAT method
	to estimate the grid point changes simulated by the GCM, based on
	regression relationships trained using simulated rather than observed
	values of the predictor and the predictand variables (i.e. STAT-SIM).
	In most areas the temperature changes predicted by STAT-SIM and the
	GCM itself are similar, indicating that the statistical relationships
	trained from present climate anomalies remain valid in the perturbed
	climate. However, STAT-SIM underestimates the surface warming in
	areas where advective predictors are important predictors of natural
	variability but not of climate change. For precipitation, STAT-SIM
	estimates the simulated changes with lower skill, especially in January
	when increases in simulated precipitation related to a moister atmosphere
	are not captured. This occurs because moisture is rarely a strong
	enough predictor of natural variability to be included in the specification
	equation. The predictor/predictand relationships found in the GCM
	do not always match those found in observations. In January, the
	link between surface and lower tropospheric temperature is too strong.
	This is also true in July, when the links between precipitation and
	various atmospheric predictors are also too strong. These biases
	represent a likely source of error in both dynamical and statistical
	downscaling predictions. For example, simulated reductions in precipitation
	over southern Europe in summer may be too large},
  doi = {10.1002/(SICI)1097-0088(200004)20:5<489::AID-JOC484>3.0.CO;2-6},
  keywords = {Climate modification, Forecasting, Statistical method, Dynamic method,
	Scale reduction, Atmospheric precipitation, Surface temperature,
	Ocean atmosphere interaction, Numerical simulation, Europe, General
	circulation models, Climate models},
  tags = {Downscaling}
}

@ARTICLE{murphy1999,
  author = {Murphy, J.},
  title = {An evaluation of statistical and dynamical techniques for downscaling
	local climate},
  journal = {Journal of Climate},
  year = {1999},
  volume = {12},
  pages = {2256--2284},
  number = {8},
  abstract = {An assessment is made of downscaling estimates of screen temperature
	and precipitation observed at 976 European stations during 1983-94.
	A statistical downscaling technique, in which local values are inferred
	from observed atmospheric predictor variables, is compared against
	two dynamical downscaling techniques, based on the use of the screen
	temperature or precipitation simulated at the nearest grid point
	in integrations of two climate models. In one integration a global
	general circulation model (GCM) is constrained to reproduce the observed
	atmospheric circulation over the period of interest, while the second
	involves a high-resolution regional climate model (RCM) nested inside
	the GCM.},
  doi = {10.1175/1520-0442(1999)012<2256:AEOSAD>2.0.CO;2},
  keywords = {GENERAL-CIRCULATION MODEL, DAILY PRECIPITATION SERIES, ATMOSPHERIC
	CIRCULATION, SURFACE CLIMATOLOGY, DAILY VARIABILITY, UNITED-STATES,
	WATER CLOUDS, SIMULATION, EUROPE, GCM},
  tags = {Downscaling}
}

@ARTICLE{murphy+al2004,
  author = {Murphy, J. and Sexton, D. and Barnett, D. and Jones, G. and Webb,
	M. and Collins, M.},
  title = {Quantification of modelling uncertainties in a large ensemble of
	climate change simulations},
  journal = {Nature},
  year = {2004},
  volume = {430},
  pages = {768--772},
  number = {7001},
  abstract = {Comprehensive global climate models(1) are the only tools that account
	for the complex set of processes which will determine future climate
	change at both a global and regional level. Planners are typically
	faced with a wide range of predicted changes from different models
	of unknown relative quality(2,3), owing to large but unquantified
	uncertainties in the modelling process(4). Here we report a systematic
	attempt to determine the range of climate changes consistent with
	these uncertainties, based on a 53-member ensemble of model versions
	constructed by varying model parameters. We estimate a probability
	density function for the sensitivity of climate to a doubling of
	atmospheric carbon dioxide levels, and obtain a 5 - 95 per cent probability
	range of 2.4-5.4degreesC. Our probability density function is constrained
	by objective estimates of the relative reliability of different model
	versions, the choice of model parameters that are varied and their
	uncertainty ranges, specified on the basis of expert advice. Our
	ensemble produces a range of regional changes much wider than indicated
	by traditional methods based on scaling the response patterns of
	an individual simulation(5,6).},
  doi = {10.1038/nature02771},
  keywords = {AVERAGING REA METHOD, SENSITIVITY, CONSTRAINTS, PROJECTIONS, PROBABILITY,
	RELIABILITY, PREDICTION, WEATHER},
  tags = {Uncertainty, Impacts, Thesis}
}

@INPROCEEDINGS{mussi+al2009,
  author = {Mussi, L. and Cagnoni, S. and Daolio, F.},
  title = {Empirical assessment of the effects of update synchronization in
	Particle Swarm Optimization},
  booktitle = {Proceedings of the 2009 AIIA Workshop on Complexity, Evolution and
	Emergent Intelligence},
  year = {2009},
  volume = {1},
  pages = {1--10},
  note = {Published on CD, isbn 978-88-903581-1-1},
  abstract = {Despite considerable popularity, the mechanisms that govern the behavior
	of Particle Swarm Optimization (PSO) are still a subject of research.
	Regarding communication between particles, for example, many authors
	have discussed the effects of swarm topology, but few have studied
	the dynamics of the information exchange among particles. In this
	paper we show that a synchronous update of the social attractors,
	which is necessary when parallel versions of PSO are implemented,
	may influence the effectiveness of the algorithm. To do so we compare
	the synchronous and asynchronous variants of PSO on a standard benchmark.
	The results show that the "global best" topology is sensitive to
	the policy update, especially in the presence of high-dimensional
	search spaces. In contrast, sparsely-connected topologies seem to
	be much less sensitive to synchronization.},
  owner = {rojasro},
  timestamp = {2011.10.20}
}

@ARTICLE{muttil2008,
  author = {Muttil, N. and Jayawardena, A.},
  title = {Shuffled Complex Evolution model calibration algorithm: enhancing
	its robustness and efficiency},
  journal = {Hydrological Processes},
  year = {2008},
  volume = {22},
  pages = {4628--4638},
  number = {23},
  abstract = {Shuffled Complex Evolution—University of Arizona (SCE-UA) has been
	used extensively and proved to be a robust and efficient global optimization
	method for the calibration of conceptual models. In this paper, two
	enhancements to the SCE-UA algorithm are proposed, one to improve
	its exploration and another to improve its exploitation of the search
	space. A strategically located initial population is used to improve
	the exploration capability and a modification to the downhill simplex
	search method enhances its exploitation capability. This enhanced
	version of SCE-UA is tested, first on a suite of test functions and
	then on a conceptual rainfall-runoff model using synthetically generated
	runoff values. It is observed that the strategically located initial
	population drastically reduces the number of failures and the modified
	simplex search also leads to a significant reduction in the number
	of function evaluations to reach the global optimum, when compared
	with the original SCE-UA. Thus, the two enhancements significantly
	improve the robustness and efficiency of the SCE-UA model calibrating
	algorithm.},
  doi = {10.1002/hyp.7082},
  tags = {SWAT, Calibration}
}

@ARTICLE{najafi+al2011,
  author = {Najafi, M. and Moradkhani, H. and Jung, I.},
  title = {Assessing the uncertainties of hydrologic model selection in climate
	change impact studies},
  journal = {Hydrologcial Processes},
  year = {2011},
  volume = {25},
  pages = {2814--2826},
  number = {18},
  abstract = {The uncertainties associated with atmosphere-ocean General Circulation
	Models (GCMs) and hydrologic models are assessed by means of multi-modelling
	and using the statistically downscaled outputs from eight GCM simulations
	and two emission scenarios. The statistically downscaled atmospheric
	forcing is used to drive four hydrologic models, three lumped and
	one distributed, of differing complexity: the Sacramento Soil Moisture
	Accounting (SAC-SMA) model, Conceptual HYdrologic MODel (HYMOD),
	Thornthwaite-Mather model (TM) and the Precipitation Runoff Modelling
	System (PRMS). The models are calibrated based on three objective
	functions to create more plausible models for the study. The hydrologic
	model simulations are then combined using the Bayesian Model Averaging
	(BMA) method according to the performance of each models in the observed
	period, and the total variance of the models. The study is conducted
	over the rainfall-dominated Tualatin River Basin (TRB) in Oregon,
	USA. This study shows that the hydrologic model uncertainty is considerably
	smaller than GCM uncertainty, except during the dry season, suggesting
	that the hydrologic model selection-combination is critical when
	assessing the hydrologic climate change impact. The implementation
	of the BMA in analysing the ensemble results is
	
	found to be useful in integrating the projected runoff estimations
	from different models, while enabling to assess the model structural
	uncertainty.},
  doi = {10.1002/hyp.8043},
  owner = {rojasro},
  timestamp = {2011.07.11}
}

@BOOK{nakicenovic+al2000,
  title = {IPCC {S}pecial {R}eport on {E}missions {S}cenarios},
  publisher = {Cambridge University Press},
  year = {2000},
  editor = {Nakicenovic, N. and Swart, R.},
  author = {Nakicenovic, N. and Swart, R.},
  address = {Cambridge, UK},
  tags = {IPCC}
}

@ARTICLE{nashsutcliffe1970,
  author = {Nash, J. and Sutcliffe, J.},
  title = {River flow forecasting through conceptual models. {P}art {I}--{A}
	discussion of principles},
  journal = {Journal of Hydrology},
  year = {1970},
  volume = {10},
  pages = {282--290},
  number = {3},
  abstract = {The principles governing the application of the conceptual model technique
	to river flow forecasting are discussed. The necessity for a systematic
	approach to the development and testing of the model is explained
	and some preliminary ideas suggested.
	
	This is the first of a series of papers which it is hoped to publish
	from time to time reporting the results of the continuing work in
	this field of the Institute of Hydrology, Wallingford, Berkshire,
	U.K.},
  doi = {10.1016/0022-1694(70)90255-6},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{nasr+al2007,
  author = {Nasr, A. and Bruen, M. and Jordan, P. and Moles, R. and Kiely, G.
	and Byrne, P.},
  title = {A comparison of {SWAT}, {HSPF} and {SHETRAN/GOPC} for modelling phosphorus
	export from three catchments in {I}reland},
  journal = {Water Research},
  year = {2007},
  volume = {41},
  pages = {1065--1073},
  number = {5},
  abstract = {Recent extensive water quality surveys in Ireland revealed that diffuse
	phosphorus (P) pollution originating from agricultural land and transported
	by runoff and subsurface flows is the primary cause of the deterioration
	of surface water quality. P transport from land to water can be described
	by mathematical models that vary in modelling approach, complexity
	and scale (plot, field and catchment). Here, three mathematical models
	(soil water and analysis tools (SWAT), hydrological simulation program-FORTRAN
	(HSPF) and systeme hydrologique Europeen TRANsport (SHETRAN)/grid
	oriented phosphorus component (GOPC)) of diffuse P pollution have
	been tested in three Irish catchments to explore their suitability
	in Irish conditions for future use in implementing the European Water
	Framework Directive. After calibrating the models, their daily flows
	and total phosphorus (TP) exports are compared and assessed. The
	HSPF model was the best at simulating the mean daily discharge while
	SWAT gave the best calibration results for daily TP loads. Annual
	TP exports for the three models and for two empirical models were
	compared with measured data. No single model is consistently better
	in estimating the annual TP export for all three catchments. (C)
	2006 Elsevier Ltd. All rights reserved.},
  doi = {10.1016/j.watres.2006.11.026},
  keywords = {phosphorus, SWAT, HSPF, SHETRAN, GOPC, RIVERS, CALIBRATION, EVAPORATION,
	MANAGEMENT},
  pmid = {17258266},
  tags = {SWAT}
}

@ARTICLE{ndiritudaniell2001,
  author = {Ndiritu, J. and Daniell, T.},
  title = {An improved genetic algorithm for rainfall-runoff model calibration
	and function optimization},
  journal = {Mathematical and Computer Modelling},
  year = {2001},
  volume = {33},
  pages = {695--706},
  abstract = {The standard binary-coded genetic algorithm (GA) has been improved
	using the three strategies of automatic search space shifting to
	achieve hill-climbing, automatic search space reduction to effect
	time-tuning, and the use of independent subpopulation searches coupled
	with shuffling to deal with the occurrence of multiple regions of
	attraction. The degrees of search space shifting and reduction are
	determined by the distribution of the best parameter values in the
	previous generations and are implemented after every specified number
	of generations. If the best parameter value in successive generations
	is clustering in a small part of the search range, a higher level
	of range reduction is used. The search shift is based on the deviation
	from the middle of the current search range of the best parameter
	values of a specified number of previous generations. With each independent
	subpopulation, a search is performed until an optimum is reached.
	Shuffling is then performed and new subpopulation search spaces are
	obtained from the shuffled subpopulations. The improved GA performs
	remarkably better than the standard GA with three global optimum
	location problems. The standard GA achieves 11\% success with the
	Hartman function and fails totally with the SIXPAR rainfall-runoff
	model calibration and the Griewank function while the improved GA
	effectively locates the global optima. Taking the number of function
	evaluations used to locate the global optimum as a measure of efficiency,
	the improved GA is about two times less efficient, three times more
	efficient, and 34 times less efficient than the shuffled complex
	evolution (SCE-UA) method for the SIXPAR rainfall-runoff model calibration,
	the Hartman function, and the Griewank function, respectively. The
	modified GA can therefore be considered effective but not always
	efficient. (C) 2001 Elsevier Science Ltd. All rights reserved.},
  doi = {10.1016/S0895-7177(00)00273-9 },
  keywords = {fine-tuning, hillclimbing, independent subpopulation searches, genetic
	algorithm, GLOBAL OPTIMIZATION, HYDROLOGICAL MODELS, VALIDATION},
  tags = {Calibration}
}

@ARTICLE{ndomba+al2008,
  author = {Ndomba, P. and Mtalo, F. and Killingtveit, A.},
  title = {{SWAT} model application in a data scarce tropical complex catchment
	in {T}anzania},
  journal = {Physics and Chemistry of the Earth},
  year = {2008},
  volume = {33},
  pages = {626--632},
  number = {8-13},
  abstract = {This study intended to validate the Soil and Water Assessment Tool
	(SWAT) model in data scarce environment in a complex tropical catchment
	in the Pangani River Basin located in northeast Tanzania. The validation
	process involved the model initialization, calibration, verification
	and sensitivity analysis. Both manual and auto-calibration procedures
	were used to facilitate the comparison of the results with past studies
	in the same catchment. For this study, some model parameters including
	Soil depth (SOL\_Z) and Saturated hydraulic conductivity (SOL-K)
	were assumed uniform within the study catchment and were therefore
	lumped comprising the huge computation resource requirement of the
	SWAT model. Results indicated that the same set of important parameters
	was identified with or without the use of observed flows data. Some
	of the parameters had physical interpretation and could therefore
	relate directly to hydrological controlling factors within the catchment.
	Despite swapping ranking importance of parameters, these results
	suggest the suitability of the SWAT model for identifying hydrological
	controlling factors/parameters in ungauged catchments. Results of
	calibration and validation at the daily timescale gave moderately
	satisfactory Nash-Sutcliffe Coefficient of Efficiency (CE) of 54.6\%
	for calibration and 68\% for validation while simulated and observed
	mean annual flow discharges gave an Index of Volumetric Fit (IVF)
	of 100\%. The Study further indicated the improvement of model estimation
	when more reliable spatial representation of rainfall was used. Although
	in this study SWAT model has performed satisfactorily in data poor
	and complex catchment, the authors recommend a wider validation effort
	of the model before it is adopted for operational purpose. (c) 2008
	Elsevier Ltd. All rights reserved.},
  doi = {10.1016/j.pce.2008.06.013},
  keywords = {SWAT model, auto-calibration, sensitivity analysis, RAINFALL-RUNOFF
	MODELS, AUTOMATIC CALIBRATION, SENSITIVITY},
  tags = {SWAT}
}

@MANUAL{neitsch+al2005a,
  title = {Soil and Water Assessment Tool Theoretical Documentation Version
	2005},
  author = {Neitsch, S. and Arnold, J. and Kiniry, J. and Srinivasan, R. and
	Williams, J.},
  address = {Grassland, Soil and Water Research Laboratory; Agricultural Research
	Service 808 East Blackland Road; Temple, Texas 76502; Blackland Research
	\& Extension Center; Texas Agricultural Experiment Station 720 East
	Blackland Road Temple, Texas 76502, USA},
  year = {2005},
  bibkey = {SWAT},
  institution = {Grassland, Soil and Water Research Laboratory; Agricultural Research
	Service},
  tags = {SWAT},
  url = {www.brc.tamus.edu/swat/doc.html}
}

@MANUAL{neitsch+al2005b,
  title = {Soil and Water Assessment Tool Input/Output file documentation, Version
	2005},
  author = {Neitsch, S. and Arnold, J. and Kiniry, J. and Srinivasan, R. and
	Williams, J.},
  address = {Grassland, Soil and Water Research Laboratory; Agricultural Research
	Service 808 East Blackland Road; Temple, Texas 76502; Blackland Research
	\& Extension Center; Texas Agricultural Experiment Station 720 East
	Blackland Road Temple, Texas 76502, USA},
  year = {2005},
  institution = {Grassland, Soil and Water Research Laboratory; Agricultural Research
	Service},
  keywords = {SWAT},
  tags = {SWAT},
  url = {www.brc.tamus.edu/swat/doc.html}
}

@MANUAL{swat2005,
  title = {Soil and water assessment tool - theoretical documentation (SWAT-2005)},
  author = {Neitsch, S. and Arnold, J. and Kiniry, J. and Williams, J.},
  organization = {Grassland, Soil and Water Research Laboratory - Blackland Research
	Centre},
  address = {Temple, Texas},
  edition = {1st},
  year = {2005},
  owner = {rojasro},
  timestamp = {2011.10.18},
  url = {http://swatmodel.tamu.edu/media/1292/SWAT2005theory.pdf}
}

@ARTICLE{neuman2004,
  author = {Neuman, S.},
  title = {Stochastic groundwater models in practice},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2004},
  volume = {18},
  pages = {268--270},
  number = {4},
  doi = {10.1007/s00477-004-0192-6},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{neuman2003,
  author = {Neuman, S.},
  title = {Maximum likelihood {B}ayesian averaging of uncertain model predictions},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2003},
  volume = {17},
  pages = {291-305},
  number = {5},
  abstract = {Hydrologic analyses typically rely on a single conceptual-mathematical
	model. Yet hydrologic environments are open and complex, rendering
	them prone to multiple interpretations and mathematical descriptions.
	Adopting only one of these may lead to statistical bias and underestimation
	of uncertainty. A comprehensive strategy for constructing alternative
	conceptual-mathematical models of subsurface flow and transport,
	selecting the best among them, and using them jointly to render optimum
	predictions under uncertainty has recently been developed by Neuman
	and Wierenga (2003). This paper describes a key formal element of
	this much broader and less formal strategy that concerns rendering
	optimum hydrologic predictions by means of several competing deterministic
	or stochastic models and assessing their joint predictive uncertainty.
	The paper proposes a Maximum Likelihood Bayesian Model Averaging
	(MLBMA) method to accomplish this goal. MLBMA incorporates both site
	characterization and site monitoring data so as to base the outcome
	on an optimum combination of prior information (scientific knowledge
	plus data) and model predictions. A preliminary example based on
	real data is included in the paper.},
  doi = {10.1007/s00477-003-0151-7},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{neumanwierenga2003,
  author = {Neuman, S. and Wierenga, P.},
  title = {A comprehensive strategy of hydrogeologic modeling and uncertainty
	analysis for nuclear facilities and sites},
  institution = {US Nuclear Regulatory Commission},
  year = {2003},
  type = {Report NUREG/CR-6805},
  address = {Washington USA},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{newhulme2000,
  author = {New, M. and Hulme, M.},
  title = {Representing uncertainties in climate change scenarios: {A} {M}onte
	{C}arlo approach},
  journal = {Integrated Assessment},
  year = {2000},
  volume = {1},
  pages = {203--213},
  number = {3},
  abstract = {Climate change impact assessment is subject to a range of uncertainties
	due to both incomplete and unknowable knowledge. This paper presents
	an approach to quantifying some of these uncertainties within a probabilistic
	framework. A hierarchical impact model is developed that addresses
	uncertainty about future greenhouse gas emissions, the climate sensitivity,
	and limitations and unpredictability in general circulation models.
	The hierarchical model is used in Bayesian Monte-Carlo simulations
	to define posterior probability distributions for changes in seasonal-mean
	temperature and precipitation over the United Kingdom that are conditional
	on prior distributions for the model parameters. The application
	of this approach to an impact model is demonstrated using a hydrological
	example},
  doi = {10.1023/A:1019144202120},
  keywords = {cascade of uncertainty},
  tags = {Uncertainty, Scenarios}
}

@ARTICLE{new+al2000,
  author = {New, M. and Hulme, M. and Jones, P.},
  title = {{Representing twentieth-century space–time climate variability. Part
	II: Development of 1901–96 monthly grids of terrestrial surface climate}},
  journal = {Journal of Climate},
  year = {2000},
  volume = {13},
  pages = {2217--2238},
  number = {13},
  abstract = {The authors describe the construction of a 0.5° lat–long gridded dataset
	of monthly terrestrial surface climate for the period of 1901–96.
	The dataset comprises a suite of seven climate elements: precipitation,
	mean temperature, diurnal temperature range, wet-day frequency, vapor
	pressure, cloud cover, and ground frost frequency. The spatial coverage
	extends over all land areas, including oceanic islands but excluding
	Antarctica. Fields of monthly climate anomalies, relative to the
	1961–90 mean, were interpolated from surface climate data. The anomaly
	grids were then combined with a 1961–90 mean monthly climatology
	(described in Part I) to arrive at grids of monthly climate over
	the 96-yr period. The primary variables—precipitation, mean temperature,
	and diurnal temperature range—were interpolated directly from station
	observations. The resulting time series are compared with other coarser-resolution
	datasets of similar temporal extent. The remaining climatic elements,
	termed secondary variables, were interpolated from merged datasets
	comprising station observations and, in regions where there were
	no station data, synthetic data estimated using predictive relationships
	with the primary variables. These predictive relationships are described
	and evaluated. It is argued that this new dataset represents an advance
	over other products because (i) it has higher spatial resolution
	than other datasets of similar temporal extent, (ii) it has longer
	temporal coverage than other products of similar spatial resolution,
	(iii) it encompasses a more extensive suite of surface climate variables
	than available elsewhere, and (iv) the construction method ensures
	that strict temporal fidelity is maintained. The dataset should be
	of particular relevance to a number of applications in applied climatology,
	including large-scale biogeochemical and hydrological modeling, climate
	change scenario construction, evaluation of regional climate models,
	and comparison with satellite products. The dataset is available
	from the Climatic Research Unit and is currently being updated to
	1998.},
  doi = {10.1175/1520-0442(2000)013<2217:RTCSTC>2.0.CO;2},
  owner = {rojasro},
  timestamp = {2011.03.29}
}

@ARTICLE{nikulin2011,
  author = {Nikulin, G. and Kjellstr\"om, E. and Hansson, U. and Strandberg,
	G. and Ullerstig, A.},
  title = {Evaluation and future projections of temperature, precipitation and
	wind extremes over {E}urope in an ensemble of regional climate simulations},
  journal = {Tellus {A}},
  year = {2011},
  volume = {63},
  pages = {41--55},
  number = {1},
  abstract = {Temperature, precipitation and wind extremes over Europe are examined
	in an ensemble of RCA3 regional climate model simulations driven
	by six different global climate models (ECHAM5, CCSM3, HadCM3, CNRM,
	BCM and IPSL) under the SRES A1B emission scenario. The extremes
	are expressed in terms of the 20-year return values of annual temperature
	and wind extremes and seasonal precipitation extremes. The ensemble
	shows reduction of recurrence time of warm extremes from 20 years
	in 1961–1990 (CTL) to 1–2 years over southern Europe and to 5 years
	over Scandinavia in 2071–2100 (SCN) while cold extremes, defined
	for CTL, almost disappear in the future. The recurrence time of intense
	precipitation reduces from 20 years in CTL to 6–10 years in SCN over
	northern and central Europe in summer and even more to 2–4 years
	in Scandinavia in winter. The projected changes in wind extremes
	have a large spread among the six simulations with a disperse tendency
	(1–2 m s?1) of strengthening north of 45°N and weakening south of
	it which is sensitive to the number of simulations in the ensemble.
	Changes in temperature extremes are more robust compared to those
	in precipitation extremes while there is less confidence on changes
	in wind extremes.},
  doi = {10.1111/j.1600-0870.2010.00466.x},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{nilsson2007,
  author = {Nilsson, B. and H{\o}jberg, A. and Refsgaard, J. and Troldborg, L.},
  title = {Uncertainty in geological and hydrogeological data},
  journal = {Hydrology and Earth System Sciences},
  year = {2007},
  volume = {11},
  pages = {1551--1561},
  number = {5},
  abstract = {Uncertainty in conceptual model structure and in environmental data
	is of essential interest when dealing with uncertainty in water resources
	management. To make quantification of uncertainty possible is it
	necessary to identify and characterise the uncertainty in geological
	and hydrogeological data. This paper discusses a range of available
	techniques to describe the uncertainty related to geological model
	structure and scale of support. Literature examples on uncertainty
	in hydrogeological variables such as saturated hydraulic conductivity,
	specific yield, specific storage, effective porosity and dispersivity
	are given. Field data usually have a spatial and temporal scale of
	support that is different from the one on which numerical models
	for water resources management operate. Uncertainty in hydrogeological
	data variables is characterised and assessed within the methodological
	framework of the HarmoniRiB classification.},
  doi = {10.5194/hess-11-1551-2007},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{nunes2008,
  author = {Nunes, J. and Seixas, J. and Pacheco, R.},
  title = {Vulnerability of water resources, vegetation productivity and soil
	erosion to climate change in {M}editerranean watersheds},
  journal = {Hydrological Processes},
  year = {2008},
  volume = {22},
  pages = {3115--3134},
  number = {16},
  abstract = {Climate change is expected to increase temperatures and lower rainfall
	in Mediterranean regions; however, there is a great degree of uncertainty
	as to the amount of change. This limits the prediction capacity of
	models to quantify impacts on water resources, vegetation productivity
	and erosion. This work circumvents this problem by analysing the
	sensitivity of these variables to varying degrees of temperature
	change (increased by up to 6·4 °C), rainfall (reduced by up to 40%)
	and atmospheric CO2 concentrations (increased by up to 100%). The
	SWAT watershed model was applied to 18 large watersheds in two contrasting
	regions of Portugal, one humid and one semi-arid; incremental changes
	to climate variables were simulated using a stochastic weather generator.
	The main results indicate that water runoff, particularly subsurface
	runoff, is highly sensitive to these climate change trends (down
	by 80%). The biomass growth of most species showed a declining trend
	(wheat down by 40%), due to the negative impacts of increasing temperatures,
	dampened by higher CO2 concentrations. Mediterranean species, however,
	showed a positive response to milder degrees of climate change. Changes
	to erosion depended on the interactions between the decline in surface
	runoff (driving erosion rates downward) and biomass growth (driving
	erosion rates upward). For the milder rainfall changes, soil erosion
	showed a significant increasing trend in wheat fields (up to 150%
	in the humid watersheds), well above the recovery capacity of the
	soil. Overall, the results indicate a shift of the humid watersheds
	to acquire semi-arid characteristics, such as more irregular river
	flows and increasingly marginal conditions for agricultural production.},
  doi = {10.1002/hyp.6897},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{olea2006,
  author = {Olea, R.},
  title = {A six--step practical approach to semivariogram modeling},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2006},
  volume = {20},
  pages = {307--318},
  number = {5},
  abstract = {Geostatistical prediction and simulation are being increasingly used
	in the earth sciences and engineering to address the imperfect knowledge
	of attributes that fluctuate over large areas or volumes—pollutant
	concentration, electromagnetic fields, porosity, thickness of a geological
	formation. Central to the application of such techniques is the need
	to know the spatial continuity, knowledge that is commonly condensed
	in the form of covariance or semivariogram models. Their preparation
	is subdivided here into the following steps: (1) Data editing, (2)
	Exploratory data analysis, (3) Semivariogram estimation, (4) Directional
	investigation, (5) Simple modeling, (6) Nested modeling. I illustrate
	these stages practically with a real data set from a geophysical
	survey from Elk County, Kansas, USA. The applicability of the approach
	is not limited by the physical nature of the attribute of interest.},
  doi = {10.1007/s00477-005-0026-1},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{olesen+al2007,
  author = {Olesen, J. and Carter, T. and {D\'iaz-Ambrona}, C. and Fronzek, S.
	and Heidmann, T. and Hickler, T. and Holt, T. and Minguez, M. and
	Morales, P. and Palutikof, J. and Quemada, M. and {Ruiz-Ramos}, M.
	and Rub{\ae}k, G. and Sau, F. and Smith, B. and Sykes, M.},
  title = {Uncertainties in projected impacts of climate change on {E}uropean
	agriculture and terrestrial ecosystems based on scenarios from regional
	climate models},
  journal = {Climatic Change},
  year = {2007},
  volume = {81},
  pages = {123--143},
  number = {1},
  abstract = {The uncertainties and sources of variation in projected impacts of
	climate change on agriculture and terrestrial ecosystems depend not
	only on the emission scenarios and climate models used for projecting
	future climates, but also on the impact models used, and the local
	soil and climatic conditions of the managed or unmanaged ecosystems
	under study. We addressed these uncertainties by applying different
	impact models at site, regional and continental scales, and by separating
	the variation in simulated relative changes in ecosystem performance
	into the different sources of uncertainty and variation using analyses
	of variance. The crop and ecosystem models used output from a range
	of global and regional climate.},
  doi = {10.1007/s10584-006-9216-1},
  tags = {Impacts}
}

@ARTICLE{oliver1997,
  author = {Oliver, D. and Cunha, L. and Reynolds, A.},
  title = {Markov {C}hain {M}onte {C}arlo methods for conditioning a permeability
	field to pressure data},
  journal = {Mathematical Geology},
  year = {1997},
  volume = {29},
  pages = {61--91},
  number = {1},
  abstract = {Generating one realization of a random permeability field that is
	consistent with observed pressure data and a known variogram model
	is not a difficult problem. If, however, one wants to investigate
	the uncertainty of reservior behavior, one must generate a large
	number of realizations and ensure that the distribution of realizations
	properly reflects the uncertainty in reservoir properties. The most
	widely used method for conditioning permeability fields to production
	data has been the method of simulated annealing, in which practitioners
	attempt to minimize the difference between the ’ ’true and simulated
	production data, and “true” and simulated variograms. Unfortunately,
	the meaning of the resulting realization is not clear and the method
	can be extremely slow. In this paper, we present an alternative approach
	to generating realizations that are conditional to pressure data,
	focusing on the distribution of realizations and on the efficiency
	of the method. Under certain conditions that can be verified easily,
	the Markov chain Monte Carlo method is known to produce states whose
	frequencies of appearance correspond to a given probability distribution,
	so we use this method to generate the realizations. To make the method
	more efficient, we perturb the states in such a way that the variogram
	is satisfied automatically and the pressure data are approximately
	matched at every step. These perturbations make use of sensitivity
	coefficients calculated from the reservoir simulator.},
  doi = {10.1007/BF02769620},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@ARTICLE{olsthoom2006,
  author = {Olsthoom, T. and Kamps, P.},
  title = {Challenges to calibration: {F}acing an increasingly critical environment},
  journal = {Ground Water},
  year = {2006},
  volume = {44},
  pages = {876--879},
  number = {6},
  doi = {10.1111/j.1745-6584.2006.00247.x},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@ARTICLE{oz2003,
  author = {Oz, B. and Deutsch, C. and Tran, T. and Xie, Y--L},
  title = {D{SSIM-HR}: {A} {FORTRAN 90} program for direct sequential simulation
	with histogram reproduction},
  journal = {Computers \& Geosciences},
  year = {2003},
  volume = {29},
  pages = {39--51},
  number = {1},
  abstract = {Sequential simulation is a frequently used geostatistical simulation
	technique. The most widely used version of this technique is sequential
	Gaussian simulation, where the data are transformed to follow a Gaussian
	distribution and the entire multivariate distribution is then assumed
	to be Gaussian. This critical assumption greatly simplifies the simulation
	process since every conditional distribution is Gaussian with parameters
	given by kriging. Direct sequential simulation does not require any
	Gaussian assumption and simulates directly the data space; however,
	a longstanding problem of direct simulation is that the histogram
	of the variable is not reproduced even though the mean, variance,
	and variogram are reproduced. This lack of histogram reproduction
	is due to the unknown shape of the conditional distributions, which
	are used for drawing the simulated values.
	
	
	We derive a simple and theoretically valid approach by establishing
	the shapes of the sequentially constructed conditional distributions.
	These shapes ensure histogram reproduction. The approach has been
	coded in FORTRAN 90 and called DSSIM-HR, where the extension HR refers
	to the feature of “Histogram Reproduction”.},
  doi = {10.1016/S0098-3004(02)00071-7},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{pappenberger2006a,
  author = {Pappenberger, F. and Beven, K.},
  title = {Ignorance is bliss: {O}r seven reasons not to use uncertainty analysis},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W05302},
  abstract = {Uncertainty analysis of models has received increasing attention over
	the last two decades in water resources research. However, a significant
	part of the community is still reluctant to embrace the estimation
	of uncertainty in hydrological and hydraulic modeling. In this paper,
	we summarize and explore seven common arguments: uncertainty analysis
	is not necessary given physically realistic models; uncertainty analysis
	cannot be used in hydrological and hydraulic hypothesis testing;
	uncertainty (probability) distributions cannot be understood by policy
	makers and the public; uncertainty analysis cannot be incorporated
	into the decision-making process; uncertainty analysis is too subjective;
	uncertainty analysis is too difficult to perform; uncertainty does
	not really matter in making the final decision. We will argue that
	none of the arguments against uncertainty analysis rehearsed are,
	in the end, tenable. Moreover, we suggest that one reason why the
	application of uncertainty analysis is not normal and expected part
	of modeling practice is that mature guidance on methods and applications
	does not exist. The paper concludes with suggesting that a Code of
	Practice is needed as a way of formalizing such guidance.},
  doi = {10.1029/2005WR004820},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{pappenberger+al2007,
  author = {Pappenberger, F. and Beven, K. and Frodsham, K. and Romanowicz, R.
	and Matgen, P.},
  title = {Grasping the unavoidable subjectivity in calibration of flood inundation
	models: {A} vulnerability weighted approach},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {333},
  pages = {275--287},
  number = {2-4},
  abstract = {Quantitative modeling of risk and hazard from flooding involves decisions
	regarding the choice of model and goal of the modeling exercise,
	expressed by some measure of performance. This paper shows how the
	subjectivity in the choices of performance measures and observation
	sets used for model calibration inevitably results in variability
	in the estimation of flood hazard. We compare the predictions of
	a 2D flood inundation model obtained using different global and local
	evaluation criteria. It is shown that traditional area averaging
	performance measures are inadequate in the face of model imperfection,
	especially when such models are calibrated for flood hazard studies.
	In this study we include flood risk weighting into the performance
	measure of the model. This allows us to calibrate the model to places
	that are important, e.g. location of houses. The quantification of
	the importance of places requires the necessity of engaging stakeholders
	into the model calibration process.},
  doi = {10.1016/j.jhydrol.2006.08.017},
  keywords = {Flood inundation model, LISFLOOD-FP, GLUE, Raster map comparison,
	Utility function, Flood risk, Flood hazard},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{pappenberger2006b,
  author = {Pappenberger, F. and Matgen, P. and Beven, K. and Henry, J. and Pfister,
	L. and {de Fraipont}, P.},
  title = {Influence of uncertain boundary conditions and model structure on
	flood inundation predictions},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {1430--1449},
  number = {10},
  abstract = {In this study, the GLUE methodology is applied to establish the sensitivity
	of flood inundation predictions to uncertainty of the upstream boundary
	condition and bridges within the modelled region. An understanding
	of such uncertainties is essential to improve flood forecasting and
	floodplain mapping. The model has been evaluated on a large data
	set. This paper shows uncertainty of the upstream boundary can have
	significant impact on the model results, exceeding the importance
	of model parameter uncertainty in some areas. However, this depends
	on the hydraulic conditions in the reach e.g. internal boundary conditions
	and, for example, the amount of backwater within the modelled region.
	The type of bridge implementation can have local effects, which is
	strongly influenced by the bridge geometry (in this case the area
	of the culvert). However, the type of bridge will not merely influence
	the model performance within the region of the structure, but also
	other evaluation criteria such as the travel time. This also highlights
	the difficulties in establishing which parameters have to be more
	closely examined in order to achieve better fits. In this study no
	parameter set or model implementation that fulfils all evaluation
	criteria could be established. We propose four different approaches
	to this problem: closer investigation of anomalies; introduction
	of local parameters; increasing the size of acceptable error bounds;
	and resorting to local model evaluation. Moreover, we show that it
	can be advantageous to decouple the classification into behavioural
	and non-behavioural model data/parameter sets from the calculation
	of uncertainty bounds.},
  doi = {10.1016/j.advwatres.2005.11.012},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@ARTICLE{parajka+al2007,
  author = {Parajka, J. and Merz, R. and Bl{\"o}schl, G.},
  title = {Uncertainty and multiple objective calibration in regional water
	balance modelling: case study in 320 {A}ustrian catchments},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {435--446},
  number = {4},
  abstract = {We examine the value of additional information in multiple objective
	calibration in terms of model performance and parameter uncertainty.
	We calibrate and validate a semi-distributed conceptual catchment
	model for two 11-year periods in 320 Austrian catchments and test
	three approaches of parameter calibration: (a) traditional single
	objective calibration (SINGLE) on daily runoff; (b) multiple objective
	calibration (MULTI) using daily runoff and snow cover data; (c) multiple
	objective calibration (APRIORI) that incorporates an a priori expert
	guess about the parameter distribution as additional information
	to runoff and snow cover data. Results indicate that the MULTI approach
	performs slightly poorer than the SINGLE approach in terms of runoff
	simulations, but significantly better in terms of snow cover simulations.
	The APRIORI approach is essentially as good as the SINGLE approach
	in terms of runoff simulations but is slightly poorer than the MULTI
	approach in terms of snow cover simulations. An analysis of the parameter
	uncertainty indicates that the MULTI approach significantly decreases
	the uncertainty of the model parameters related to snow processes
	but does not decrease the uncertainty of other model parameters as
	compared to the SINGLE case. The APRIORI approach tends to decrease
	the uncertainty of all model parameters as compared to the SINGLE
	case},
  doi = {10.1002/hyp.6253},
  keywords = {multiple objective calibration, parameter uncertainty, water balance
	modelling},
  tags = {Uncertainty, Calibration}
}

@ARTICLE{parasuraman2008,
  author = {Parasuraman, K. and Elshorbagy, A.},
  title = {Toward improving the reliability of hydrologic prediction: {M}odel
	structure uncertainty and its quantification using ensemble-based
	genetic programming framework},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W12406},
  abstract = {Uncertainty analysis is starting to be widely acknowledged as an integral
	part of hydrological modeling. The conventional treatment of uncertainty
	analysis in hydrologic modeling is to assume a deterministic model
	structure, and treat its associated parameters as imperfectly known,
	thereby neglecting the uncertainty associated with the model structure.
	In this paper, a modeling framework that can explicitly account for
	the effect of model structure uncertainty has been proposed. The
	modeling framework is based on initially generating different realizations
	of the original data set using a non-parametric bootstrap method,
	and then exploiting the ability of the self-organizing algorithms,
	namely genetic programming, to evolve their own model structure for
	each of the resampled data sets. The resulting ensemble of models
	is then used to quantify the uncertainty associated with the model
	structure. The performance of the proposed modeling framework is
	analyzed with regards to its ability in characterizing the evapotranspiration
	process at the Southwest Sand Storage facility, located near Ft.
	McMurray, Alberta. Eddy-covariance-measured actual evapotranspiration
	is modeled as a function of net radiation, air temperature, ground
	temperature, relative humidity, and wind speed. Investigating the
	relation between model complexity, prediction accuracy, and uncertainty,
	two sets of experiments were carried out by varying the level of
	mathematical operators that can be used to define the predictand-predictor
	relationship. While the first set uses just the additive operators,
	the second set uses both the additive and the multiplicative operators
	to define the predictand-predictor relationship. The results suggest
	that increasing the model complexity may lead to better prediction
	accuracy but at an expense of increasing uncertainty. Compared to
	the model parameter uncertainty, the relative contribution of model
	structure uncertainty to the predictive uncertainty of a model is
	shown to be more important. Furthermore, the study advocates that
	the search to find the optimal model could be replaced by the quest
	to unearth possible models for characterizing hydrological processes.},
  doi = {10.1029/2007WR006451},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@BOOK{parry+al2007,
  title = {Climate Change 2007: Impacts, Adaptation and Vulnerability. Contribution
	of Working Group II to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {Parry, M. and Canziani, O. and Palutikof, J. and {van der Linden},
	P. and Hanson, C.},
  author = {Parry, M. and Canziani, O. and Palutikof, J. and {van der Linden},
	P. and Hanson, C.},
  pages = {1000pp.},
  address = {Cambridge, UK},
  tags = {IPCC}
}

@ARTICLE{pasquier2006,
  author = {Pasquier, P. and Marcotte, D.},
  title = {Steady-- and transient--state inversion in hydrogeology by successive
	flux estimation},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {1934--1952},
  number = {12},
  abstract = {A calibration method to solve the groundwater inverse problem under
	steady- and transient-state conditions is presented. The method compares
	kriged and numerical head field gradients to modify hydraulic conductivity
	without the use of non-linear optimization techniques. The process
	is repeated iteratively until a close match with piezometric data
	is reached. The approach includes a damping factor to avoid divergence
	and oscillation of the solution in areas of low hydraulic gradient
	and a weighting factor to account for temporal head variation in
	transient simulations. The efficiency of the method in terms of computing
	time and calibration results is demonstrated with a synthetic field.
	It is shown that the proposed method provides parameter fields that
	reproduce both hydraulic conductivity and piezometric data in few
	forward model solutions. Stochastic numerical experiments are conducted
	to evaluate the sensitivity of the method to the damping function
	and to the head field estimation errors.},
  doi = {10.1016/j.advwatres.2006.02.001},
  owner = {RRojas},
  timestamp = {2009.03.13}
}

@ARTICLE{patriarche2005,
  author = {Patriarche, D. and Castro, M. and Goovaerts, P.},
  title = {Estimating regional hydraulic conductivity fields--{A} comparative
	study of geostatistical methods},
  journal = {Mathematical Geology},
  year = {2005},
  volume = {37},
  pages = {587--613},
  number = {6},
  abstract = {Geostatistical estimations of the hydraulic conductivity field (K)
	in the Carrizo aquifer, Texas, are performed over three regional
	domains of increasing extent: 1) the domain corresponding to a three-dimensional
	groundwater flow model previously built (model domain); 2) the area
	corresponding to the 10 counties encompassing the model domain (County
	domain), and; 3) the full extension of the Carrizo aquifer within
	Texas (Texas domain). Two different approaches are used: 1) an indirect
	approach where transmissivity (T) is estimated first and K is retrieved
	through division of the T estimate by the screen length of the wells,
	and; 2) a direct approach where K data are kriged directly. Due to
	preferential well screen emplacement, and scarcity of sampling in
	the deeper portions of the formation (> 1 km), the available data
	set is biased toward high values of hydraulic conductivities. Kriging
	combined with linear regression, simple kriging with varying local
	means, kriging with an external drift, and cokriging allow the incorporation
	of specific capacity as secondary information. Prediction performances
	(assessed through cross-validation) differ according to the chosen
	approach, the considered variable (log-transformed or back-transformed),
	and the scale of interest. For the indirect approach, kriging of
	log T with varying local means yields the best estimates for both
	log-transformed and back-transformed variables in the model domain.
	For larger regional scales (County and Texas domains), cokriging
	performs generally better than other kriging procedures when estimating
	both (log T)? and T?. Among procedures using the direct approach,
	the best prediction performances are obtained using kriging of log
	K with an external drift. Overall, geostatistical estimation of the
	hydraulic conductivity field at regional scales is rendered difficult
	by both preferential well location and preferential emplacement of
	well screens in the most productive portions of the aquifer. Such
	bias creates unrealistic hydraulic conductivity values, in particular,
	in sparsely sampled areas.},
  doi = {10.1007/s11004-005-7308-5},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{payne+al2004,
  author = {Payne, J. and Wood, A. and Hamlet, A. and Palmer, R. and Lettenmaier,
	D.},
  title = {Mitigating the effects of climate change on the water resources of
	the {C}olumbia {R}iver basin},
  journal = {Climatic Change},
  year = {2004},
  volume = {62},
  pages = {233--256},
  number = {1--3},
  abstract = {The potential effects of climate change on the hydrology and water
	resources of the Columbia River Basin (CRB) were evaluated using
	simulations from the U.S. Department of Energy and National Center
	for Atmospheric Research Parallel Climate Model (DOE/NCAR PCM). This
	study focuses on three climate projections for the 21st century based
	on a `business as usual' (BAU) global emissions scenario, evaluated
	with respect to a control climate scenario based on static 1995 emissions.
	Time-varying monthly PCM temperature and precipitation changes were
	statistically downscaled and temporally disaggregated to produce
	daily forcings that drove a macro-scale hydrologic simulation model
	of the Columbia River basin at 1/4-degree spatial resolution. For
	comparison with the direct statistical downscaling approach, a dynamical
	downscaling approach using a regional climate model (RCM) was also
	used to derive hydrologic model forcings for 20-year subsets from
	the PCM control climate (1995--2015) scenario and from the three
	BAU climate(2040--2060) projections. The statistically downscaled
	PCM scenario results were assessed for three analysis periods (denoted
	Periods 1--3: 2010--2039,2040--2069, 2070--2098) in which changes
	in annual average temperature were +0.5,+1.3 and +2.1 Â°C, respectively,
	while critical winter season precipitation changes were --3, +5 and
	+1 percent. For RCM, the predicted temperature change for the 2040--2060
	period was +1.2 Â°C and the average winter precipitation change was
	--3 percent, relative to the RCM controlclimate. Due to the modest
	changes in winter precipitation, temperature changes dominated the
	simulated hydrologic effects by reducing winter snow accumulation,
	thus shifting summer streamflow to the winter. The hydrologic changes
	caused increased competition for reservoir storage between firm hydropower
	and instream flow targets developed pursuant to the Endangered Species
	Act listing of Columbia River salmonids. We examined several alternative
	reservoir operating policies designed to mitigate reservoir system
	performance losses. In general, the combination of earlier reservoir
	refill with greater storage allocations for instream flow targets
	mitigated some of the negative impacts to flow, but only with significant
	losses in firm hydropower production (ranging from --9 percent in
	Period1 to --35 percent for RCM). Simulated hydropower revenue changes
	were lessthan 5 percent for all scenarios, however, primarily due
	to small changes inannual runoff.},
  doi = {10.1023/B:CLIM.0000013694.18154.d6},
  tags = {Impacts}
}

@ARTICLE{pebesma2004,
  author = {Pebesma, E.},
  title = {Multivariable geostatistics in {S}: {T}he gstat package},
  journal = {Computers \& Geosciences},
  year = {2004},
  volume = {30},
  pages = {683--691},
  number = {7},
  abstract = {This paper discusses advantages and shortcomings of the S environment
	for multivariable geostatistics, in particular when extended with
	the gstat package, an extension package for the S environments (R,
	S-Plus). The gstat S package provides multivariable geostatistical
	modelling, prediction and simulation, as well as several visualisation
	functions. In particular, it makes the calculation, simultaneous
	fitting, and visualisation of a large number of direct and cross
	(residual) variograms very easy. Gstat was started 10 years ago and
	was released under the GPL in 1996; gstat.org was started in 1998.
	Gstat was not initially written for teaching purposes, but for research
	purposes, emphasising flexibility, scalability and portability. It
	can deal with a large number of practical issues in geostatistics,
	including change of support (block kriging), simple/ordinary/universal
	(co)kriging, fast local. neighbourhood selection, flexible trend
	modelling, variables with different sampling configurations, and
	efficient simulation of large spatially correlated. random fields,
	indicator kriging and simulation, and (directional) variogram and
	cross variogram modelling. The formula/models interface of the S
	language is used to define multivariable geostatistical models. This
	paper introduces the gstat S package, and discusses a number of design
	and implementation issues. It also draws attention to a number of
	papers on integration of spatial statistics software, GIS and the
	S environment that were presented on the spatial statistics workshop
	and sessions during the conference (C) 2004 Elsevier Ltd. All rights
	reserved.},
  doi = {10.1016/j.cageo.2004.03.012},
  keywords = {kriging, cokriging, linear model of coregionalisation, open source
	software, S language, Stochastic simulation, PREDICTION},
  tags = {R, interpolation}
}

@ARTICLE{pengxu2009,
  author = {Peng, D. and Xu, Z.},
  title = {Simulating the impact of climate change on streamflow in the {T}arim
	{R}iver basin by using a modified semi--distributed monthly water
	balance model},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {24},
  pages = {209--216},
  number = {2},
  abstract = {A modified semi-distributed monthly water balance model is proposed
	to simulate the streamflow in the Tarim River. With the comparative
	study among TOPMODEL, Xinanjiang model and the modified semi-distributed
	monthly water balance model in the headwater catchment of the Tarim
	River, it showed that the proposed model with simple structure and
	two parameters as well as TOPMODEL, Xinanjiang model perform well
	in the study area (the Nash-Sutcliffe efficiencies R-2 were between
	0.60 and 0.69, and the relative errors of the volumetric fitness
	R-E were between -19% and 19%). At Shaliguilank hydrological station
	in the Aksu River basin, R-2 is around 0.60. Those values at other
	stations are generally between 0.60 and 0.66 in headwater catchment
	of the Tarim River. The high-flow year, average year and dry year
	were determined by analysing the frequency of annual runoff in the
	Tarim River from 1957 to 2005 using Pearson type III (P-III) distribution,
	and the L-moment approach was applied to the parameter estimation
	for P-III distribution. With the reference years (high-flow year,
	average year and dry year) and the long-term trend, different scenarios
	for the precipitation in 2010 and 2020 have been generated. The streamflow
	was simulated by using the proposed model with different scenarios,
	and the impact of climate change on streamflow was analysed. The
	results showed that the streamflow in 2010 and 2020 exhibited an
	increasing tendency in the Aksu, Yarkant, and Hotan River basins},
  doi = {10.1002/hyp.7485},
  tags = {Climate Change}
}

@ARTICLE{perrin+al2001,
  author = {Perrin, C. and Michel, C. and Andreassian, V.},
  title = {Does a large number of parameters enhance model performance? {C}omparative
	assessment of common catchment model structures on 429 catchments},
  journal = {Journal of Hydrology},
  year = {2001},
  volume = {242},
  pages = {275--301},
  number = {3-4},
  abstract = {Hydrological models must be reliable and robust as these qualities
	influence all applications based on model output. Previous studies
	on conceptual rainfall-runoff models have shown that one of the root
	causes of their output uncertainty is model over-parameterisation.
	The problem of poorly defined parameters has attracted much attention
	but has not yet been satisfactorily solved. We believe that the most
	fruitful way forward is to improve the structures where these parameters
	act. The main objective of this paper is to examine the role of complexity
	in hydrological models by studying the relation between the number
	of optimised parameters and model performance. An extensive comparative
	performance assessment of the structures of 19 daily lumped models
	was carried out on 429 catchments, mostly in France but also in the
	United States, Australia, the Ivory Coast and Brazil. Bulk treatment
	of the data showed that the complex models outperform the simple
	ones in calibration mode but not in verification mode. We argue that
	the main reason why complex models lack stability is that the structure,
	i.e. the way components are organised, is not suited to extracting
	information available in hydrological time-series An inadequate complexity
	typically results in model over-parameterisation and parameter uncertainty.
	Although complexity has been used as a response to the challenge
	of predicting the hydrological effects of environmental changes,
	this study suggests that such models may have been developed with
	excessive confidence and that they could face difficulties of parameter
	estimation and structure validation when confronted with hydro-meteorological
	time-series. This comparative study indicates that some parsimonious
	models can yield promising results and should be further developed,
	although they are not able to tackle all types of problems, which
	would be the case if their complexity were ideally adapted.},
  doi = {10.1016/S0022-1694(00)00393-0},
  keywords = {France, United States, Australia, Ivory Coast, Brazil, models, rain
	water, runoff, evapotranspiration, optimization, digital simulation,
	calibration, drainage basins, Western Europe, Europe, North America,
	Australasia, West Africa, Africa, South America},
  tags = {Calibration, conceptual model}
}

@ARTICLE{piani+al2010a,
  author = {Piani, C. and Haerter, J. and Coppola, E.},
  title = {Statistical bias correction for daily precipitation in regional climate
	models over {E}urope},
  journal = {Theoretical and Applied Climatology},
  year = {2010},
  volume = {99},
  pages = {187--192},
  number = {1--2},
  month = {January},
  abstract = {We design, apply, and validate a methodology for correcting climate
	model output to produce internally consistent fields that have the
	same statistical intensity distribution as the observations. We refer
	to this as a statistical bias correction. Validation of the methodology
	is carried out using daily precipitation fields, defined over Europe,
	from the ENSEMBLES climate model dataset. The bias correction is
	calculated using data from 1961 to 1970, without distinguishing between
	seasons, and applied to seasonal data from 1991 to 2000. This choice
	of time periods is made to maximize the lag between calibration and
	validation within the ERA40 reanalysis period. Results show that
	the method performs unexpectedly well. Not only are the mean and
	other moments of the intensity distribution improved, as expected,
	but so are a drought and a heavy precipitation index, which depend
	on the autocorrelation spectra. Given that the corrections were derived
	without seasonal distinction and are based solely on intensity distributions,
	a statistical quantity oblivious of temporal correlations, it is
	encouraging to find that the improvements are present even when seasons
	and temporal statistics are considered. This encourages the application
	of this method to multi-decadal climate projections.},
  doi = {10.1007/s00704-009-0134-9},
  owner = {rojasro},
  timestamp = {2010.06.18}
}

@ARTICLE{piani+al2010b,
  author = {Piani, C. and Weedon, G. and Best, M. and Gomes, S. and Viterbo,
	P. and Hagemann, S. and Haerter, J.},
  title = {Statistical bias correction of global simulated daily precipitation
	and temperature for the application of hydrological models},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {395},
  pages = {199--215},
  number = {3--4},
  abstract = {A statistical bias correction methodology for global climate simulations
	is developed and applied to daily land precipitation and mean, minimum
	and maximum daily land temperatures. The bias correction is based
	on a fitted histogram equalization function. This function is defined
	daily, as opposed to earlier published versions in which they were
	derived yearly or seasonally at best, while conserving properties
	of robustness and eliminating unrealistic jumps at seasonal or monthly
	transitions. The methodology is tested using the newly available
	global dataset of observed hydrological forcing data of the last
	50 years from the EU project WATCH (WATer and global CHange) and
	an initial conditions ensemble of simulations performed with the
	ECHAM5 global climate model for the same period. Bias corrections
	are derived from 1960 to 1969 observed and simulated data and then
	applied to 1990–1999 simulations. Results confirm the effectiveness
	of the methodology for all tested variables. Bias corrections are
	also derived using three other non-overlapping decades from 1970
	to 1999 and all members of the ECHAM5 initial conditions ensemble.
	A methodology is proposed to use the resulting “ensemble of bias
	corrections” to quantify the error in simulated scenario projections
	of components of the hydrological cycle.},
  doi = {10.1016/j.jhydrol.2010.10.024},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{pillingjones2002,
  author = {Pilling, C. and Jones, J.},
  title = {The impact of future climate change on seasonal discharge, hydrological
	processes and extreme flows in the {U}pper {W}ye experimental catchment,
	{M}id-{W}ales},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {1201--1213},
  number = {6},
  abstract = {Analysing the impact of future climate change on hydrological regimes
	is hampered by the disparity of scales between general circulation
	model (GCM) output and the spatial resolution required by catchment-scale
	hydrological simulation models. In order to overcome this, statistical
	relationships were established between three indices of atmospheric
	circulation (vorticity and the strength and direction of geostrophic
	windflow) and daily catchment precipitation and potential evapotranspiration
	(PET) to downscale from the HadCM2 GCM to the Upper Wye experimental
	catchment in mid-Wales. The atmospheric circulation indices were
	calculated from daily grid point sea-level pressure data for: (a)
	the Climatic Research Unit observed data set (1975-90); (b) the HadCM2SUL
	simulation representing the present climate (1980-99); and (c) the
	HadCM2SUL simulation representing future climate conditions (2080-99).
	The performance of the downscaling approach was evaluated by comparing
	diagnostic statistics from the three downscaled precipitation and
	PET scenarios with those recorded from the Upper Wye catchment. The
	most significant changes between the downscaled HadCM2SUL 1980-99
	and 2080-99 scenarios are decreases in precipitation occurrence and
	amount in summer and autumn combined with a shortening of mean wet
	spell length, which is most pronounced in autumn. A hydrological
	simulation model (HYSIM) was calibrated on recorded flow data for
	the Upper Wye catchment and forced with the three downscaled precipitation
	and PET scenarios to model changes in river flow and hillslope hydrological
	processes. Results indicate increased seasonality of flows, with
	markedly drier summers. Analysis of extreme events suggests significant
	increases in the frequency of both high- and low-flow events},
  doi = {10.1002/hyp.1057},
  tags = {Impacts}
}

@ARTICLE{plavcova2011,
  author = {Plavcov\'a, E. and Kysel\'y, J.},
  title = {{Evaluation of daily temperatures in Central Europe and their links
	to large-scale circulation in an ensemble of regional climate models}},
  journal = {Tellus {A}},
  year = {2011},
  volume = {63},
  pages = {763--781},
  number = {4},
  abstract = {Reproduction of daily maximum and minimum temperatures, including
	tails of their distributions and links to large-scale circulation,
	is evaluated in an ensemble of high-resolution regional climate model
	(RCM) simulations over the Czech Republic. RCM data for recent climate
	(1961–1990) are validated against observed data gridded from a high-density
	station network. We find large biases in mean monthly temperatures
	and in seasonal extremes, which are significant in most RCMs throughout
	the year. The results suggest that an RCM's formulation plays a much
	more important role in summer, whereas in winter RCM performance
	is closely linked to the driving GCM. Biases are usually larger for
	extremes than central parts of temperature distributions, and RCMs
	tend to underestimate the severity of extremes in both seasons. Substantial
	underestimation of diurnal temperature range throughout the year
	in all RCMs and a shift of maximum in its annual cycle suggest general
	errors in simulating climate processes affecting the difference between
	daytime and night-time temperatures. Some features of the temperature
	biases in RCMs are related to deficiencies in the simulation of atmospheric
	circulation, particularly too strong advection and overestimation
	of westerly flow at the expense of easterly flow in most RCMs. The
	general biases in simulating anticyclonic, cyclonic and straight
	flow also contribute to the underestimated diurnal temperature range.},
  doi = {10.1111/j.1600-0870.2011.00514.x},
  owner = {rojasro},
  timestamp = {2011.02.07}
}

@ARTICLE{poeterander2005,
  author = {Poeter, E. and Anderson, D.},
  title = {Multimodel ranking and inference in ground water modelling},
  journal = {Ground Water},
  year = {2005},
  volume = {43},
  pages = {597--605},
  number = {4},
  abstract = {Uncertainty of hydrogeologic conditions makes it important to consider
	alternative plausible models in an effort to evaluate the character
	of a ground water system, maintain parsimony, and make predictions
	with reasonable definition of their uncertainty. When multiple models
	are considered, data collection and analysis focus on evaluation
	of which model(s) is(are) most supported by the data. Generally,
	more than one model provides a similar acceptable fit to the observations;
	thus, inference should be made from multiple models. Kullback-Leibler
	(K-L) information provides a rigorous foundation for model inference
	that is simple to compute, is easy to interpret, selects parsimonious
	models, and provides a more realistic measure of precision than evaluation
	of any one model or evaluation based on other commonly referenced
	model selection criteria. These alternative criteria strive to identify
	the true (or quasi-true) model, assume it is represented by one of
	the models in the set, and given their preference for parsimony regardless
	of the available number of observations the selected model may be
	underfit. This is in sharp contrast to the K-L information approach,
	where models are considered to be approximations to reality, and
	it is expected that more details of the system will be revealed when
	more data are available. We provide a simple, computer-generated
	example to illustrate the procedure for multimodel inference based
	on K-L information and present arguments, based on statistical underpinnings
	that have been overlooked with time, that its theoretical basis renders
	it preferable to other approaches.},
  doi = {10.1111/j.1745-6584.2005.0061.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{poeterhill1999,
  author = {Poeter, E. and Hill, M.},
  title = {{UCODE, a computer code for universal inverse modeling}},
  journal = {Computer \& Geosciences},
  year = {1999},
  volume = {25},
  pages = {457--462},
  number = {4},
  abstract = {This article presents the US Geological Survey computer program UCODE,
	which was developed in collaboration with the US Army Corps of Engineers
	Waterways Experiment Station and the International Ground Water Modeling
	Center of the Colorado School of Mines. UCODE performs inverse modeling,
	posed as a parameter-estimation problem, using nonlinear regression.
	Any application model or set of models can be used; the only requirement
	is that they have numerical (ASCII or text only) input and output
	files and that the numbers in these files have sufficient significant
	digits. Application models can include preprocessors and postprocessors
	as well as models related to the processes of interest (physical,
	chemical and so on), making UCODE extremely powerful for model calibration.
	Estimated parameters can be defined flexibly with user-specified
	functions. Observations to be matched in the regression can be any
	quantity for which a simulated equivalent value can be produced,
	thus simulated equivalent values are calculated using values that
	appear in the application model output files and can be manipulated
	with additive and multiplicative functions, if necessary. Prior,
	or direct, information on estimated parameters also can be included
	in the regression. The nonlinear regression problem is solved by
	minimizing a weighted least-squares objective function with respect
	to the parameter values using a modified Gauss–Newton method. Sensitivities
	needed for the method are calculated approximately by forward or
	central differences and problems and solutions related to this approximation
	are discussed. Statistics are calculated and printed for use in (1)
	diagnosing inadequate data or identifying parameters that probably
	cannot be estimated with the available data, (2) evaluating estimated
	parameter values, (3) evaluating the model representation of the
	actual processes and (4) quantifying the uncertainty of model simulated
	values. UCODE is intended for use on any computer operating system:
	it consists of algorithms programmed in perl, a freeware language
	designed for text manipulation and Fortran90, which efficiently performs
	numerical calculations.},
  doi = {10.1016/S0098-3004(98)00149-6},
  owner = {rojasro},
  timestamp = {2011.10.13}
}

@ARTICLE{poeterhill1997,
  author = {Poeter, E. and Hill, M.},
  title = {Inverse models: {A} necessary step in {G}round--{W}ater modeling},
  journal = {Ground Water},
  year = {1997},
  volume = {35},
  pages = {250--260},
  number = {2},
  abstract = {Inverse models using, for example, nonlinear least-squares regression,
	provide capabilities that help modelers take full advantage of the
	insight available from ground-water models. However, lack of information
	about the requirements and benefits of inverse models is an obstacle
	to their widespread use. This paper presents a simple ground-water
	flow problem to illustrate the requirements and benefits of the nonlinear
	least-squares regression method of inverse modeling and discusses
	how these attributes apply to field problems. The benefits of inverse
	modeling include: (1) expedited determination of best fit parameter
	values; (2) quantification of the (a) quality of calibration, (b)
	data shortcomings and needs, and (c) confidence limits on parameter
	estimates and predictions; and (3) identification of issues that
	are easily overlooked during nonautomated calibration.},
  doi = {10.1111/j.1745-6584.1997.tb00082.x},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{poeter2005,
  author = {Poeter, E. and Hill, M. and Banta, E. and Mehl, S. and Christensen,
	S.},
  title = {U{CODE}--2005 and six other computer codes for universal sensitivity
	analysis, calibration, and uncertainty evaluation},
  institution = {United States Geological Survey},
  year = {2005},
  type = {Technical Methods 6--A11},
  address = {Reston, Virginia, USA},
  booktitle = {Technical Methods 6-A11},
  owner = {RRojas},
  pages = {--283 pp},
  publisher = {U.S. Geological Survey},
  refid = {POETER2005A},
  timestamp = {2008.11.04}
}

@ARTICLE{poeter1995,
  author = {Poeter, E. and McKenna, S.},
  title = {Reducing uncertainty associated with groundw--water flow and transport
	predictions},
  journal = {Ground Water},
  year = {1995},
  volume = {33},
  pages = {899--904},
  number = {6},
  abstract = {Effective evaluation of ground-water flow and transport problems requires
	consideration of the range of possible interpretations of the subsurface
	given the available, disparate types of data. Geostatistical simulation
	(using a modified version of ISIM3D) of hydrofacies units produces
	many realizations that honor the available geologic data and represent
	the range of subsurface interpretations of unit geometry. Hydraulic
	observations are utilized to accept or reject the geometric configurations
	of hydrofacies units and to estimate ground-water flow parameters
	for the units (using MODFLOWP). These realizations are employed to
	evaluate the uncertainty of the resulting value of the response function
	(ground-water flow velocity and contaminant concentration) using
	MT3D. The process is illustrated with a synthetic data set for which
	the "truth" is known, and produces a striking reduction in the distribution
	of predicted contaminant concentrations. The same system is evaluated
	three times: first with only hard data, then with both hard and soft
	data, and finally with only the realizations that honor the hydraulic
	data (i.e., those accepted after parameter estimation via inverse
	flow modeling). Using only hard data, the mean concentration predicted
	for all realizations at the point of interest is nearly two orders
	of magnitude lower than the true value and the standard deviation
	of the log of concentration is two. The addition of soft data brings
	the mean concentration within one order of magnitude of the true
	value and reduces the standard deviation of the log of concentration
	to one. After eliminating realizations using inverse flow modeling,
	the mean concentration is one-third of the true value and the standard
	deviation of the log of concentration less than 0.5.},
  doi = {10.1111/j.1745-6584.1995.tb00034.x},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{pokhrel+al2010,
  author = {Pokhrel, P. and Gupta, H.},
  title = {On the use of spatial regularization strategies to improve calibration
	of distributed watershed models},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W01505},
  number = {1},
  doi = {10.1029/2009WR008066},
  tags = {Calibration}
}

@ARTICLE{pokhrel+al2008,
  author = {Pokhrel, P. and Gupta, H. and Wagener, T.},
  title = {A spatial regularization approach to parameter estimation for a distributed
	watershed model},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = { W12419},
  abstract = {Partial support for this work provided by the National Weather Service
	Office of Hydrology under grant NA04NWS462001 and by SAHRA under
	NSF-STC grant EAR-9876800 is gratefully acknowledged. The first author
	was also partially supported by a grant from the World Laboratory
	International Center for Scientific Culture.},
  doi = {10.1029/2007WR006615},
  keywords = {RAINFALL-RUNOFF MODELS, INTERCOMPARISON PROJECT, GLOBAL OPTIMIZATION,
	HYDROLOGIC-MODELS, AUTOMATIC CALIBRATION, REGIONALIZATION, IDENTIFICATION,
	UNCERTAINTY, INFORMATION, SIMULATION},
  tags = {Calibration}
}

@ARTICLE{pokhrel+al2011,
  author = {Pokhrel, P. and Yilmaz, K. and Gupta, H.},
  title = {Multiple-criteria calibration of a distributed watershed model using
	spatial regularization and response signatures},
  journal = {Journal of Hydrology},
  year = {2011},
  note = {in press.},
  abstract = {This paper explores the use of a semi-automated multiple-criteria
	calibration approach for estimating the parameters of the spatially
	distributed HL-DHM model to the Blue River basin, Oklahoma. The study
	was performed in the context of Phase 2 of the DMIP project organized
	by the Hydrology Lab of the NWS. To deal with the problem of ill
	conditioning, we employ a regularization approach that constrains
	the search space using information contained in a priori estimates
	of the spatially distributed parameter fields developed from soils
	and other geo-spatial datasets. Unlike the commonly used spatial-multiplier
	method, our more general approach allows the parameters to depart
	non-uniformly (to some degree) from the a priori spatial pattern.
	The approach reduces the number of unknowns to be estimated using
	historical input{--}output data from 860 to 35. Two commonly used
	summary statistics of the model residuals, MSE and MSEL, are used
	to optimize fitting of the model to both the peaks and the recession
	periods of the time series data. A signature measure approach is
	used to select parameter sets that are close to Pareto-optimal in
	terms of MSE and MSEL, but which provide more consistent representation
	of the hydrologic behavior of the watershed as summarized by measures
	derived from the flow duration curve. While the results support the
	methods used in this analysis and show considerable improvement over
	the a priori parameter estimates, we find that the basin has some
	peculiar behaviors (including time non-stationarity) that the HL-DHM
	model as implemented is not set up to reproduce},
  doi = {10.1016/j.jhydrol.2008.12.004},
  keywords = {Distributed watershed models, Regularization, Multiple-criteria calibration,
	Diagnostics, Parameter estimation, Information},
  tags = {Calibration, Signatures}
}

@ARTICLE{poli2008,
  author = {Poli, Riccardo},
  title = {Analysis of the Publications on the Applications of Particle Swarm
	Optimisation},
  journal = {Journal of Artificial Evolution and Applications},
  year = {2008},
  volume = {2008},
  pages = {1--10},
  number = {685175},
  abstract = {Particle swarm optimisation (PSO) has been enormously successful.
	Within little more than a decade hundreds of papers have reported
	successful applications of PSO. In fact, there are so many of them,
	that it is difficult for PSO practitioners and researchers to have
	a clear up-to-date vision of what has been done in the area of PSO
	applications. This brief paper attempts to fill this gap, by categorising
	a large number of publications dealing with PSO applications stored
	in the IEEE Xplore database at the time of writing.},
  doi = {10.1155/2008/685175},
  tags = {PSO}
}

@ARTICLE{poli+al2007,
  author = {Poli, R. and Kennedy, J. and Blackwell, T.},
  title = {Particle swarm optimization},
  journal = {Swarm Intelligence},
  year = {2007},
  volume = {1},
  pages = {33--57},
  number = {1},
  abstract = {Particle swarm optimization (PSO) has undergone many changes since
	its introduction in 1995. As researchers have learned about the technique,
	they have derived new versions, developed new applications, and published
	theoretical studies of the effects of the various parameters and
	aspects of the algorithm. This paper comprises a snapshot of particle
	swarming from the authors{'} perspective, including variations in
	the algorithm, current and ongoing research, applications and open
	problems. },
  doi = {10.1007/s11721-007-0002-0},
  keywords = {Particle swarms - Particle swarm optimization - PSO - Social networks
	- Swarm theory - Swarm dynamics - Real world applications },
  tags = {PSO}
}

@TECHREPORT{pollock1994,
  author = {Pollock, D.},
  title = {User's guide for {MODPATH/MODPATH}--{PLOT}, Version 3: {A} particle
	tracking post--processing package for {MODFLOW}, the {U}.{S}. {G}eological
	{S}urvey finite--difference ground--water flow model},
  institution = {United States Geological Survey},
  year = {1994},
  type = {Open--File Report 94--464},
  address = {Reston, Virginia, USA},
  booktitle = {Open-File Report 94-464},
  owner = {RRojas},
  pages = {--249 pp},
  publisher = {U.S. Geological Survey},
  refid = {POLLOCK1994},
  timestamp = {2008.11.04}
}

@ARTICLE{pope+al2000,
  author = {Pope, V. and Gallani, M. and Rowntree, P. and Stratton, R.},
  title = {The impact of new physical parametrizations in the {H}adley {C}entre
	climate model: {HadAM3}},
  journal = {Climate Dynamics},
  year = {2000},
  volume = {16},
  pages = {123--146},
  number = {2--3},
  abstract = {Results are presented from the latest version of the Hadley Centre
	climate model, HadAM3 (Hadley Centre Atmospheric Model version 3).
	It represents a significant improvement over the previous version,
	HadAM2b. This is demonstrated using a series of ten year integrations
	with AMIP (Atmospheric Model Intercomparison Project) boundary conditions.
	The work covers three aspects of model performance: (1) it shows
	the improvements in the mean climate in changing from HadAM2b to
	HadAM3; (2) it demonstrates that the model now compares well with
	observations and (3) it isolates the impacts of new physical parametrizations.},
  doi = {10.1007/s003820050009},
  keywords = {HadAM3 climate model},
  tags = {Climate Models}
}

@ARTICLE{praskieviczchang2009,
  author = {Praskievicz, S. and Chang, H.},
  title = {A review of hydrological modelling of basin--scale climate change
	and urban development impacts},
  journal = {Progress in Physical Geography},
  year = {2009},
  volume = {33},
  pages = {650--671},
  number = {5},
  abstract = {Hydrological modelling is a valuable tool for researchers in geography
	and other disciplines for studying the processes governing impacts
	of climate change and urban development on water resources and for
	projecting potential ranges of impacts from scenarios of future change.
	Modelling is an inherently probabilistic exercise, with uncertainty
	amplified at each stage of the process, from scenario generation
	to issues of scale, to simulation of hydrological processes, to management
	impacts. At the basin scale, significant factors affecting hydrological
	impacts of climate change include latitude, topography, geology,
	and land use. Under scenarios of future climate change, many basins
	are likely to experience changes not only in their mean hydrology,
	but also in the frequency and magnitude of extreme hydrological events.
	Impacts of climate change on water quality are largely determined
	by hydrological changes and by the nature of pollutants as flushingor
	dilution-controlled. The most significant impact of urban development
	on water resources is an increase in overall surface runoff and the
	flashiness of the storm hydrograph. The increase in impervious surface
	area associated with urban development also contributes to degradation
	of water quality as a result of non-point source pollution. Modelling
	studies on the combined impacts of climate change and urban development
	have found that either change may be more significant, depending
	on scenario assumptions and basin characteristics, and that each
	type of change may amplify or ameliorate the effects of the other.
	Hydrological impacts of climate change and urban development are
	likely to significantly affect future water resource management.},
  doi = {10.1177/0309133309348098},
  tags = {Impacts}
}

@INPROCEEDINGS{prudhomme2006,
  author = {Prudhomme, C.},
  title = {G{CM} and downscaling uncertainty in modelling of current river flow:
	{W}hy is it important for future impacts?},
  booktitle = {5th FRIEND World Conf.},
  year = {2006},
  pages = {375--381},
  publisher = {Havana. IAHS Publication 308},
  tags = {Downscaling}
}

@ARTICLE{prudhommedavies2009b,
  author = {Prudhomme, C. and Davies, C.},
  title = {Assessing uncertainties in climate change impact analyses on the
	river flow regimes in the {UK}. {P}art 2: {F}uture climate},
  journal = {Climatic Change},
  year = {2009},
  volume = {93},
  pages = {197--222},
  number = {1--2},
  abstract = {The first part of this paper demonstrated the existence of bias in
	GCM-derived precipitation series, downscaled using either a statistical
	technique (here the Statistical Downscaling Model) or dynamical method
	(here high resolution Regional Climate Model HadRM3) propagating
	to river flow estimated by a lumped hydrological model. This paper
	uses the same models and methods for a future time horizon (2080s)
	and analyses how significant these projected changes are compared
	to baseline natural variability in four British catchments. The UKCIP02
	scenarios, which are widely used in the UK for climate change impact,
	are also considered. Results show that GCMs are the largest source
	of uncertainty in future flows. Uncertainties from downscaling techniques
	and emission scenarios are of similar magnitude, and generally smaller
	than GCM uncertainty. For catchments where hydrological modelling
	uncertainty is smaller than GCM variability for baseline flow, this
	uncertainty can be ignored for future projections, but might be significant
	otherwise. Predicted changes are not always significant compared
	to baseline variability, less than 50% of projections suggesting
	a significant change in monthly flow. Insignificant changes could
	occur due to climate variability alone and thus cannot be attributed
	to climate change, but are often ignored in climate change studies
	and could lead to misleading conclusions. Existing systematic bias
	in reproducing current climate does impact future projections and
	must, therefore, be considered when interpreting results. Changes
	in river flow variability, important for water management planning,
	can be easily assessed from simple resampling techniques applied
	to both baseline and future time horizons. Assessing future climate
	and its potential implication for river flows is a key challenge
	facing water resource planners. This two-part paper demonstrates
	that uncertainty due to hydrological and climate modelling must and
	can be accounted for to provide sound, scientifically-based advice
	to decision makers.},
  doi = {10.1007/s10584-008-9461-6},
  tags = {Uncertainty, Thesis}
}

@ARTICLE{prudhommedavies2009a,
  author = {Prudhomme, C. and Davies, H.},
  title = {Assessing uncertainties in climate change impact analyses on the
	river flow regimes in the {UK}. {P}art 1: {B}aseline climate},
  journal = {Climatic Change},
  year = {2009},
  volume = {93},
  pages = {177--195},
  number = {1--2},
  abstract = {Assessing future climate and its potential implications on river flows
	is a key challenge facing water resource planners. Sound, scientifically-based
	advice to decision makers also needs to incorporate information on
	the uncertainty in the results. Moreover, existing bias in the reproduction
	of the â€˜currentâ€™ (or baseline) river flow regime is likely to
	transfer to the simulations of flow in future time horizons, and
	it is thus critical to undertake baseline flow assessment while undertaking
	future impacts studies. This paper investigates the three main sources
	of uncertainty surrounding climate change impact studies on river
	flows: uncertainty in GCMs, in downscaling techniques and in hydrological
	modelling. The study looked at four British catchmentsâ€™ flow series
	simulated by a lumped conceptual rainfall--runoff model with observed
	and GCM-derived rainfall series representative of the baseline time
	horizon (1961--1990). A block-resample technique was used to assess
	climate variability, either from observed records (natural variability)
	or reproduced by GCMs. Variations in mean monthly flows due to hydrological
	model uncertainty from different model structures or model parameters
	were also evaluated. Three GCMs (HadCM3, CCGCM2, and CSIRO-mk2) and
	two downscaling techniques (SDSM and HadRM3) were considered. Results
	showed that for all four catchments, GCM uncertainty is generally
	larger than downscaling uncertainty, and both are consistently greater
	than uncertainty from hydrological modelling or natural variability.
	No GCM or downscaling technique was found to be significantly better
	or to have a systematic bias smaller than the others. This highlights
	the need to consider more than one GCM and downscaling technique
	in impact studies, and to assess the bias they introduce when modelling
	river flows},
  doi = {10.1007/s10584-008-9464-3},
  tags = {Thesis, Uncertainty}
}

@INCOLLECTION{prudhommedavies2007,
  author = {Prudhomme, C. and Davies, H.},
  title = {Comparison of different sources of uncertainty in climate change
	impact studies in {G}reat {B}ritain},
  booktitle = {Climatic and anthropogenic impacts on the variability of water resources},
  publisher = {FRIEND},
  year = {2007},
  pages = {183--190},
  address = {Paris. Paris, UNESCO},
  abstract = {The paper assesses the range of changes from a comprehensive set of
	scenarios describing uncertainties due to climate modelling and climate
	projections for the 2080s. The study focuses on the mean annual flow
	ANN and the low flow regime indicator Q95. The changes are represented
	by confidence bands including 90% of the future simulations and are
	compared with estimate variations in ANN and Q95 due to natural climatic
	variability. The climatic projections include incertainty in future
	emissions of greenhouse gases, in midolling global climate and in
	downscaling methodologies, while the natural variability is assessed
	through data resampling. Results are analysed to assess which of
	the considered uncertainties is largest for one British test catchment,
	and to provide guidance for incorporating uncertainty in future impact
	studies.},
  journal = {Hydrol. Process},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{prudhomme+al2003,
  author = {Prudhomme, C. and Jakob, D. and Svensson, C.},
  title = {Uncertainty and climate change impact on the flood regime of small
	{UK} catchments},
  journal = {Journal of Hydrology},
  year = {2003},
  volume = {277},
  pages = {1--23},
  number = {1--2},
  abstract = {A rigorous methodology is described for quantifying some of the uncertainties
	of climate change impact studies, excluding those due to downscaling
	techniques, and applied on a set of five catchments in Great Britain.
	Uncertainties in climate change are calculated from a set of 25,000
	climate scenarios randomly generated by a Monte Carlo simulation,
	using several Global Climate Models, SRES-98 emission scenarios and
	climate sensitivities. Flow series representative of current and
	future conditions were simulated using a conceptual hydrological
	model. Generalised Pareto Distributions were fitted to Peak-Over-Threshold
	series for each scenario, and future flood scenarios were compared
	to current conditions for four typical flood events. Most scenarios
	show an increase in both the magnitude and the frequency of flood
	events, generally not greater than the 95% confidence limits. The
	largest uncertainty can be attributed to the type of GCM used, with
	the magnitude of changes varying by up to a factor 9 in Northern
	England and Scotland. It is therefore essential that climate change
	impact studies consider a range of climate scenarios derived from
	different GCMs, and that adaptation policies do not rely on results
	from only very few scenarios},
  doi = {10.1016/S0022-1694(03)00065-9},
  keywords = {Climate change impact, Flood regime, UK, Resampling, Uncertainty,
	GCMs, Natural variability, Bootstrapping, Confidence intervals, Flood
	magnitude, Flood frequency, Risk assessment},
  tags = {Uncertainty}
}

@ARTICLE{prudhomme+al2011,
  author = {Prudhomme, C. and Parry, S. and Hannaford, J. and Clarck, D. and
	Hagemann, S. and Voss, F.},
  title = {{How well do large-scale models reproduce regional hydrological extremes
	in Europe?}},
  journal = {Journal of Hydrometeorology},
  year = {2011},
  volume = {12},
  pages = {1181--1204},
  number = {6},
  abstract = {This paper presents a new methodology for assessing the ability of
	gridded hydrological models to reproduce large-scale hydrological
	high and low flow events (as a proxy for hydrological extremes) as
	described by catalogues of historical droughts [using the regional
	deficiency index (RDI)] and high flows [regional flood index (RFI)]
	previously derived from river flow measurements across Europe. Using
	the same methods, total runoff simulated by three global hydrological
	models from the Water Model Intercomparison Project (WaterMIP) [Joint
	U.K. Land Environment Simulator (JULES), Water Global Assessment
	and Prognosis (WaterGAP), and Max Planck Institute Hydrological Model
	(MPI-HM)] run with the same meteorological input (watch forcing data)
	at the same spatial 0.5° grid was used to calculate simulated RDI
	and RFI for the period 1963–2001 in the same European regions, directly
	comparable with the observed catalogues. Observed and simulated RDI
	and RFI time series were compared using three performance measures:
	the relative mean error, the ratio between the standard deviation
	of simulated over observed series, and the Spearman correlation coefficient.
	Results show that all models can broadly reproduce the spatiotemporal
	evolution of hydrological extremes in Europe to varying degrees.
	JULES tends to produce prolonged, highly spatially coherent events
	for both high and low flows, with events developing more slowly and
	reaching and sustaining greater spatial coherence than observed—this
	could be due to runoff being dominated by slow-responding subsurface
	flow. In contrast, MPI-HM shows very high variability in the simulated
	RDI and RFI time series and a more rapid onset of extreme events
	than observed, in particular for regions with significant water storage
	capacity—this could be due to possible underrepresentation of infiltration
	and groundwater storage, with soil saturation reached too quickly.
	WaterGAP shares some of the issues of variability with MPI-HM—also
	attributed to insufficient soil storage capacity and surplus effective
	precipitation being generated as surface runoff—and some strong spatial
	coherence of simulated events with JULES, but neither of these are
	dominant. Of the three global models considered here, WaterGAP is
	arguably best suited to reproduce most regional characteristics of
	large-scale high and low flow events in Europe. Some systematic weaknesses
	emerge in all models, in particular for high flows, which could be
	a product of poor spatial resolution of the input climate data (e.g.,
	where extreme precipitation is driven by local convective storms)
	or topography. Overall, this study has demonstrated that RDI and
	RFI are powerful tools that can be used to assess how well large-scale
	hydrological models reproduce large-scale hydrological extremes—an
	exercise rarely undertaken in model intercomparisons.},
  doi = {10.1175/2011JHM1387.1},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{prudhomme+al2002,
  author = {Prudhomme, C. and Reynard, N. and Crooks, S.},
  title = {Downscaling of global climate models for flood frequency analysis:
	{W}here are we now?},
  journal = {Hydrological Processes},
  year = {2002},
  volume = {16},
  pages = {1137--1150},
  number = {6},
  abstract = {The issues of downscaling the results from global climate models (GCMs)
	to a scale relevant for hydrological impact studies are examined.
	GCM outputs, typically at a spatial resolution of around 3Â° latitude
	and 4Â° longitude, are currently not considered reliable at time
	scales shorter than 1 month. Continuous rainfall-runoff modelling
	for flood regime assessment requires input at the daily or even hourly
	time-step. A review of the different methodologies suggested in the
	literature to downscale GCM results at smaller spatial and temporal
	resolutions is presented. The methods, from simple interpolation
	to more sophisticated dynamical modelling, through multiple regression
	and weather generators, are, however, mostly based directly on GCM
	outputs, sometimes at daily time-step. The approach adopted is a
	simple, empirical methodology based on modelled monthly changes from
	the HadCM2 greenhouse gases experiment for the time horizon 2050s.
	Three daily rainfall scenarios are derived from the same set of monthly
	changes, representing different possible changes in the rainfall
	regime. The first scenario represents an increase of the occurrence
	of frontal systems, corresponding to a decrease in the rainfall intensity;
	the second corresponds to an increase in convective storm-type rainfall,
	characterized by extreme events with higher intensity; the third
	one assumes an increase in the monthly rainfall without any change
	in rainfall variability. A continuous daily rainfall-runoff model,
	calibrated for the Severn catchment, was used to generate daily flow
	series for the 1961-90 baseline period and the 2050s, and a peaks-over-threshold
	analysis was undertaken to produce flood frequency distributions
	for the two time horizons. Though the three scenarios lead to an
	increase in the magnitude and the frequency of the extreme flood
	events, the impact is strongly influenced by the type of daily rainfall
	scenario applied. We conclude that if the next generation of GCMs
	produce more reliable rainfall variance estimates, then more appropriate
	ways of deriving rainfall scenarios could be developed using weather
	generators rather than empirical methods.},
  doi = {10.1002/hyp.1054.},
  tags = {Downscaling}
}

@MANUAL{R2011,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Development Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2011},
  note = {{ISBN} 3-900051-07-0},
  organisation = {R Foundation for Statistical Computing},
  tags = {Technical reports, R},
  url = {http://www.R-project.org}
}

@ARTICLE{raisaneneklund2011,
  author = {R\"ais\"anen, J. and Eklund, J.},
  title = {{21st Century changes in snow climate in Northern Europe: a high-resolution
	view from ENSEMBLES regional climate models}},
  journal = {Climate Dynamics},
  year = {2011},
  pages = {1--17},
  abstract = {Changes in snow amount in northern Europe are analysed from 11 regional
	model simulations of 21st century climate under the Special Report
	on Emissions Scenarios A1B scenario. These high-resolution models
	collectively indicate a future decrease in the water equivalent of
	the snow pack (SWE). Although winter precipitation increases, this
	is insufficient to compensate for the increased fraction of liquid
	precipitation and increased snowmelt caused by higher temperatures.
	The multi-model mean results suggest a slight increase in March mean
	SWE only locally in mountains of northern Sweden, and even there,
	snow is reduced earlier in winter and later in spring. The nature
	of the changes remains the same throughout the 21st century, but
	their magnitude increases with time as the greenhouse gas forcing
	grows larger. The geographical patterns of the change support the
	physically intuitive view that snow is most vulnerable to warming
	in areas with relatively mild winter climate. A similar relationship
	emerges when comparing the 11 simulations with each other: the ratio
	between the relative SWE decrease and winter mean temperature change
	is larger (smaller) for simulations with higher (lower) late 20th
	century winter temperatures. Despite the decrease in long-term mean
	SWE, individual snow-rich winters do occur in the simulations, but
	they become increasingly uncommon towards the end of the 21st century.},
  doi = {10.1007/s00382-011-1076-3},
  owner = {rojasro},
  timestamp = {2011.07.12}
}

@ARTICLE{raisanen2004,
  author = {R\"ais\"anen, J. and Hansson, U. and Ullerstig, A. and Graham, L.
	and Jones, C. and Meier, H. and Samuelsson, P. and Will\'en, U.},
  title = {European climate in the late twenty-first century: regional simualtions
	with two driving global models and two forcing scenarios},
  journal = {Climate Dynamics},
  year = {2004},
  volume = {22},
  pages = {13--31},
  number = {1},
  month = {January},
  abstract = {A basic analysis is presented for a series of regional climate change
	simulations that were conducted by the Swedish Rossby Centre and
	contribute to the PRUDENCE (Prediction of Regional scenarios and
	Uncertainties for Defining EuropeaN Climate change risks and Effects)
	project. For each of the two driving global models HadAM3H and ECHAM4/OPYC3,
	a 30-year control run and two 30-year scenario runs (based on the
	SRES A2 and B2 emission scenarios) were made with the regional model.
	In this way, four realizations of climate change from 1961–1990 to
	2071–2100 were obtained. The simulated changes are larger for the
	A2 than the B2 scenario (although with few qualitative differences)
	and in most cases in the ECHAM4/OPYC3-driven (RE) than in the HadAM3H-driven
	(RH) regional simulations. In all the scenario runs, the warming
	in northern Europe is largest in winter or late autumn. In central
	and southern Europe, the warming peaks in summer when it locally
	reaches 10 °C in the RE-A2 simulation and 6–7 °C in the RH-A2 and
	RE-B2 simulations. The four simulations agree on a general increase
	in precipitation in northern Europe especially in winter and on a
	general decrease in precipitation in southern and central Europe
	in summer, but the magnitude and the geographical patterns of the
	change differ markedly between RH and RE. This reflects very different
	changes in the atmospheric circulation during the winter half-year,
	which also lead to quite different simulated changes in windiness.
	All four simulations show a large increase in the lowest minimum
	temperatures in northern, central and eastern Europe, most likely
	due to reduced snow cover. Extreme daily precipitation increases
	even in most of those areas where the mean annual precipitation decreases.},
  doi = {10.1007/s00382-003-0365-x},
  owner = {rojasro},
  timestamp = {2009.08.14}
}

@ARTICLE{raisanenpalmer2001,
  author = {R\"ais\"anen, J. and Palmer, T.},
  title = {A probability and decision--model analysis of a multimodel ensemble
	of climate change simulations},
  journal = {Journal of Climate},
  year = {2001},
  volume = {14},
  pages = {3212--3226},
  number = {15},
  abstract = {Because of the inherent uncertainties in the computational representation
	of climate and because of unforced chaotic climate variability, it
	is argued that climate change projections should be expressed in
	probabilistic form. In this paper, 17 Coupled Model Intercomparison
	Project second-phase experiments sharing the same gradual increase
	in atmospheric CO2 are treated as a probabilistic multimodel ensemble
	projection of future climate. Tools commonly used for evaluation
	of probabilistic weather and seasonal forecasts are applied to this
	climate change ensemble. The probabilities of some temperature- and
	precipitation-related events defined for 20-yr seasonal means of
	climate are first studied. A cross-verification exercise is then
	used to obtain an upper estimate of the quality of these probability
	forecasts in terms of Brier skill scores, reliability diagrams, and
	potential economic value. Skill and value estimates are consistently
	higher for temperature- related events (e.g., will the 20-yr period
	around the doubling of CO2 be at least 1 degreesC warmer than the
	present?) than for precipitation-related events (e.g., will the mean
	precipitation decrease by 10% or more?). For large enough CO2 forcing,
	however, probabilistic projections of precipitation-related events
	also exhibit substantial potential economic value for a range of
	cost-loss ratios. The treatment of climate change information in
	a probabilistic rather than deterministic manner (e.g., using the
	ensemble consensus forecast) can greatly enhance its potential value.},
  doi = {10.1175/1520-0442(2001)014<3212:APADMA>2.0.CO;2},
  keywords = {OCEAN-ATMOSPHERE MODEL, TRANSIENT CO2 EXPERIMENT, COUPLED MODEL, INTEGRATIONS,
	FORECASTS, VERSION, WEATHER, SYSTEM, SPINUP, GCM},
  tags = {Multimodel - Ensambles}
}

@TECHREPORT{raftery2003,
  author = {Raftery, A. and Balabdaoui, F. and Gneiting, T. and Polakowski, M.},
  title = {Using {B}ayesian model averaging to calibrate forecast ensembles},
  institution = {Department of Statistics, University of Washington, Seattle, Washington},
  year = {2003},
  type = {Technical Report no. 440},
  address = {Washington, USA},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{rafteryzheng2003,
  author = {Raftery, A. and Zhang, Y.},
  title = {Discussion: {P}erformance of {B}ayesian model averaging},
  journal = {Journal of the American Statistical Association},
  year = {2003},
  volume = {98},
  pages = {931--938},
  number = {464},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://ideas.repec.org/a/bes/jnlasa/v98y2003p931-938.html}
}

@ARTICLE{rahmstorf+al2007,
  author = {Rahmstorf, S. and Cazenave, A. and Church, J. and Hansen, J. and
	Keeling, R. and Parker, D. and Somerville, R.},
  title = {Recent climate observations compared to projections},
  journal = {Science},
  year = {2007},
  volume = {316},
  pages = {709},
  number = {5825},
  abstract = {We present recent observed climate trends for carbon dioxide concentration,
	global mean air temperature, and global sea level, and we compare
	these trends to previous model projections as summarized in the 2001
	assessment report of the Intergovernmental Panel on Climate Change
	(IPCC). The IPCC scenarios and projections start in the year 1990,
	which is also the base year of the Kyoto protocol, in which almost
	all industrialized nations accepted a binding commitment to reduce
	their greenhouse gas emissions. The data available for the period
	since 1990 raise concerns that the climate system, in particular
	sea level, may be responding more quickly to climate change than
	our current generation of models indicates.},
  doi = {10.1126/science.1136843},
  pmid = {17272686},
  tags = {Global Cycle}
}

@ARTICLE{rajemujumdar2010,
  author = {Raje, D. and Mujumdar, P.},
  title = {Reservoir performance under uncertainty in hydrologic impacts of
	climate change},
  journal = {Advances in Water Resources},
  year = {2010},
  volume = {33},
  pages = {312--326},
  number = {3},
  abstract = {Relatively few studies have addressed water management and adaptation
	measures in the face of changing water balances due to climate change.
	The current work studies climate change impact on a multipurpose
	reservoir performance and derives adaptive policies for possible
	future scenarios. The method developed in this work is illustrated
	with a case study of Hirakud reservoir on the Mahanadi river in Orissa,
	India, which is a multipurpose reservoir serving flood control, irrigation
	and power generation. Climate change effects on annual hydropower
	generation and four performance indices (reliability with respect
	to three reservoir functions, viz. hydropower, irrigation and flood
	control, resiliency, vulnerability and deficit ratio with respect
	to hydropower) are studied. Outputs from three general circulation
	models (GCMs) for three scenarios each are downscaled to monsoon
	streamflow in the Mahanadi river for two future time slices, 2045--65
	and 2075--95. Increased irrigation demands, rule curves dictated
	by increased need for flood storage and downscaled projections of
	streamflow from the ensemble of GCMs and scenarios are used for projecting
	future hydrologic scenarios. It is seen that hydropower generation
	and reliability with respect to hydropower and irrigation are likely
	to show a decrease in future in most scenarios, whereas the deficit
	ratio and vulnerability are likely to increase as a result of climate
	change if the standard operating policy (SOP) using current rule
	curves for flood protection is employed. An optimal monthly operating
	policy is then derived using stochastic dynamic programming (SDP)
	as an adaptive policy for mitigating impacts of climate change on
	reservoir operation. The objective of this policy is to maximize
	reliabilities with respect to multiple reservoir functions of hydropower,
	irrigation and flood control. In variations to this adaptive policy,
	increasingly more weightage is given to the purpose of maximizing
	reliability with respect to hydropower for two extreme scenarios.
	It is seen that by marginally sacrificing reliability with respect
	to irrigation and flood control, hydropower reliability and generation
	can be increased for future scenarios. This suggests that reservoir
	rules for flood control may have to be revised in basins where climate
	change projects an increasing probability of droughts. However, it
	is also seen that power generation is unable to be restored to current
	levels, due in part to the large projected increases in irrigation
	demand. This suggests that future water balance deficits may limit
	the success of adaptive policy options},
  doi = {10.1016/j.advwatres.2009.12.008},
  keywords = {Reservoir performance, Climate change, Uncertainty, Adaptive policy},
  tags = {Uncertainty, Impacts, Disturbed Catchment}
}

@ARTICLE{ramarao1995,
  author = {{RamaRao}, B. and {LaVenue}, A. and {de Marsily}, G. and Marietta,
	M.},
  title = {Pilot point methodology for automated calibration of an ensemble
	of conditionally simulated transmissivity fields. 1. {T}heory and
	computational experiments},
  journal = {Water Resources Research},
  year = {1995},
  volume = {31},
  pages = {475--493},
  number = {3},
  abstract = {A new methodology for solution of the inverse problem in groundwater
	hydrology is proposed and applied to a site in southeastern New Mexico
	with extensive hydrogeologic data. The methodology addresses the
	issue of nonuniqueness of the inverse solutions by generating an
	ensemble of transmissivity fields considered to be equally likely,
	each of which is in agreement with the measured transmissivity and
	pressure data. It consists of generating a selected number of conditionally
	simulated transmissivity fields and then calibrating each of the
	fields to match the measured steady state or transient pressures,
	in a least squares sense. The calibration phase involves an iterative
	implementation of an automated pilot point approach coupled with
	conditional simulations. Pilot points are the parameters of calibration.
	They are synthetic transmissivity data which are added to the transmissivity
	database to produce a revised conditional simulation during calibration.
	Coupled kriging and adjoint sensitivity analysis is employed for
	the optimal location of pilot points, and gradient search methods
	are used to derive their optimal transmissivities. The pilot point
	methodology is well suited for characterizing the spatial variability
	of the transmissivity field in contrast to methods using zonation.
	Pilot points are located where their potential for minimizing the
	objective function is the highest. This minimizes the perturbations
	in the transmissivities which are optimally assigned to the pilot
	point and results in minimal changes to the covariance structure
	of the transmissivity field. The calibrated fields honor the transmissivity
	measurements at their locations, preserve the variogram, and match
	the measured pressures in a least squares sense. Since there are
	numerous options in the execution of this methodology, computational
	experiments have been conducted to identify the most efficient among
	them. The method has been applied to the Waste Isolation Pilot Plant
	(WIPP) site, in southeastern New Mexico, where the U.S. Department
	of Energy is conducting probabilistic system assessment for the permanent
	disposal of transuranic nuclear waste. The resulting calibrated transmissivity
	fields are input to a Monte Carlo analysis of the total system performance.
	The present paper, paper 1 of a two-paper presentation, describes
	the methodology. Paper 2, a companion paper, presents the methodology's
	application to the WIPP site.},
  doi = {10.1029/94WR02258},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@INCOLLECTION{randall+al2007,
  author = {Randall, D. and Wood, R. and Bony, S. and Colman, R. and Fichefet,
	T. and Fyfe, J. and Kattsov, V. and Pitman, A. and Shukla, J. and
	Srinivasan, J. and Stouffer, R. and Sumi, A. and Taylor, K.},
  title = {Climate models and their evaluation},
  booktitle = {Climate Change 2007: The Physical Science Basis. Contribution of
	Working Group I to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {S. Solomon and D. Qin and M. Manning and Z. Chen and M. Marquis and
	K. B. Averyt and M. Tignor and H. L. Miller},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  tags = {IPCC}
}

@ARTICLE{rankinen2006,
  author = {Rankinen, K. and Karvonen, T. and Butterfield, D.},
  title = {An application of the {GLUE} methodology for estimating the parameters
	of the {INCA}--{N} model},
  journal = {Science of the Total Environment},
  year = {2006},
  volume = {365},
  pages = {123--139},
  number = {1--3},
  abstract = {The conceptual and parameter uncertainty of the semi-distributed INCA-N
	(Integrated Nutrients in Catchments—Nitrogen) model was studied using
	the GLUE (Generalized Likelihood Uncertainty Estimation) methodology
	combined with quantitative experimental knowledge, the concept known
	as ‘soft data’. Cumulative inorganic N leaching, annual plant N uptake
	and annual mineralization proved to be useful soft data to constrain
	the parameter space. The INCA-N model was able to simulate the seasonal
	and inter-annual variations in the stream-water nitrate concentrations,
	although the lowest concentrations during the growing season were
	not reproduced. This suggested that there were some retention processes
	or losses either in peatland/wetland areas or in the river which
	were not included in the INCA-N model. The results of the study suggested
	that soft data was a way to reduce parameter equifinality, and that
	the calibration and testing of distributed hydrological and nutrient
	leaching models should be based both on runoff and/or nutrient concentration
	data and the qualitative knowledge of experimentalist.},
  doi = {10.1016/j.scitotenv.2006.02.034},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{rapaiczeljko2009,
  author = {Rapai{\'c}, M. and Kanovi{\'c}, {\v{Z}}},
  title = {Time-varying PSO - convergence analysis, convergence-related parameterization
	and new parameter adjustment schemes},
  journal = {Information Processing Letters},
  year = {2009},
  volume = {109},
  pages = {548--552},
  number = {11},
  abstract = {In this paper, a formal convergence analysis of the conventional PSO
	algorithms with time-varying parameters is presented. Based on this
	analysis, a new convergence-related parametric model for the conventional
	PSO is introduced. Finally, several new schemes for parameter adjustment,
	providing significant performance benefits, are introduced. Performance
	of these schemes is empirically compared to conventional PSO algorithms
	on a set of selected benchmarks. The tests prove effectiveness of
	the newly introduced schemes, especially regarding their ability
	to efficiently explore the search space.},
  doi = {10.1016/j.ipl.2009.01.021},
  keywords = {Analysis of algorithms, Global optimization, Particle Swarm Optimization},
  tags = {Calibration, PSO}
}

@ARTICLE{ratnaweera+al2004,
  author = {Ratnaweera, A. and Halgamuge, S. and Watson, H.},
  title = {Self-Organizing Hierarchical Particle Swarm Optimizer With Time-Varying
	Acceleration Coefficients},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2004},
  volume = {8},
  pages = {240--255},
  number = {3},
  abstract = {This paper introduces a novel parameter automation strategy for the
	particle swarm algorithm and two further extensions to improve its
	performance after a predefined number of generations. Initially,
	to efficiently control the local search and convergence to the global
	optimum solution, time-varying acceleration coefficients (TVAC) are
	introduced in addition to the time-varying inertia weight factor
	in particle swarm optimization (PSO). From the basis of TVAC, two
	new strategies are discussed to improve the performance of the PSO.
	First, the concept of "mutation" is introduced to the particle swarm
	optimization along with TVAC (MPSO-TVAC), by adding a small perturbation
	to a randomly selected modulus of the velocity vector of a random
	particle by predefined probability. Second, we introduce a novel
	particle swarm concept "self-organizing hierarchical particle swarm
	optimizer with TVAC (HPSO-TVAC)". Under this method, only the "social"
	part and the "cognitive" part of the particle swarm strategy are
	considered to estimate the new velocity of each particle and particles
	are reinitialized whenever they are stagnated in the search space.
	In addition, to overcome the difficulties of selecting an appropriate
	mutation step size for different problems, a time-varying mutation
	step size was introduced. Further, for most of the benchmarks, mutation
	probability is found to be insensitive to the performance of MPSO-TVAC
	method. On the other hand, the effect of reinitialization velocity
	on the performance of HPSO-TVAC method is also observed. Time-varying
	reinitialization step size is found to be an efficient parameter
	optimization strategy for HPSO-TVAC method. The HPSO-TVAC strategy
	outperformed all the methods considered in this investigation for
	most of the functions. Furthermore, it has also been observed that
	both the MPSO and HPSO strategies perform poorly when the acceleration
	coefficients are fixed at two.},
  doi = {10.1109/TEVC.2004.826071},
  keywords = {internal combustion spark ignition engines, parameter automation strategy,
	parameter estimation, self-organizing hierarchical particle swarm
	optimizer, time-varying acceleration coefficients, time-varying inertia
	weight factor, optimisation, parameter estimation},
  mzbnote = {TVc1, TVc2},
  tags = {Calibration, PSO}
}

@ARTICLE{rauscher+al2010,
  author = {Rauscher, S. and Coppola, E. and PIani, C. and Girogi, F.},
  title = {{Resolution effects on regional climate model simulations of seasonal
	precipitation over Europe}},
  journal = {Climate Dynamics},
  year = {2010},
  volume = {35},
  pages = {685--711},
  number = {4},
  abstract = {We analyze a set of nine regional climate model simulations for the
	period 1961–2000 performed at 25 and 50 km horizontal grid spacing
	over a European domain in order to determine the effects of horizontal
	resolution on the simulation of precipitation. All of the models
	represent the seasonal mean spatial patterns and amount of precipitation
	fairly well. Most models exhibit a tendency to over-predict precipitation,
	resulting in a domain-average total bias for the ensemble mean of
	about 20% in winter (DJF) and less than 10% in summer (JJA) at both
	resolutions, although this bias could be artificially enhanced by
	the lack of a gauge correction in the observations. A majority of
	the models show increased precipitation at 25 km relative to 50 km
	over the oceans and inland seas in DJF, JJA, and ANN (annual average),
	although the response is strongest during JJA. The ratio of convective
	precipitation to total precipitation decreases over land for most
	models at 25 km. In addition, there is an increase in interannual
	variability in many of the models at 25 km grid spacing. Comparison
	with gridded observations indicates that a majority of models show
	improved skill in simulating both the spatial pattern and temporal
	evolution of precipitation at 25 km compared to 50 km during the
	summer months, but not in winter or on an annual mean basis. Model
	skill at higher resolution in simulating the spatial and temporal
	character of seasonal precipitation is found especially for Great
	Britain. This geographic dependence of the increased skill suggests
	that observed data of sufficient density are necessary to capture
	fine-scale climate signals. As climate models increase their horizontal
	resolution, it is thus a key priority to produce high quality fine
	scale observations for model evaluation.},
  doi = {10.1007/s00382-009-0607-7},
  owner = {rojasro},
  timestamp = {2011.01.12}
}

@ARTICLE{reddykumar2007,
  author = {Reddy, J. and Kumar, N.},
  title = {Multi-objective particle swarm optimization for generating optimal
	trade-offs in reservoir operation},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {2897--2909},
  number = {21},
  abstract = {A multi-objective particle swarm optimization (MOPSO) approach is
	presented for generating Pareto-optimal solutions for reservoir operation
	problems. This method is developed by integrating Pareto dominance
	principles into particle swarm optimization (PSO) algorithm. In addition,
	a variable size external repository and an efficient elitist-mutation
	(EM) operator are introduced. The proposed EM-MOPSO approach is first
	tested for few test problems taken from the literature and evaluated
	with standard performance measures. It is found that the EM-MOPSO
	yields efficient solutions in terms of giving a wide spread of solutions
	with good convergence to true Pareto optimal solutions. On achieving
	good results for test cases, the approach was applied to a case study
	of multi-objective reservoir operation problem, namely the Bhadra
	reservoir system in India. The solutions of EM-MOPSOs yield a trade-off
	curve/surface, identifying a set of alternatives that define optimal
	solutions to the problem. Finally, to facilitate easy implementation
	for the reservoir operator, a simple but effective decision-making
	approach was presented. The results obtained show that the proposed
	approach is a viable alternative to solve multi-objective water resources
	and hydrology problems.},
  doi = {10.1002/hyp.6507},
  keywords = {Multi-objective optimization, Particle swarm optimization, Elitist-mutation,
	Reservoir operation, Hydropower, Irrigation, Water quality, Pareto
	optimal solutions},
  tags = {Disturbed Catchments, PSO, Calibration}
}

@ARTICLE{refsgaard+al2010,
  author = {Refsgaard, J. and Hansen, J.},
  title = {A good-looking catchment can turn into a modeller's nightmare},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {899--912},
  number = {6},
  abstract = {This paper tells a real-life story of a modelling study that in many
	respects, failed to achieve its objectives. The study was carried
	out in 2003 as part of a Danish government policy process aiming
	to identify cost-effective measures to reduce nitrate loads from
	agricultural diffuse pollution in order to be able to meet Water
	Framework Directive requirements (of achieving good ecological status
	in surface water by 2015). To support this process, we established
	a distributed model that we expected could provide reliable predictions
	of the impacts of local-scale measures. The distributed model established
	for the 1312-km2 Odense Pilot River Basin was based on a combination
	of two model codes: (a) DAISY, which simulated root zone processes;
	and (b) MIKE SHE/MIKE 11, which simulated the catchment flow and
	transport processes in surface water and groundwater using a 500-m
	horizontal grid and nine subsurface layers. The reliability of the
	modelling results turned out to be significantly poorer than the
	expectations. The simulations of catchment discharge were not as
	good as previous simulations with lumped models, owing to inappropriate
	conceptualisation of the catchment heterogeneity. Subsequent analysis
	in a PhD study showed that the dominating nitrate reduction process
	in the subsurface (where more than 50\% of the nitrate disappears)
	is governed by geological heterogeneity with length scales smaller
	than the grid size of the numerical model. This poses a severe limitation
	for predicting the effects of local-scale measures, which was one
	of the reasons for choosing a distributed modelling approach. The
	entire modelling process was carried out in a policy context with
	very short deadlines, few allocated resources, and confusion about
	the terms of reference and the roles and functions of the actors
	involved. This context, which is far from the ideal modelling process
	recommended in protocols for good modelling practice, contributed
	significantly to the problems experienced. },
  doi = {10.1080/02626667.2010.505571},
  tags = {Calibration, Philosophical}
}

@ARTICLE{refsgaard2004,
  author = {Refsgaard, J. and Henriksen, H.},
  title = {Modelling guidelines--terminology and guiding principles},
  journal = {Advances in Water Resources},
  year = {2004},
  volume = {27},
  pages = {71--82},
  number = {1},
  abstract = {Some scientists argue, with reference to Popper’s scientific philosophical
	school, that models cannot be verified or validated. Other scientists
	and many practitioners nevertheless use these terms, but with very
	different meanings. As a result of an increasing number of examples
	of model malpractice and mistrust to the credibility of models, several
	modelling guidelines are being elaborated in recent years with the
	aim of improving the quality of modelling studies. This gap between
	the views and the lack of consensus experienced in the scientific
	community and the strongly perceived need for commonly agreed modelling
	guidelines is constraining the optimal use and benefits of models.
	This paper proposes a framework for quality assurance guidelines,
	including a consistent terminology and a foundation for a methodology
	bridging the gap between scientific philosophy and pragmatic modelling.
	A distinction is made between the conceptual model, the model code
	and the site-specific model. A conceptual model is subject to confirmation
	or falsification like scientific theories. A model code may be verified
	within given ranges of applicability and ranges of accuracy, but
	it can never be universally verified. Similarly, a model may be validated,
	but only with reference to site-specific applications and to pre-specified
	performance (accuracy) criteria. Thus, a model’s validity will always
	be limited in terms of space, time, boundary conditions and types
	of application. This implies a continuous interaction between manager
	and modeller in order to establish suitable accuracy criteria and
	predictions associated with uncertainty analysis.},
  doi = {10.1016/j.advwatres.2003.08.006},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{refsgaard2006,
  author = {Refsgaard, J. and {Van der Sluijs}, J. and Brown, J. and {Van der
	Keur}, P.},
  title = {A framework for dealing with uncertainty due to model structure error},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {1586--1597},
  number = {11},
  abstract = {Although uncertainty about structures of environmental models (conceptual
	uncertainty) is often acknowledged to be the main source of uncertainty
	in model predictions, it is rarely considered in environmental modelling.
	Rather, formal uncertainty analyses have traditionally focused on
	model parameters and input data as the principal source of uncertainty
	in model predictions. The traditional approach to model uncertainty
	analysis, which considers only a single conceptual model, may fail
	to adequately sample the relevant space of plausible conceptual models.
	As such, it is prone to modelling bias and underestimation of predictive
	uncertainty. In this paper we review a range of strategies for assessing
	structural uncertainties in models. The existing strategies fall
	into two categories depending on whether field data are available
	for the predicted variable of interest. To date, most research has
	focussed on situations where inferences on the accuracy of a model
	structure can be made directly on the basis of field data. This corresponds
	to a situation of ‘interpolation’. However, in many cases environmental
	models are used for ‘extrapolation’; that is, beyond the situation
	and the field data available for calibration. In the present paper,
	a framework is presented for assessing the predictive uncertainties
	of environmental models used for extrapolation. It involves the use
	of multiple conceptual models, assessment of their pedigree and reflection
	on the extent to which the sampled models adequately represent the
	space of plausible models.},
  doi = {10.1016/j.advwatres.2005.11.013},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{refsgaard2007,
  author = {Refsgaard, J. and {Van der Sluijs}, J. and H{\o}jberg, A. and Vanrolleghem,
	P.},
  title = {Uncertainty in the environmental modelling process--{A} framework
	and guidance},
  journal = {Environmental Modelling \& Software},
  year = {2007},
  volume = {22},
  pages = {1543--1556},
  number = {11},
  abstract = {A terminology and typology of uncertainty is presented together with
	a framework for the modelling process, its interaction with the broader
	water management process and the role of uncertainty at different
	stages in the modelling processes. Brief reviews have been made of
	14 different (partly complementary) methods commonly used in uncertainty
	assessment and characterisation: data uncertainty engine (DUE), error
	propagation equations, expert elicitation, extended peer review,
	inverse modelling (parameter estimation), inverse modelling (predictive
	uncertainty), Monte Carlo analysis, multiple model simulation, NUSAP,
	quality assurance, scenario analysis, sensitivity analysis, stakeholder
	involvement and uncertainty matrix. The applicability of these methods
	has been mapped according to purpose of application, stage of the
	modelling process and source and type of uncertainty addressed. It
	is concluded that uncertainty assessment is not just something to
	be added after the completion of the modelling work. Instead uncertainty
	should be seen as a red thread throughout the modelling study starting
	from the very beginning, where the identification and characterisation
	of all uncertainty sources should be performed jointly by the modeller,
	the water manager and the stakeholders.},
  doi = {10.1016/j.envsoft.2007.02.004},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@TECHREPORT{refsgaard2005,
  author = {Refsgaard, J. and {Van der Sluijs}, J. and H{\o}jberg, A. and Vanrolleghem,
	P.},
  title = {Uncertainty analysis. {H}armoni--{CA} {Guidance N0. 1}},
  year = {2005},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{reilly+al2001,
  author = {Reilly, J. and Stone, P. and Forest, C. and Webster, M. and Jacoby,
	H. and Prinn, R.},
  title = {Climate change. {U}ncertainty and climate change assessments},
  journal = {Science},
  year = {2001},
  volume = {293},
  pages = {430--433},
  number = {5529},
  abstract = {Clear and quantitative discussion of uncertainties is critical for
	public policy making on climate change. The recently completed report
	of the Intergovernmental Panel on Climate Change assessed the uncertainty
	in its findings and forecasts. The uncertainty assessment process
	of the IPCC should be improved in the future by using a consistent
	approach to quantifying uncertainty, focusing the quantification
	on the few key results most important for policy making. The uncertainty
	quantification procedure should be fully documented, and if expert
	judgment is used, a specific list of the experts consulted should
	be included},
  doi = {10.1126/science.1062001},
  pmid = {11463897},
  tags = {Uncertainty, Thesis}
}

@ARTICLE{renard2010,
  author = {Renard, B. and Kavetski, D. and Kuczera, G. and Thyer, M. and Franks,
	S.},
  title = {Understanding predictive uncertainty in hydrologic modeling: {T}he
	challenge of identifying input and structural errors},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W05521},
  abstract = {Meaningful quantification of data and structural uncertainties in
	conceptual rainfall-runoff modeling is a major scientific and engineering
	challenge. This paper focuses on the total predictive uncertainty
	and its decomposition into input and structural components under
	different inference scenarios. Several Bayesian inference schemes
	are investigated, differing in the treatment of rainfall and structural
	uncertainties, and in the precision of the priors describing rainfall
	uncertainty. Compared with traditional lumped additive error approaches,
	the quantification of the total predictive uncertainty in the runoff
	is improved when rainfall and/or structural errors are characterized
	explicitly. However, the decomposition of the total uncertainty into
	individual sources is more challenging. In particular, poor identifiability
	may arise when the inference scheme represents rainfall and structural
	errors using separate probabilistic models. The inference becomes
	ill-posed unless sufficiently precise prior knowledge of data uncertainty
	is supplied; this ill-posedness can often be detected from the behavior
	of the Monte Carlo sampling algorithm. Moreover, the priors on the
	data quality must also be sufficiently accurate if the inference
	is to be reliable and support meaningful uncertainty decomposition.
	Our findings highlight the inherent limitations of inferring inaccurate
	hydrologic models using rainfall-runoff data with large unknown errors.
	Bayesian total error analysis can overcome these problems using independent
	prior information. The need for deriving independent descriptions
	of the uncertainties in the input and output data is clearly demonstrated.},
  doi = {10.1029/2009WR008328},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{renard+al2008,
  author = {Renard, B. and Lang, M. and Bois, P. and Dupeyrat, A. and Mestre,
	O. and Niel, H. and Sauquet, E. and Prudhomme, C. and Parey, S. and
	Paquet, E. and Neppel, L. and Gailhard, J.},
  title = {Regional methods for trend detection: Assessing field significance
	and regional consistency},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W08419},
  doi = {10.1029/2007WR006268},
  owner = {rojasro},
  timestamp = {2012.06.26}
}

@ARTICLE{renard2007,
  author = {Renard, P.},
  title = {Stochastic hydrogeology: {W}hat professionals really need?},
  journal = {Ground Water},
  year = {2007},
  volume = {45},
  pages = {531--541},
  number = {5},
  abstract = {Quantitative hydrogeology celebrated its 150th anniversary in 2006.
	Geostatistics is younger but has had a very large impact in hydrogeology.
	Today, geostatistics is used routinely to interpolate deterministically
	most of the parameters that are required to analyze a problem or
	make a quantitative analysis. In a small number of cases, geostatistics
	is combined with deterministic approaches to forecast uncertainty.
	At a more academic level, geostatistics is used extensively to study
	physical processes in heterogeneous aquifers. Yet, there is an important
	gap between the academic use and the routine applications of geostatistics.
	The reasons for this gap are diverse. These include aspects related
	to the hydrogeology consulting market, technical reasons such as
	the lack of widely available software, but also a number of misconceptions.
	A change in this situation requires acting at different levels. First,
	regulators must be convinced of the benefit of using geostatistics.
	Second, the economic potential of the approach must be emphasized
	to customers. Third, the relevance of the theories needs to be increased.
	Last, but not least, software, data sets, and computing infrastructure
	such as grid computing need to be widely available.},
  doi = {10.1111/j.1745-6584.2007.00340.x},
  owner = {RRojas},
  timestamp = {2009.02.25}
}

@ARTICLE{reynard+al2001,
  author = {Reynard, N. and Prudhomme, C. and Crooks, S.},
  title = {The flood characteristics of large {U.K.} rivers: potential effects
	of climate and land use},
  journal = {Climatic Change},
  year = {2001},
  volume = {48},
  pages = {343--359},
  number = {2--3},
  abstract = {A continuous flow simulation model(CLASSIC) has been used to assess
	the potential impactof climate and land use changes on the flood
	regimesof large U.K. catchments. Climate change scenarios,based on
	the HadCM2 experiments from the HadleyCentre, are applied to the
	Severn and Thames rivers.The analysis shows that, for the 2050s,
	the climatechange scenarios result in an increase in both thefrequency
	and magnitude of flooding events in theserivers. The various ways
	of applying the rainfallscenario can have a significant effect on
	thesegeneral conclusions, although generally do not affecteither
	the direction or consistency of the changes.While lsquobest guessrsquo
	land use changes show little impacton flood response, a 50% increase
	in forest covercould counter-act the impact of climate change. Aswould
	be expected, a large change in the urban coverof the catchments does
	have a large effect on theflood regimes, increasing both the frequency
	andmagnitude of floods significantly beyond the changesdue to climate
	alone. Further research is requiredinto the potential impacts of
	seasonal changes in thedaily rainfall and potential evaporation regimes,
	landuse changes and the interaction between the two.},
  doi = {10.1023/A:1010735726818},
  owner = {rojasro},
  timestamp = {2010.08.04}
}

@TECHREPORT{rijks+al1998,
  author = {Rijks, D. and Terres, J. and Vossen, P.},
  title = {Agrometeorological applications for regional crop monitoring and
	production assessment},
  institution = {European Commission, Joint Research Centre},
  year = {1998},
  number = {EUR 17735},
  address = {Ispra, Italy},
  owner = {rojasro},
  timestamp = {2010.08.17}
}

@ARTICLE{risacher2003,
  author = {Risacher, F. and Alonso, H. and Salazar, C.},
  title = {The origin of brines and salts in {C}hilean salar: {A} hydrochemical
	review},
  journal = {Earth-Sciences Reviews},
  year = {2003},
  volume = {63},
  pages = {249--293},
  number = {3--4},
  abstract = {Northern Chile is characterized by a succession of north–south-trending
	ranges and basins occupied by numerous saline lakes and salt crusts,
	collectively called salars. Fossil salt crusts are found to the west
	in the extremely arid Central Valley, while active salars receiving
	permanent inflows fill many intravolcanic basins to the east in the
	semiarid Cordillera. Sea salts and desert dust are blown eastward
	over the Cordillera, where they constitute an appreciable fraction
	of the solute load of very dilute waters (salt content<0.1 g/l).
	The weathering of volcanic rocks contributes most components to inflow
	waters with salt content ranging from 0.1 to 0.6 g/l. However, the
	average salt content of all inflows is much higher: about 3.2 g/l.
	Chemical composition, Cl/Br ratio, and 18O–2H isotope contents point
	to the mixing of very dilute meteoric waters with present lake brines
	for the origin of saline inflows. Ancient gypsum in deep sedimentary
	formations seems to be the only evaporitic mineral recycled in present
	salars. Saline lakes and subsurface brines are under steady-state
	regime. The average residence time of conservative components ranges
	from a few years to some thousands years, which indicates a permanent
	leakage of the brines through bottom sediments. The infiltrating
	brines are recycled in the hydrologic system where they mix with
	dilute meteoric waters. High heat flow is the likely driving force
	that moves the deep waters in this magmatic arc region. Active Chilean
	salars cannot be considered as terminal lakes nor, strictly speaking,
	as closed basin lakes. Almost all incoming salts leave the basin
	and are transported elsewhere. Moreover, the dissolution of fossil
	salt crusts in some active salars also carries away important fluxes
	of components in percolating brines. Evaporative concentration of
	inflow waters leads to sulfate-rich or calcium-rich, near-neutral
	brines. Alkaline brines are almost completely lacking. The alkalinity/calcium
	ratio of inflow waters is lowered by the oxidation of native sulfur
	(reducing alkalinity) and the deposition of eolian gypsum (increasing
	Ca concentration). Theoretically, SO4-rich inflow waters and their
	derived SO4-rich brines should be found in the intravolcanic basins
	of the Cordillera because of the ubiquity of native sulfur, while
	Ca-rich brines should prevail in sedimentary basins where Ca-rich
	minerals are abundant. This relation is perfectly observed in the
	salar de Atacama, the largest in Chile. However, several salars located
	within the volcanic Cordillera belong to the Ca-rich group. Inflows
	and brines may have acquired their Ca-rich composition in Pleistocene
	time when their drainage basins were mainly sedimentary. Later on,
	recent lava flows and ignimbrites covered the sedimentary formations.
	Underground waters may have kept their early sedimentary signature
	by continuous recycling. However, the weathering of volcanic rocks
	tend to slowly shift the water compositions from the Ca-rich to the
	SO4-rich type.},
  doi = {10.1016/S0012-8252(03)00037-0},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@BOOK{robert2007,
  title = {The {B}ayesian {C}hoice--{F}rom decision--theoretic foundations to
	computational implementation},
  publisher = {Springer-Verlag},
  year = {2007},
  author = {Robert, C.},
  pages = {577},
  address = {New York},
  edition = {Second},
  booktitle = {Springer Texts in Statistics},
  owner = {RRojas},
  refid = {ROBERT2007},
  timestamp = {2008.11.04}
}

@ARTICLE{robertson2006,
  author = {Robertson, R. and Mueller, U. and Bloom, L.},
  title = {Direct sequential simulation with histogram reproduction: {A} comparison
	of algorithms},
  journal = {Computers \& Geosciences},
  year = {2006},
  volume = {32},
  pages = {382--395},
  number = {3},
  abstract = {Sequential simulation is a widely used technique applied in geostatistics
	to generate realisations that reproduce properties such as the mean,
	variance and semivariogram. Sequential Gaussian simulation requires
	the original variable to be transformed to a standard normal distribution
	before implementing variography, kriging and simulation procedures.
	Direct sequential simulation allows one to perform the simulation
	using the original variable rather than in normal score space. The
	shape of the local probability distribution from which simulated
	values are drawn is generally unknown and this results in direct
	simulation not being able to guarantee reproduction of the target
	histogram; only the Gaussian distribution ensures reproduction of
	the target distribution, and most geostatistical data sets are not
	normally distributed. This problem can be overcome by defining the
	shape of the local probability distribution through the use of constrained
	optimisation algorithms or by using the target normal-score transformation.
	We investigate two non-parametric approaches based on the minimisation
	of an objective function subject to a set of linear constraints,
	and an alternative approach that creates a lookup table using Gaussian
	transformation. These approaches allow the variography, kriging and
	simulation to be performed using original data values and result
	in the reproduction of both the histogram and semivariogram, within
	statistical fluctuations. The programs for the algorithms are written
	in Fortran 90 and follow the GSLIB format. Routines for constrained
	optimisation have been incorporated.},
  doi = {10.1016/j.cageo.2005.07.002},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{robinsonrahmatsamii2004,
  author = {Robinson, J. and {Rahmat-Samii}, Y.},
  title = {Particle Swarm Optimization in Electromagnetics},
  journal = {IEEE Transactions on Antennas and Propagation},
  year = {2004},
  volume = {52},
  pages = {397--407},
  number = {2},
  abstract = {The particle swarm optimization (PSO), new to the electromagnetics
	community, is a robust stochastic evolutionary computation technique
	based on the movement and intelligence of swarms. This paper introduces
	a conceptual overview and detailed explanation of the PSO algorithm,
	as well as how it can be used for electromagnetic optimizations.
	This paper also presents several results illustrating the swarm behavior
	in a PSO algorithm developed by the authors at UCLA specifically
	for engineering optimizations (UCLA-PSO). Also discussed is recent
	progress in the development of the PSO and the special considerations
	needed for engineering implementation including suggestions for the
	selection of parameter values. Additionally, a study of boundary
	conditions is presented indicating the invisible wall technique outperforms
	absorbing and reflecting wall techniques. These concepts are then
	integrated into a representative example of optimization of a profiled
	corrugated horn antenna.},
  doi = {10.1109/TAP.2004.823969},
  tags = {Calibration, PSO}
}

@ARTICLE{rode+al2007,
  author = {Rode, M. and Suhr, U. and Wriedt, G.},
  title = {Multi-objective calibration of a river water quality model - Information
	content of calibration data},
  journal = {Ecological Modelling},
  year = {2007},
  volume = {204},
  pages = {129--142},
  number = {1-2},
  abstract = {Substantial uncertainties exist in the identification of river water
	quality models, which partially depend on the information content
	of the calibration data. To evaluate the dependencies between available
	calibration data and model predictions, investigations were conducted
	based on a 536 km free-flowing reach of the German part of the River
	Elbe. Five extensive flow-time-related longitudinal surveys with
	14 sampling locations were used. The multi-objective calibration
	of the deterministic river water quality model QSIM was carried out
	with the nonlinear parameter estimator PEST. At the investigated
	river reach, parameter sensitivities were highly variable depending
	mainly on the growth of algal biomass. Based on 30 multi-objective
	calibration runs considering different numbers and combinations of
	the data sets, we found that calibration was only slightly improved
	using more than three data sets. Uncertainties can be decreased by
	increasing the amount of calibration data. For the calibration data
	sets, the cumulative distribution functions of the Nash and Sutcliffe
	coefficient steepen progressively and the uncertainties of model
	parameters decreased with an increased number of data sets included
	in the calibration procedure. Also the combination of different calibration
	data sets had an effect on the goodness of the model validation.
	Most uncertainties were associated with the calculation of oxygen.
	These findings are restricted to cases where data sets of different
	conditions are available. The suggested methodology for model calibration
	including a cross validation is especially suited for cases where
	available data are limited, which is common for river water quality
	modelling investigations. The results of this study will help model
	users to define appropriate data collections and monitoring schemes},
  doi = {10.1016/j.ecolmodel.2006.12.037},
  keywords = {River water quality modelling, Automatic model calibration, Data information
	content, PEST},
  tags = {Calibration, Water Quality}
}

@TECHREPORT{roeckner+al1996,
  author = {Roeckner, E. and Arpe, K. and Bengtsson, L. and Christoph, M. and
	Claussen, M. and D\"umenil, L. and Esch, M. and Giorgetta, M. and
	Schlese, U. and Schulzweida, U.},
  title = {The atmospheric general circulation model {ECHAM-4}: {M}odel description
	and simulation of present--day climate},
  institution = {Max Planck Institute for Meteorology},
  year = {1996},
  type = {Report No. 218},
  address = {Hamburg, Germany},
  note = {{ISSN} 0937-1060},
  tags = {Climate Models}
}

@TECHREPORT{echam5,
  author = {Roeckner, E. and B\"auml, G. and Bonaventura, L. and Brokopf, R.
	and Esch, M. and Giorgetta, M. and Hagemann, S. and Kirchner, I.
	and Kornblueh, L. and Manzini, E. and Rhodin, A. and Sclese, U. and
	Schulzweida, U. and Tompkins, A.},
  title = {{The atmospheric general circulation model ECHAM5 Part I: Model description}},
  institution = {Max Planck Institute for Meteorology},
  year = {2003},
  number = {349},
  abstract = {A detailed description of the fth-generation ECHAM model is presented.
	Compared to the previous version, ECHAM4, a number of substantial
	changes have been introduced in both the numerics and physics of
	the model. These include a ux-form semi-Lagrangian transport scheme
	for positive denite variables like water components and chemical
	tracers, a new longwave radiation scheme, separate prognostic equations
	for cloud liquid water and cloud ice, a new cloud microphysical scheme
	and a prognostic-statistical cloud cover parameterization. The number
	of spectral intervals is increased in both the longwave and shortwave
	part of the spectrum. Changes have also been made in the representation
	of land surface processes, including an implicit coupling between
	the surface and the atmosphere, and in the representation of orographic
	drag forces. Also, a new dataset of land surface parameters has been
	compiled for the new model. On the other hand, horizontal and vertical
	diusion, cumulus convection and also the spectral dynamics remain
	essentially unchanged.},
  owner = {rojasro},
  timestamp = {2010.08.09}
}

@ARTICLE{rojas2010a,
  author = {Rojas, R. and Batelaan, O. and Feyen, L. and Dassargues, A.},
  title = {Assessment of conceptual model uncertainty for the regional aquifer
	{P}ampa del {T}amarugal--{N}orth {C}hile},
  journal = {Hydrology and Earth System Sciences},
  year = {2010},
  volume = {14},
  pages = {171--192},
  number = {2},
  abstract = {In this work we assess the uncertainty in modelling the groundwater
	flow for the Pampa del Tamarugal Aquifer (PTA) – North Chile using
	a novel and fully integrated multi-model approach aimed at explicitly
	accounting for uncertainties arising from the definition of alternative
	conceptual models. The approach integrates the Generalized Likelihood
	Uncertainty Estimation (GLUE) and Bayesian Model Averaging (BMA)
	methods. For each member of an ensemble M of potential conceptualizations,
	model weights used in BMA for multi-model aggregation are obtained
	from GLUE-based likelihood values. These model weights are based
	on model performance, thus, reflecting how well a conceptualization
	reproduces an observed dataset D. GLUE-based cumulative predictive
	distributions for each member of M are then aggregated obtaining
	predictive distributions accounting for conceptual model uncertainties.
	For the PTA we propose an ensemble of eight alternative conceptualizations
	covering all major features of groundwater flow models independently
	developed in past studies and including two recharge mechanisms which
	have been source of debate for several years. Results showed that
	accounting for heterogeneities in the hydraulic conductivity field
	(a) reduced the uncertainty in the estimations of parameters and
	state variables, and (b) increased the corresponding model weights
	used for multi-model aggregation. This was more noticeable when the
	hydraulic conductivity field was conditioned on available hydraulic
	conductivity measurements. Contribution of conceptual model uncertainty
	to the predictive uncertainty varied between 6% and 64% for ground
	water head estimations and between 16% and 79% for ground water flow
	estimations. These results clearly illustrate the relevance of conceptual
	model uncertainty.},
  doi = {10.5194/hess-14-171-2010},
  owner = {rojasro},
  timestamp = {2009.09.10}
}

@ARTICLE{rojas2007,
  author = {Rojas, R. and Dassargues, A.},
  title = {Groundwater flow modelling of the regional aquifer of the {P}ampa
	del {T}amarugal, northern {C}hile},
  journal = {Hydrogeology Journal},
  year = {2007},
  volume = {15},
  pages = {537--551},
  number = {3},
  abstract = {The Pampa del Tamarugal Aquifer (PTA) is an important source of groundwater
	in northern Chile. In this study, a groundwater flow model of this
	aquifer is developed and calibrated for the period 1983–2004. The
	model reproduces the observed flow-field and the water balance components
	reasonably well. Five scenarios are defined to evaluate the response
	to different pumping situations. These scenarios show that groundwater
	heads will continue to decrease with the present pumping discharge
	rates. To account for variations in the model results due to uncertainties
	in average recharge rates, randomly generated recharge realizations
	with different levels of uncertainty are simulated. Evaporation flow
	rates and groundwater flowing out of the modelled area seem unaffected
	by the recharge uncertainty, whereas the storage terms can vary considerably.
	For the most intensive pumping scenario under the generated random
	recharge rates, it is unlikely that the cumulative discharged volume
	from the aquifer, at the end of the simulation period, will be larger
	than 12% of the estimated groundwater reserve. Fluctuations in simulated
	groundwater heads due to uncertainties in the average recharge values
	are more noticeable in certain areas. These fluctuations could explain
	unusual behaviour in the observed groundwater heads in these areas.},
  doi = {10.1007/s10040-006-0084-6},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{rojas2010b,
  author = {Rojas, R. and Feyen, L. and Batelaan, O. and Dassargues, A.},
  title = {On the value of conditioning data to reduce conceptual model uncertainty
	in groundwater modelling},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {1--20},
  number = {W08520},
  doi = {10.1029/2009WR008822},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{rojas+al2012,
  author = {Rojas, R. and Feyen, L. and Bianchi, A. and Dosio, A.},
  title = {{Assessment of future flood hazard in Europe using a large ensemble
	of bias-corrected regional climate simulations}},
  journal = {Journal of Geophysical Research},
  year = {2012},
  volume = {117},
  pages = {D17109},
  abstract = {We assess future flood hazard in view of climate change at pan-European
	scale using a large ensemble of climate projections. The ensemble
	consists of simulations from 12 climate experiments conducted within
	the ENSEMBLES project, forced by the SRES A1B emission scenario for
	the period 1961–2100. Prior to driving the hydrological model LISFLOOD,
	climate simulations are corrected for bias in precipitation and temperature
	using a Quantile Mapping (QM) method. For time slices of 30 years,
	a Gumbel distribution is fitted by the maximum likelihood method
	through the simulated annual maximum discharges. Changes in extreme
	river flows, here exemplified by the 100-year discharge (Q100), are
	then analyzed with respect to a control period (1961–1990). We assess
	the uncertainty arising from using alternative climate experiments
	to force LISFLOOD and from the fitting of extreme value distributions.
	Results show large discrepancies in the magnitude of change in Q100
	among the hydrological simulations for different climate experiments,
	with some regions even showing an opposite signal of change. Due
	to the low signal-to-noise ratio in some areas the projected changes
	showed not all to be statistically significant. Despite this, western
	Europe, the British Isles and northern Italy show a robust increase
	in future flood hazard, mainly due to a pronounced increase in extreme
	rainfall. A decrease in Q100, on the other hand, is projected in
	eastern Germany, Poland, southern Sweden and, to a lesser extent,
	the Baltic countries. In these areas, the signal is dominated by
	the strong reduction in snowmelt induced floods, which offsets the
	increase in average and extreme precipitation.},
  doi = {10.1029/2012JD017461},
  owner = {rojasro},
  timestamp = {2012.10.19}
}

@ARTICLE{rojas2009,
  author = {Rojas, R. and Feyen, L. and Dassargues, A.},
  title = {Sensitivity analysis of prior model probabilities and the value of
	prior knowledge in the assessment of conceptual model uncertainty
	in groundwater modelling},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {23},
  pages = {1131--1146},
  number = {8},
  abstract = {A key point in the application of multi-model Bayesian averaging techniques
	to assess the predictive uncertainty in groundwater modelling applications
	is the definition of prior model probabilities, which reflect the
	prior perception about the plausibility of alternative models. In
	this work the influence of prior knowledge and prior model probabilities
	on posterior model probabilities, multi-model predictions, and conceptual
	model uncertainty estimations is analysed. The sensitivity to prior
	model probabilities is assessed using an extensive numerical analysis
	in which the prior probability space of a set of plausible conceptualizations
	is discretized to obtain a large ensemble of possible combinations
	of prior model probabilities. Additionally, the value of prior knowledge
	about alternative models in reducing conceptual model uncertainty
	is assessed by considering three example knowledge states, expressed
	as quantitative relations among the alternative models. A constrained
	maximum entropy approach is used to find the set of prior model probabilities
	that correspond to the different prior knowledge states. For illustrative
	purposes, a three-dimensional hypothetical setup approximated by
	seven alternative conceptual models is employed. Results show that
	posterior model probabilities, leading moments of the predictive
	distributions and estimations of conceptual model uncertainty are
	very sensitive to prior model probabilities, indicating the relevance
	of selecting proper prior probabilities. Additionally, including
	proper prior knowledge improves the predictive performance of the
	multi-model approach, expressed by reductions of the multi-model
	prediction variances by up to 60% compared with a non-informative
	case. However, the ratio between-model to total variance does not
	substantially decrease. This suggests that the contribution of conceptual
	model uncertainty to the total variance cannot be further reduced
	based only on prior knowledge about the plausibility of alternative
	models. These results advocate including proper prior knowledge about
	alternative conceptualizations in combination with extra conditioning
	data to further reduce conceptual model uncertainty in groundwater
	modelling predictions.},
  doi = {10.1002/hyp.7231},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{rojas2008,
  author = {Rojas, R. and Feyen, L. and Dassargues, A.},
  title = {Conceptual model uncertainty in groundwater modeling: {C}ombining
	generalized likelihood uncertainty estimation and {B}ayesian model
	averaging},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W12418},
  number = {12},
  abstract = {Uncertainty assessments in groundwater modeling applications typically
	attribute all sources of uncertainty to errors in parameters and
	inputs, neglecting what may be the primary source of uncertainty,
	namely, errors in the conceptualization of the system. Confining
	the set of plausible system representations to a single model leads
	to underdispersive and prone-to-bias predictions. In this work, we
	present a general and flexible approach that combines generalized
	likelihood uncertainty estimation (GLUE) and Bayesian model averaging
	(BMA) to assess uncertainty in model predictions that arise from
	errors in model structure, inputs, and parameters. In a prior analysis,
	a set of plausible models is selected, and the joint prior input
	and parameter space is sampled to form potential simulators of the
	system. For each model, the likelihood measures of acceptable simulators,
	assigned to them based on their ability to reproduce observed system
	behavior, are integrated over the joint input and parameter space
	to obtain the integrated model likelihood. The latter is used to
	weight the predictions of the respective model in the BMA ensemble
	predictions. For illustrative purposes, we applied the methodology
	to a three-dimensional hypothetical setup. Results showed that predictions
	of groundwater budget terms varied considerably among competing models;
	despite this, a set of 16 head observations used for conditioning
	did not allow differentiating between the models. BMA provided average
	predictions that were more conservative than individual predictions
	obtained for individual models. Conceptual model uncertainty contributed
	up to 30% of the total uncertainty. The results clearly indicate
	the need to consider alternative conceptualizations to account for
	model uncertainty.},
  doi = {10.1029/2008WR006908},
  keywords = { GLUE, BMA, multimodel prediction, Monte Carlo methods, uncertainty
	assessment.},
  tags = {Uncertainty}
}

@ARTICLE{rojas+al2011,
  author = {Rojas, R. and Feyen, L. and Dosio, A. and Bavera, D.},
  title = {Improving {pan-European hydrological simulation of extreme events
	through statistical bias correction of RCM-driven climate simulations}},
  journal = {Hydrology and Earth System Sciences},
  year = {2011},
  volume = {15},
  pages = {2599--2620},
  number = {8},
  abstract = {In this work we asses the benefits of removing bias in climate forcing
	data used for hydrological climate change impact assessment at pan-European
	scale, with emphasis on floods. Climate simulations from the HIRHAM5-ECHAM5
	model driven by the SRES-A1B emission scenario are corrected for
	bias using a histogram equalization method. As predictand for the
	bias correction we employ gridded interpolated observations of precipitation,
	average, minimum, and maximum temperature from the E-OBS data set.
	Bias removal transfer functions are derived for the control period
	1961–1990. These are subsequently used to correct the climate simulations
	for the control period, and, under the assumption of a stationary
	error model, for the future time window 2071-2100. Validation against
	E-OBS climatology in the control period shows that the correction
	method performs successfully in removing bias in average and extreme
	statistics relevant for flood simulation over the majority of the
	European domain in all seasons. This translates into considerably
	improved simulations with the hydrological model of observed average
	and extreme river discharges at a majority of 554 validation river
	stations across Europe. Probabilities of extreme events derived employing
	extreme value techniques are also more closely reproduced. Results
	indicate that projections of future flood hazard in Europe based
	on uncorrected climate simulations, both in terms of their magnitude
	and recurrence interval, are likely subject to large errors. Notwithstanding
	the inherent limitations of the large-scale approach used herein,
	this study strongly advocates the removal of bias in climate simulations
	prior to their use in hydrological impact assessment.},
  doi = {10.5194/hess-15-2599-2011},
  owner = {rojasro},
  timestamp = {2011.04.29}
}

@ARTICLE{rojas+al2011discus,
  author = {Rojas, R. and Feyen, L. and Dosio, A. and Bavera, D.},
  title = {{Improving pan-european hydrological simulation of extreme events
	through statistical bias correction of RCM-driven climate simulations}},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2011},
  volume = {8},
  pages = {3883--3936},
  number = {2},
  doi = {10.5194/hessd-8-3883-2011},
  owner = {rojasro},
  timestamp = {2012.07.26}
}

@ARTICLE{rojas2010c,
  author = {Rojas, R. and Kahunde, S. and Peeters, L. and Batelaan, O. and Feyen,
	L. and Dassargues, A.},
  title = {Application of a multimodel approach to account for conceptual model
	and scenario uncertainties in groundwater modelling},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {394},
  pages = {416--435},
  number = {3--4},
  abstract = {Groundwater models are often used to predict the future behaviour
	of groundwater systems. These models may vary in complexity from
	simplified system conceptualizations to more intricate versions.
	It has been recently suggested that uncertainties in model predictions
	are largely dominated by uncertainties arising from the definition
	of alternative conceptual models. Different external factors such
	as climatic conditions or groundwater abstraction policies, on the
	other hand, may also play an important role. Rojas et al. (2008)
	proposed a multimodel approach to account for predictive uncertainty
	arising from forcing data (inputs), parameters and alternative conceptualizations.
	In this work we extend upon this approach to include uncertainties
	arising from the definition of alternative future scenarios and we
	apply the extended methodology to a real aquifer system underlying
	the Walenbos Nature Reserve area in Belgium. Three alternative conceptual
	models comprising different levels of geological knowledge are considered.
	Additionally, three recharge settings (scenarios) are proposed to
	evaluate recharge uncertainties. A joint estimation of the predictive
	uncertainty including parameter, conceptual model and scenario uncertainties
	is estimated for groundwater budget terms. Finally, results obtained
	using the improved approach are compared with the results obtained
	from methodologies that include a calibration step and which use
	a model selection criterion to discriminate between alternative conceptualizations.
	Results showed that conceptual model and scenario uncertainties significantly
	contribute to the predictive variance for some budget terms. Besides,
	conceptual model uncertainties played an important role even for
	the case when a model was preferred over the others. Predictive distributions
	showed to be considerably different in shape, central moment and
	spread among alternative conceptualizations and scenarios analysed.
	This reaffirms the idea that relying on a single conceptual model
	driven by a particular scenario, will likely produce bias and under-dispersive
	estimations of the predictive uncertainty. Multimodel methodologies
	based on the use of model selection criteria produced ambiguous results.
	In the frame of a multimodel approach, these inconsistencies are
	critical and can not be neglected. These results strongly advocate
	the idea of addressing conceptual model uncertainty in groundwater
	modelling practice. Additionally, considering alternative future
	recharge uncertainties will permit to obtain more realistic and,
	possibly, more reliable estimations of the predictive uncertainty.},
  doi = {10.1016/j.jhydrol.2010.09.016},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{romanowicz2010,
  author = {Romanowicz, R.},
  title = {An application of a log-transformed low-flow ({LTLF}) model to baseflow
	separation},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {952--964},
  number = {6},
  abstract = {A logarithmic transformation of flow is applied to the estimation
	of baseflow. Such a transformation has been applied in several hydrological
	studies since the discovery that flow is well described by a log-normal
	distribution. However, the main purpose of previous applications
	was normalisation of the error between simulated and observed flows.
	The new approach proposed here consists of an application of log-transform
	that leads to a formulation of the rainfall-flow process as a rate
	of change, instead of a typical mass balance problem. It applies
	a stochastic transfer function approach to log-transformed flow,
	enabling the problem to be stated in a statistically rigorous form.
	The difficulty in obtaining good hydrological predictions in extreme
	conditions lies both in the nonlinear character of processes governed
	by extreme hydrological signals and in the scarcity and inaccuracy
	of extreme flow measurements, particularly in early records. It has
	previously been shown that a log-transformed low-flow (LTLF) model
	allows for the decomposition of the simulated flow into slow and
	fast components that can be interpreted as baseflow and runoff. This
	paper compares the baseflow estimates obtained from the LTLF model
	with those obtained from other flow separation techniques, i.e. the
	Wittenberg nonlinear storage model and the Chapman linear filter.
	Also, the conditions for the applicability of the LTLF formulation
	are stated and the discussion is illustrated using some {``}monstrous{''},
	i.e. difficult to model, catchments from the UK and Poland. A rainfall-flow
	model based on the logarithm of flow gives a good representation
	of baseflow for catchments with a well-defined baseflow component.
	},
  doi = {10.1080/02626667.2010.505172},
  keywords = {catchment modelling, baseflow separation, daily streamflow, logarithmic
	transformation of flow, stochastic transfer function },
  tags = {Calibration, Low-Flows, Goodness-of-Fit}
}

@ARTICLE{romanowicz2006,
  author = {Romanowicz, R. and Beven, K.},
  title = {Comments on generalised likelihood uncertainty estimation},
  journal = {Reliability Engineering \& System Safety},
  year = {2006},
  volume = {91},
  pages = {1315--1321},
  number = {10--11},
  abstract = {The paper presents an application of the generalised likelihood uncertainty
	estimation methodology to the problem of estimating the uncertainty
	of predictions produced by environmental models. The methodology
	is placed in a wider context of different approaches to inverse modelling
	and, in particular, a comparison is made with Bayesian estimation
	techniques based on explicit structural assumptions about model error.
	Using a simple example of a rainfall-flow model, different evaluation
	measures and their influence on the prediction uncertainty and credibility
	intervals are demonstrated.},
  doi = {10.1016/j.ress.2005.11.030},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{romanowicz2003a,
  author = {Romanowicz, R. and Beven, K.},
  title = {Estimation of flood inundation probabilities as conditioned on event
	inundation maps},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1073--1084},
  number = {3},
  abstract = {The generalized likelihood uncertainty estimation (GLUE) methodology
	is applied to the problem of predicting the spatially distributed,
	time-varying probabilities of inundation of all points on a floodplain.
	Advantage is taken of the relative independence of different effective
	conveyance parameters to minimize the simulations required. Probability
	estimates are based on conditioning predictions of Monte Carlo realizations
	of a distributed quasi-two-dimensional flood routing model using
	maps of maximum inundation and aerial photographs of flooding in
	the area. The methodology allows posterior distributions of conveyance
	parameters to be estimated and maps of inundation potential probabilities
	to be drawn up for flood events of different magnitudes. The results
	suggest that combining information from different magnitude events
	should be done with care, as the distributions of effective parameter
	values may vary with event magnitude. The value of accurate topographic
	information that is consistent with mapped inundation is also highlighted.
	The methodology can be used to obtain dynamic probabilities of floodplain
	inundation in real time forecasting.},
  doi = {10.1029/2001WR001056},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@INCOLLECTION{romanowicz1994,
  author = {Romanowicz, R. and Beven, K. and Tawn, J.},
  title = {Evaluation of prediction uncertainty in non--linear hydrological
	models using a {B}ayesian approach},
  booktitle = {Statistics for the environment II - Water related issues},
  publisher = {John Wiley \& Sons Inc.},
  year = {1994},
  editor = {Barnett, V. and Trukman, F.},
  address = {Chichester},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{romanowicz2003b,
  author = {Romanowicz, R. and Young, P.},
  title = {Data assimilation and uncertainty analysis of environmental assessment
	problems--an application of {S}tochastic {T}ransfer {F}unction and
	{G}eneralized {L}ikelihood {U}ncertainty {E}stimation techniques},
  journal = {Reliability Engineering \& System Safety},
  year = {2003},
  volume = {79},
  pages = {161--174},
  number = {2},
  abstract = {Stochastic Transfer Function (STF) and Generalised Likelihood Uncertainty
	Estimation (GLUE) techniques are outlined and applied to an environmental
	problem concerned with marine dose assessment. The goal of both methods
	in this application is the estimation and prediction of the environmental
	variables, together with their associated probability distributions.
	In particular, they are used to estimate the amount of radionuclides
	transferred to marine biota from a given source: the British Nuclear
	Fuel Ltd (BNFL) repository plant in Sellafield, UK. The complexity
	of the processes involved, together with the large dispersion and
	scarcity of observations regarding radionuclide concentrations in
	the marine environment, require efficient data assimilation techniques.
	In this regard, the basic STF methods search for identifiable, linear
	model structures that capture the maximum amount of information contained
	in the data with a minimal parameterisation. They can be extended
	for on-line use, based on recursively updated Bayesian estimation
	and, although applicable to only constant or time-variable parameter
	(non-stationary) linear systems in the form used in this paper, they
	have the potential for application to non-linear systems using recently
	developed State Dependent Parameter (SDP) non-linear STF models.
	The GLUE based-methods, on the other hand, formulate the problem
	of estimation using a more general Bayesian approach, usually without
	prior statistical identification of the model structure. As a result,
	they are applicable to almost any linear or non-linear stochastic
	model, although they are much less efficient both computationally
	and in their use of the information contained in the observations.
	As expected in this particular environmental application, it is shown
	that the STF methods give much narrower confidence limits for the
	estimates due to their more efficient use of the information contained
	in the data. Exploiting Monte Carlo Simulation (MCS) analysis, the
	GLUE technique is used to estimate how the errors involved in the
	STF model structure and observations influence the model outputs
	and errors. The discussion on updating information originating from
	different locations using GLUE procedure is also given. A final aim
	of the paper is to use the results obtained in this particular example
	to explore the differences between the GLUE and STF approaches.},
  doi = {10.1016/S0951-8320(02)00227-2},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{rouhani+al2007,
  author = {Rouhani, H. and Willems, P. and Wyseure, G. and Feyen, J.},
  title = {Parameter estimation in semi-distributed hydrological catchment modelling
	using a multi-criteria objective function},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {2998--3008},
  number = {22},
  abstract = {Output generated by hydrologic simulation models is traditionally
	calibrated and validated using split-samples of observed time series
	of total water flow, measured at the drainage outlet of the river
	basin. Although this approach might yield an optimal set of model
	parameters, capable of reproducing the total flow, it has been observed
	that the flow components making up the total flow are often poorly
	reproduced. Previous research suggests that notwithstanding the underlying
	physical processes are often poorly mimicked through calibration
	of a set of parameters hydrologic models most of the time acceptably
	estimates the total flow. The objective of this study was to calibrate
	and validate a computer-based hydrologic model with respect to the
	total and slow flow. The quick flow component used in this study
	was taken as the difference between the total and slow flow. Model
	calibrations were pursued on the basis of comparing the simulated
	output with the observed total and slow flow using qualitative (graphical)
	assessments and quantitative (statistical) indicators. The study
	was conducted using the Soil and Water Assessment Tool (SWAT) model
	and a 10-year historical record (1986-1995) of the daily flow components
	of the Grote Nete River basin (Belgium). The data of the period 1986-1989
	were used for model calibration and data of the period 1990-1995
	for model validation. The predicted daily average total flow matched
	the observed values with a Nash-Sutcliff coefficient of 0{$\cdot$}67
	during calibration and 0{$\cdot$}66 during validation. The Nash-Sutcliff
	coefficient for slow flow was 0{$\cdot$}72 during calibration and
	0{$\cdot$}61 during validation. Analysis of high and low flows indicated
	that the model is unbiased. A sensitivity analysis revealed that
	for the modelling of the daily total flow, accurate estimation of
	all 10 calibration parameters in the SWAT model is justified, while
	for the slow flow processes only 4 out of the set of 10 parameters
	were identified as most sensitive},
  doi = {10.1002/hyp.6527},
  tags = {Calibration}
}

@ARTICLE{rowell2006,
  author = {Rowell, D.},
  title = {A demonstration of the uncertainty in projections of {UK} climate
	change resulting from regional model formulation},
  journal = {Climate Change},
  year = {2006},
  volume = {79},
  pages = {243--257},
  number = {3--4},
  abstract = {Regional climate models (RCMs) are now commonly used to downscale
	climate change projections provided by global coupled models to resolutions
	that can be utilised at national and finer scales. Although this
	extra tier of complexity adds significant value, it inevitably contributes
	a further source of uncertainty, due to the regional modelling uncertainties
	involved. Here, an initial attempt is made to estimate the uncertainty
	that arises from typical variations in RCM formulation, focussing
	on changes in UK surface air temperature (SAT) and precipitation
	projected for the late twenty-first century. Data are provided by
	a relatively large suite of RCM and global model integrations with
	widely varying formulations. It is found that uncertainty in the
	formulation of the RCM has a relatively small, but non-negligible,
	impact on the range of possible outcomes of future UK seasonal mean
	climate. This uncertainty is largest in the summer season. It is
	also similar in magnitude to that of large-scale internal variations
	of the coupled climate system, and for SAT, it is less than the uncertainty
	due to the emissions scenario, whereas for precipitation it is probably
	larger. The largest source of uncertainty, for both variables and
	in all seasons, is the formulation of the global coupled model. The
	scale-dependency of uncertainty due to RCM formulation is also explored
	by considering its impact on projections of the difference in climate
	change between the north and south of the UK. Finally, the implications
	for the reliability of UK seasonal mean climate change projections
	are discussed.},
  doi = {10.1007/s10584-006-9100-z},
  tags = {Uncertainty}
}

@ARTICLE{rowell2005,
  author = {Rowell, David},
  title = {A scenario of European climate change for the late twenty-first century:
	seasonal means and interannual variability},
  journal = {Climate Dynamics},
  year = {2005},
  volume = {25},
  pages = {837-849},
  abstract = {A scenario of European climate change for the late twenty-first century
	is described, using a high-resolution state-of-the-art model. A time-slice
	approach is used, whereby the atmospheric general circulation model,
	HadAM3P, was integrated for two periods, 1960–1990 and 2070–2100,
	using the SRES A2 scenario. For the first time an ensemble of such
	experiments was produced, along with appropriate statistical tests
	for assessing significance. The focus is on changes to the statistics
	of seasonal means, and includes analysis of both multi-year means
	and interannual variance. All four seasons are assessed, and anomalies
	are mapped for surface air temperature, precipitation and snow mass.
	Mechanisms are proposed where these are dominated by straightforward
	local processes. In winter, the largest warming occurs over eastern
	Europe, up to 7°C, mean snow mass is reduced by at least 80% except
	over Scandinavia, and precipitation increases over all but the southernmost
	parts of Europe. In summer, temperatures rise by 6–9°C south of about
	50°N, and mean rainfall is substantially reduced over the same area.
	In spring and autumn, anomalies tend to be weaker, but often display
	patterns similar to the preceding season, reflecting the inertia
	of the land surface component of the climate system. Changes in interannual
	variance are substantial in the solsticial seasons for many regions
	(note that for precipitation, variance estimates are scaled by the
	square of the mean). In winter, interannual variability of near-surface
	air temperature is considerably reduced over much of Europe, and
	the relative variability of precipitation is reduced north of about
	50°N. In summer, the (relative) interannual variance of both variables
	increases over much of the continent.},
  affiliation = {Hadley Centre for Climate Prediction and Research Met Office Fitzroy
	Road Exeter EX13PB UK},
  doi = {10.1007/s00382-005-0068-6},
  issn = {0930-7575},
  issue = {7},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{rozos+al2004,
  author = {Rozos, E. and Efstratiadis, A. and Nalbantis, I. and Koutsoyiannis,
	D.},
  title = {Calibration of a semi-distributed model for conjunctive simulation
	of surface and groundwater flows},
  journal = {Hydrological Sciences Journal},
  year = {2004},
  volume = {49},
  pages = {819--842},
  number = {5},
  abstract = {A hydrological simulation model was developed for conjunctive representation
	of surface and groundwater processes. It comprises a conceptual soil
	moisture accounting module, based on an enhanced version of the Thornthwaite
	model for the soil moisture reservoir, a Darcian multi-cell groundwater
	flow module and a module for partitioning water abstractions among
	water resources. The resulting integrated scheme is highly flexible
	in the choice of time (i.e. monthly to daily) and space scales (catchment
	scale, aquifer scale). Model calibration involved successive phases
	of manual and automatic sessions. For the latter, an innovative optimization
	method called evolutionary annealing-simplex algorithm is devised.
	The objective function involves weighted goodness-of-fit criteria
	for multiple variables with different observation periods, as well
	as penalty terms for restricting unrealistic water storage trends
	and deviations from observed intermittency of spring flows. Checks
	of the unmeasured catchment responses through manually changing parameter
	bounds guided choosing final parameter sets. The model is applied
	to the particularly complex Boeoticos Kephisos basin, Greece, where
	it accurately reproduced the main basin response, i.e. the runoff
	at its outlet, and also other important components. Emphasis is put
	on the principle of parsimony which resulted in a computationally
	effective modelling. This is crucial since the model is to be integrated
	within a stochastic simulation framework. },
  doi = {10.1623/hysj.49.5.819.55130},
  keywords = {conjunctive surface and groundwater use, Thornthwaite model, multi-cell
	model, global optimization, evolutionary annealing-simplex algorithm,
	hydrological simulation},
  tags = {Calibration}
}

@BOOK{rubin2003,
  title = {Applied stochastic hydrogeology},
  publisher = {Oxford University Press},
  year = {2003},
  author = {Rubin, Y.},
  pages = {416},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{rubin1987,
  author = {Rubin, Y. and Dagan, G.},
  title = {Stochastic identification of transmissivity and effective recharge
	in steady groundwater flow 1. {T}heory},
  journal = {Water Resources Research},
  year = {1987},
  volume = {23},
  pages = {1185--1192},
  number = {7},
  abstract = {The study is a continuation and extension of a previous work (Dagan,
	1985a) whose aim was to identify the values of the log-transmissivity
	Y for steady flow. The common basic assumptions are that Y is a normal
	and stationary random space function, the aquifer is unbounded, and
	a first-order approximation of the flow equation is adopted. The
	expected value of the water head H, as well as the Y unconditional
	autocovariance, are supposed to have analytical expressions which
	depend on a parameters vector ?. The proposed solution of the inverse
	problem consists of identifying ? with the aid of the model and of
	the measurements of Y and H and subsequently computing the statistical
	moments of Y conditioned on the same data, The additional features
	of the present study are (1) incorporation of a constant, but random,
	effective recharge and its identification and (2) accounting for
	the fact that ? estimation is associated with some uncertainty, whereas
	before ? was assumed to be identified with certainty. Analytical
	expressions are derived for the Y and H covariances for an exponential
	autocovariance of Y. Paper 2 (Rubin and Dagan, this issue) of the
	study illustrates the applications of the method to a real-life case.},
  doi = {10.1029/WR023i007p01185},
  owner = {rojasro},
  timestamp = {2009.10.21}
}

@ARTICLE{dunn+al2008,
  author = {Dunn; S. and Freer, J. and Weiler, M. and Kirkby, M. and Seibert,
	J. and Quinn, P. and Lischeid, G. and Tetzlaff, D. and Soulsby, C.},
  title = {Conceptualization in catchment modelling: simply learning?},
  journal = {Hydrological Processes},
  year = {2008},
  volume = {22},
  pages = {2389--2393},
  number = {13},
  doi = {10.1002/hyp.7070},
  keywords = {SENSITIVITY-ANALYSIS, SPATIAL VARIABILITY, DYNAMIC TOPMODEL, WATER,
	UNCERTAINTY, CALIBRATION, HYDROLOGY, RUNOFF, PREDICTION, FRAMEWORK},
  tags = {Calibration, Philosophical, conceptual model}
}

@ARTICLE{sanchez2009,
  author = {S\'anchez, E. and Romera, R. Gaertner, M. and Gallardo, C. and Castro,
	M.},
  title = {A weighting proposal for an ensemble of regional climate models over
	{E}urope driven by 1961-2000 {ERA40} based on monthly precipitation
	probability density functions},
  journal = {Atmospheric Science Letters},
  year = {2009},
  volume = {10},
  pages = {241--248},
  number = {4},
  abstract = {Present climate over Europe is simulated by 12 regional climate models
	(RCMs), forced by ERA40 reanalysis. A method is proposed to score
	models from the 1961-1990 monthly precipitation cumulative density
	functions (CDFs) for each season and eight chosen subregions, compared
	with the CRU observational database. Ensemble CDF curves compare
	well against observations for all the subregions and seasons. Higher
	percentiles (heavy precipitation amounts) show a larger spread among
	results. Important differences in scores are obtained among models,
	regions and seasons. Applying the scores to compute 1991-2000 weighted
	ensemble precipitation, results are slightly closer to observations
	than the direct (unweighted) ensemble, and some cases show a larger
	improvement.},
  doi = {10.1002/asl.230},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@INCOLLECTION{sahuquillo1992,
  author = {Sahuquillo, A. and Capilla, J. and G\'omez-Hern\'andez, J. and Andreu,
	J.},
  title = {Conditional simulation of transmissivity fields honouring piezometric
	data},
  booktitle = {Hydraulic Engineering Software IV. Fluid Flow Modelling},
  publisher = {Kluwer Academic Publishers},
  year = {1992},
  editor = {Blain, W. and Cabrera, E.},
  pages = {210--214},
  journal = {Hydraulic Engineering Software},
  owner = {rojasro},
  timestamp = {2009.09.11}
}

@ARTICLE{salathe2005,
  author = {Salath\'e, E.},
  title = {Downscaling simulations of future global climate with application
	to hydrologic modelling},
  journal = {International Journal of Climatology},
  year = {2005},
  volume = {25},
  pages = {419--436},
  number = {4},
  abstract = {This study approaches the problem of downscaling global climate model
	simulations with an emphasis on validating and selecting global models.
	The downscaling method makes minimal, physically based corrections
	to the global simulation while preserving much of the statistics
	of interannual variability in the climate model. Differences among
	the downscaled results for simulations of present-day climate form
	a basis for model evaluation. The downscaled results are used to
	simulate streamflow in the Yakima River, a mountainous basin in Washington,
	USA, to illustrate how model differences affect streamflow simulations.
	The downscaling is applied to the output of three models (ECHAM4,
	HADCM3, and NCAR-PCM) for simulations of historic conditions (1900-2000)
	and two future emissions scenarios (A2 and B2 for 2000-2100) from
	the IPCC assessment. The ECHAM4 simulation closely reproduces the
	observed statistics of temperature and precipitation for the 42 year
	period 1949-90. Streamflow computed from this climate simulation
	likewise produces similar statistics to streamflow computed from
	the observed data. Downscaled climate-change scenarios from these
	models are examined in light of the differences in the present-day
	simulations. Streamflows simulated from the ECHAM4 results show the
	greatest sensitivity to climate change, with the peak in summertime
	flow occurring 2 months earlier by the end of the 21st century},
  doi = {10.1002/joc.1125},
  keywords = {Pacific Northwest, statistical downscaling, hydrology, hydrologic
	modelling, precipitation, streamflow, climate change},
  tags = {Thesis, Downscaling, Impacts}
}

@ARTICLE{salathe2003,
  author = {Salath\'e, E.},
  title = {Comparison of various precipitation downscaling methods for the simulation
	of streamflow in a rainshadow river basin},
  journal = {International Journal of Climatology},
  year = {2003},
  volume = {23},
  pages = {887--901},
  number = {8},
  abstract = {Global simulations of precipitation from climate models lack sufficient
	resolution and contain large biases that make them unsuitable for
	regional studies, such as forcing hydrologic simulations. In this
	study, the effectiveness of several methods to downscale large-scale
	precipitation is examined. To facilitate comparisons with observations
	and to remove uncertainties in other fields, large-scale predictor
	fields to be downscaled are taken from the National Centers for Environmental
	Prediction-National Center for Atmospheric Research reanalyses. Three
	downscaling methods are used: (1): a local scaling of the simulated
	large-scale precipitation; (2) a modified scaling of simulated precipitation
	that takes into account the large-scale wind field; and (3) an analogue
	method with 1000 hPa heights as predictor. A hydrologic model of
	the Yakima River in central Washington state, USA, is then forced
	by the three downscaled precipitation datasets. Simulations with
	the raw large-scale precipitation and gridded observations are also
	made. Comparisons among these simulated flows reveal the effectiveness
	of the downscaling methods. The local scaling of the simulated large-scale
	precipitation is shown to be quite successful and simple to implement.
	Furthermore, the tuning of the downscaling methods is valid across
	phases of the Pacific decadal oscillation, suggesting that the methods
	are applicable to climate-change studies},
  doi = {10.1002/joc.922},
  tags = {Downscaling}
}

@CONFERENCE{salazar1999,
  author = {Salazar, C. and Rojas, L. and Pollastri, A.},
  title = {Evaluaci\'on de recursos h\'idricos en el sector de Pica hoya de
	la {Pampa del Tamarugal I Regi\'on-Chile}},
  booktitle = {VI Jornadas del CONAPHI Chile},
  year = {1999},
  address = {Santiago, Chile},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{saltelli+al2000,
  author = {Saltelli, A. and Tarantola, S. and Campolango, F.},
  title = {Sensitivity analysis as an ingredient of modeling},
  journal = {Statistical Science},
  year = {2000},
  volume = {15},
  pages = {377--395},
  number = {4},
  abstract = {We explore the tasks where sensitivity analysis (SA) can be useful
	and try to assess the relevance of SA within the modeling process.
	We suggest that SA could considerably assist in the use of models,
	by providing objective criteria of judgement for different phases
	of the model-building process: model identification and discrimination;
	model calibration; model corroboration. We review some new global
	quantitative SA methods and suggest that these might enlarge the
	scope for sensitivity analysis in computational and statistical modeling
	practice. Among the advantages of the new methods are their robustness,
	model independence and computational convenience. The discussion
	is based on worked examples.},
  owner = {rojasro},
  timestamp = {2011.10.10},
  url = {http://www.jstor.org/stable/2676831}
}

@BOOK{saltelli2008,
  title = {Sensitivity analysis in practice: {A} guide to assessing scientific
	models},
  publisher = {John Wiley \& Sons},
  year = {2008},
  author = {Saltelli, A. and Tarantola, S. and Campolango, F. and Ratto, M.},
  pages = {232},
  address = {Chichester},
  edition = {First},
  owner = {RRojas},
  timestamp = {2009.03.26}
}

@ARTICLE{salvadori2004,
  author = {Salvadori, G. and {De Michele}, C.},
  title = {{Frequency analysis via copulas: Theoretical aspects and applications
	to hydrological events}},
  journal = {Water Resources Research},
  year = {2004},
  volume = {40},
  pages = {W12511},
  abstract = {In this paper we provide a general theoretical framework exploiting
	copulas for studying the return periods of hydrological events; in
	particular, we consider events depending upon the joint behavior
	of two nonindependent random variables, an approach which can easily
	be generalized to the multivariate case. We show that using copulas
	may greatly simplify the calculations and may even yield analytical
	expressions for the isolines of the return periods, both in the unconditional
	and in the conditional case. In addition, we show how a new probability
	distribution may be associated with the return period of specific
	events and introduce the definitions of sub-, super-, and critical
	events as well as those of primary and secondary return periods.
	An illustration of the techniques proposed is provided by analyzing
	some case studies already examined in literature.},
  doi = {10.1029/2004WR003133},
  owner = {rojasro},
  timestamp = {2011.06.23}
}

@ARTICLE{samaniego+al2010,
  author = {Samaniego, L. and Kumar, R. and Attinger, S.},
  title = {Multiscale parameter regionalization of a grid-based hydrologic model
	at the mesoscale},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W05523},
  number = {5},
  abstract = {The requirements for hydrological models have increased considerably
	during the previous decades to cope with the resolution of extensive
	remotely sensed data sets and a number of demanding applications.
	Existing models exhibit deficiencies such as overparameterization,
	the lack of an effective technique to integrate the spatial heterogeneity
	of physiographic characteristics, and the nontransferability of parameters
	across scales and locations. A multiscale parameter regionalization
	(MPR) technique is proposed as a way to address these issues simultaneously.
	Using this technique, parameters at a coarser scale, in which the
	dominant hydrological processes are represented, are linked with
	their corresponding ones at a finer resolution in which input data
	sets are available. The linkage is done with upscaling operators
	such as the harmonic mean, among others. Parameters at the finer
	scale are regionalized through nonlinear transfer functions which
	link basin predictors with global parameters to be determined through
	calibration. MPR was compared with a standard regionalization (SR)
	method in which basin predictors instead of model parameters are
	first aggregated. Both methods were tested in a basin located in
	Germany using a distributed hydrologic model. Results indicate that
	MPR is superior to SR in many respects, especially if global parameters
	are transferred from coarser to finer scales. Furthermore, MPR, as
	opposed to SR, preserves the spatial variability of state variables
	and conserves the mass balance with respect to a control scale. Cross-validation
	tests indicate that the transferability of the global parameters
	to ungauged locations is possible},
  doi = {10.1029/2008WR007327},
  tags = {Regionalization}
}

@ARTICLE{sanchezetal2009,
  author = {Sanchez-Gomez, E. and Somot, S. and D\'equ\'e, M.},
  title = {Ability of an ensemble of regional climate models to reproduce weather
	regimes over {E}urope-{A}tlantic during the period 1961--2000},
  journal = {Climate Dynamics},
  year = {2009},
  volume = {33},
  pages = {723--736},
  number = {5},
  month = {October},
  abstract = {One of the main concerns in regional climate modeling is to which
	extent limited-area regional climate models (RCM) reproduce the large-scale
	atmospheric conditions of their driving general circulation model
	(GCM). In this work we investigate the ability of a multi-model ensemble
	of regional climate simulations to reproduce the large-scale weather
	regimes of the driving conditions. The ensemble consists of a set
	of 13 RCMs on a European domain, driven at their lateral boundaries
	by the ERA40 reanalysis for the time period 1961–2000. Two sets of
	experiments have been completed with horizontal resolutions of 50
	and 25 km, respectively. The spectral nudging technique has been
	applied to one of the models within the ensemble. The RCMs reproduce
	the weather regimes behavior in terms of composite pattern, mean
	frequency of occurrence and persistence reasonably well. The models
	also simulate well the long-term trends and the inter-annual variability
	of the frequency of occurrence. However, there is a non-negligible
	spread among the models which is stronger in summer than in winter.
	This spread is due to two reasons: (1) we are dealing with different
	models and (2) each RCM produces an internal variability. As far
	as the day-to-day weather regime history is concerned, the ensemble
	shows large discrepancies. At daily time scale, the model spread
	has also a seasonal dependence, being stronger in summer than in
	winter. Results also show that the spectral nudging technique improves
	the model performance in reproducing the large-scale of the driving
	field. In addition, the impact of increasing the number of grid points
	has been addressed by comparing the 25 and 50 km experiments. We
	show that the horizontal resolution does not affect significantly
	the model performance for large-scale circulation.},
  doi = {10.1007/s00382-008-0502-7},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{sankarasubra+al2009,
  author = {Sankarasubramanian, A. and Lall, U. and {Souza Filho}, F. and Sharma,
	A.},
  title = {Improved water allocation utilizing probabilistic climate forecasts:
	{S}hort--term water contracts in a risk management framework},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W11409},
  number = {11},
  abstract = {Probabilistic, seasonal to interannual streamflow forecasts are becoming
	increasingly available as the ability to model climate teleconnections
	is improving. However, water managers and practitioners have been
	slow to adopt such products, citing concerns with forecast skill.
	Essentially, a management risk is perceived in {\grqq}gamblingâ€?
	with operations using a probabilistic forecast, while a system failure
	upon following existing operating policies is {\grqq}protectedâ€?
	by the official rules or guidebook. In the presence of a prescribed
	system of prior allocation of releases under different storage or
	water availability conditions, the manager has little incentive to
	change. Innovation in allocation and operation is hence key to improved
	risk management using such forecasts. A participatory water allocation
	process that can effectively use probabilistic forecasts as part
	of an adaptive management strategy is introduced here. Users can
	express their demand for water through statements that cover the
	quantity needed at a particular reliability, the temporal distribution
	of the {\grqq}allocation,â€? the associated willingness to pay, and
	compensation in the event of contract nonperformance. The water manager
	then assesses feasible allocations using the probabilistic forecast
	that try to meet these criteria across all users. An iterative process
	between users and water manager could be used to formalize a set
	of short-term contracts that represent the resulting prioritized
	water allocation strategy over the operating period for which the
	forecast was issued. These contracts can be used to allocate water
	each year/season beyond long-term contracts that may have precedence.
	Thus, integrated supply and demand management can be achieved. In
	this paper, a single period multiuser optimization model that can
	support such an allocation process is presented. The application
	of this conceptual model is explored using data for the Jaguaribe
	Metropolitan Hydro System in Ceara, Brazil. The performance relative
	to the current allocation process is assessed in the context of whether
	such a model could support the proposed short-term contract based
	participatory process. A synthetic forecasting example is also used
	to explore the relative roles of forecast skill and reservoir storage
	in this framework},
  doi = {10.1029/2009WR007821},
  keywords = {water contracts, drought management, reservoir rule curves, probabilistic
	forecasts},
  tags = {Multimodel - Ensambles, Impacts}
}

@ARTICLE{sankarasubramanian+al2001,
  author = {Sankarasubramanian, A. and Vogel, R. and Limbrunner, J.},
  title = {Climate elasticity of streamflow in the {U}nited {S}tates},
  journal = {Water Resources Research},
  year = {2001},
  volume = {37},
  pages = {1771--1781},
  number = {6},
  abstract = {Precipitation elasticity of streamflow, ? P , provides a measure of
	the sensitivity of streamflow to changes in rainfall. Watershed model-based
	estimates of ? P are shown to be highly sensitive to model structure
	and calibration error. A Monte Carlo experiment compares a nonparametric
	estimator of ? P with various watershed model-based approaches. The
	nonparametric estimator is found to have low bias and is as robust
	as or more robust than alternate model-based approaches. The nonparametric
	estimator is used to construct a map of ? P for the United States.
	Comparisons with 10 detailed climate change studies reveal that the
	contour map of ? P introduced here provides a validation metric for
	past and future climate change investigations in the United States.
	Further investigations reveal that ? P tends to be low for basins
	with significant snow accumulation and for basins whose moisture
	and energy inputs are seasonally in phase with one another. The Budyko
	hypothesis can only explain variations in ? P for very humid basins.},
  doi = {10.1029/2000WR900330},
  tags = {Rainfall, conceptual model}
}

@ARTICLE{santhi+al2001,
  author = {Santhi, C. and Arnold, J. and Williams, J. and Dugas, W. and Srinivasan,
	R. and Hauck, L.},
  title = {Validation of the swat model on a large river basin with point and
	nonpoint sources},
  journal = {Journal of the American Water Resources Association},
  year = {2001},
  volume = {37},
  pages = {1169--1188},
  abstract = {The State of Texas has initiated the development of a Total Maximum
	Daily Load program in the Bosque River Watershed, where point and
	nonpoint sources of pollution are a concern. Soil Water Assessment
	Tool (SWAT) was validated for flow, sediment, and nutrients in the
	watershed to evaluate alternative management scenarios and estimate
	their effects in controlling pollution. This paper discusses the
	calibration and validation at two locations, Hico and Valley Mills,
	along the North Bosque River. Calibration for flow was performed
	from 1960 through 1998. Sediment and nutrient calibration was done
	from 1993 through 1997 at Hico and from 1996 through 1997 at Valley
	Mills. Model validation was performed for 1998. Time series plots
	and statistical measures were used to verify model predictions. Predicted
	values generally matched well with the observed values during calibration
	and validation (R-2 greater than or equal to 0.6 and Nash-Suttcliffe
	Efficiency greater than or equal to 0.5, in most instances) except
	for some underprediction of nitrogen during calibration at both locations
	and sediment and organic nutrients during validation at Valley Mills.
	This study showed that SWAT was able to predict flow, sediment, and
	nutrients successfully and can be used to study the effects of alternative
	management scenarios.},
  doi = {10.1111/j.1752-1688.2001.tb03630.x},
  keywords = {watershed management, total maximum daily load, erosion, sedimentation,
	phosphorus loading, dairy manure management, BALANCE, QUALITY, FLOW},
  tags = {SWAT}
}

@ARTICLE{santhi+al2008,
  author = {Santhi, C. and Kannan, N. and Arnold, J. and {Di Luzio}, M.},
  title = {Spatial calibration and temporal validation of flow for regional
	scale hydrologic modeling},
  journal = {JAWRA Journal Of The American Water Resources Association},
  year = {2008},
  volume = {44},
  pages = {829--846},
  abstract = {The USDA-NRCS Resource Inventory Assessment Division provided funding
	for this work as part of the Conservation Effects Assessment Project
	(CEAP). Thanks to the editor and the anonymous reviewers for their
	constructive comments. The Agricultural Policy/Environmental extender
	(APEX) modeling team's contribution is acknowledged.},
  doi = {10.1111/j.1752-1688.2008.00207.x},
  keywords = {spatially distributed calibration, validation, hydrologic modeling,
	regional scale, HUMUS, SWAT, CEAP, CONTERMINOUS UNITED-STATES, MISSISSIPPI
	RIVER-BASIN, SWAT, PRECIPITATION, TEMPERATURE, CLIMATE},
  tags = {SWAT, Calibration},
  url = {http://ddr.nal.usda.gov/dspace/bitstream/10113/18990/1/IND44097230.pdf}
}

@ARTICLE{santos2010,
  author = {Santos, J. and {Pulido-Calvo}, I. and Portela, M.},
  title = {Spatial and temporal variability of droughts in {P}ortugal},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W03503},
  abstract = {An analysis of droughts in mainland Portugal based on monthly precipitation
	data, from September 1910 to October 2004, in 144 rain gages distributed
	uniformly over the country is presented. The drought events were
	characterized by means of the Standardized Precipitation Index (SPI)
	applied to different time scales (1, 6, and 12 consecutive months
	and 6 months from April to September and 12 months from October to
	September). To assess spatial and temporal patterns of droughts,
	a principal component analysis (PCA) and K-means clustering (KMC)
	were applied to the SPI series. In this way, three different and
	spatially well-defined regions with different temporal evolution
	of droughts were identified (north, central, and south regions of
	Portugal). A spectral analysis of the SPI patterns obtained with
	principal component analysis and clusters analysis, using the fast
	Fourier transform algorithm (FFT), showed that there is a manifest
	3.6-year cycle in the SPI pattern in the south of Portugal and evident
	2.4-year and 13.4-year cycles in the north of Portugal. The observation
	of the drought periods supports the occurrence of more frequent cycles
	of dry events in the south (droughts from moderate to extreme approximately
	every 3.6 years) than in the north (droughts from severe to extreme
	approximately every 13.4 years). These results suggest a much stronger
	immediate influence of the NAO in the south than in the north of
	Portugal, although these relations remain a challenging task.},
  doi = {10.1029/2009WR008071},
  owner = {rojasro},
  timestamp = {2010.03.08}
}

@ARTICLE{schar+al2004,
  author = {Sch\"ar, C. and Vidale, P. and Luthi, D. and Frei, C. and Haberli,
	C. and Liniger, M. and Appenzeller, C.},
  title = {{The role of increasing temperature variability in European summer
	heatwaves}},
  journal = {Nature},
  year = {2004},
  volume = {427},
  pages = {332--336},
  number = {6972},
  doi = {10.1038/nature02300},
  owner = {rojasro},
  timestamp = {2012.06.26}
}

@ARTICLE{schaefligupta2007,
  author = {Schaefli, B. and Gupta, H.},
  title = {Do {N}ash values have value?},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {2075--2080},
  number = {15},
  doi = {10.1002/hyp.6825},
  mzbnote = {Benchmark Efficiency (BE), scaling the rainfall to match the mean
	discharge and to shift the sequence in time by some optimum lag.},
  tags = {Goodness-of-Fit, Calibration}
}

@ARTICLE{schaefli+al2005,
  author = {Schaefli, B. and Hingray, B. and Niggli, M. and Musy, A.},
  title = {A conceptual glacio-hydrological model for high mountainous catchments},
  journal = {Hydrology and Earth System Sciences},
  year = {2005},
  volume = {9},
  pages = {95--109},
  abstract = {In high mountainous catchments, the spatial precipitation and therefore
	the overall water balance is generally difficult to estimate. The
	present paper describes the structure and calibration of a semi-lumped
	conceptual glacio-hydrological model for the joint simulation of
	daily discharge and annual glacier mass balance that represents a
	better integrator of the water balance. The model has been developed
	for climate change impact studies and has therefore a parsimonious
	structure; it requires three input times series - precipitation,
	temperature and potential evapotranspiration - and has 7 parameters
	to calibrate. A multi-signal approach considering daily discharge
	and - if available - annual glacier mass balance has been developed
	for the calibration of these parameters. The model has been calibrated
	for three different catchments in the Swiss Alps having glaciation
	rates between 37\% and 52\%. It simulates well the observed daily
	discharge, the hydrological regime and some basic glaciological features,
	such as the annual mass balance.},
  doi = {10.5194/hess-9-95-2005},
  keywords = {EQUILIBRIUM-LINE ALTITUDES, CLIMATE-CHANGE, MASS-BALANCE, HYDROMETEOROLOGICAL
	MODEL, RUNOFF MODEL, SNOW, GLACIERS, RIVER, ICE, HYDROLOGY},
  tags = {Snow, conceptual model}
}

@ARTICLE{scheerlinck+al2009,
  author = {Scheerlinck, K. and Pauwels, V. and Vernieuwe, H. and {De Baets},
	B.},
  title = {Calibration of a water and energy balance model: Recursive parameter
	estimation versus particle swarm optimization},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {1--22},
  number = {10},
  abstract = {It is well known that one of the major problems in the application
	of land surface models is the determination of the various model
	parameters. In most cases, only one or a limited number of variables
	are used to estimate these parameters. This study evaluates the use
	of two fundamentally different global optimization methods, multistart
	weight-adaptive recursive parameter estimation (MWARPE) and particle
	swarm optimization (PSO), for the estimation of hydrologic model
	parameters on the basis of data for multiple variables. MWARPE iteratively
	uses the linear recursive filter equations in a Monte Carlo setting
	and therefore does not rely on the explicit minimization of an objective
	function. However, a major drawback of the MWARPE method is the high
	dimensionality, determined by the number of observations, of the
	matrix to be inverted. On the other hand, PSO is a stochastic optimization
	method based on the collective strength of a population of individuals
	with flocking or herding behavior, as observed in a wide number of
	biological systems. In situ observations of net radiation; latent,
	sensible, and ground heat fluxes; and the soil moisture profile are
	used to determine the parameters of a simplified water and energy
	balance model. Both optimization methods are analyzed in terms of
	model performance and computational efficiency. Comparable results,
	expressed in terms of the root mean square error values, were obtained
	for both methods. However, it was found that MWARPE tends to slightly
	overfit the data. },
  doi = {10.1029/2009WR008051},
  tags = {PSO, Calibration}
}

@ARTICLE{schertzer+al2010,
  author = {Schertzer, D. and Tchiguirinskaia, I. and Lovejoy, S. and Hubert,
	P.},
  title = {No monsters, no miracles: in nonlinear sciences hydrology is not
	an outlier!},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {965--979},
  number = {6},
  abstract = {The end users of hydrological models may be justified for being tired
	of the excessive uncertainty of these models, not to mention their
	simplistic approximations and crude modelling. The ever-increasing
	sophistication of model parameter fitting is simply a smoke-screen
	that hides the models' lack of physical basis, their scale dependence,
	and their inability to fit widely diverse behaviours. More generally,
	we have to admit a lack of qualitative improvement in hydrological
	modelling in recent times. In fact, operational hydrology may have
	suffered for some time from ignoring the advances in theoretical
	hydrology, which have, in contrast, greatly stimulated the nonlinear
	sciences. For instance, more than a century ago fractals were considered
	as geometrical monsters, whereas decades ago river networks became
	classical fractal objects, and rainfall and discharges are now classical
	examples of multifractal fields. These hydrological characteristics
	are still often ignored by operational hydrology, whereas they explain
	not only its current limitations, but also how to overcome them.
	To illustrate these problems, this paper focuses on the fact that
	hydrological fields are most likely singular with respect to measures
	of time and volume. This would not only explain the ubiquitous scale
	dependence of hydrological observations, but would also give the
	possibility to transform them into scale-independent quantities.
	The upscaling of a rainfall time series from an hour to a year is
	therefore discussed in detail, and enables us to quickly introduce
	other examples. },
  doi = {10.1080/02626667.2010.505173},
  keywords = {scales, measure, singularities, balance equation, fractals, multifractals
	},
  tags = {Calibration, Philosophical, Outliers}
}

@ARTICLE{schmidli+al2006,
  author = {Schmidli, J. and Frei, C. and Vidale, P.},
  title = {Downscaling from {GCM} precipitation: {A} benchmark for dynamical
	and statistical downscaling methods},
  journal = {International Journal of Climatology},
  year = {2006},
  volume = {26},
  pages = {679--689},
  number = {5},
  abstract = {A precipitation downscaling method is presented using precipitation
	from a general circulation model (GCM) as predictor. The method extends
	a previous method from monthly to daily temporal resolution. The
	simplest form of the method corrects for biases in wet-day frequency
	and intensity. A more sophisticated variant also takes account of
	flow-dependent biases in the GCM. The method is flexible and simple
	to implement. It is proposed here as a correction of GCM output for
	applications where sophisticated methods are not available, or as
	a benchmark for the evaluation of other downscaling methods. Applied
	to output from reanalyses (ECMWF, NCEP) in the region of the European
	Alps, the method is capable of reducing large biases in the precipitation
	frequency distribution, even for high quantiles. The two variants
	exhibit similar performances, but the ideal choice of method can
	depend on the GCM/reanalysis and it is recommended to test the methods
	in each case. Limitations of the method are found in small areas
	with unresolved topographic detail that influence higher-order statistics
	(e.g. high quantiles). When used as benchmark for three regional
	climate models (RCMs), the corrected reanalysis and the RCMs perform
	similarly in many regions, but the added value of the latter is evident
	for high quantiles in some small regions},
  doi = {10.1002/joc.1287},
  keywords = {statistical downscaling, European alps, precipitation statistics,
	regional climate model, reanalysis},
  tags = {Downscaling}
}

@ARTICLE{schmidli+al2007,
  author = {Schmidli, J. and Goodess, C. and Frei, C. and Haylock, M. and Hundecha,
	Y. and Ribalaygua, J. and Schmith, T.},
  title = {Statistical and dynamical downscaling of precipitation: {A}n evaluation
	and comparison of scenarios for the {E}uropean {A}lps},
  journal = {Journal of Geophysical Research},
  year = {2007},
  volume = {112},
  pages = {D04105},
  number = {D4},
  abstract = {This paper compares six statistical downscaling models (SDMs) and
	three regional climate models (RCMs) in their ability to downscale
	daily precipitation statistics in a region of complex topography.
	The six SDMs include regression methods, weather typing methods,
	a conditional weather generator, and a bias correction and spatial
	disaggregation approach. The comparison is carried out over the European
	Alps for current and future (2071--2100) climate. The evaluation
	of simulated precipitation for the current climate shows that the
	SDMs and RCMs tend to have similar biases but that they differ with
	respect to interannual variations. The SDMs strongly underestimate
	the magnitude of the year-to-year variations. Clear differences emerge
	also with respect to the year-to-year anomaly correlation skill:
	In winter, over complex terrain, the better RCMs achieve significantly
	higher skills than the SDMs. Over flat terrain and in summer, the
	differences are smaller. Scenario results using A2 emissions show
	that in winter mean precipitation tends to increase north of about
	45Â°N and insignificant or opposite changes are found to the south.
	There is good agreement between the downscaling models for most precipitation
	statistics. In summer, there is still good qualitative agreement
	between the RCMs but large differences between the SDMs and between
	the SDMs and the RCMs. According to the RCMs, there is a strong trend
	toward drier conditions including longer periods of drought. The
	SDMs, on the other hand, show mostly nonsignificant or even opposite
	changes. Overall, the present analysis suggests that downscaling
	does significantly contribute to the uncertainty in regional climate
	scenarios, especially for the summer precipitation climate},
  doi = {10.1029/2005JD007026},
  tags = {Downscaling}
}

@ARTICLE{schneider2002,
  author = {Schneider, S.},
  title = {Can we estimate the likelihood of climatic changes at 2100?},
  journal = {Climatic Change},
  year = {2002},
  volume = {52},
  pages = {441--451},
  number = {4},
  doi = {10.1023/A:1014276210717},
  keywords = {SRES, scenarios, equally plausible, equally sound, uncertainty cascade},
  tags = {Uncertainty, Scenarios}
}

@ARTICLE{schoofpryor2001,
  author = {Schoof, J. and Pryor, S.},
  title = {Downscaling temperature and precipitation: {A} comparison of regression--based
	methods and artificial neural networks},
  journal = {International Journal of Climatology},
  year = {2001},
  volume = {21},
  pages = {773--790},
  number = {7},
  abstract = {A comparison of two statistical downscaling methods for daily maximum
	and minimum surface air temperature, total daily precipitation and
	total monthly precipitation at Indianapolis, IN, USA, is presented.
	The analysis is conducted for two seasons, the growing season and
	the non-growing season, defined based on variability of surface air
	temperature. The predictors used in the downscaling are indices of
	the synoptic scale circulation derived from rotated principal components
	analysis (PCA) and cluster analysis of variables extracted from an
	18-year record from seven rawinsonde stations in the Midwest region
	of the United States. PCA yielded seven significant components for
	the growing season and five significant components for the non-growing
	season. These PCs explained 86% and 83% of the original rawinsonde
	data for the growing and non-growing seasons, respectively. Cluster
	analysis of the PC scores using the average linkage method resulted
	in eight growing season synoptic types and twelve non-growing synoptic
	types. The downscaling of temperature and precipitation is conducted
	using PC scores and cluster frequencies in regression models and
	artificial neural networks (ANNs). Regression models and ANNs yielded
	similar results, but the data for each regression model violated
	at least one of the assumptions of regression analysis. As expected,
	the accuracy of the downscaling models for temperature was superior
	to that for precipitation. The accuracy of all temperature models
	was improved by adding an autoregressive term, which also changed
	the relative importance of the dominant anomaly patterns as manifest
	in the PC scores. Application of the transfer functions to model
	daily maximum and minimum temperature data from an independent time
	series resulted in correlation coefficients of 0.34-0.89. In accord
	with previous studies, the precipitation models exhibited lesser
	predictive capabilities. The correlation coefficient for predicted
	versus observed daily precipitation totals was less than 0.5 for
	both seasons, while that for monthly total precipitation was below
	0.65. The downscaling techniques are discussed in terms of model
	performance, comparison of techniques and possible model improvements},
  doi = {10.1002/joc.655},
  tags = {Downscaling}
}

@ARTICLE{schoups+al2008,
  author = {Schoups, G. and {van de Giesen}, N. and Savenije, H.},
  title = {Model Complexity Control for Hydrologic Prediction},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = { W00B03},
  note = {They conclude (end of Section 3) that temporally variable model parameters,
	by season or by event, result in increasingly better fits to the
	data, as shown by Re values in Table 3. However, models with time-variable
	parameters exhibit parameter non-uniqueness and equifinality,},
  abstract = {A common concern in hydrologic modeling is over-parameterization of
	complex models given limited and noisy data. This leads to problems
	of parameter non-uniqueness and equifinality, which may negatively
	affect prediction uncertainties. A systematic way of controlling
	model complexity is therefore needed. We compare three model complexitycontrol
	methods for hydrologic prediction, namely Cross-Validation (CV),
	Akaike{'}s Information Criterion (AIC), and Structural Risk Minimization
	(SRM). Results show that simulation of water flow using non-physically
	based models (polynomials in this case) leads to increasingly better
	calibration fits as the model complexity (polynomial order)increases.
	However, prediction uncertainty worsens for complex non-physically
	based models due to over-fitting of noisy data. Incorporation of
	physically based constraints into the model (e.g. storage-discharge
	relationship) effectively bounds prediction uncertainty, even as
	the number of parameters increases. The conclusion is that overparameterization
	and equifinality do not lead to a continued increase in prediction
	uncertainty, as long as models are constrained by such physical principles.
	Complexity control of hydrologic models reduces parameter equifinality
	and identifies the simplestmodel that adequately explains the data,
	thereby providing a means of hydrologic generalization and classification.
	SRM is a promising technique for this purpose, as it (i) provides
	analytic upper bounds on prediction uncertainty, hence avoiding the
	computational burden of CV, and (ii) extends the applicability of
	classic methods such asAIC to finite data. The main hurdle in applying
	SRM is the need for an a priori estimation of the complexity of the
	hydrologic model, as measured by its Vapnik-Chernovenkis (VC) dimension.
	Further research is needed in this area.},
  doi = {10.1029/2008WR006836},
  organization = {Department of Water Management, Delft University of Technology},
  tags = {Applications}
}

@ARTICLE{schoups+al2005,
  author = {Schoups, G. and Hopmans, J. and Young, C. and Vrugt, J. and Wallender,
	W.},
  title = {Multi-criteria optimization of a regional spatially-distributed subsurface
	water flow model},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {311},
  pages = {20--48},
  number = {1-4},
  abstract = {This paper presents the multi-criteria calibration of a regional distributed
	subsurface water flow model for a 1400 km2 irrigated agricultural
	area in the western San Joaquin Valley of California. Two global
	optimization algorithms were used to identify model parameters using
	data on spatially distributed local water table depth measurements,
	district-average groundwater pumping and district-average subsurface
	drainage data. Model parameters that were subjected to calibration
	included irrigation efficiency, effective drain depth and conductance,
	crop evapotranspiration correction coefficient, saturated hydraulic
	conductivity and specific yield values of coarse and fine fractions,
	and saturated hydraulic conductivity values defining water fluxes
	across domain boundaries. Using the single-objective function approach,
	the three measurement types were weighted into a single-objective
	function for global optimization purposes. Additionally, a three-objective
	multi-criteria optimization problem was formulated in which no prior
	weighting of the individual objectives was specified. The single-objective
	optimization approach resulted in identifiable parameters with relatively
	small uncertainties, however, most likely values for various optimized
	parameter approached the outer bounds of their physical-realistic
	ranges. The normalized weighting of the single-objective function
	approach emphasized the pumping and drainage data more than the water
	table depth data. In the multi-objective approach, the objective
	function of each measurement type was treated independently, so that
	no subjective preferences were assigned a priori. Within a single
	optimization run, a Pareto set of solutions was generated, which
	included the optimal results for each end-member of each of the three
	objective functions. The results showed a moderate trade-off between
	pumping and water table predictions, and a slight independence of
	drainage predictions from the other two measurements. The estimated
	Pareto set exhibited large parameter uncertainty, indicating possible
	model structural inadequacies. We further show that the magnitude
	of prediction uncertainties associated with the Pareto parameter
	uncertainty is large for making water table predictions, but much
	smaller for drainage and pumping predictions. Trade-offs between
	fitting shallow and deep water tables were revealed by considering
	additional performance criteria for model evaluation, namely BIAS
	and RMSE values for six water table depth groups. These results point
	to possible model improvements by spatially distributing some of
	the model parameters.},
  doi = {10.1016/j.jhydrol.2005.01.001},
  keywords = {Vadose zone hydrology, Salinity, Drainage, Irrigation efficiency,
	Measurement error},
  tags = {Calibration}
}

@ARTICLE{schuol2006,
  author = {Schuol, J. and Abbaspour, K.},
  title = {Calibration and uncertainty issues of a hydrological model (SWAT)
	applied to West Africa},
  journal = {Advances in Geosciences},
  year = {2006},
  volume = {9},
  pages = {137--143},
  number = {9},
  doi = {10.5194/adgeo-9-137-2006},
  tags = {Uncertainty, SWAT}
}

@ARTICLE{schuol+al2008a,
  author = {Schuol, J. and Abbaspour, K. and Srinivasan, R. and Yang, H.},
  title = {Estimation of freshwater availability in the {W}est {A}frican sub-continent
	using the {SWAT} hydrologic model},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {352},
  pages = {30},
  number = {1-2},
  abstract = {Accurate knowledge of freshwater availability is indispensable for
	water resources management at regional or national level. This information,
	however, has historically been very difficult to obtain because of
	lack of data, difficulties in the aggregation of spatial information,
	and problems in the quantification of distributed hydrological processes.
	The currently available estimates of freshwater availability by a
	few large international organizations such as FAO and UNESCO are
	often not sufficient as they only provide aggregated rough quantities
	of river discharge and groundwater recharge (blue water) at a national
	level and on a yearly basis. This paper aims to provide a procedure
	to improve the estimations of freshwater availability at subbasin
	level and monthly intervals. Applying the distributed hydrological
	model {``}Soil and Water Assessment Tool{''} (SWAT), the freshwater
	availability is quantified for a 4-million km2 area covering some
	18 countries in West Africa. The procedure includes model calibration
	and validation based on measured river discharges, and quantification
	of the uncertainty in model outputs using {``}Sequential Uncertainty
	Fitting Algorithm{''} (SUFI-2) The aggregated results for 11 countries
	are compared with two other studies. It was seen that for most countries,
	the estimates from the other two studies fall within our calculated
	prediction uncertainty ranges. The uncertainties are, in general,
	within reasonable ranges but larger in subbasins containing features
	such as dams and wetlands, or subbasins with inadequate climate or
	landuse information. As the modelling procedure in this study proved
	quite successful, its application for quantification of freshwater
	availability at a global scale is already underway. There are, however,
	two limitations in the West African model: (1) not all the components
	of the water balance model such as soil moisture or deep aquifer
	recharge could be directly calibrated because of lack of data and
	(2) the full capabilities of the SWAT model could not be realized
	because of the lack of local water and agricultural management information},
  doi = {10.1016/j.jhydrol.2007.12.025},
  keywords = {SWAT, Blue water, Green water, Prediction uncertainty, SUFI-2, dGen},
  tags = {SWAT, Uncertainty, Applications}
}

@ARTICLE{schwartz1978,
  author = {Schwartz, G.},
  title = {Estimating the dimension of a model},
  journal = {Annals of Statistics},
  year = {1978},
  volume = {6},
  pages = {461--464},
  number = {2},
  abstract = {The problem of selecting one of a number of models of different dimensions
	is treated by finding its Bayes solution, and evaluating the leading
	terms of its asymptotic expansion. These terms are a valid large-sample
	criterion beyond the Bayesian context, since they do not depend on
	the a priori distribution.},
  owner = {RRojas},
  refid = {SCHWARTZ1978},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2958889}
}

@BOOK{schwartz1996,
  title = {The Art of the Long View: Planning for the Future in an Uncertain
	World},
  publisher = {Currency Doubleday},
  year = {1996},
  author = {Schwartz, P.},
  tags = {Scenarios}
}

@ARTICLE{sefton1997,
  author = {Sefton, C. and Boorman, D.},
  title = {A regional investigation of climate change impacts on {UK} streamflows},
  journal = {Journal of Hydrology},
  year = {1997},
  volume = {195},
  pages = {26--44},
  number = {1--4},
  abstract = {A framework has been developed that enables the estimation of climate
	change impacts on flow regimes of catchments in England and Wales.
	Within this framework, rainfall-runoff processes are represented
	by a unit hydrograph-based model that was calibrated against historical
	data from 39 catchments. Rainfall and temperature inputs were perturbed
	according to an equilibrium climate change scenario. Three study
	catchments, of widely differing flow regime, are presented as examples
	of the versatility of the model, and to illustrate variations in
	catchment response under the modified climate. Results, quantified
	by percentage change in low, mean and flood flows, were interpolated
	spatially to provide a regional picture of hydrological response.
	Catchments in central and eastern England suffer the most severe
	reductions in low flows whilst flooding is seen to increase in the
	north and west. The linking of model parameters to physical landscape
	characteristics allows estimation of flows, both historical and climate-changed,
	at ungauged sites. Since these characteristics include land use indices,
	it is possible to quantify second-order effects resulting from changes
	in land use.},
  doi = {10.1016/S0022-1694(96)03257-X},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{seibert2001,
  author = {Seibert, J.},
  title = {On the need for benchmarks in hydrological modelling},
  journal = {Hydrological Processes},
  year = {2001},
  volume = {15},
  pages = {1063--1064},
  number = {6},
  doi = {10.1002/hyp.446},
  mzbnote = {What is "acceptable accurracy" ?},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{seibert1997,
  author = {Seibert, J.},
  title = {Estimation of parameter uncertainty in the HBV model},
  journal = {Nordic Hydrology},
  year = {1997},
  volume = {28},
  pages = {247--262},
  number = {4--5},
  doi = {10.2166/nh.1997.015},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{seibertmcdonnell2002,
  author = {Seibert, J. and McDonnell, J.},
  title = {On the dialog between experimentalist and modeler in catchment hydrology:
	{U}se of soft data for multicriteria model calibration},
  journal = {Water Resources Research},
  year = {2002},
  volume = {38},
  pages = {1241},
  number = {11},
  abstract = { The dialog between experimentalist and modeler in catchment hydrology
	has been minimal to date. The experimentalist often has a highly
	detailed yet highly qualitative understanding of dominant runoff
	processes; thus there is often much more information content on the
	catchment than we use for calibration of a model. While modelers
	often appreciate the need for {``}hard data{''} for the model calibration
	process, there has been little thought given to how modelers might
	access this {``}soft{''} or process knowledge. We present a new method
	where soft data (i.e., qualitative knowledge from the experimentalist
	that cannot be used directly as exact numbers) are made useful through
	fuzzy measures of model simulation and parameter value acceptability.
	We developed a three-box lumped conceptual model for the Maimai catchment
	in New Zealand, a particularly well-studied process-hydrological
	research catchment. The boxes represent the key hydrological reservoirs
	that are known to have distinct groundwater dynamics, isotopic composition,
	and solute chemistry. The model was calibrated against hard data
	(runoff and groundwater levels) as well as a number of criteria derived
	from the soft data (e.g., percent new water, reservoir volume, etc.).
	We achieved very good fits for the three-box model when optimizing
	the parameter values with only runoff (Reff = 0.93). However, parameter
	sets obtained in this way showed in general a poor goodness of fit
	for other criteria such as the simulated new water contributions
	to peak runoff. Inclusion of soft data criteria in the model calibration
	process resulted in lower Reff values (around 0.84 when including
	all criteria) but led to better overall performance, as interpreted
	by the experimentalist{'}s view of catchment runoff dynamics. The
	model performance with respect to soft data (like, for instance,
	the new water ratio) increased significantly, and parameter uncertainty
	was reduced by 60\% on average with the introduction of the soft
	data multicriteria calibration. We argue that accepting lower model
	efficiencies for runoff is {``}worth it{''} if one can develop a
	more {``}real{''} model of catchment behavior. The use of soft data
	is an approach to formalize this exchange between experimentalist
	and modeler and to more fully utilize the information content from
	experimental catchments.},
  doi = {10.1029/2001WR000978},
  keywords = {multicriteria calibration, conceptual catchment modeling, hillslope
	hydrology, qualitative knowledge, soft data, HUMID HEADWATER CATCHMENTS,
	STORM RUNOFF GENERATION, PARAMETER UNCERTAINTY, NEW-ZEALAND, HBV
	MODEL, HILLSLOPE, SCALE, WATER, EQUIFINALITY, METHODOLOGY},
  tags = {Philosophical}
}

@ARTICLE{seifert2008,
  author = {Seifert, D. and Sonnenberg, T. and Scharling, P. and Hinsby, K.},
  title = {Use of alternative conceptual models to assess the impact of a buried
	valley on groundwater vulnerability},
  journal = {Hydrogeology Journal},
  year = {2008},
  volume = {16},
  pages = {659--674},
  number = {4},
  abstract = {A buried valley incised into a sequence of pre-Quaternary sediments
	is shown to seriously affect the vulnerability of groundwater. Often
	the existence of buried valleys is not known or is not described
	explicitly in a hydrogeological model. In the present study, two
	numerical groundwater models, representing two alternative conceptual
	models, were produced to help quantify the effect of the valley on
	groundwater vulnerability. One model included the buried valley and
	the other did not. Both models were subjected to calibration and
	were found to describe hydraulic head and river discharge equally
	well. Even though the two models showed similar calibration statistics;
	fluxes, travel paths and travel times were affected by the inclusion
	of the buried valley. The recharge area and the groundwater age of
	potential abstraction wells placed in the pre-Quaternary deep aquifers
	surrounding the buried valley were different for the two models,
	with significantly higher vulnerability when the valley was included
	in the model. Based on the results of the present study, it is concluded
	that a buried valley may not always be detectable when calibrating
	a wrong conceptual model. If reliable results should be obtained
	a good geological model has to be constructed.},
  doi = {10.1007/s10040-007-0252-3},
  owner = {RRojas},
  timestamp = {2008.04.24}
}

@ARTICLE{selroos2002,
  author = {Selroos, {J-O}. and Walker, D. and Str{\"o}m, A. and Gylling, B.
	and Follin, S.},
  title = {Comparison of alternative modelling approaches for groundwater flow
	in fractured rock},
  journal = {Journal of Hydrology},
  year = {2002},
  volume = {257},
  pages = {174--188},
  number = {1--4},
  abstract = {In performance assessment studies of radioactive waste disposal in
	crystalline rocks, one source of uncertainty is the appropriateness
	of conceptual models of the physical processes contributing to the
	potential transport of radionuclides. The Alternative Models Project
	(AMP) evaluates the uncertainty of models of groundwater flow, an
	uncertainty that arises from alternative conceptualisations of groundwater
	movement in fractured media. The AMP considers three modelling approaches
	for simulating flow and advective transport from the waste canisters
	to the biosphere: Stochastic Continuum, Discrete Fracture Network,
	and Channel Network. Each approach addresses spatial variability
	via Monte Carlo simulation, whose realisations are summarised by
	the statistics of three simplified measures of geosphere performance:
	travel time, transport resistance (a function of travel distance,
	flow-wetted surface per volume of rock, and Darcy velocity along
	a flowpath), and canister flux (Darcy velocity at repository depth).
	The AMP uses a common reference case defined by a specific model
	domain, boundary conditions, and layout of a hypothetical repository,
	with a consistent set of summary statistics to facilitate the comparison
	of the three approaches. The three modelling approaches predict similar
	median travel times and median canister fluxes, but dissimilar variability.
	The three modelling approaches also predict similar values for minimum
	travel time and maximum canister flux, and predict similar locations
	for particles exiting the geosphere. The results suggest that the
	problem specifications (i.e. boundary conditions and gross hydrogeology)
	constrain the flow modelling, limiting the impact of this conceptual
	uncertainty on performance assessment.},
  doi = {10.1016/S0022-1694(01)00551-0},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@INBOOK{SREX2012,
  author = {Seneviratne, S. and Nicholls, N. and Easterling, D. and Goodess,
	C. and Kanae, S. and Kossin, J. and Luo, Y. and Marengo, J. and {McInnes},
	K. and Rahimi, M. and Reichstein, M. and Sorteberg, A. and Vera,
	C. and Zhang, X.},
  editor = {Field, C. and Barros, V. and Stocker, T. and Qin, D. and Dokken,
	D. and Ebi, K. and Mastrandrea, M. and Mach, K. and Plattner, G.
	and Allen, S. and Tignor, M. and Midgley, P.},
  title = {{A Special Report of Working Groups I and II of th eIntergovernmental
	Panel on Climate Change (IPCC)}},
  chapter = {Changes in climate extremes and their impacts on the natural physical
	environment. In: Managing the Risks of Extreme Events and Disasters
	to Advance Climate Change Adaptation},
  pages = {109--230},
  year = {2012},
  publisher = {{Cambridge University Press}},
  owner = {rojasro},
  timestamp = {2012.05.16}
}

@ARTICLE{setegn+al2009,
  author = {Setegn, S. and Srinivasan, R. and Melesse, A. and Dargahi, B.},
  title = {{SWAT} model application and prediction uncertainty analysis in the
	{L}ake {T}ana {B}asin, {E}thiopia},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {4},
  pages = {357--367},
  number = {3},
  abstract = {Lake Tana Basin is of significant importance to Ethiopia concerning
	water resources aspects and the ecological balance of the area. Many
	years of mismanagement, wetland losses due to urban encroachment
	and population growth, and droughts are causing its rapid deterioration.
	The main objective of this study was to assess the performance and
	applicability of the soil water assessment tool (SWAT) model for
	prediction of streamflow in the Lake Tana Basin, so that the influence
	of topography, land use, soil and climatic condition on the hydrology
	of Lake Tana Basin can be well examined. The physically based SWAT
	model was calibrated and validated for four tributaries of Lake Tana.
	Sequential uncertainty fitting (SUFI-2), parameter solution (ParaSol)
	and generalized likelihood uncertainty estimation (GLUE) calibration
	and uncertainty analysis methods were compared and used for the set-up
	of the SWAT model. The model evaluation statistics for streamflows
	prediction shows that there is a good agreement between the measured
	and simulated flows that was verified by coefficients of determination
	and Nash Sutcliffe efficiency greater than 0{$\cdot$}5. The hydrological
	water balance analysis of the basin indicated that baseflow is an
	important component of the total discharge within the study area
	that contributes more than the surface runoff. More than 60\% of
	losses in the watershed are through evapotranspiration},
  doi = {10.1002/hyp.7457},
  keywords = {SWAT, Lake Tana, hydrological modelling, SUFI-2, GLUE, ParaSol},
  tags = {SWAT, Uncertainty}
}

@ARTICLE{shabalova+al2003,
  author = {Shabalova, M. and {van Deursen}, W. and Buishand, T.},
  title = {Assessing future discharge of the river {R}hine using regional climate
	model integrations and a hydrological model},
  journal = {Climate Research},
  year = {2003},
  volume = {23},
  pages = {233--246},
  number = {3},
  abstract = {Climate change scenarios based on integrations of the Hadley Centre
	regional climate model HadRM2 are used to determine the change in
	the flow regime of the river Rhine by the end of the 21st century.
	Two scenarios are formulated: Scenario 1 accounting for the temperature
	increase (4.8°C on average over the basin) and changes in the mean
	precipitation, and Scenario 2 accounting additionally for changes
	in the temperature variance and an increase in the relative variability
	of precipitation. These scenarios are used as input into the RhineFlow
	hydrological model, a distributed water balance model of the Rhine
	basin that simulates river flow, soil moisture, snow pack and groundwater
	storage with a 10 d time step. Both scenarios result in higher mean
	discharges of the Rhine in winter (approx. +30%), but lower mean
	discharges in summer (approx. -30%), particularly in August (approx.
	-50%). RhineFlow simulations also indicate that the variability of
	the 10 d discharges increases significantly, even if the variability
	of the climatic inputs remains unchanged. The annual maximum discharge
	increases in magnitude throughout the Rhine and tends to occur more
	frequently in winter, thus suggesting an increasing risk of winter
	floods. This is especially pronounced in Scenario 2. At the Netherlands-German
	border, the magnitude of the 20 yr maximum discharge event increases
	by 14% in Scenario 1 and by 29% in Scenario 2; the present-day 20
	yr event tends to reappear every 5 yr in Scenario 1 and every 3 yr
	in Scenario 2. The frequency of occurrence of low and very low flows
	increases, in both scenarios alike.},
  doi = {10.3354/cr023233},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{shannon1948,
  author = {Shannon, C.},
  title = {A mathematical theory of communication},
  journal = {Bell System Technical Journal},
  year = {1948},
  volume = {27},
  pages = {623--656},
  number = {279--423},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {10.1145/584091.584093}
}

@ARTICLE{sheffieldwood2008,
  author = {Sheffield, Justin and Wood, Eric},
  title = {Projected changes in drought occurrence under future global warming
	from multi-model, multi-scenario, IPCC AR4 simulations},
  journal = {Climate Dynamics},
  year = {2008},
  volume = {31},
  pages = {79--105},
  abstract = {Recent and potential future increases in global temperatures are likely
	to be associated with impacts on the hydrologic cycle, including
	changes to precipitation and increases in extreme events such as
	droughts. We analyze changes in drought occurrence using soil moisture
	data for the SRES B1, A1B and A2 future climate scenarios relative
	to the PICNTRL pre-industrial control and 20C3M twentieth century
	simulations from eight AOGCMs that participated in the IPCC AR4.
	Comparison with observation forced land surface model estimates indicates
	that the models do reasonably well at replicating our best estimates
	of twentieth century, large scale drought occurrence, although the
	frequency of long-term (more than 12-month duration) droughts are
	over-estimated. Under the future projections, the models show decreases
	in soil moisture globally for all scenarios with a corresponding
	doubling of the spatial extent of severe soil moisture deficits and
	frequency of short-term (4–6-month duration) droughts from the mid-twentieth
	century to the end of the twenty-first. Long-term droughts become
	three times more common. Regionally, the Mediterranean, west African,
	central Asian and central American regions show large increases most
	notably for long-term frequencies as do mid-latitude North American
	regions but with larger variation between scenarios. In general,
	changes under the higher emission scenarios, A1B and A2 are the greatest,
	and despite following a reduced emissions pathway relative to the
	present day, the B1 scenario shows smaller but still substantial
	increases in drought, globally and for most regions. Increases in
	drought are driven primarily by reductions in precipitation with
	increased evaporation from higher temperatures modulating the changes.
	In some regions, increases in precipitation are offset by increased
	evaporation. Although the predicted future changes in drought occurrence
	are essentially monotonic increasing globally and in many regions,
	they are generally not statistically different from contemporary
	climate (as estimated from the 1961–1990 period of the 20C3M simulations)
	or natural variability (as estimated from the PICNTRL simulations)
	for multiple decades, in contrast to primary climate variables, such
	as global mean surface air temperature and precipitation. On the
	other hand, changes in annual and seasonal means of terrestrial hydrologic
	variables, such as evaporation and soil moisture, are essentially
	undetectable within the twenty-first century. Changes in the extremes
	of climate and their hydrological impacts may therefore be more detectable
	than changes in their means.},
  affiliation = {Princeton University Department of Civil and Environmental Engineering
	Princeton NJ 08544 USA},
  doi = {10.1007/s00382-007-0340-z},
  issn = {0930-7575},
  issue = {1},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{shen2008,
  author = {Shen, Y. and Oki, T. and Utsumi, N. and Kanae, S. and Hanasaki, N.},
  title = {Projection of future world water resources under {SRES} scenarios:
	water withdrawal},
  journal = {Hydrological Sciences Journal},
  year = {2008},
  volume = {53},
  pages = {11--33},
  number = {1},
  month = {February},
  doi = {10.1623/hysj.53.1.11},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{shi+al2008,
  author = {Shi, X. and Wood, A. and Lettenmaier, D.},
  title = {How Essential is Hydrologic Model Calibration to Seasonal Streamflow
	Forecasting?},
  journal = {Journal of Hydrometeorology},
  year = {2008},
  volume = {9},
  pages = {1350--1363},
  number = {6},
  abstract = {Hydrologic model calibration is usually a central element of streamflow
	forecasting based on the ensemble streamflow prediction (ESP) method.
	Evaluation measures of forecast errors such as root-mean-square error
	(RMSE) are heavily influenced by bias, which in turn is readily reduced
	by calibration. On the other hand, bias can also be reduced by postprocessing
	(e.g., {``}training{''} bias correction schemes based on retrospective
	simulation error statistics). This observation invites the question:
	How much is forecast error reduced by calibration, beyond what can
	be accomplished by postprocessing to remove bias? The authors address
	this question through retrospective evaluation of forecast errors
	at eight streamflow forecast locations distributed across the western
	United States. Forecast periods of length ranging from 1 to 6 months
	are investigated, for forecasts initiated from 1 December to 1 June,
	which span the period when most runoff occurs from snowmelt-dominated
	western U.S. rivers. ESP forecast errors are evaluated both for uncalibrated
	forecasts to which a percentile mapping bias correction approach
	is applied, and for forecasts from an objectively calibrated model
	without explicit bias correction. Using the coefficient of prediction
	(Cp), which essentially is a measure of the fraction of variance
	explained by the forecast, the authors find that the reduction in
	forecast error as measured by Cp that is achieved by bias correction
	alone is nearly as great as that resulting from hydrologic model
	calibration..},
  doi = {10.1175/2008JHM1001.1},
  tags = {Calibration}
}

@INPROCEEDINGS{shieberhart1999,
  author = {Shi, Y. and Eberhart, R.},
  title = {Empirical study of particle swarm optimization},
  booktitle = {Proceedings of the 1999 Congress on Evolutionary Computation-CEC99
	(Cat. No. 99TH8406)},
  year = {1999},
  pages = {1945},
  abstract = {We empirically study the performance of the particle swarm optimizer
	(PSO). Four different benchmark functions with asymmetric initial
	range settings are selected as testing functions. The experimental
	results illustrate the advantages and disadvantages of the PSO. Under
	all the testing cases, the PSO always converges very quickly towards
	the optimal positions but may slow its convergence speed when it
	is near a minimum. Nevertheless, the experimental results show that
	the PSO is a promising optimization method and a new approach is
	suggested to improve PSO's performance near the optima, such as using
	an adaptive inertia weight},
  doi = {10.1109/CEC.1999.785511},
  tags = {Calibration, PSO}
}

@INPROCEEDINGS{shieberhart1998a,
  author = {Shi, Y. and Eberhart, R.},
  title = {Parameter Selection in Particle Swarm Optimization},
  booktitle = {EP '98 Proceedings of the 7th International Conference on Evolutionary
	Programming VII},
  year = {1998},
  tags = {Calibration, PSO}
}

@INPROCEEDINGS{shieberhart1998b,
  author = {Shi, Y. and Eberhart, R.},
  title = {A modified particle swarm optimizer},
  booktitle = {Evolutionary Computation Proceedings, 1998. IEEE World Congress on
	Computational Intelligence., The 1998 IEEE International Conference
	on},
  year = {1998},
  pages = {69--73},
  abstract = {Evolutionary computation techniques, genetic algorithms, evolutionary
	strategies and genetic programming are motivated by the evolution
	of nature. A population of individuals, which encode the problem
	solutions are manipulated according to the rule of survival of the
	fittest through ldquo;genetic rdquo; operations, such as mutation,
	crossover and reproduction. A best solution is evolved through the
	generations. In contrast to evolutionary computation techniques,
	Eberhart and Kennedy developed a different algorithm through simulating
	social behavior (R.C. Eberhart et al., 1996; R.C. Eberhart and J.
	Kennedy, 1996; J. Kennedy and R.C. Eberhart, 1995; J. Kennedy, 1997).
	As in other algorithms, a population of individuals exists. This
	algorithm is called particle swarm optimization (PSO) since it resembles
	a school of flying birds. In a particle swarm optimizer, instead
	of using genetic operators, these individuals are ldquo;evolved rdquo;
	by cooperation and competition among the individuals themselves through
	generations. Each particle adjusts its flying according to its own
	flying experience and its companions' flying experience. We introduce
	a new parameter, called inertia weight, into the original particle
	swarm optimizer. Simulations have been done to illustrate the significant
	and effective impact of this new parameter on the particle swarm
	optimizer},
  doi = {10.1109/ICEC.1998.699146},
  keywords = {competition, cooperation, evolutionary computation techniques, evolutionary
	strategies, flying birds, flying experience, genetic algorithms,
	genetic programming, inertia weight, modified particle swarm optimizer,
	particle swarm optimization, social behavior simulation, survival
	of the fittest, genetic algorithms, iterative methods, search problems},
  tags = {Calibration, PSO}
}

@ARTICLE{shiau2011,
  author = {Shiau, {J.-T.} and {Wen Shen}, H.},
  title = {Recurrence analysis of hydrologic droughts of differeing severity},
  journal = {Journal of Water Resources Planning and Management},
  year = {2011},
  volume = {127},
  pages = {30--40},
  number = {1},
  abstract = {Droughts are stochastic in nature, therefore, statistical assessment
	of droughts is an attractive approach for water resources planning
	and management. In this study, a methodology of frequency and risk
	analysis of hydrologic droughts, defined as an event during which
	the streamflow is continuously below a certain truncation level,
	is formulated. The theoretical derivation of the recurrence interval
	of hydrologic droughts with a certain severity or greater is obtained
	based on the concept of stochastic processes. For risk analysis,
	the distribution of the number of droughts occurring within a specific
	period of time and the distribution of the interarrival time of hydrologic
	droughts with a certain severity or greater are determined. To illustrate
	the feasibility of the proposed methodology, a recorded reservoir
	inflow is used as a case study. Satisfactory agreement is found between
	the observed data and the results of the proposed models.},
  doi = {10.1061/(ASCE)0733-9496(2001)127:1(30)},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@BOOK{shiklomanovrodda2003,
  title = {World Water Resources at the Beginning of the 21st Century},
  publisher = {Cambridge University Press},
  year = {2003},
  author = {Shiklomanov, I. and Rodda, J.},
  address = {Cambridge}
}

@ARTICLE{shuklawood2008,
  author = {Shukla, S. and Wood, A.},
  title = {Use of standardized runoff index for characterizing hydrologic drought},
  journal = {Geophysical Research Letters},
  year = {2008},
  volume = {35},
  pages = {L02505},
  abstract = {Many current metrics of drought are derived solely from analyses of
	climate variables such as precipitation and temperature. Drought
	is clearly a consequence of climate anomalies, as well as of human
	water use practices, but many impacts to society are more directly
	related to hydrologic conditions resulting from these two factors.
	Modern hydrology models can provide a valuable counterpart to existing
	climate-based drought indices by simulating hydrologic variables
	such as land surface runoff. We contrast the behavior of a standardized
	runoff index (SRI) with that of the well-known standardized precipitation
	index (SPI) during drought events in a snowmelt region. Although
	the SRI and SPI are similar when based on long accumulation periods,
	the SRI incorporates hydrologic processes that determine seasonal
	lags in the influence of climate on streamflow. As a result, on monthly
	to seasonal time scales, the SRI is a useful complement to the SPI
	for depicting hydrologic aspects of drought.},
  doi = {10.1029/2007GL032487},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{sieberuhlenbrook2005,
  author = {Sieber, A. and Uhlenbrook, S.},
  title = {Sensitivity analyses of a distributed catchment model to verify the
	model structure},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {310},
  pages = {216--235},
  number = {1--4},
  abstract = {Sensitivity analyses are valuable tools for identifying important
	model parameters, testing the model conceptualization, and improving
	the model structure. They help to apply the model efficiently and
	to enable a focussed planning of future research and field measurement.
	Two different methods were used for sensitivity analyses of the complex
	process-oriented model TACD (tracer aided catchment model, distributed)
	that was applied to the meso-scale Brugga basin (40 km2) and the
	sub-basin St Wilhelmer Talbach (15.2 km2). Five simulations periods
	were investigated: two summer events, two snow melt induced events
	and one summer low flow period. The model was applied using 400 different
	parameter sets, which were generated by Monte Carlo simulations using
	latin hypercube sampling. The regional sensitivity analysis (RSA)
	allowed determining the most significant parameters for the complete
	simulation periods using a graphical method. The results of the regression-based
	sensitivity analysis were more detailed and complex. The temporal
	variability of the simulation sensitivity could be observed continuously
	and the significance of the parameters could be determined in a quantitative
	way. A dependency of the simulation sensitivity on initial- and boundary
	conditions and the temporal and spatial variability of the sensitivity
	to some model parameters was revealed by the regression-based sensitivity
	analysis. Thus, the difficulty of transferring the results to different
	time periods or model applications in other catchments became obvious.
	The analysis of the temporal course of the simulation sensitivity
	to parameter values in conjunction with simulated and measured additional
	data sets (precipitation, temperature, reservoir volumes etc.) gave
	further insight into the internal model behaviour and demonstrated
	the plausibility of the model structure and process conceptionalizations.},
  doi = {10.1016/j.jhydrol.2005.01.004},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{sillmann2008,
  author = {Sillmann, J. and Roeckner, E.},
  title = {Indices for extreme events in projections of anthropogenic climate
	change},
  journal = {Climatic Change},
  year = {2008},
  volume = {86},
  pages = {83--104},
  number = {1--2},
  month = {January},
  abstract = {Indices for temperature and precipitation extremes are calculated
	on the basis of the global climate model ECHAM5/MPI-OM simulations
	of the twentieth century and SRES A1B and B1 emission scenarios for
	the twenty-first century. For model evaluation, the simulated indices
	representing the present climate were compared with indices based
	on observational data. This comparison shows that the model is able
	to realistically capture the observed climatological large-scale
	patterns of temperature and precipitation indices, although the quality
	of the simulations depends on the index and region under consideration.
	In the climate projections for the twenty-first century, all considered
	temperature-based indices, minimum Tmin, maximum Tmax, and the frequency
	of tropical nights, show a significant increase worldwide. Similarly,
	extreme precipitation, as represented by the maximum 5-day precipitation
	and the 95th percentile of precipitation, is projected to increase
	significantly in most regions of the world, especially in those that
	are relatively wet already under present climate conditions. Analogously,
	dry spells increase particularly in those regions that are characterized
	by dry conditions in present-day climate. Future changes in the indices
	exhibit distinct regional and seasonal patterns as identified exemplarily
	in three European regions.},
  doi = {10.1007/s10584-007-9308-6},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{singh2009,
  author = {Singh, A. and Mishra, S. and Ruskauff, G.},
  title = {Model averaging techniques for quantifying conceptual model uncertainty},
  journal = {Ground Water},
  year = {2009},
  volume = {48},
  pages = {701--715},
  number = {5},
  month = {September},
  abstract = {In recent years a growing understanding has emerged regarding the
	need to expand the modeling paradigm to include conceptual model
	uncertainty for groundwater models. Conceptual model uncertainty
	is typically addressed by formulating alternative model conceptualizations
	and assessing their relative likelihoods using statistical model
	averaging approaches. Several model averaging techniques and likelihood
	measures have been proposed in the recent literature for this purpose
	with two broad categories—Monte Carlo-based techniques such as Generalized
	Likelihood Uncertainty Estimation or GLUE (Beven and Binley 1992)
	and criterion-based techniques that use metrics such as the Bayesian
	and Kashyap Information Criteria (e.g., the Maximum Likelihood Bayesian
	Model Averaging or MLBMA approach proposed by Neuman 2003) and Akaike
	Information Criterion-based model averaging (AICMA) (Poeter and Anderson
	2005). These different techniques can often lead to significantly
	different relative model weights and ranks because of differences
	in the underlying statistical assumptions about the nature of model
	uncertainty. This paper provides a comparative assessment of the
	four model averaging techniques (GLUE, MLBMA with KIC, MLBMA with
	BIC, and AIC-based model averaging) mentioned above for the purpose
	of quantifying the impacts of model uncertainty on groundwater model
	predictions. Pros and cons of each model averaging technique are
	examined from a practitioner's perspective using two groundwater
	modeling case studies. Recommendations are provided regarding the
	use of these techniques in groundwater modeling practice.},
  doi = {10.1111/j.1745-6584.2009.00642.x},
  owner = {rojasro},
  timestamp = {2010.01.05}
}

@INBOOK{sorooshiangupta1995,
  author = {Singh, V.},
  editor = {Singh, V.},
  title = {Computer Models of Watershed Hydrology},
  chapter = {Model calibration},
  pages = {23--68},
  year = {1995},
  publisher = {Water Resources Publications},
  address = {Colorado},
  altauthor = {S. Sorooshian and V.K. Gupta},
  optannote = {Sorooshian, S. and Gupta, V.K., 1995. Model calibration. In: Singh,
	V.P., Editor, , 1995. Computer Models of Watershed Hydrology, Water
	Resources Publications, Colorado, pp. 23{--}68.},
  tags = {conceptual model}
}

@ARTICLE{skahill2006,
  author = {Skahill, B. and Doherty, J.},
  title = {Efficient accomodation of local minima in watershed model calibration},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {329},
  pages = {122--139},
  number = {1--2},
  abstract = {The Gauss–Marquardt–Levenberg (GML) method of computer-based parameter
	estimation, in common with other gradient-based approaches, suffers
	from the drawback that it may become trapped in local objective function
	minima, and thus report “optimized” parameter values that are not,
	in fact, optimized at all. This can seriously degrade its utility
	in the calibration of watershed models where local optima abound.
	Nevertheless, the method also has advantages, chief among these being
	its model-run efficiency, and its ability to report useful information
	on parameter sensitivities and covariances as a by-product of its
	use. It is also easily adapted to maintain this efficiency in the
	face of potential numerical problems (that adversely affect all parameter
	estimation methodologies) caused by parameter insensitivity and/or
	parameter correlation. The present paper presents two algorithmic
	enhancements to the GML method that retain its strengths, but which
	overcome its weaknesses in the face of local optima. Using the first
	of these methods an “intelligent search” for better parameter sets
	is conducted in parameter subspaces of decreasing dimensionality
	when progress of the parameter estimation process is slowed either
	by numerical instability incurred through problem ill-posedness,
	or when a local objective function minimum is encountered. The second
	methodology minimizes the chance of successive GML parameter estimation
	runs finding the same objective function minimum by starting successive
	runs at points that are maximally removed from previous parameter
	trajectories. As well as enhancing the ability of a GML-based method
	to find the global objective function minimum, the latter technique
	can also be used to find the locations of many non-global optima
	(should they exist) in parameter space. This can provide a useful
	means of inquiring into the well-posedness of a parameter estimation
	problem, and for detecting the presence of bimodal parameter and
	predictive probability distributions. The new methodologies are demonstrated
	by calibrating a Hydrological Simulation Program-FORTRAN (HSPF) model
	against a time series of daily flows. Comparison with the SCE-UA
	method in this calibration context demonstrates a high level of comparative
	model run efficiency for the new method.},
  doi = {10.1016/j.jhydrol.2006.02.005},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{smakhtin2001,
  author = {V.U. Smakhtin},
  title = {Low flow hydrology: a review},
  journal = {Journal of Hydrology},
  year = {2001},
  volume = {240},
  pages = {147--186},
  number = {3--4},
  abstract = {The paper intends to review the current status of low-flow hydrology
	— a discipline which deals with minimum flow in a river during the
	dry periods of the year. The discussion starts with the analysis
	of low-flow generating mechanisms operating in natural conditions
	and the description of anthropogenic factors which directly or indirectly
	affect low flows. This is followed by the review of existing methods
	of low-flow estimation from streamflow time-series, which include
	flow duration curves, frequency analysis of extreme low-flow events
	and continuous low-flow intervals, baseflow separation and characterisation
	of streamflow recessions. The paper describes the variety of low-flow
	characteristics (indices) and their applications. A separate section
	illustrates the relationships between low-flow characteristics. The
	paper further focuses on the techniques for low-flow estimation in
	ungauged river catchments, which include a regional regression approach,
	graphical representation of low-flow characteristics, construction
	of regional curves for low-flow prediction and application of time-series
	simulation methods. The paper presents a summary of recent international
	low-flow related research initiatives. Specific applications of low-flow
	data in river ecology studies and environmental flow management as
	well as the problem of changing minimum river flows as the result
	of climate variability are also discussed. The review is largely
	based on the research results reported during the last twenty years.},
  doi = {10.1016/S0022-1694(00)00340-1},
  issn = {0022-1694},
  keywords = {Low flow}
}

@ARTICLE{smith+al2008b,
  author = {Smith, P. and Beven, K. and Tawn, J.},
  title = {Detection of structural inadequacy in process-based hydrological
	models: A particle-filtering approach},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = { W01410},
  number = {1},
  abstract = {In recent years, increasing computational power has been used to weight
	competing hydrological models in a Bayesian framework to improve
	predictive power. This may suggest that for a given measure of association
	with the observed data, one hydrological model is superior to another.
	However, careful analyses of the residuals of the model fit are required
	to propose further improvements to the model. In this paper we consider
	an alternative method of analyzing the shortcomings in a hydrological
	model. The hydrological model parameters are treated as varying in
	time. Simulation using a particle filter algorithm then reveals the
	parameter distribution needed at each time to reproduce the observed
	data. The resulting parameter, and the corresponding model state,
	distributions can be analyzed to propose improvements to the hydrological
	model. A demonstrative example is presented using rainfall-runoff
	data from the Leaf River, United States. This indicates that even
	when explicitly representing the uncertainty of the observed rainfall
	and discharge series, the technique shows shortcomings in the model
	structure.},
  doi = {10.1029/2006WR005205},
  keywords = {MONTE-CARLO METHODS, RECURSIVE PARAMETER-ESTIMATION, EXTENDED KALMAN
	FILTER, RAINFALL-RUNOFF MODEL, DATA ASSIMILATION, STREAMFLOW SIMULATION,
	DYNAMIC-SYSTEMS, UNCERTAINTY, CALIBRATION, OPTIMIZATION},
  tags = {conceptual model}
}

@ARTICLE{smith2008,
  author = {Smith, P. and Beven, K. and Tawn, J.},
  title = {Informal likelihood measures in model assessment: {T}heoretic development
	and investigation},
  journal = {Advances in Water Resources},
  year = {2008},
  volume = {31},
  pages = {1087--1100},
  number = {8},
  abstract = {Within hydrology performance criteria such as the Nash–Sutcliffe efficiency
	have been used to condition the parameter space of a model. Their
	use is motivated by the fact that the stochastic error series between
	a model output and corresponding observations is the result of the
	composite effect of multiple error sources which cannot be described,
	even in form, a priori. This paper formalises the use of such performance
	criteria within a Bayesian framework, such as Generalised Likelihood
	Uncertainty Estimation (GLUE), by introducing the concept of Informal
	Likelihoods. Informal Likelihoods are used to characterise desirable
	features in the relationship between the model output and corresponding
	observed data. A number of common performance criteria are considered
	as Informal Likelihoods. Analytical results and a simulation indicate
	all of the performance criteria considered as Informal Likelihoods
	in this paper have one or more properties which may be considered
	undesirable, but may perform no less well in conditioning model parameters
	than formal likelihoods for which the assumptions are only mildly
	incorrect.},
  doi = {10.1016/j.advwatres.2008.04.012},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@ARTICLE{snowling2001,
  author = {Snowling, S. and Kramer, J.},
  title = {Evaluating modelling uncertainty for model selection},
  journal = {Ecological Modelling},
  year = {2001},
  volume = {138},
  pages = {17--30},
  number = {1--3},
  abstract = {Modelling uncertainty is evaluated with respect to model complexity,
	sensitivity and error. The hypothesis that more complex models simulate
	reality better, but with more sensitivity and less error, is tested.
	An Index of Complexity is proposed. Improvements in fit of more complex
	models are weighed against the increase in model sensitivity. A simple
	index of utility is then proposed for model selection. The index
	of utility evaluates model sensitivity (response to changes in input)
	and model error (closeness of simulation to measurement). Model utility
	is evaluated for several models in two case studies, a simple system
	involving sorption of metals on sediments, and a more complex system,
	involving groundwater transport of a hydrophobic contaminant. Moderately
	complex models are found to be the more utile of those tested.},
  doi = {doi:10.1016/S0304-3800(00)00390-2},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{sohn2000,
  author = {Sohn, M. and Small, M. and Pantazidou, M.},
  title = {Reducing uncertainty in site characterization using {B}ayes {M}onte
	{C}arlo methods},
  journal = {Journal of Environmental Engineering--ASCE},
  year = {2000},
  volume = {126},
  pages = {893--902},
  number = {10},
  abstract = {A Bayesian uncertainty analysis approach is developed as a tool for
	assessing and reducing uncertainty in ground-water flow and chemical
	transport predictions. The method is illustrated for a site contaminated
	with chlorinated hydrocarbons. Uncertainty in source characterization,
	in chemical transport parameters, and in the assumed hydrogeologic
	structure was evaluated using engineering judgment and updated using
	observed field data. The updating approach using observed hydraulic
	head data was able to differentiate between reasonable and unreasonable
	hydraulic conductivity fields but could not differentiate between
	alternative conceptual models for the geological structure of the
	subsurface at the site. Updating using observed chemical concentration
	data reduced the uncertainty in most parameters and reduced uncertainty
	in alternative conceptual models describing the geological structure
	at the site, source locations, and the chemicals released at these
	sources. Thirty-year transport projections for no-action and source
	containment scenarios demonstrate a typical application of the methods.},
  doi = {10.1061/(ASCE)0733-9372(2000)126:10(893)},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@INCOLLECTION{solomon+al2007,
  author = {S. Solomon and D. Qin and M. Manning and R.B. Alley and T. Berntsen
	and N. L. Bindoff and Z. Chen and A. Chidthaisong and J. M. Gregory
	and G.C. Hegerl and M. Heimann and B. Hewitson and B.J. Hoskins and
	F. Joos and J. Jouzel and V. Kattsov and U. Lohmann and T. Matsuno
	and M. Molina and N. Nicholls and J. Overpeck and G. Raga and V.
	Ramaswamy and J. Ren and M. Rusticucci and R. Somerville and T. F.
	Stocker and P. Whetton and R. A. Wood and D. Wratt},
  title = {Technical Summary},
  booktitle = {Climate Change 2007: The Physical Science Basis. Contribution of
	Working Group I to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {S. Solomon and D. Qin and M. Manning and Z. Chen and M. Marquis and
	K. B. Averyt and M. Tignor and H.L. Miller},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  tags = {IPCC}
}

@BOOK{sorensen2002,
  title = {Likelihood, {B}ayesian, and {MCMC} methods in quantitative genetics},
  publisher = {Springer-Verlag},
  year = {2002},
  author = {Sorensen, D. and Gianola, D.},
  volume = {I},
  pages = {740},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  refid = {SORENSEN2002},
  timestamp = {2008.11.04}
}

@ARTICLE{sorooshiandracup1980,
  author = {Sorooshian, S. and Dracup, J.},
  title = {Stochastic Parameter Estimation Procedures for Hydrologic Rainfall-Runoff
	Models: Correlated and Heteroscedastic Error Cases},
  journal = {Water Resources Research},
  year = {1980},
  volume = {16},
  pages = {430--442},
  number = {2},
  abstract = {A maximum likelihood estimation procedure is presented through which
	two aspects of the streamflow measurement errors of the calibration
	phase are accounted for. First, the correlated error case is considered
	where a first-order autoregressive scheme is presupposed for the
	additive errors. This proposed procedure first determines the anticipated
	correlation coefficient of the errors and then uses it in the objective
	function to estimate the best values of the model parameters. Second,
	the heteroscedastic error case (changing variance) is considered
	for which a weighting approach, using the concept of power transformation,
	is developed. The performances of the new procedures are tested with
	synthetic data for various error conditions on a two-parameter model.
	In comparison with the simple least squares criterion and the weighted
	least squares scheme of the HEC-1 of the U.S. Army Corps of Engineers
	for the heteroschedastic case, the new procedures constantly produced
	better estimates. The procedures were found to be easy to implement
	with no convergence problem. In the absence of correlated errors,
	as theoretically expected, the correlated error procedure produces
	the exact same estimates as the simple least squares criterion. Likewise,
	the self-correcting ability of the heteroschedastic error procedure
	was effective in reducing the objective function to that of the simple
	least squares as data gradually became homoscedastic. Finally, the
	effective residual tests for detection of the above-mentioned error
	situations are discussed. },
  doi = {10.1029/WR016i002p00430 },
  tags = {Calibration}
}

@ARTICLE{sorooshianduangupta2003,
  author = {Sorooshian, S. and Duan, Q. and Gupta, V.},
  title = {Calibration of Rainfall-Runoff Models - Application Of Global Optimization
	To The Sacramento Soil-Moisture Accounting Model},
  journal = {Water Resources Research},
  year = {1993},
  volume = {29},
  pages = {1185--1194},
  number = {4},
  abstract = {Conceptual rainfall-runoff models are difficult to calibrate by means
	of automatic methods; one major reason for this is the inability
	of conventional procedures to locate the globally optimal set of
	parameters. This paper investigates the consistency with which two
	global optimization methods, the shuffled complex evolution (SCE-UA)
	method (developed by the authors) and the multistart simplex (MSX)
	method, are able to find the optimal parameter set during calibration
	of the Sacramento soil moisture accounting model (SAC-SMA) of the
	National Weather Service River Forecast System (NWSRFS). In the first
	phase of this study, error-free synthetic data are used to conduct
	a comparative evaluation of the algorithms under ''ideal'' conditions.
	In 10 independent trials of each algorithm in which 13 parameters
	of the SAC-SMA model were optimized simultaneously, the SCE-UA method
	achieved a 100\% success rate in locating the precise global optimum
	(i.e., the ''true'' parameter values) while the MSX method failed
	in all trials even with more than twice the number of function evaluations.
	In the second phase, historical data from the Leaf River watershed
	are used to conduct a comparative evaluation of the algorithms under
	''real'' conditions, using two different estimation criteria, DRMS
	and HMLE; the SCE-UA algorithm obtained consistently lower function
	values and more closely grouped parameter estimates, while using
	one-third fewer function evaluations than the MSX algorithm.},
  doi = {10.1029/92WR02617 },
  keywords = {SMALL HYPOTHETICAL CATCHMENTS, AUTOMATIC CALIBRATION, OBSERVABILITY,
	UNCERTAINTY, UNIQUENESS, ALGORITHMS, REPRESENT, DYNAMICS, ABILITY},
  tags = {Calibration}
}

@ARTICLE{sorooshiangupta1983,
  author = {Sorooshian, S. and Gupta, V.},
  title = {Automatic Calibration of Conceptual Rainfall-Runoff Models: The Question
	of Parameter Observability and Uniqueness},
  journal = {Water Resources Research},
  year = {1983},
  volume = {19},
  pages = {260--268},
  number = {1},
  doi = {10.1029/WR019i001p00260},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{sorooshian+al1983,
  author = {Sorooshian, S. and Gupta, V. and Fulton, J.},
  title = {Evaluation of Maximum Likelihood Parameter Estimation Techniques
	for Conceptual Rainfall-Runoff Models : Influence of Calibration
	Data Variability and Length on Model Credibility},
  journal = {Water Resources Research},
  year = {1983},
  volume = {19},
  pages = {251--259},
  number = {1},
  abstract = {The success of an automatic calibration procedure is highly dependent
	on the choice of the objective function and the nature (quantity
	and quality) of the data used. The objective function should be selected
	on the basis of the stochastic properties of the errors present in
	the data and in the model. Also, the data should be chosen so as
	to contain as much valuable information about the process as possible.
	In this paper we compare the performance of two maximum likelihood
	estimators, the AMLE, which assumes the presence of first lag autocorrelated
	homogeneous variance errors, and the HMLE, which assumes the presence
	of uncorrelated inhomogeneous variance errors, to the commonly used
	simple least squares criterion, SLS. The model calibrated was the
	soil moisture accounting model of the U.S. National Weather Service's
	river forecast system (SMA-NWSRFS). The results indicate that a properly
	chosen objective function can enhance the possibility of obtaining
	unique and conceptually realistic parameter estimates. Furthermore,
	the sensitivity of the estimation results to various characteristics
	of the calibration data, such as hydrologic variability and length,
	are substantially reduced.},
  doi = {10.1029/WR019i001p00251},
  tags = {conceptual model, Calibration}
}

@ARTICLE{soussana+al2009,
  author = {Soussana, {J.-F.} and Tubiello, F. and Graux, {A.-I.}},
  title = {Use and misuse of modelling for projections of climate change impacts
	on crops and pastures},
  journal = {Comparative Biochemistry and Physiology - Part A: Molecular \& Integrative
	Physiology},
  year = {2009},
  volume = {153},
  pages = {S223--S224},
  number = {2},
  abstract = {Projections of climate change impacts on global food supply are largely
	based on crop and pasture modelling. The consistency of these models
	with experimental data and their ability to simulate the effects
	of elevated CO2 and of increased climate variability has been debated.
	It has recently been argued that most models tend to overestimate
	the CO2 response of crops compared to Free Air Carbon dioxide Enrichment
	(FACE) data. In addition, the effects of high temperatures, of increased
	climate variability and of several limiting factors such as nutrients,
	air quality, pests and weeds, may reduce the effects of elevated
	CO2 and such interactions are neither well understood nor well implemented
	in leading models. We discuss possible improvements in crop and pasture
	models based on fundamental knowledge at the plant and plot level.
	We conclude by making recommendations for current and future research
	needs, with a focus on stable and increased support for long-term
	studies and multi-factor experiments, explicit inclusion of biodiversity,
	disturbance, and extreme events in experiments and models and increased
	support for model–model and model–experiment comparisons.},
  abstracts = {Projections of climate change impacts on global food supply are largely
	based on crop and pasture modelling. The consistency of these models
	with experimental data and their ability to simulate the effects
	of elevated CO2 and of increased climate variability has been debated.
	It has recently been argued that most models tend to overestimate
	the CO2 response of crops compared to Free Air Carbon dioxide Enrichment
	(FACE) data. In addition, the effects of high temperatures, of increased
	climate variability and of several limiting factors such as nutrients,
	air quality, pests and weeds, may reduce the effects of elevated
	CO2 and such interactions are neither well understood nor well implemented
	in leading models. We discuss possible improvements in crop and pasture
	models based on fundamental knowledge at the plant and plot level.
	We conclude by making recommendations for current and future research
	needs, with a focus on stable and increased support for long-term
	studies and multi-factor experiments, explicit inclusion of biodiversity,
	disturbance, and extreme events in experiments and models and increased
	support for model--model and model--experiment comparisons},
  doi = {10.1016/j.cbpa.2009.04.555},
  tags = {Agriculture}
}

@ARTICLE{spear1980,
  author = {Spear, R. and Hornberger, G.},
  title = {Eutrophication in peel inlet--{II}. Identification of critical uncertainties
	via generalized sensitivity analysis},
  journal = {Water Research},
  year = {1980},
  volume = {14},
  pages = {43--49},
  number = {1},
  abstract = {A generalized sensitivity analysis was carried out on a phosphorous
	based model of cultural eutrophication processes in the Peel Inlet
	of Western Australia. The object of the analysis was to identify
	critical uncertainties in present knowledge of the system for the
	direction and planning of future research. The main hypothesis suggested
	by the results is that the nuisance alga, Cladophora aff. battersii,
	have access to nutrients in the interstitial water of the sediments
	in the Inlet and that a significant quantity of nutrient is deposited
	in the major area of Cladophora growth by river-borne sediment. Suggested
	areas for future research include investigation of mechanisms of
	deposition, release and remineralization of nutrients in the sediment,
	and of certain aspects of the physiology of Cladophora.},
  doi = {10.1016/0043-1354(80)90040-8},
  owner = {RRojas},
  timestamp = {2009.02.26}
}

@ARTICLE{spelucci1998,
  author = {Spellucci, P.},
  title = {An {SQP} method for general nonlinear programs using only equality
	constrained subproblems},
  journal = {Mathematical Programming},
  year = {1998},
  volume = {82},
  pages = {413--448},
  number = {3},
  abstract = {In this paper we describe a new version of a sequential equality constrained
	quadratic programming method for general nonlinear programs with
	mixed equality and inequality constraints. Compared with an older
	version [P. Spellucci, Han's method without solving QP, in: A. Auslender,
	W. Oettli, J. Stoer (Eds), Optimization and Optimal Control, Lecture
	Notes in Control and Information Sciences, vol. 30, Springer, Berlin,
	1981, pp. 123–141.] it is much simpler to implement and allows any
	kind of changes of the working set in every step. Our method relies
	on a strong regularity condition. As far as it is applicable the
	new approach is superior to conventional SQP-methods, as demonstrated
	by extensive numcrical tests.},
  doi = {10.1007/BF01580078},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{sperna2010,
  author = {{Sperna Weiland}, F. and {van Beek}, L. and Kwadijk, J. and Bierkens,
	M.},
  title = {The ability of a {GCM}--forced hydrological model to reproduce global
	discharge variability},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2010},
  volume = {7},
  pages = {687--724},
  number = {1},
  abstract = {Data from General Circulation Models (GCMs) are often used in studies
	investigating hydrological impacts of climate change. However GCM
	data are known to have large biases, especially for precipitation.
	In this study the usefulness of GCM data for hydrological studies
	was tested by applying bias-corrected daily climate data of the 20CM3
	control experiment from an ensemble of twelve GCMs as input to the
	global hydrological model PCR-GLOBWB. Results are compared with discharges
	calculated from a model run based on a reference meteorological dataset
	constructed from the CRU TS2.1 data and ERA-40 reanalysis time-series.
	Bias-correction was limited to monthly mean values as our focus was
	on the reproduction of runoff variability. The bias-corrected GCM
	based runs resemble the reference run reasonably well, especially
	for rivers with strong seasonal patterns. However, GCM derived discharge
	quantities are overall too low. Furthermore, from the arctic regimes
	it can be seen that a few deviating GCMs can bias the ensemble mean.
	Moreover, the GCMs do not well represent intra- and inter-year variability
	as exemplified by a limited persistence. This makes them less suitable
	for the projection of future runoff extremes.},
  doi = {10.5194/hessd-7-687-2010},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@BOOK{spitzmoreno1996,
  title = {A practical guide to groundwater and solute transport modeling},
  publisher = {John Wiley \& Sons Inc.},
  year = {1996},
  author = {Spitz, K. and Moreno, J.},
  pages = {461},
  address = {New York},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{Stahl2010,
  author = {Stahl, K. and Hisdal, H. and Hannaford, J. and Tallaksen, L. M. and
	{van Lanen}, H. A. J. and Sauquet, E. and Demuth, S. and Fendekova,
	M. and J\'odar, J.},
  title = {Streamflow trends in Europe: evidence from a dataset of near-natural
	catchments},
  journal = {Hydrology and Earth System Sciences},
  year = {2010},
  volume = {14},
  pages = {2367--2382},
  number = {12},
  abstract = {Streamflow observations from near-natural catchments are of paramount
	importance for detection and attribution studies, evaluation of large-scale
	model simulations, and assessment of water management, adaptation
	and policy options. This study investigates streamflow trends in
	a newly-assembled, consolidated dataset of near-natural streamflow
	records from 441 small catchments in 15 countries across Europe.
	The period 1962–2004 provided the best spatial coverage, but analyses
	were also carried out for longer time periods (with fewer stations),
	starting in 1932, 1942 and 1952. Trends were calculated by the slopes
	of the Kendall-Theil robust line for standardized annual and monthly
	streamflow, as well as for summer low flow magnitude and timing.
	A regionally coherent picture of annual streamflow trends emerged,
	with negative trends in southern and eastern regions, and generally
	positive trends elsewhere. Trends in monthly streamflow for 1962–2004
	elucidated potential causes for these changes, as well as for changes
	in hydrological regimes across Europe. Positive trends were found
	in the winter months in most catchments. A marked shift towards negative
	trends was observed in April, gradually spreading across Europe to
	reach a maximum extent in August. Low flows have decreased in most
	regions where the lowest mean monthly flow occurs in summer, but
	vary for catchments which have flow minima in winter and secondary
	low flows in summer. The study largely confirms findings from national
	and regional scale trend analyses, but clearly adds to these by confirming
	that these tendencies are part of coherent patterns of change, which
	cover a much larger region. The broad, continental-scale patterns
	of change are mostly congruent with the hydrological responses expected
	from future climatic changes, as projected by climate models. The
	patterns observed could hence provide a valuable benchmark for a
	number of different studies and model simulations.},
  doi = {10.5194/hess-14-2367-2010}
}

@ARTICLE{stainforth+al2005,
  author = {Stainforth, D. and Aina, T. and Christensen, C. and Collins, M. and
	Faull, F. and Frame, D. and Kettleborough, J. and Knight, S. and
	Martin, A. and Murphy, J. and Piani, C. and Sexton, D. and Smith,
	L. and Spicer, R. and Thorpe, A. and Allen, M.},
  title = {Uncertainty in predictions of the climate response to rising levels
	of greenhouse gases},
  journal = {Nature},
  year = {2005},
  volume = {433},
  pages = {403--406},
  number = {7024},
  abstract = {The range of possibilities for future climate evolution needs to be
	taken into account when planning climate change mitigation and adaptation
	strategies. This requires ensembles of multi-decadal simulations
	to assess both chaotic climate variability and model response uncertainty.
	Statistical estimates of model response uncertainty, based on observations
	of recent climate change admit climate sensitivities—defined as the
	equilibrium response of global mean temperature to doubling levels
	of atmospheric carbon dioxide—substantially greater than 5 K. But
	such strong responses are not used in ranges for future climate change
	because they have not been seen in general circulation models. Here
	we present results from the 'climateprediction.net' experiment, the
	first multi-thousand-member grand ensemble of simulations using a
	general circulation model and thereby explicitly resolving regional
	details. We find model versions as realistic as other state-of-the-art
	climate models but with climate sensitivities ranging from less than
	2 K to more than 11 K. Models with such extreme sensitivities are
	critical for the study of the full range of possible responses of
	the climate system to rising greenhouse gas levels, and for assessing
	the risks associated with specific targets for stabilizing these
	levels.},
  doi = {10.1038/nature03301},
  pmid = {15674288},
  tags = {Uncertainty, Thesis}
}

@ARTICLE{stedinger2008,
  author = {Stedinger, J. and Vogel, R. and Lee, S. and Batchelder, R.},
  title = {Appraisal of the generalized likelihood uncertainty estimation {(GLUE)}
	method},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W00B06},
  abstract = {Recent research documents that the widely accepted generalized likelihood
	uncertainty estimation (GLUE) method for describing forecasting precision
	and the impact of parameter uncertainty in rainfall/runoff watershed
	models fails to achieve the intended purpose when used with an informal
	likelihood measure. In particular, GLUE generally fails to produce
	intervals that capture the precision of estimated parameters, and
	the difference between predictions and future observations. This
	paper illustrates these problems with GLUE using a simple linear
	rainfall/runoff model so that model calibration is a linear regression
	problem for which exact expressions for prediction precision and
	parameter uncertainty are well known and understood. The simple regression
	example enables us to clearly and simply illustrate GLUE deficiencies.
	Beven and others have suggested that the choice of the likelihood
	measure used in a GLUE computation is subjective and may be selected
	to reflect the goals of the modeler. If an arbitrary likelihood is
	adopted that does not reasonably reflect the sampling distribution
	of the model errors, then GLUE generates arbitrary results without
	statistical validity that should not be used in scientific work.
	The traditional subjective likelihood measures that have been used
	with GLUE also fail to reflect the nonnormality, heteroscedasticity,
	and serial correlation among the residual errors generally found
	in real problems, and hence are poor metrics for even simple sensitivity
	analyses and model calibration. Most previous applications of GLUE
	only produce uncertainty intervals for the average model prediction,
	which by construction should not be expected to include future observations
	with the prescribed probability. We show how the GLUE methodology
	when properly implemented with a statistically valid likelihood function
	can provide prediction intervals for future observations which will
	agree with widely accepted and statistically valid analyses.},
  doi = {10.1029/2008WR006822},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{steeledunne+al2008,
  author = {{Steele-Dunne}, S. and Lynch, P. and McGrath, R. and Semmler, T.
	and Wang, S. and Hanafin, J. and Nolan, P.},
  title = {The impacts of climate change on hydrology in {I}reland},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {356},
  pages = {28--45},
  number = {1--2},
  abstract = {A study of nine Irish catchments was carried out to quantify the expected
	impact of climate change on hydrology in Ireland. Boundary data from
	the European Centre Hamburg Model Version 5 (ECHAM 5) general circulation
	model were used to force the Rossby Centre Atmosphere Model (RCA3)
	regional climate model, producing dynamically downscaled precipitation
	and temperature data under past and future climate scenarios. This
	data was used to force the HBV-Light conceptual rainfall-runoff model
	to simulate stream flow in the reference period (1961--2000) and
	in the future (2021--2060) under the Special Report on Emissions
	Scenarios (SRES) A1B scenario. A Monte-Carlo approach to calibration
	was used to obtain 100 parameter sets which reproduced observed stream
	flow well. Use of an ensemble provided results in terms of a range
	rather than a single value. Results suggested an amplification of
	the seasonal cycle across the country, driven by increased winter
	precipitation, decreased summer precipitation and increased temperature.
	The expected changes in mean winter and summer flows as well as annual
	maximum daily mean flow varied depending on catchment characteristics
	and the timing and magnitude of expected changes in precipitation
	in each catchment.},
  doi = {10.1016/j.jhydrol.2008.03.025},
  keywords = {Climate change, Ireland, Catchment hydrology, Parameter uncertainty,
	Calibration},
  tags = {Climate Change}
}

@ARTICLE{stone+al2005,
  author = {Stone, D. and Allen, M.},
  title = {The end--to--end attribution problem: {F}rom emissions to impacts},
  journal = {Climatic Change},
  year = {2005},
  volume = {71},
  pages = {303--318},
  number = {3},
  abstract = {When a damaging extreme meteorological event occurs, the question
	often arises as to whether that event was caused by anthropogenic
	greenhouse gas emissions. The question is more than academic, since
	people affected by the event will be interested in recurring damages
	if they find that someone is at fault. However, since this extreme
	event could have occurred by chance in an unperturbed climate, we
	are currently unable to properly respond to this question. A solution
	lies in recognising the similarity with the cause-effect issue in
	the epidemiological field. The approach there is to consider the
	changes in the risk of the event occurring as attributable, as against
	the occurrence of the event itself. Inherent in this approach is
	a recognition that knowledge of the change in risk as well as the
	amplitude of the forcing itself are uncertain. Consequently, the
	fraction of the risk attributable to the external forcing is a probabilistic
	quantity. Here we develop and demonstrate this methodology in the
	context of the climate change},
  doi = {10.1007/s10584-005-6778-2},
  tags = {Thesis, Uncertainty}
}

@ARTICLE{stonestrom2009,
  author = {Stonestrom, D.},
  title = {Introduction to special section on Impacts of Land Use Change on
	Water Resources},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = {W00A00},
  abstract = {Changes in land use have potentially large impacts on water resources,
	yet quantifying these impacts remains among the more challenging
	problems in hydrology. Water, food, energy, and climate are linked
	through complex webs of direct and indirect effects and feedbacks.
	Land use is undergoing major changes due not only to pressures for
	more efficient food, feed, and fiber production to support growing
	populations but also due to policy shifts that are creating markets
	for biofuel and agricultural carbon sequestration. Hydrologic systems
	embody flows of water, solutes, sediments, and energy that vary even
	in the absence of human activity. Understanding land use impacts
	thus necessitates integrated scientific approaches. Field measurements,
	remote sensing, and modeling studies are shedding new light on the
	modes and mechanisms by which land use changes impact water resources.
	Such studies can help deconflate the interconnected influences of
	human actions and natural variations on the quantity and quality
	of soil water, surface water, and groundwater, past, present, and
	future.},
  doi = {10.1029/2009WR007937},
  tags = {Climate Change}
}

@ARTICLE{stottkettleborough2002,
  author = {Stott, P. and Kettleborough, J.},
  title = {Origins and estimates of uncertainty in prediction of twenty--first
	century temperature rise},
  journal = {Nature},
  year = {2002},
  volume = {416},
  pages = {723--726},
  number = {6882},
  abstract = {Predictions of temperature rise over the twenty-first century are
	necessarily uncertain, both because the sensitivity of the climate
	system to changing atmospheric greenhouse-gas concentrations, as
	well as the rate of ocean heat uptake, is poorly quantified and because
	future influences on climate—of anthropogenic as well as natural
	origin—are difficult to predict. Past observations have been used
	to help constrain the range of uncertainties in future warming rates,
	but under the assumption of a particular scenario of future emissions.
	Here we investigate the relative importance of the uncertainty in
	climate response to a particular emissions scenario versus the uncertainty
	caused by the differences between future emissions scenarios for
	our estimates of future change. We present probabilistic forecasts
	of global-mean temperatures for four representative scenarios for
	future emissions, obtained with a comprehensive climate model. We
	find that, in the absence of policies to mitigate climate change,
	global-mean temperature rise is insensitive to the differences in
	the emissions scenarios over the next four decades. We also show
	that in the future, as the signal of climate change emerges further,
	the predictions will become better constrained.},
  doi = {10.1038/416723a},
  tags = {Uncertainty}
}

@ARTICLE{strzepek1997,
  author = {Strzepek, K. and Yates, D.},
  title = {Climate change impacts on the hydrologic resources of {Europe}: a
	simplified continental scale analysis},
  journal = {Climatic Change},
  year = {1997},
  volume = {36},
  pages = {79--92},
  number = {1--2},
  abstract = {U.S. Country Studies supported analyses of climate change impacts
	on water resources have been completed or are underway in the following
	Central and Eastern European nations: Czech Republic, Slovakia, Poland,
	Romania, Estonia, Russian Federation, and the Ukraine. Climate change
	impacts on the hydrologic resources of these countries is being performed
	at the river basin scale using monthly water balance models using
	GCM-based climate scenarios. The authors have performed a regional
	analysis of climate change impacts on the Hydrologic Resources of
	Europe using the Turc Annual Model. The regional analysis was done
	with GIS methodolgies using regional climate databases. The regional
	results were compared to the U.S. Country Studies hydrologic assessmnent
	results to validiate the use of this simplified methodolgy for making
	regional climate change assessment. Results from three countries
	showed acceptable performace of the annual approach . Using GCM-based
	climate scenarios regional analysis of potential climate change impacts
	on the hydrologic resources of Europe was conducted and national
	and regional results are presented.},
  doi = {10.1023/A:1005305827527},
  owner = {rojasro},
  timestamp = {2010.08.05}
}

@ARTICLE{suklitsch+al2010,
  author = {Suklitsch, M. and Gobiet, A. and Truhetz, H. and Awan, N. and G\"ottel,
	H. and Jacob, D.},
  title = {Error characteristics of high resolution regional climate models
	over the {Alpine area}},
  journal = {Climate Dynamics},
  year = {2010},
  volume = {37},
  pages = {377-390},
  number = {1--2},
  abstract = {This study describes typical error ranges of high resolution regional
	climate models operated over complex orography and investigates the
	scale-dependence of these error ranges. The results are valid primarily
	for the European Alpine region, but to some extent they can also
	be transferred to other orographically complex regions of the world.
	We investigate the model errors by evaluating a set of 62 one-year
	hindcast experiments for the year 1999 with four different regional
	climate models. The analysis is conducted for the parameters mean
	sea level pressure, air temperature (mean, minimum and maximum) and
	precipitation (mean, frequency and intensity), both as an area average
	over the whole modeled domain (the “Greater Alpine Region”, GAR)
	and in six subregions. The subregional seasonal error ranges, defined
	as the interval between the 2.5th percentile and the 97.5th percentile,
	lie between ?3.2 and +2.0 K for temperature and between ?2.0 and
	+3.1 mm/day (?45.7 and +94.7%) for precipitation, respectively. While
	the temperature error ranges are hardly broadened at smaller scales,
	the precipitation error ranges increase by 28%. These results demonstrate
	that high resolution RCMs are applicable in relatively small scale
	climate impact studies with a comparable quality as on well investigated
	larger scales as far as temperature is concerned. For precipitation,
	which is a much more demanding parameter, the quality is moderately
	degraded on smaller scales.},
  doi = {10.1007/s00382-010-0848-5},
  owner = {rojasro},
  timestamp = {2011.02.02}
}

@BOOK{sun1994,
  title = {Inverse {P}roblems in {G}roundwater {M}odeling ({T}heory and {A}pplications
	of {T}ransport in {P}orous {M}edia)},
  publisher = {Kluwer Academic Publishers},
  year = {1994},
  author = {Sun, N--Z.},
  pages = {352},
  address = {The Netherlands},
  edition = {First},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{susuki1985,
  author = {Suzuki, O. and Aravena, R.},
  title = {Hidrolog\'ia isot\'opica y el recurso agua del sector {Esmeralda--Pica--Matilla}},
  journal = {Nucleot\'ecnica},
  year = {1985},
  volume = {4},
  pages = {41--51},
  number = {8},
  owner = {RRojas},
  timestamp = {2009.04.07}
}

@ARTICLE{svensson+al2005,
  author = {Svensson, Cecilia and Kundzewicz, W. Zbigniew and Maurer, Thomas},
  title = {Trend detection in river flow series: 2. Flood and low-flow index
	series},
  journal = {Hydrological Sciences Journal},
  year = {2005},
  volume = {50},
  pages = {811--824},
  number = {5},
  abstract = { Abstract Major floods in Europe and North America during the past
	decade have provoked the question of whether or not they are an effect
	of a changing climate. This study investigates changes in observational
	data, using up to 100-year-long daily mean river flow records at
	21 stations worldwide. Trends in seven flood and low-flow index series
	are assessed using Mann-Kendall and linear regression methods. Emphasis
	was on the comparison of trends in these flow index series, particularly
	in peak-over-threshold (POT) series as opposed to annual maximum
	(AM) river flow series. There is a larger number of significant trends
	in the AM than in the POT flood magnitude series, probably relating
	to the way the series are constructed. Low flood peaks occurring
	at the beginning or end of a time series with trend may be too low
	to be selected for the POT analysis. However, one peak per year will
	always be selected for the AM series, making the slope steeper and/or
	the series longer, resulting in a more significant trend. There is
	no general pattern of increasing or decreasing numbers or magnitudes
	of floods, but there are significant increases in half of the low-flow
	series. },
  doi = {10.1623/hysj.2005.50.5.811}
}

@ARTICLE{hyung2007,
  author = {{Tae-Choi}, H. and Beven, K.},
  title = {Multi--period and multi--criteria model conditioning to reduce prediction
	uncertainty in an application of {TOPMODEL} within the {GLUE} framework},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {332},
  pages = {316--336},
  number = {3--4},
  abstract = {A new approach to multi-criteria model evaluation is presented. The
	approach is consistent with the equifinality thesis and is developed
	within the Generalised Likelihood Uncertainty Estimation (GLUE) framework.
	The predictions of Monte Carlo realisations of TOPMODEL parameter
	sets are evaluated using a number of performance measures calibrated
	for both global (annual) and seasonal (30 day) periods. The seasonal
	periods were clustered using a Fuzzy C-means algorithm, into 15 types
	representing different hydrological conditions. The model shows good
	performance on a classical efficiency measure at the global level,
	but no model realizations were found that were behavioural over all
	multi-period clusters and all performance measures, raising questions
	about what should be considered as an acceptable model performance.
	Prediction uncertainties can still be calculated by allowing that
	different clusters require different parameter sets. Variations in
	parameter distributions between clusters, as well as examination
	of where observed discharges depart from model prediction bounds,
	give some indication of model structure deficiencies.},
  doi = {10.1016/j.jhydrol.2006.07.012},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{tallaksen+al2009,
  author = {Lena M. Tallaksen and Hege Hisdal and Henny A.J. Van Lanen},
  title = {Space–time modelling of catchment scale drought characteristics},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {375},
  pages = {363--372},
  number = {3–-4},
  abstract = {Summary Drought may affect all components of the water cycle and covers
	commonly a large part of the catchment area. This paper examines
	drought propagation at the catchment scale using spatially aggregated
	drought characteristics and illustrates the importance of catchment
	processes in modifying the drought signal in both time and space.
	Analysis is conducted using monthly time series covering the period
	1961–1997 for the Pang catchment, UK. The time series include observed
	rainfall and groundwater recharge, head and discharge simulated by
	physically-based soil water and groundwater models. Drought events
	derived separately for each unit area and variable are combined to
	yield catchment scale drought characteristics. The study reveals
	relatively large differences in the spatial and temporal characteristics
	of drought for the different variables. Meteorological droughts cover
	frequently the whole catchment; and they are more numerous and last
	for a short time (1–2&#xa0;months). In comparison, droughts in recharge
	and hydraulic head cover typically a smaller area and last longer
	(4–5&#xa0;months). Hydraulic head and groundwater discharge exhibit
	similar drought characteristics, which can be expected in a groundwater
	fed catchment. Deficit volume is considered a robust measure of the
	severity of a drought event over the catchment area for all variables;
	whereas, duration is less sensitive, particular for rainfall. Spatial
	variability in drought characteristics for groundwater recharge,
	head and discharge are primarily controlled by catchment properties.
	It is recommended not to use drought area separately as a measure
	of drought severity at the catchment scale, rather it should be used
	in combination with other drought characteristics like duration and
	deficit volume.},
  doi = {10.1016/j.jhydrol.2009.06.032},
  issn = {0022-1694},
  keywords = {Drought}
}

@ARTICLE{tallaksen+al1997,
  author = {Tallaksen, L. M. and Madsen, H. and Clausen, B.},
  title = {On the definition and modelling of streamflow drought duration and
	deficit volume},
  journal = {Hydrological Sciences Journal},
  year = {1997},
  volume = {42},
  pages = {15--33},
  number = {1},
  abstract = {The threshold level approach is used to define drought characteristics,
	i.e. drought duration and deficit volume from time series of daily
	streamflow. Three different procedures for pooling dependent droughts
	are compared: a method based on an inter-event time and volume criterion
	(IC), a moving average procedure (MA), and a method based on the
	sequent peak algorithm (SPA). The extreme values of drought duration
	and deficit volume are analysed using both an annual maximum series
	(AMS) and a partial duration series (PDS) approach. Two Danish catchments
	with very different flow regimes were used in the study. The IC and
	MA methods provided virtually the same sample statistics of the AMS
	of drought duration and deficit volume for all thresholds considered.
	The results of the SPA method differed significantly from the other
	two methods for high thresholds due to the presence of multi-year
	droughts. For analysis of seasonal droughts the SPA method is restricted
	to low thresholds. The occurrence of a large number of zerodrought
	years for low thresholds may significantly reduce the information
	content of the AMS, and in this case the PDS model is superior. The
	problem of minor droughts in the PDS was implicitly reduced by using
	the MA and SPA methods, and in this respect these methods have an
	important advantage as compared to the IC method.},
  doi = {10.1080/02626669709492003},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{tamura2007,
  author = {Tamura, R.},
  title = {Rdonlp2: {A}n {R} extension library to use {P}eter {S}pellucci's
	{DONLP}2 from {R}. {R} package version 0.3--1},
  year = {2007},
  owner = {RRojas},
  timestamp = {2008.04.23},
  url = {http://arumat.net/Rdonlp2/}
}

@ARTICLE{tangbagchi2010,
  author = {Tang, Z. and Bagchi, K.},
  title = {Globally Convergent Particle Swarm Optimization via Branch-and-Bound},
  journal = {Computer and Information Science},
  year = {2010},
  volume = {3},
  pages = {60--71},
  number = {4},
  abstract = {Particle swarm optimization (PSO) is a recently developed optimization
	method that has attracted interest of researchers in various areas.
	PSO has been shown to be effective in solving a variety of complex
	optimization problems. With properly chosen parameters, PSO can converge
	to local optima. However, conventional PSO does not have global convergence.
	Empirical evidences indicate that the PSO algorithm may fail to reach
	global optimal solutions for complex problems. We propose to combine
	the branch-and-bound framework with the particle swarm optimization
	algorithm. With this integrated approach, convergence to global optimal
	solutions is theoretically guaranteed. We have developed and implemented
	the BB-PSO algorithm that combines the efficiency of PSO and effectiveness
	of the branch-and-bound method. The BB-PSO method was tested with
	a set of standard benchmark optimization problems. Experimental results
	confirm that BB-PSO is effective in finding global optimal solutions
	to problems that may cause difficulties for the PSO algorithm.},
  tags = {PSO},
  url = {http://www.ccsenet.org/journal/index.php/cis/article/view/7039/6114}
}

@MANUAL{RTeam2011b,
  title = {{R} Data Import/Export},
  author = {Team, R. Development Core},
  year = {2011},
  note = {ISBN 3-900051-10-0},
  tags = {R}
}

@ARTICLE{tebaldi+al2004,
  author = {Tebaldi, C.},
  title = {Regional probabilities of precipitation change: A {B}ayesian analysis
	of multimodel simulations},
  journal = {Geophysical Research Letters},
  year = {2004},
  volume = {31},
  pages = {L24213},
  number = {24},
  abstract = {Tebaldi et al. [2005] present a Bayesian approach to determining probability
	distribution functions (PDFs) of temperature change at regional scales,
	from the output of a multi-model ensemble, run under the same scenario
	of future anthropogenic emissions. The main characteristic of the
	method is the formalization of the two criteria of bias and convergence
	that the REA method [Giorgi and Mearns, 2002] first quantified as
	a way of assessing model reliability. Thus, the General Circulation
	Models (AOGCMs) of the ensemble are combined in a way that accounts
	for their performance with respect to current climate and a measure
	of each model's agreement with the majority of the ensemble. We apply
	the Bayesian model to a set of transient experiments under two SRES
	scenarios. We focus on predictions of precipitation change, for land
	regions of subcontinental size. We highlight differences in the PDFs
	of precipitation change derived in regions where models find easy
	agreement, and perform well in simulating present day precipitation,
	compared to regions where models have large biases, and/or their
	future projections disagree. We compare results from the two scenarios,
	thus assessing the consequences of the two alternative hypotheses,
	and present summaries based on their averaging},
  doi = {10.1029/2004GL021276},
  tags = {Multimodel - Ensambles}
}

@ARTICLE{tebaldiknutti2007,
  author = {Tebaldi, C. and Knutti, R.},
  title = {The use of the multi--model ensemble in probabilistic climate projections},
  journal = {Philosophical Transactions of the Royal Society A Mathematical, Physical
	\& Engineering Sciences},
  year = {2007},
  volume = {365},
  pages = {2053--2075},
  number = {1857},
  abstract = {Recent coordinated efforts, in which numerous climate models have
	been run for a common set of experiments, have produced large datasets
	of projections of future climate for various scenarios. Those multi-model
	ensembles sample initial condition, parameter as well as structural
	uncertainties in the model design, and they have prompted a variety
	of approaches to quantify uncertainty in future climate in a probabilistic
	way. This paper outlines the motivation for using multi-model ensembles,
	reviews the methodologies published so far and compares their results
	for regional temperature projections. The challenges in interpreting
	multi-model results, caused by the lack of verification of climate
	projections, the problem of model dependence, bias and tuning as
	well as the difficulty in making sense of an â€˜ensemble of opportunityâ€™,
	are discussed in detail},
  doi = {10.1098/rsta.2007.2076},
  pmid = {17569654},
  tags = {Multimodel - Ensambles}
}

@ARTICLE{tebaldi+al2005,
  author = {Tebaldi, C. and Smith, R. and Nychka, D. and Mearns, L.},
  title = {Quantifying uncertainty in projections of regional climate change:
	A {B}ayesian approach to the analysis of multi--model ensembles},
  journal = {Journal of Climate},
  year = {2005},
  volume = {18},
  pages = {1524--1540},
  number = {10},
  abstract = {A Bayesian statistical model is proposed that combines information
	from a multimodel ensemble of atmosphere–ocean general circulation
	models (AOGCMs) and observations to determine probability distributions
	of future temperature change on a regional scale. The posterior distributions
	derived from the statistical assumptions incorporate the criteria
	of bias and convergence in the relative weights implicitly assigned
	to the ensemble members. This approach can be considered an extension
	and elaboration of the reliability ensemble averaging method. For
	illustration, the authors consider the output of mean surface temperature
	from nine AOGCMs, run under the A2 emission scenario from the Synthesis
	Report on Emission Scenarios (SRES), for boreal winter and summer,
	aggregated over 22 land regions and into two 30-yr averages representative
	of current and future climate conditions. The shapes of the final
	probability density functions of temperature change vary widely,
	from unimodal curves for regions where model results agree (or outlying
	projections are discounted) to multimodal curves where models that
	cannot be discounted on the basis of bias give diverging projections.
	Besides the basic statistical model, the authors consider including
	correlation between present and future temperature responses, and
	test alternative forms of probability distributions for the model
	error terms. It is suggested that a probabilistic approach, particularly
	in the form of a Bayesian model, is a useful platform from which
	to synthesize the information from an ensemble of simulations. The
	probability distributions of temperature change reveal features such
	as multimodality and long tails that could not otherwise be easily
	discerned. Furthermore, the Bayesian model can serve as an interdisciplinary
	tool through which climate modelers, climatologists, and statisticians
	can work more closely. For example, climate modelers, through their
	expert judgment, could contribute to the formulations of prior distributions
	in the statistical model.},
  doi = {10.1175/JCLI3363.1},
  tags = {Multimodel - Ensambles}
}

@ARTICLE{teegavarapuchandramouli2005,
  author = {Teegavarapu, R. and Chandramouli, V.},
  title = {Improved weighting methods, deterministic and stochastic data--driven
	models for estimation of missing precipitation records},
  journal = {Journal of Hydrology},
  year = {2005},
  volume = {312},
  pages = {191},
  number = {1--4},
  abstract = {Distance-weighted and data-driven methods are extensively used for
	estimation of missing rainfall data. Inverse distance weighting method
	(IDWM) is one of the most frequently used methods for estimating
	missing rainfall values at a gage based on values recorded at all
	other available recording gages. In spite of the method's wide success
	and acceptability, it suffers from major conceptual limitations.
	Conceptual improvements are incorporated in the IDWM method that
	led to several modified distance-based methods. A data-driven model
	that uses artificial neural network concepts and a stochastic interpolation
	technique, kriging, are also developed and tested in the current
	study. These methods are tested for estimation of missing precipitation
	data. Historical precipitation data from 20 rain-gauging stations
	in the state of Kentucky, USA, are used to test the improvised methods
	and derive conclusions about the efficacy of incorporated improvements.
	Results suggest that the conceptual revisions can improve estimation
	of missing precipitation records by defining better weighting parameters
	and surrogate measures for distances that are used in the IDWM.},
  doi = {10.1016/j.jhydrol.2005.02.015},
  keywords = {Missing rainfall data, Inverse distance weighting method, Thiessen
	polygon method, Geometric patterns, Weighting schemes, Kriging, Artificial
	neural networks, Kentucky, USA},
  tags = {Rainfall}
}

@ARTICLE{terink+al2010,
  author = {Terink, W. and Hurkmans, R. and Torfs, P. and Uijlenhoet, R.},
  title = {Evaluation of a bias correction method applied to downscaled precipitation
	and temperature reanalysis data for the {R}hine basin},
  journal = {Hydrology and Earth System Sciences},
  year = {2010},
  volume = {14},
  pages = {684--703},
  number = {4},
  abstract = {In many climate impact studies hydrological models are forced with
	meteorological data without an attempt to assess the quality of these
	data. The objective of this study is to compare downscaled ERA15
	(ECMWF-reanalysis data) precipitation and temperature with observed
	precipitation and temperature and apply a bias correction to these
	forcing variables. Precipitation is corrected by fitting the mean
	and coefficient of variation (CV) of the observations. Temperature
	is corrected by fitting the mean and standard deviation of the observations.
	It appears that the uncorrected ERA15 is too warm and too wet for
	most of the Rhine basin. The bias correction leads to satisfactory
	results, precipitation and temperature differences decreased significantly,
	although there are a few years for which the correction of precipitation
	is less satisfying. Corrections were largest during summer for both
	precipitation and temperature, and for September and October for
	precipitation only. Besides the statistics the correction method
	was intended to correct for, it is also found to improve the correlations
	for the fraction of wet days and lag-1 autocorrelations between ERA15
	and the observations. For the validation period temperature is corrected
	very well, but for precipitation the RMSE of the daily difference
	between modeled and observed precipitation has increased for the
	corrected situation. When taking random years for calibration, and
	the remaining years for validation, the spread in the mean bias error
	(MBE) becomes larger for the corrected precipitation during validation,
	but the overal average MBE has decreased.},
  doi = {10.5194/hess-14-687-2010},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{terink+al2009,
  author = {Terink, W. and Hurkmans, R. and Torfs, P. and Uijlenhoet, R.},
  title = {Bias correction of temperature and precipitation data for regional
	climate model application to the {R}hine basin},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2009},
  volume = {6},
  pages = {5377--5413},
  number = {4},
  abstract = {In many climate impact studies hydrological models are forced with
	meteorological forcing data without an attempt to assess the quality
	of these forcing data. The objective of this study is to compare
	downscaled ERA15 (ECMWF-reanalysis data) precipitation and temperature
	with observed precipitation and temperature and apply a bias correction
	to these forcing variables. The bias-corrected precipitation and
	temperature data will be used in another study as input for the Variable
	Infiltration Capacity (VIC) model. Observations were available for
	134 sub-basins throughout the Rhine basin at a temporal resolution
	of one day from the International Commission for the Hydrology of
	the Rhine basin (CHR). Precipitation is corrected by fitting the
	mean and coefficient of variation (CV) of the observations. Temperature
	is corrected by fitting the mean and standard deviation of the observations.
	It seems that the uncorrected ERA15 is too warm and too wet for most
	of the Rhine basin. The bias correction leads to satisfactory results,
	precipitation and temperature differences decreased significantly.
	Corrections were largest during summer for both precipitation and
	temperature, and for September and October for precipitation only.
	Besides the statistics the correction method was intended to correct
	for, it is also found to improve the correlations for the fraction
	of wet days and lag-1 autocorrelations between ERA15 and the observations.},
  doi = {10.5194/hessd-6-5377-2009},
  owner = {rojasro},
  timestamp = {2010.07.29}
}

@ARTICLE{teutschbeinSeibert2010,
  author = {Teutschbein, C. and Seibert, J.},
  title = {Regional climate models for hydrological impact studies at the catchment
	scale: {A} review of recent model strategies},
  journal = {Geography Compass},
  year = {2010},
  volume = {4},
  pages = {834--860},
  number = {7},
  abstract = {This article reviews recent applications of regional climate model
	(RCM) output for hydrological impact studies. Traditionally, simulations
	of global climate models (GCMs) have been the basis of impact studies
	in hydrology. Progress in regional climate modeling has recently
	made the use of RCM data more attractive, although the application
	of RCM simulations is challenging due to often considerable biases.
	The main modeling strategies used in recent studies can be classified
	into (i) very simple constructed modeling chains with a single RCM
	(S-RCM approach) and (ii) highly complex and computing-power intensive
	model systems based on RCM ensembles (E-RCM approach). In the literature
	many examples for S-RCM can be found, while comprehensive E-RCM studies
	with consideration of several sources of uncertainties such as different
	greenhouse gas emission scenarios, GCMs, RCMs and hydrological models
	are less common. Based on a case study using control-run simulations
	of fourteen different RCMs for five Swedish catchments, the biases
	of and the variability between different RCMs are demonstrated. We
	provide a short overview of possible bias-correction methods and
	show that inter-RCM variability also has substantial consequences
	for hydrological impact studies in addition to other sources of uncertainties
	in the modeling chain. We propose that due to model bias and inter-model
	variability, the S-RCM approach is not advised and ensembles of RCM
	simulations (E-RCM) should be used. The application of bias-correction
	methods is recommended, although one should also be aware that the
	need for bias corrections adds significantly to uncertainties in
	modeling climate change impacts.},
  doi = {10.1111/j.1749-8198.2010.00357.x},
  tags = {Multimodel - Ensambles, Uncertainty, Impacts}
}

@ARTICLE{themessl+al2010,
  author = {Theme{\ss}l, M. and Gobiet, A. and Leuprecht, A.},
  title = {Empirical-statistical downscaling and error correction of daily precipitation
	from regional climate models},
  journal = {International Journal of Climatology},
  year = {2010},
  volume = {31},
  pages = {1530--1544},
  number = {10},
  abstract = {Although regional climate models (RCMs) are powerful tools for describing
	regional and even smaller scale climate conditions, they still feature
	severe systematic errors. In order to provide optimized climate scenarios
	for climate change impact research, this study merges linear and
	nonlinear empirical-statistical downscaling techniques with bias
	correction methods and investigates their ability for reducing RCM
	error characteristics. An ensemble of seven empirical-statistical
	downscaling and error correction methods (DECMs) is applied to post-process
	daily precipitation sums of a high-resolution regional climate hindcast
	simulation over the Alpine region, their error characteristics are
	analysed and compared to the raw RCM results. Drastic reductions
	in error characteristics due to application of DECMs are demonstrated.
	Direct point-wise methods like quantile mapping and local intensity
	scaling as well as indirect spatial methods as nonlinear analogue
	methods yield systematic improvements in median, variance, frequency,
	intensity and extremes of daily precipitation. Multiple linear regression
	methods, even if optimized by predictor selection, transformation
	and randomization, exhibit significant shortcomings for modelling
	daily precipitation due to their linear framework. Comparing the
	well-performing methods to each other, quantile mapping shows the
	best performance, particularly at high quantiles, which is advantageous
	for applications related to extreme precipitation events. The improvements
	are obtained regardless of season and region, which indicates the
	potential transferability of these methods to other regions.},
  doi = {10.1002/joc.2168},
  owner = {rojasro},
  timestamp = {2010.06.18}
}

@ARTICLE{thiemann+al2001,
  author = {Thiemann, M. and Trosset, M. and Gupta, H. and Sorooshian, S.},
  title = {Bayesian Recursive Parameter Estimation For Hydrologic Models},
  journal = {Water Resources Research},
  year = {2001},
  volume = {37},
  pages = {2521},
  abstract = {The uncertainty in a given hydrologic prediction is the compound effect
	of the parameter, data, and structural uncertainties associated with
	the underlying model. In general, therefore, the confidence in a
	hydrologic prediction can be improved by reducing the uncertainty
	associated with the parameter estimates. However, the classical approach
	to doing this via model calibration typically requires that considerable
	amounts of data be collected and assimilated before the model can
	be used. This limitation becomes immediately apparent when hydrologic
	predictions must be generated for a previously ungauged watershed
	that has only recently been instrumented. This paper presents the
	framework for a Bayesian recursive estimation approach to hydrologic
	prediction that can be used for simultaneous parameter estimation
	and prediction in an operational setting. The prediction is described
	in terms of the probabilities associated with different output values.
	The uncertainty associated with the parameter estimates is updated
	(reduced) recursively, resulting in smaller prediction uncertainties
	as measurement data are successively assimilated. The effectiveness
	and efficiency of the method are illustrated in the context of two
	models: a simple unit hydrograph model and the more complex Sacramento
	soil moisture accounting model, using data from the Leaf River basin
	in Mississippi.},
  doi = {10.1029/2000WR900405},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{thiemig+al2010,
  author = {Thiemig, V. and Pappenberger, F. and Thielen, J. and Gadian, H. and
	{de Roo}, A. and Bodis, K. and {del Medico}, M. and Muthusi, F.},
  title = {Ensemble flood forecasting in {Africa}: {A} feasibility study in
	the {Juba-Shabelle} river basin},
  journal = {Atmospheric Science Letters},
  year = {2010},
  volume = {11},
  pages = {123--131},
  number = {2},
  abstract = {The European flood alert system (EFAS) achieves early flood warnings
	for large to mediumsize river basins with lead times of 10 days.
	This is based on probabilistic weather forecasts, the exceedance
	of alert thresholds and persistence. The methodologies have been
	tested for different events and time scales in mid-latitude basins
	in Europe. In this article, the transferability of the EFAS-methodologies
	to equatorial African basins is assessed with the analysis of the
	Juba-Shabelle river basin as an example using a variety of different
	meteorological data sources. In this context, ERA-40 and CHARM have
	been used for the calculation of climatologies; re-forecasts of the
	current operational European Centre for Medium-Range Weather Forecasts
	model provided hindcasts of historic flood events. The results show
	that flood events have been detected successfully in more than 85%
	of all cases, with a high accuracy in terms of timing and magnitude.},
  doi = {10.1002/asl.266},
  owner = {rojasro},
  timestamp = {2010.08.10}
}

@ARTICLE{thiemig+al2012,
  author = {Thiemig, V. and Rojas, R. and {Zambrano-Bigiarini}, M. and Levizzani,
	V. and {de Roo}, A.},
  title = {Validation of Satellite-Based Precipitation Products Over Sparsely-Gauged
	African River Basins},
  journal = {Journal of Hydrometeorology},
  year = {2012},
  volume = {in press},
  doi = {10.1175/JHM-D-12-032.1},
  owner = {rojasro},
  timestamp = {2012.07.31}
}

@ARTICLE{thodsen2007,
  author = {Thodsen, H.},
  title = {The influence of climate change on stream flow in {D}anish rivers},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {333},
  pages = {226--238},
  number = {2--4},
  abstract = {The influence of climate change on river discharges in five major
	Danish rivers divided into 29 sub-catchments is investigated for
	the future period of 2071--2100. Climate changes are modelled by
	the HIRHAM regional climate model on the basis of the IPCC A2 scenario.
	A hydrological model (NAM) is used to convert precipitation to river
	discharges. Difficulties are found in the direct use of climate model
	generated precipitation and potential evapotranspiration (reference
	evaporation) because of too many rainy days, deviations from mean
	annual values of precipitation and potential evapotranspiration from
	observed values, and poor agreement on seasonality. Therefore climate
	model generated data is corrected to match observed mean annual values
	and the mean monthly distribution. Mean annual precipitation is found
	to increase 7%, potential evapotranspiration to increase 3% and river
	discharges to increase 12% on average, between a control period (1961--1990)
	and the future period. Because of increased precipitation from October
	to March and reduced precipitation from July to September the monthly
	river discharges are found to increase from December to August and
	decrease in September and October. Extreme values of precipitation
	and river discharge are examined and the level of the highest precipitation
	and the highest river discharge events are estimated to increase.
	The precipitation amount exceeded 0.1% of all days increases by an
	average of 7%, the river discharge exceeded 0.1% of all days increases
	approximately 15%. The 100-year flood is modelled to increase 11%
	on average},
  doi = {10.1016/j.jhydrol.2006.08.012},
  keywords = {NAM model},
  tags = {Impacts}
}

@ARTICLE{thomson+al2005,
  author = {Thomson, A. and Brown, R. and Rosenberg, N. and Srinivasan, R. and
	Izaurralde, R.},
  title = {Climate change impacts for the conterminous {USA}: {A}n integrated
	assessment: {P}art4. {W}ater {R}esources},
  journal = {Climatic Change},
  year = {2005},
  volume = {69},
  pages = {67--88},
  number = {1},
  abstract = {Global climate change will impact the hydrologic cycle by increasing
	the capacity of the atmosphere to hold moisture. Anticipated impacts
	are generally increased evaporation at low latitudes and increased
	precipitation at middle and high latitudes. General Circulation Models
	(GCMs) used to simulate climate disagree on whether the U.S. as a
	whole and its constituent regions will receive more or less precipitation
	as global warming occurs. The impacts on specific regions will depend
	on changes in weather patterns and are certain to be complex. Here
	we apply the suite of 12 potential climate change scenarios, previously
	described in Part 1, to the Hydrologic Unit Model of the United States
	(HUMUS) to simulate water supply in the conterminous United States
	in reference to a baseline scenario. We examine the sufficiency of
	this water supply to meet changing demands of irrigated agriculture.
	The changes in water supply driven by changes in climate will likely
	be most consequential in the semi-arid western parts of the country
	where water yield is currently scarce and the resource is intensively
	managed. Changes of greater than Â±50% with respect to present day
	water yield are projected in parts of the Midwest and Southwest U.S.
	Interannual variability in the water supply is likely to increase
	where conditions become drier and to decrease under wetter conditions.},
  doi = {10.1007/s10584-005-3610-y},
  keywords = {SWAT, Climate Change},
  tags = {application, Climate Change, SWAT}
}

@ARTICLE{thorne2011,
  author = {Thorne, R.},
  title = {{Uncertainty in the impacts of projected climate change on the hydrology
	of a subarctic environment: Liard River basin}},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2011},
  volume = {15},
  pages = {1483--1492},
  number = {5},
  abstract = {Freshwater inputs from the Mackenzie River into the Arctic Ocean contribute
	to the control of oceanic dynamics and sea ice cover duration. Half
	of the annual runoff from the Mackenzie River drains from mountainous
	regions, where the Liard River, with a drainage area of 275 000 km2,
	is especially influential. The impact of projected atmospheric warming
	on the discharge of the Liard River is unclear. Here, uncertainty
	in climate projections associated with GCM structure (2 °C prescribed
	warming) and magnitude of increases in global mean air temperature
	(1 to 6 °C) on the river discharge are assessed using SLURP, a well-tested
	hydrological model. Most climate projections indicate (1) warming
	in this subarctic environment that is greater than the global mean
	and (2) an increase in precipitation across the basin. These changes
	lead to an earlier spring freshet (1 to 12 days earlier), a decrease
	in summer runoff (up to 22%) due to enhanced evaporation, and an
	increase in autumn flow (up to 48%), leading to higher annual discharge
	and more freshwater input from the Liard River to the Arctic Ocean.
	All simulations project that the subarctic nival regime will be preserved
	in the future but the magnitude of changes in river discharge is
	highly uncertain (ranging from a decrease of 3% to an increase of
	15% in annual runoff), due to differences in GCM projections of basin-wide
	temperature and precipitation.},
  doi = {10.5194/hess-15-1483-2011},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{thornton+al2009,
  author = {Thornton, V. and {van de Steeg}, J. and Notenbaert, A. and Herrero,
	M.},
  title = {The impacts of climate change on livestock and livestock systems
	in developing countries: {A} review of what we know and what we need
	to know},
  journal = {Agricultural Systems},
  year = {2009},
  volume = {101},
  pages = {113--127},
  number = {3},
  abstract = {Despite the importance of livestock to poor people and the magnitude
	of the changes that are likely to befall livestock systems, the intersection
	of climate change and livestock in developing countries is a relatively
	neglected research area. Little is known about the interactions of
	climate and increasing climate variability with other drivers of
	change in livestock systems and in broader development trends. In
	many places in the tropics and subtropics, livestock systems are
	changing rapidly, and the spatial heterogeneity of household response
	to change may be very large. While opportunities may exist for some
	households to take advantage of more conducive rangeland and cropping
	conditions, for example, the changes projected will pose serious
	problems for many other households. We briefly review the literature
	on climate change impacts on livestock and livestock systems in developing
	countries, and identify some key knowledge and data gaps. We also
	list some of the broad researchable issues associated with how smallholders
	and pastoralists might respond to climate change. The agendas of
	research and development organisations may need adjustment if the
	needs of vulnerable livestock keepers in the coming decades are to
	be met effectively.},
  doi = {10.1016/j.agsy.2009.05.002},
  keywords = {Development, Research, Poverty, Impact, Livestock, Systems, Climate
	change},
  tags = {Impacts}
}

@ARTICLE{thyer+al1999,
  author = {Thyer, M. and Kuczera, G. and Bates, B.},
  title = {Probabilistic optimization for conceptual rainfall-runoff models:
	{A} comparison of the shuffled complex evolution and simulated annealing
	algorithms},
  journal = {Water Resources Research},
  year = {1999},
  volume = {35},
  pages = {767--773},
  number = {3},
  abstract = {Automatic optimization algorithms are used routinely to calibrate
	conceptual rainfall-runoff (CRR) models. The goal of calibration
	is to estimate a feasible and unique (global) set of parameter estimates
	that best fit the observed runoff data. Most if not all optimization
	algorithms have difficulty in locating the global optimum because
	of response surfaces that contain multiple local optima with regions
	of attraction of differing size, discontinuities, and long ridges
	and valleys. Extensive research has been undertaken to develop efficient
	and robust global optimization algorithms over the last 10 years.
	This study compares the performance of two probabilistic global optimization
	methods: the shuffled complex evolution algorithm SCE-UA, and the
	three-phase simulated annealing algorithm SA-SX. Both algorithms
	are used to calibrate two parameter sets of a modified version of
	Boughton's [1984] SFB model using data from two Australian catchments
	that have low and high runoff yields. For the reduced, well-identified
	parameter set the algorithms have a similar efficiency for the low-yielding
	catchment, but SCE-UA is almost twice as robust. Although the robustness
	of the algorithms is similar for the high-yielding catchment, SCE-UA
	is six times more efficient than SA-SX. When fitting the full parameter
	set the performance of SA-SX deteriorated markedly for both catchments.
	These results indicated that SCE-UA's use of multiple complexes and
	shuffling provided a more effective search of the parameter space
	than SA-SX's single simplex with stochastic step acceptance criterion,
	especially when the level of parameterization is increased. Examination
	of the response surface for the low-yielding catchment revealed some
	reasons why SCE-UA outperformed SA-SX and why probabilistic optimization
	algorithms can experience difficulty in locating the global optimum.
	},
  doi = {10.1029/1998WR900058},
  kywords = {SCE, SCE-UA, SA},
  tags = {Calibration}
}

@ARTICLE{thyer+al2002,
  author = {Thyer, M. and Kuczera, G. and Wang, Q.},
  title = {Quantifying parameter uncertainty in stochastic models using the
	Box–Cox transformation},
  journal = {Journal of Hydrology},
  year = {2002},
  volume = {265},
  pages = {246--257},
  number = {1–-4},
  abstract = {The Box–Cox transformation is widely used to transform hydrological
	data to make it approximately Gaussian. Bayesian evaluation of parameter
	uncertainty in stochastic models using the Box–Cox transformation
	is hindered by the fact that there is no analytical solution for
	the posterior distribution. However, the Markov chain Monte Carlo
	method known as the Metropolis algorithm can be used to simulate
	the posterior distribution. This method properly accounts for the
	nonnegativity constraint implicit in the Box–Cox transformation.
	Nonetheless, a case study using the AR(1) model uncovered a practical
	problem with the implementation of the Metropolis algorithm. The
	use of a multivariate Gaussian jump distribution resulted in unacceptable
	convergence behaviour. This was rectified by developing suitable
	parameter transformations for the mean and variance of the AR(1)
	process to remove the strong nonlinear dependencies with the Box–Cox
	transformation parameter. Applying this methodology to the Sydney
	annual rainfall data and the Burdekin River annual runoff data illustrates
	the efficacy of these parameter transformations and demonstrate the
	value of quantifying parameter uncertainty.},
  doi = {10.1016/S0022-1694(02)00113-0},
  issn = {0022-1694},
  keywords = {Lag-one autoregressive models}
}

@ARTICLE{thyer+al2011,
  author = {Mark Thyer and Michael Leonard and Dmitri Kavetski and Stephen Need
	and Benjamin Renard},
  title = {The open source RFortran library for accessing R from Fortran, with
	applications in environmental modelling},
  journal = {Environmental Modelling \&amp; Software},
  year = {2011},
  volume = {26},
  pages = {219--234},
  number = {2},
  abstract = {The open source RFortran library is introduced as a convenient tool
	for accessing the functionality and packages of the R programming
	language from Fortran programs. It significantly enhances Fortran
	programming by providing a set of easy-to-use functions that enable
	access to R′s very rapidly growing statistical, numerical and visualization
	capabilities, and support a richer and more interactive model development,
	debugging and analysis setup. RFortran differs from current approaches
	that require calling Fortran Dynamic link libraries (DLL) from R,
	and instead enables the Fortran program to transfer data to/from
	R and invoke R-based procedures via the R command interpreter. More
	generally, RFortran obviates the need to re-organize Fortran code
	into DLLs callable from R, or to re-write existing R packages in
	Fortran, or to jointly compile their Fortran code with the R language
	itself. Code snippets illustrate the basic transfer of data and commmands
	to and from R using RFortran, while two case studies discuss its
	advantages and limitations in realistic environmental modelling applications.
	These case studies include the generation of automated and interactive
	inference diagnostics in hydrological model calibration, and the
	integration of R statistical packages into a Fortran-based numerical
	quadrature code for joint probability analysis of coastal flooding
	using numerical hydraulic models. Currently, RFortran uses the Component
	Object Model (COM) interface for data/command transfer and is supported
	on the Microsoft Windows operating system and the Intel and Compaq
	Visual Fortran compilers. Extending its support to other operating
	systems and compilers is planned for the future. We hope that RFortran
	expedites method and software development for scientists and engineers
	with primary programming expertise in Fortran, but who wish to take
	advantage of R′s extensive statistical, mathematical and visualization
	packages by calling them from their Fortran code. Further information
	can be found at www.rfortran.org.},
  doi = {10.1016/j.envsoft.2010.05.007},
  issn = {1364--8152},
  keywords = {R}
}

@ARTICLE{thyer+al2009,
  author = {Thyer, M. and Renard, B. and Kavetski, D. and Kuczera, G. and Franks,
	S. and Srikanthan, S.},
  title = {Critical evaluation of parameter consistency and predictive uncertainty
	in hydrological modeling: A case study using Bayesian total error
	analysis},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  number = {W00B14},
  abstract = {The lack of a robust framework for quantifying the parametric and
	predictive uncertainty of conceptual rainfall‐runoff (CRR) models
	remains a key challenge in hydrology. The Bayesian total error analysis
	(BATEA) methodology provides a comprehensive framework to hypothesize,
	infer, and evaluate probability models describing input, output,
	and model structural error. This paper assesses the ability of BATEA
	and standard calibration approaches (standard least squares (SLS)
	and weighted least squares (WLS)) to address two key requirements
	of uncertainty assessment: (1) reliable quantification of predictive
	uncertainty and (2) reliable estimation of parameter uncertainty.
	The case study presents a challenging calibration of the lumped GR4J
	model to a catchment with ephemeral responses and large rainfall
	gradients. Postcalibration diagnostics, including checks of predictive
	distributions using quantile‐quantile analysis, suggest that while
	still far from perfect, BATEA satisfied its assumed probability models
	better than SLS and WLS. In addition, WLS/SLS parameter estimates
	were highly dependent on the selected rain gauge and calibration
	period. This will obscure potential relationships between CRR parameters
	and catchment attributes and prevent the development of meaningful
	regional relationships. Conversely, BATEA provided consistent, albeit
	more uncertain, parameter estimates and thus overcomes one of the
	obstacles to parameter regionalization. However, significant departures
	from the calibration assumptions remained even in BATEA, e.g., systematic
	overestimation of predictive uncertainty, especially in validation.
	This is likely due to the inferred rainfall errors compensating for
	simplified treatment of model structural error.},
  doi = {10.1029/2008WR006825},
  owner = {rojasro},
  timestamp = {2012.09.13}
}

@ARTICLE{tiedeman2004,
  author = {Tiedeman, C. and Ely, D. and Hill, M. and {O'Brien}, G.},
  title = {A method for evaluating the importance of system state observations
	to model predictions, with application to the {D}eath {V}alley regional
	groundwater flow system},
  journal = {Water Resources Research},
  year = {2004},
  volume = {40},
  pages = {W12411},
  abstract = {We develop a new observation-prediction (OPR) statistic for evaluating
	the importance of system state observations to model predictions.
	The OPR statistic measures the change in prediction uncertainty produced
	when an observation is added to or removed from an existing monitoring
	network, and it can be used to guide refinement and enhancement of
	the network. Prediction uncertainty is approximated using a first-order
	second-moment method. We apply the OPR statistic to a model of the
	Death Valley regional groundwater flow system (DVRFS) to evaluate
	the importance of existing and potential hydraulic head observations
	to predicted advective transport paths in the saturated zone underlying
	Yucca Mountain and underground testing areas on the Nevada Test Site.
	Important existing observations tend to be far from the predicted
	paths, and many unimportant observations are in areas of high observation
	density. These results can be used to select locations at which increased
	observation accuracy would be beneficial and locations that could
	be removed from the network. Important potential observations are
	mostly in areas of high hydraulic gradient far from the paths. Results
	for both existing and potential observations are related to the flow
	system dynamics and coarse parameter zonation in the DVRFS model.
	If system properties in different locations are as similar as the
	zonation assumes, then the OPR results illustrate a data collection
	opportunity whereby observations in distant, high-gradient areas
	can provide information about properties in flatter-gradient areas
	near the paths. If this similarity is suspect, then the analysis
	produces a different type of data collection opportunity involving
	testing of model assumptions critical to the OPR results.},
  doi = {10.1029/2004WR003313},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{tiedeman1998,
  author = {Tiedeman, C. and Goode, D. and Hsieh, P.},
  title = {Characterizing a ground water basin in a {New England} mountain and
	valley terrain},
  journal = {Ground Water},
  year = {1998},
  volume = {36},
  pages = {611--620},
  number = {4},
  abstract = {A ground water basin is defined as the volume of subsurface through
	which ground water flows from the water table to a specified discharge
	location. Delineating the topographically defined surface water basin
	and extending it vertically downward does not always define the ground
	water basin. Instead, a ground water basin is more appropriately
	delineated by tracking ground water flowpaths with a calibrated,
	three-dimensional ground water flow model. To determine hydrologic
	and chemical budgets of the basin, it is also necessary to quantify
	flow through each hydrogeologic unit in the basin. In particular,
	partitioning ground water flow through unconsolidated deposits versus
	bedrock is of significant interest to hillslope hydrologic studies.
	To address these issues, a model is developed and calibrated to simulate
	ground water flow through glacial deposits and fractured crystalline
	bedrock in the vicinity of Mirror Lake, New Hampshire. Tracking of
	ground water flowpaths suggests that Mirror Lake and its inlet streams
	drain a ground water recharge area that is about 1.5 times the area
	of the surface water basin. Calculation of the ground water budget
	suggests that, of the recharge that enters the Mirror Lake ground
	water basin, about 40% travels through the basin along flowpaths
	that stay exclusively in the glacial deposits, and about 60% travels
	along flowpaths that involve movement in bedrock.},
  doi = {10.1111/j.1745-6584.1998.tb02835.x},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@TECHREPORT{tiedeman1997,
  author = {Tiedeman, C. and Goode, D. and Hsieh, P.},
  title = {Numerical simulation of ground--water flow through glacial deposits
	and crystalline bedrock in the {M}irror {L}ake area, {G}rafton {C}ounty,
	{N}ew {H}ampshire},
  institution = {United States Geological Survey},
  year = {1997},
  type = {Professional Paper},
  number = {1572},
  address = {U.S., Reston, Virginia.},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{tiedeman2003,
  author = {Tiedeman, C. and Hill, M. and {D'Agnese}, F. and Faunt, C.},
  title = {Methods for using groundwater model predictions to guide hydrogeologic
	data collection, with application to the {D}eath {V}alley regional
	groundwater flow system},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1010--1027},
  number = {1},
  abstract = {Calibrated models of groundwater systems can provide substantial information
	for guiding data collection. This work considers using such models
	to guide hydrogeologic data collection for improving model predictions
	by identifying model parameters that are most important to the predictions.
	Identification of these important parameters can help guide collection
	of field data about parameter values and associated flow system features
	and can lead to improved predictions. Methods for identifying parameters
	important to predictions include prediction scaled sensitivities
	(PSS), which account for uncertainty on individual parameters as
	well as prediction sensitivity to parameters, and a new “value of
	improved information” (VOII) method presented here, which includes
	the effects of parameter correlation in addition to individual parameter
	uncertainty and prediction sensitivity. In this work, the PSS and
	VOII methods are demonstrated and evaluated using a model of the
	Death Valley regional groundwater flow system. The predictions of
	interest are advective transport paths originating at sites of past
	underground nuclear testing. Results show that for two paths evaluated
	the most important parameters include a subset of five or six of
	the 23 defined model parameters. Some of the parameters identified
	as most important are associated with flow system attributes that
	do not lie in the immediate vicinity of the paths. Results also indicate
	that the PSS and VOII methods can identify different important parameters.
	Because the methods emphasize somewhat different criteria for parameter
	importance, it is suggested that parameters identified by both methods
	be carefully considered in subsequent data collection efforts aimed
	at improving model predictions.},
  doi = {10.1029/2001WR001255},
  owner = {rojasro},
  timestamp = {2010.02.12}
}

@ARTICLE{tierney1994,
  author = {Tierney, L.},
  title = {Markov chains for exploring posterior distributions},
  journal = {The Annals of Statistics},
  year = {1994},
  volume = {22},
  pages = {1701--1728},
  number = {4},
  abstract = {Several Markov chain methods are available for sampling from a poste-
	rior distribution. Two important examples are the Gibbs sampler and
	the Metropolis algorithm. In addition, several strategies are available
	for con- structing hybrid algorithms. This paper outlines some of
	the basic methods and strategies and discusses some related theoretical
	and practical issues. On the theoretical side, results from the theory
	of general state space Markov chains can be used to obtain convergence
	rates, laws of large numbers and central limit theorems for estimates
	obtained from Markov chain methods. These theoretical results can
	be used to guide the construction of more effi- cient algorithms.
	For the practical use of Markov chain methods, standard simulation
	methodology provides several variance reduction techniques and also
	gives guidance on the choice of sample size and allocation.},
  owner = {RRojas},
  refid = {TIERNEY1994},
  timestamp = {2008.11.04},
  url = {http://www.jstor.org/stable/2242477}
}

@ARTICLE{todini2007,
  author = {Todini, E. and Mantovan, P.},
  title = {Commnet on ``{O}n undermining the science?'' by {K}eith {B}even},
  journal = {Hydrological Processes},
  year = {2007},
  volume = {21},
  pages = {1633--1638},
  number = {12},
  doi = {10.1002/hyp.6670},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{tolson2007,
  author = {Tolson, B. and Shoemaker, C.},
  title = {The dynamically dimensional search algorithm for computationally
	effcient automatic calibration of environmental simulation models},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {1--16},
  number = {W01413},
  abstract = {A new global optimization algorithm, dynamically dimensioned search
	(DDS), is introduced for automatic calibration of watershed simulation
	models. DDS is designed for calibration problems with many parameters,
	requires no algorithm parameter tuning, and automatically scales
	the search to find good solutions within the maximum number of user-specified
	function (or model) evaluations. As a result, DDS is ideally suited
	for computationally expensive optimization problems such as distributed
	watershed model calibration. DDS performance is compared to the shuffled
	complex evolution (SCE) algorithm for multiple optimization test
	functions as well as real and synthetic SWAT2000 model automatic
	calibration formulations. Algorithms are compared for optimization
	problems ranging from 6 to 30 dimensions, and each problem is solved
	in 1000 to 10,000 total function evaluations per optimization trial.
	Results are presented so that future modelers can assess algorithm
	performance at a computational scale relevant to their modeling case
	study. In all four of the computationally expensive real SWAT2000
	calibration formulations considered here (14, 14, 26, and 30 calibration
	parameters), results show DDS to be more efficient and effective
	than SCE. In two cases, DDS requires only 15–20% of the number of
	model evaluations used by SCE in order to find equally good values
	of the objective function. Overall, the results also show that DDS
	rapidly converges to good calibration solutions and easily avoids
	poor local optima. The simplicity of the DDS algorithm allows for
	easy recoding and subsequent adoption into any watershed modeling
	application framework.},
  doi = {10.1029/2005WR004723},
  owner = {rojasro},
  timestamp = {2011.10.11}
}

@ARTICLE{tonkin2005,
  author = {Tonkin, M. and Doherty, J.},
  title = {A hybrid regularized inversion methodology for highly parameterized
	environmental models},
  journal = {Water Resources Research},
  year = {2005},
  volume = {41},
  pages = {W10412},
  abstract = {A hybrid approach to the regularized inversion of highly parameterized
	environmental models is described. The method is based on constructing
	a highly parameterized base model, calculating base parameter sensitivities,
	and decomposing the base parameter normal matrix into eigenvectors
	representing principal orthogonal directions in parameter space.
	The decomposition is used to construct super parameters. Super parameters
	are factors by which principal eigenvectors of the base parameter
	normal matrix are multiplied in order to minimize a composite least
	squares objective function. These eigenvectors define orthogonal
	axes of a parameter subspace for which information is available from
	the calibration data. The coordinates of the solution are sought
	within this subspace. Super parameters are estimated using a regularized
	nonlinear Gauss-Marquardt-Levenberg scheme. Though super parameters
	are estimated, Tikhonov regularization constraints are imposed on
	base parameters. Tikhonov regularization mitigates over fitting and
	promotes the estimation of reasonable base parameters. Use of a large
	number of base parameters enables the inversion process to be receptive
	to the information content of the calibration data, including aspects
	pertaining to small-scale parameter variations. Because the number
	of super parameters sustainable by the calibration data may be far
	less than the number of base parameters used to define the original
	problem, the computational burden for solution of the inverse problem
	is reduced. The hybrid methodology is described and applied to a
	simple synthetic groundwater flow model. It is then applied to a
	real-world groundwater flow and contaminant transport model. The
	approach and programs described are applicable to a range of modeling
	disciplines.},
  doi = {10.1029/2005WR003995},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@TECHREPORT{tonkin2007,
  author = {Tonkin, M. and Tiedeman, C. and Ely, D. and Hill, M.},
  title = {O{PR-PPR, a computer program for assessing data importance to model
	predictions using linear statistics}},
  institution = {{U.S. Geological Survey}},
  year = {2007},
  type = {{Techniques and Methods 6-E2}},
  owner = {rojasro},
  timestamp = {2010.02.17}
}

@ARTICLE{trelea2003,
  author = {Trelea, I.},
  title = {The particle swarm optimization algorithm: convergence analysis and
	parameter selection},
  journal = {Information Processing Letters},
  year = {2003},
  volume = {85},
  pages = {317--325},
  number = {6},
  abstract = {The particle swarm optimization algorithm is analyzed using standard
	results from the dynamic system theory. Graphical parameter selection
	guidelines are derived. The exploration{--}exploitation tradeoff
	is discussed and illustrated. Examples of performance on benchmark
	functions superior to previously published results are give},
  doi = {10.1016/S0020-0190(02)00447-7 },
  keywords = {Particle swarm optimization, Stochastic optimization, Analysis of
	algorithms, Parallel algorithms},
  tags = {PSO}
}

@INCOLLECTION{trenberth+al2007,
  author = {Trenberth, K. and Jones, P. and Ambenje, P. and Bojariu, R. and Easterling,
	D. and Klein, T. and Parker, D. and Rahimzadeh, F. and Renwick, J.
	and Rusticucci, M. and Soden, B. and Zhai, P.},
  title = {Observations: {S}urface and atmospheric climate change},
  booktitle = {Climate Change 2007: The Physical Science Basis. Contribution of
	Working Group I to the Fourth Assessment Report of the Intergovernmental
	Panel on Climate Change},
  publisher = {Cambridge University Press},
  year = {2007},
  editor = {S. Solomon and D. Qin and M. Manning and Z. Chen and M. Marquis and
	K.B. Averyt and M. Tignor and H.L. Miller},
  address = {Cambridge, United Kingdom and New York, NY, USA},
  tags = {IPCC}
}

@ARTICLE{troldborg2007,
  author = {Troldborg, L. and Refsgaard, J. and Jensen, K. and Engesgaard, P.},
  title = {The importance of alternative conceptual models for simulation of
	concentrations in a multi--aquifer system},
  journal = {Hydrogeology Journal},
  year = {2007},
  volume = {15},
  pages = {843--860},
  number = {5},
  abstract = {Four different conceptual models based on alternative geological interpretations
	were formulated for a shallow 600 km2 aquifer system in Denmark comprising
	Quaternary deposits. Each of the four models was calibrated against
	groundwater heads and discharge measurements through inverse modeling.
	Subsequently, the transport capabilities of the four models were
	compared to 32 concentration measurements of environmental tracers
	(tritium 3H, helium-3 3He, chlorofluorocarbons CFC11, CFC12 and CFC113).
	The flow simulations showed only minor differences in spatial head
	distribution associated with alternative conceptualizations despite
	the complexity of the aquifer system and the significant differences
	in geological interpretations. The models, however, showed major
	differences in predictions of the age of the groundwater and environmental
	tracer concentrations, differences that are seen as an effect of
	model structure uncertainty, because no additional calibrations to
	these data were performed. A single conceptualization may be adequate
	in characterizing the natural behavior of a field system after calibration,
	because the calibration procedure is able to compensate for errors
	in the data or in the conceptual model through biased parameter values.
	However, once extrapolation beyond the calibration base is attempted,
	different conceptual model formulations result in significantly different
	results. Consequently, it is crucial to take model conceptual uncertainty
	into account when making predictions beyond the calibration base.},
  doi = {10.1007/s10040-007-0192-y},
  owner = {RRojas},
  timestamp = {2008.04.24}
}

@ARTICLE{tsai2010,
  author = {Tsai, F. and Li, X.},
  title = {Reply to comment by {Ming Ye et al. on "Inverse groundwater modeling
	for hydraulic conductivity estimation using Bayesian model averaging
	and variance window"}},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W02802},
  doi = {10.1029/2009WR008591},
  owner = {rojasro},
  timestamp = {2010.01.21}
}

@ARTICLE{tsai2008,
  author = {Tsai, F. and Li, X.},
  title = {Inverse groundwater modeling for hydraulic conductivity estimation
	using {B}ayesian model averaging and variance window},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W09434},
  abstract = {This study proposes a Bayesian model averaging (BMA) method to address
	parameter estimation uncertainty arising from nonuniqueness in parameterization
	methods. BMA is able to incorporate multiple parameterization methods
	for prediction through the law of total probability and to obtain
	an ensemble average of hydraulic conductivity estimates. Two major
	issues in applying BMA to hydraulic conductivity estimation are discussed.
	The first problem is using Occam's window in usual BMA applications
	to measure approximated posterior model probabilities. Occam's window
	only accepts models in a very narrow range, tending to single out
	the best method and discard other good methods. We propose a variance
	window to replace Occam's window to cope with this problem. The second
	problem is the Kashyap information criterion (KIC) in the approximated
	posterior model probabilities, which tends to prefer highly uncertain
	parameterization methods by considering the Fisher information matrix.
	With sufficient amounts of observation data, the Bayesian information
	criterion (BIC) is a good approximation and is able to avoid controversial
	results from using KIC. This study adopts multiple generalized parameterization
	(GP) methods such as the BMA models to estimate spatially correlated
	hydraulic conductivity. Numerical examples illustrate the issues
	of using KIC and Occam's window and show the advantages of using
	BIC and the variance window in BMA application. Finally, we apply
	BMA to the hydraulic conductivity estimation of the “1500-foot” sand
	in East Baton Rouge Parish, Louisiana.},
  doi = {10.1029/2007WR006576},
  owner = {rojasro},
  timestamp = {2010.02.17}
}

@ARTICLE{gensa1996,
  author = {Tsallis, C. and Stariolo, D.},
  title = {Generalized simulated annealing},
  journal = {Physica A: Statistical Mechanics and its Applications},
  year = {1996},
  volume = {233},
  pages = {395--406},
  number = {1-2},
  abstract = {We discuss and illustrate a new stochastic algorithm (generalized
	simulated annealing) for computationally finding the global minimum
	of a given (not necessarily convex) energy/cost function defined
	in a continuous D-dimensional space. This algorithm recovers, as
	particular cases, the so-called classical (“Boltzmann machine”) and
	fast (“Cauchy machine”) simulated annealings, and turns out to be
	quicker than both.},
  doi = {10.1016/S0378-4371(96)00271-3},
  issn = {0378-4371},
  keywords = {Simulated annealing}
}

@ARTICLE{uhlenbrook+al1999,
  author = {Uhlenbrook, S. and Seibert, J. and Leibundgut, C. and Rodhe, A.},
  title = {Prediction uncertainty of conceptual rainfall-runoff models caused
	by problems in identifying model parameters and structure},
  journal = {Hydrological Sciences Journal},
  year = {1999},
  volume = {44},
  pages = {779--797},
  number = {5},
  abstract = {The uncertainties arising from the problem of identifying a representative
	model structure and model parameters in a conceptual rainfall-runoff
	model were investigated. A conceptual model, the HBV model, was applied
	to the mountainous Brugga basin (39.9 km”) in the Black Forest, southwestern
	Germany. In a first step, a Monte Carlo procedure with randomly generated
	parameter sets was used for calibration. For a ten-year calibration
	period, different parameter sets resulted in an equally good correspondence
	between observed and simulated runoff. A few parameters were well
	defined (i.e. best parameter values were within small ranges), but
	for most parameters good simulations were found with values varying
	over wide ranges. In a second step, model variants with different
	numbers of elevation and landuse zones and various runoff generation
	conceptualizations were tested. In some cases, representation of
	more spatial variability gave better simulations in terms of discharge.
	However, good results could be obtained with different and even unrealistic
	concepts. The computation of design floods and low flow predictions
	illustrated that the parameter uncertainty and the uncertainty of
	identifying a unique best model variant have implications for model
	predictions. The flow predictions varied considerably. The peak discharge
	of a flood with a probability of 0.01 year?1, for instance, varied
	from 40 to almost 60 mm day?1. It was concluded that model predictions,
	particularly in applied studies, should be given as ranges rather
	than as single values.},
  doi = {10.1080/02626669909492273},
  owner = {rojasro},
  timestamp = {2011.10.10}
}

@ARTICLE{valery+al2010,
  author = {Val{\'e}ry, A. and Andr{\'e}assian, V. and Perrin, C.},
  title = {Regionalization of precipitation and air temperature over high-altitude
	catchments - learning from outliers},
  journal = {Hydrological Sciences Journal},
  year = {2010},
  volume = {55},
  pages = {928--940},
  number = {6},
  abstract = {Closing the water balance of mountainous catchments may sometimes
	become tricky, even before applying a hydrological model. In this
	paper, we focus on mountainous, snow-affected catchments and try
	to understand the reasons for the {``}unrealistic{''} hydrological
	behaviours that they sometimes show: when annual runoff is greater
	than the annual areal precipitation estimate, something is obviously
	wrong, but finding the appropriate means to adjust the water balance
	is far from a trivial matter. This paper aims to improve our knowledge
	of the water balance of mountainous and snow-affected catchments
	in two different countries: Sweden and France. We use a simple non-dimensional
	framework to detect outliers and then propose an regionalization
	of precipitation and air temperature in order to better estimate
	inputs over high-altitude catchments. Since we are interested in
	catchment water balance, we evaluate our regionalized input estimates
	by comparing them to streamflow measurements. The results are mixed:
	in Sweden, our approach is successful because it can make most outlier
	catchments become regular ones; but in France it is disappointing
	because it does not solve most of the water balance problems identified.
	However, for both countries, the regionalization approach significantly
	improves the performance of a rainfall-runoff model at a daily time
	step.},
  doi = {10.1080/02626667.2010.504676},
  keywords = {water balance, altitudinal gradients, precipitation, air temperature,
	mountainous and snow-affected catchments, biased inputs },
  tags = {Calibration, Rainfall, Philosophical, Outliers}
}

@ARTICLE{vandersluijs2005,
  author = {{Van der Sluijs}, J.},
  title = {Uncertainty as a monster in the science--policy interface: {F}our
	coping strategies},
  journal = {Water Science \& Technology},
  year = {2005},
  volume = {52},
  pages = {87--92},
  number = {6},
  abstract = {Using the metaphor of monsters, an analysis is made of the different
	ways in which the scientific community responds to uncertainties
	that are hard to tame. A monster is understood as a phenomenon that
	at the same moment fits into two categories that were considered
	to be mutually excluding, such as knowledge versus ignorance, objective
	versus subjective, facts versus values, prediction versus speculation,
	science versus policy. Four styles of coping with monsters in the
	science-policy interface can be distinguished with different degrees
	of tolerance towards the abnormal: monster-exorcism, monster-adaptation,
	monster-embracement, and monster-assimilation. Each of these responses
	can be observed in the learning process over the past decades and
	current practices of coping with uncertainties in the science policy
	interface on complex environmental problems. We might see this ongoing
	learning process of the scientific community of coping with complex
	systems as a dialectic process where one strategy tends to dominate
	the field until its limitations and shortcomings are recognized,
	followed by a rise of one of the other strategies. We now seem to
	find ourselves in a phase with growing focus on monster assimilation
	placing uncertainty at the heart of the science-policy and science-society
	interfaces. Keywords Anomalies; monster theory; uncertainty},
  owner = {RRojas},
  refid = {VANDERSLUIJS2005},
  timestamp = {2008.11.04},
  url = {http://www.iwaponline.com/wst/05206/wst052060087.htm}
}

@ARTICLE{vanliewgarbrecht2003,
  author = {{Van Liew}, M. and Garbrecht, J.},
  title = {Hydrologic simulation of the {L}ittle {W}ashita {R}iver Experimental
	Watershed using {SWAT}},
  journal = {Journal of the American Water Resources Association},
  year = {2003},
  volume = {39},
  pages = {413--426},
  number = {2},
  month = {April},
  abstract = {Precipitation and streamflow data from three nested subwatersheds
	within the Little Washita River Experimental Watershed (LWREW) in
	southwestern Oklahoma were used to evaluate the capabilities of the
	Soil and Water Assessment Tool (SWAT) to predict streamflow under
	varying climatic conditions. Eight years of precipitation and streamflow
	data were used to calibrate parameters in the model, and 15 years
	of data were used for model validation. SWAT was calibrated on the
	smallest and largest sub-watersheds for a wetter than average period
	of record. The model was then validated on a third subwatershed for
	a range in climatic conditions that included dry, average, and wet
	periods. Calibration of the model involved a multistep approach.
	A preliminary calibration was conducted to estimate model parameters
	so that measured versus simulated yearly and monthly runoff were
	in agreement for the respective calibration periods. Model parameters
	were then fine tuned based on a visual inspection of daily hydrographs
	and flow frequency curves. Calibration on a daily basis resulted
	in higher baseflows and lower peak runoff rates than were obtained
	in the preliminary calibration. Test results show that once the model
	was calibrated for wet climatic conditions, it did a good job in
	predicting streamflow responses over wet, average, and dry climatic
	conditions selected for model validation. Monthly coefficients of
	efficiencies were 0.65, 0.86, and 0.45 for the dry, average, and
	wet validation periods, respectively. Results of this investigation
	indicate that once calibrated, SWAT is capable of providing adequate
	simulations for hydrologic investigations related to the impact of
	climate variations on water resources of the LWREW.},
  doi = {10.1111/j.1752-1688.2003.tb04395.x},
  keywords = {SWAT, calibration/validation, baseflow separation, modeling/statistics,
	hydrology, watershed, runoff curve number, climate variability},
  tags = {SWAT}
}

@ARTICLE{vanwerkhoven+al2008,
  author = {{Van Werkhoven}, K. and Wagener, T. and Reed, P. and Tang, Y.},
  title = {Characterization of watershed model behavior across a hydroclimatic
	gradient},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = { W01429},
  abstract = {A fundamental tradeoff exists in watershed modeling between a model's
	flexibility for representing watersheds with different characteristics
	versus its potential for overparameterization. This study uses global
	sensitivity analysis to investigate how a commonly used intermediate-complexity
	model, the Sacramento Soil Moisture Accounting Model (SAC-SMA), represents
	a wide range of watersheds with diverse physical and hydroclimatic
	characteristics. The analysis aims to establish a detailed understanding
	of model behavior across watersheds and time periods with the ultimate
	objective to guide model calibration and evaluation studies. Sobol's
	sensitivity analysis is used to evaluate the SAC-SMA in 12 Model
	Parameter Estimation Experiment (MOPEX) watersheds in the US. The
	watersheds span a wide hydroclimatic gradient from arid to humid
	systems. Four evaluation metrics reflecting base flows, midrange
	flows, peak flows, and long-term water balance were used to comprehensively
	characterize trends in sensitivity and model behavior. Results show
	significant variation in parameter sensitivities that are correlated
	with the hydroclimatic characteristics of the watersheds and time
	periods analyzed. The sensitivity patterns are consistent with the
	expected dominant processes and demonstrate the need for moderate
	model complexity to represent different hydroclimatic regimes. The
	analysis reveals that the primary model controls for some aspects
	of the simulated hydrograph are different from those typically assumed
	for the SAC-SMA. Results also show that between 6 and 10 parameters
	are regularly identifiable from daily hydrologic data, which is about
	twice the range that is often assumed (i. e., 3 to 5). Synthesized
	results provide comprehensive SAC-SMA calibration guidance, demonstrate
	the flexibility of the model for representing multiple hydroclimatic
	regimes, and highlight the great difficulty in generalizing model
	behavior across watersheds.},
  doi = {10.1029/2007WR006271},
  keywords = {GLOBAL SENSITIVITY-ANALYSIS, MOISTURE ACCOUNTING MODEL, RAINFALL-RUNOFF
	MODELS, HYDROLOGIC-MODELS, AUTOMATIC CALIBRATION, PARAMETER-ESTIMATION,
	MULTIOBJECTIVE OPTIMIZATION, EVOLUTIONARY ALGORITHMS, CATCHMENT MODELS,
	SACRAMENTO MODEL},
  tags = {Applications}
}

@ARTICLE{vanhaute+al2012,
  author = {Vanhaute, W. J. and Vandenberghe, S. and Scheerlinck, K. and De Baets,
	B. and Verhoest, N. E. C.},
  title = {Calibration of the modified {B}artlett-{L}ewis model using global
	optimization techniques and alternative objective functions},
  journal = {Hydrology and Earth System Sciences},
  year = {2012},
  volume = {16},
  pages = {873-891},
  number = {3},
  abstract = {The calibration of stochastic point process rainfall models, such
	as of the Bartlett-Lewis type, suffers from the presence of multiple
	local minima which local search algorithms usually fail to avoid.
	To meet this shortcoming, four relatively new global optimization
	methods are presented and tested for their ability to calibrate the
	Modified Bartlett-Lewis Model. The list of tested methods consists
	of: the Downhill Simplex Method, Simplex-Simulated Annealing, Particle
	Swarm Optimization and Shuffled Complex Evolution. The parameters
	of these algorithms are first optimized to ensure optimal performance,
	after which they are used for calibration of the Modified Bartlett-Lewis
	model. Furthermore, this paper addresses the choice of weights in
	the objective function. Three alternative weighing methods are compared
	to determine whether or not simulation results (obtained after calibration
	with the best optimization method) are influenced by the choice of
	weights.},
  doi = {10.5194/hess-16-873-2012},
  tags = {Calibration, PSO}
}

@ARTICLE{varis+al2004,
  author = {Varis, O. and Kajander, T. and Lemmela, R.},
  title = {Climate and water: {F}rom climate models to water resources management
	and vice versa},
  journal = {Climatic Change},
  year = {2004},
  volume = {66},
  pages = {321--344},
  number = {3},
  abstract = {This article reviews the recent developments in the functional chain
	from climate models to climate scenarios, through hydrology all the
	way to water resources management, design and policy making. Although
	climate models, such as Global Circulation Models (GCMs) continue
	to evolve, their outputs remain crude and often even inappropriate
	to watershed-scale hydrological analyses. The bridging techniques
	are evolving, though. Many families of regionalization technologies
	are under progress in parallel. Perhaps the most important advances
	are in the field of regional weather patterns, such as ENSO (El Nino-Southern
	Oscillation), NAO (North Atlantic Oscillation) and many more. The
	gap from hydrology to water resources development is by far not that
	wide. The traditional and contemporary practices are well in place.
	In climate change studies, the bottleneck is not in this link itself
	but in the climatic input. The tendency seems to be towards integrated
	water resources assessments, where climate is only one among many
	changes that are expected to occur, such as demography, land cover
	and land use, economy, technologies, and so forth. In such a pragmatic
	setting a risk-analytic interpretation of those scenarios is often
	called for. The above-outlined continuum from climate to water is
	a topic where the physically based modelers, the empiricists and
	the pragmatists should not get restricted to their own way of thinking.
	The issues should develop hand in hand. Perhaps the greatest challenge
	is to incorporate and respect the pragmatic policy-related component
	to the two other branches. For this purpose, it is helpful to reverse
	the direction of thinking from time to time to start-instead of climate
	models-from practical needs and think how the climate scenarios and
	models help really in the difficult task of designing better water
	structures, outline better policies and formulate better operational
	rules in the water field.},
  doi = {10.1023/B:CLIM.0000044622.42657.d4},
  keywords = {GENERAL-CIRCULATION MODEL, DAILY PRECIPITATION MODELS, OCEAN-ATMOSPHERE
	MODEL, HIGH-RESOLUTION, INTERANNUAL VARIABILITY, CONTROL SIMULATIONS,
	CHANGE SCENARIOS, VAPOR FEEDBACK, UNITED-STATES, GOOD SCIENCE},
  tags = {Climate Change}
}

@ARTICLE{veijalainen+al2010,
  author = {Noora Veijalainen and Eliisa Lotsari and Petteri Alho and Bertel
	Vehviläinen and Jukka Käyhkö},
  title = {National scale assessment of climate change impacts on flooding in
	Finland},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {391},
  pages = {333 - 350},
  number = {3–4},
  abstract = {This paper provides a general overview of changes in flooding caused
	by climate change in Finland for the periods 2010–2039 and 2070–2099.
	Changes in flooding were evaluated at 67 sites in Finland with variable
	sizes of runoff areas using a conceptual hydrological model and 20
	climate scenarios from both global and regional climate models with
	the delta change approach. Floods with a 100-year return period were
	estimated with frequency analysis using the Gumbel distribution.
	At four study sites depicting different watershed types and hydrology,
	the inundation areas of the 100-year floods were simulated with a
	2D hydraulic model. The results demonstrate that the impacts of climate
	change are not uniform within Finland due to regional differences
	in climatic conditions and watershed properties. In snowmelt-flood
	dominated areas, annual floods decreased or remained unchanged due
	to decreasing snow accumulation. On the other hand, increased precipitation
	resulted in growing floods in major central lakes and their outflow
	rivers. The changes in flood inundation did not linearly follow the
	changes in 100-year discharges, due to varying characteristics of
	river channels and floodplains. The results highlight the importance
	of comprehensive climatological and hydrological knowledge and the
	use of several climate scenarios in estimation of climate change
	impacts on flooding. Generalisations based on only a few case studies,
	or large scale flood assessments using only a few climate scenarios
	should be avoided in countries with variable hydrological conditions.},
  doi = {10.1016/j.jhydrol.2010.07.035},
  issn = {0022-1694},
  keywords = {Climate change}
}

@CONFERENCE{mads2012,
  author = {Vesselinov, V. and Harp, D.},
  title = {{Model analysis and decision support (MADS) for comple physics models}},
  booktitle = {XIX International Conference on Water Resources - CMWR 2012},
  year = {2012},
  address = {Illinois, USA},
  organization = {University of Illinois Urbana-Champaign},
  owner = {rojasro},
  timestamp = {2012.09.26}
}

@ARTICLE{squads2012,
  author = {Vesselinov, V. and Harp, D.},
  title = {Adaptive hydbrid optimisation strategy for calibration and parameter
	estimation of physical process models},
  journal = {Computers \& Geosciences},
  year = {2012},
  volume = {in press},
  doi = {10.1016/j.cageo.2012.05.027},
  owner = {rojasro},
  timestamp = {2012.09.26}
}

@ARTICLE{vicenteserrano+al2012,
  author = {{Vicente-Serrano}, S. and {L\'opez-Moreno}, J. and Beguer\'ia, S.
	and {Lorenzo-Lacruz}, J. and {Azorin-Molina}, C. and {Mor\'an-Tejeda},
	E.},
  title = {Accurate computation of streamflow drought index},
  journal = {Journal of Hydrologic Engineering},
  year = {2012},
  volume = {17},
  pages = {318--322},
  number = {2},
  abstract = {In this study, the authors investigated an approach to calculate the
	standardized streamflow index (SSI), which allows accurate spatial
	and temporal comparison of the hydrological conditions of a stream
	or set of streams. For this purpose, the capability of six three-parameter
	distributions (lognormal, Pearson Type III, log-logistic, general
	extreme value, generalized Pareto, and Weibull) and two different
	approaches to select the most suitable distribution the best monthly
	fit (BMF) and the minimum orthogonal distance (MD), were tested by
	using a monthly streamflow data set for the Ebro Basin (Spain). This
	large Mediterranean basin is characterized by high variability in
	the magnitude of streamflows and in seasonal regimes. The results
	show that the most commonly used probability distributions for flow
	frequency analysis provided good fits to the streamflow series. Thus,
	the visual inspection of the L-moment diagrams and the results of
	the Kolmogorov-Smirnov test did not enable the selection of a single
	optimum probability distribution. However, no single probability
	distribution for all the series was suitable for obtaining a robust
	standardized streamflow series because each of the distributions
	had one or more limitations. The BMF and MD approaches improved the
	results in the expected average, standard deviation, and the frequencies
	of extreme events of the SSI series in relation to the selection
	of a unique distribution for each station. The BMF and MD approaches
	involved using different probability distributions for each gauging
	station and month of the year to calculate the SSI. Both approaches
	are easy to apply and they provide very similar results in the quality
	of the obtained hydrological drought indexes. The proposed procedures
	are very flexible for analyses involving contrasting hydrological
	regimes and flow characteristics.},
  doi = {10.1061/(ASCE)HE.1943-5584.0000433},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{vicuna+al2010,
  author = {Vicuna, S. and Dracup, J. and Lund, J. and Dale, L. and Maurer, E.},
  title = {Basin--scale water system operations with uncertain future climate
	conditions: {M}ethodology and case studies},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W04505},
  number = {4},
  abstract = {The old and useful paradigm used by water resource engineers, that
	hydrology in a given place is stationary, and hence it is sufficient
	to look into the past to plan for the future, does not hold anymore,
	according to climate change projections. This becomes especially
	true in snow-dominated regions like California, where not only the
	magnitude but also the timing of streamflow could be affected by
	changes in precipitation and temperature. To plan and operate water
	resources systems at the basin scale, it is necessary to develop
	new tools that are suited for this nonstationary world. In this paper
	we develop an optimization algorithm that can be used for different
	studies related to climate change and water resources management.
	Three applications of this algorithm are developed for the Merced
	River basin. The first of these gives an assessment of the climate
	change effects on the operations of this basin considering an adaptive
	management strategy embedded in the optimization algorithm. In a
	second application we explore different long-term adaptation strategies
	intended to mitigate the effects of climate change. A final application
	is developed to determine how beneficial it is to build a new reservoir
	considering explicitly the uncertainty about future climate projections.},
  doi = {10.1029/2009WR007838},
  tags = {Climate Change, Disturbed Catchments}
}

@ARTICLE{vidalwade2009,
  author = {Vidal, {J.P.} and Wade, S.},
  title = {{A multimodel assessment of future climatological droughts in the
	United Kingdom}},
  journal = {Journal of Climatology},
  year = {2009},
  volume = {29},
  pages = {2056--2071},
  number = {14},
  abstract = {This paper presents a detailed assessment of future rainfall drought
	patterns over the United Kingdom. Previously developed bias-corrected
	high-resolution gridded precipitation time series are aggregated
	to the scale relevant for water resources management, in order to
	provide 21st-century time series for 183 hydrologic areas, as computed
	by six General Circulation Models (GCMs) under two emissions scenarios.
	The control run data are used as a ‘learning time series’ to compute
	the Standardized Precipitation Index (SPI) at four different time
	scales. SPI values for three 30-year future time slices are computed
	with respect to these learning time series in order to assess the
	changes in drought frequency. Multimodel results under the A2 scenario
	show a dramatic increase in the frequency of short-term extreme drought
	class for most of the country. A decrease of long-term droughts is
	expected in Scotland, due to the projected increase in winter precipitation.
	The analysis for two catchment case studies also showed that changes
	under the B2 scenario are generally consistent with those of the
	A2 scenario, with a reduced magnitude in changes. The overall increase
	with time in the spread of individual GCM results demonstrates the
	utility of multimodel statistics when assessing the uncertainty in
	future drought indices to be used in long-term water resources planning.},
  doi = {10.1002/joc.1843},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@INCOLLECTION{viner2003,
  author = {Viner, D.},
  title = {A qualitative assessment of the sources of uncertainty in climate
	change impacts assessment studies},
  booktitle = {Climatic Change: Implications for the Hydrological Cycle and for
	Water Management},
  publisher = {Springer Netherlands},
  year = {2003},
  volume = {10},
  series = {Advances in Global Change Research},
  pages = {139--149},
  abstract = {This paper, to be used as a guide by impacts assessors, identifies
	and then presents a qualitative assessment of the sources of uncertainty
	that are encapsulated in any climate change impacts assessment. The
	initial source, that of emissions scenarios derived from socio-economic
	projections is described and from here the cascade of the probable
	ranges of uncertainty associated with each step in an impacts assessment
	study is discussed. This assessment of the sources of uncertainty
	is coupled to the evolution of the science during the course of the
	Intergovernmental Panel on Climate Change (IPCC) process. This is
	used to explain (in specific cases) why certain de facto standards
	have been adopted in climate change research},
  doi = {10.1007/0-306-47983-4\_8},
  tags = {Thesis, Uncertainty}
}

@ARTICLE{LUCHEM-II,
  author = {Viney, N. and Bormann, H. and Breuer, L. and Bronstert, A. and Croke,
	B. and Frede, H. and Graff, T. and Hubrechts, L. and Huisman, J.
	and Jakeman, A. and Kite, G. and Lanini, J. and Leavesley, G. and
	Lettenmaier, D. and Lindstrom, G. and Seibert, J. and Sivapalan,
	M. and Willems, P.},
  title = {Assessing the impact of land use change on hydrology by ensemble
	modelling {(LUCHEM) II}: {E}nsemble combinations and predictions},
  journal = {Advances in Water Resources},
  year = {2009},
  volume = {32},
  pages = {147-158},
  number = {2},
  abstract = {This paper reports on a project to compare predictions from a range
	of catchment models applied to a mesoscale river basin in central
	Germany and to assess various ensemble predictions of catchment streamflow.
	The models encompass a large range in inherent complexity and input
	requirements. In approximate order of decreasing complexity, they
	are DHSVM, MIKE-SHE, TOPLATS, WASIM-ETH, SWAT, PRMS, SLURP, HBV,
	LASCAM and IHACRES. The models are calibrated twice using different
	sets of input data. The two predictions from each model are then
	combined by simple averaging to produce a single-model ensemble.
	The 10 resulting single-model ensembles are combined in various ways
	to produce multi-model ensemble predictions. Both the single-model
	ensembles and the multi-model ensembles are shown to give predictions
	that are generally superior to those of their respective constituent
	models, both during a 7-year calibration period and a 9-year validation
	period. This occurs despite a considerable disparity in performance
	of the individual models. Even the weakest of models is shown to
	contribute useful information to the ensembles they are part of.
	The best model combination methods are a trimmed mean (constructed
	using the central four or six predictions each day) and a weighted
	mean ensemble (with weights calculated from calibration performance)
	that places relatively large weights on the better performing models.
	Conditional ensembles, in which separate model weights are used in
	different system states (e.g. summer and winter, high and low flows)
	generally yield little improvement over the weighted mean ensemble.
	However a conditional ensemble that discriminates between rising
	and receding flows shows moderate improvement. An analysis of ensemble
	predictions shows that the best ensembles are not necessarily those
	containing the best individual models. Conversely, it appears that
	some models that predict well individually do not necessarily combine
	well with other models in multi-model ensembles. The reasons behind
	these observations may relate to the effects of the weighting schemes,
	non-stationarity of the climate series and possible cross-correlations
	between models.},
  doi = {10.1016/j.advwatres.2008.05.006},
  owner = {Rodrigo},
  timestamp = {2009.06.17}
}

@ARTICLE{visser+al2000,
  author = {Visser, H. and Folkert, R. and Hoekstra, J. and {De Wolff}, J.},
  title = {Identifying key sources of uncertainty in climate change projections},
  journal = {Climatic Change},
  year = {2000},
  volume = {45},
  pages = {421--457},
  number = {3--4},
  abstract = {What sources of uncertainty should be included in climate change projections
	and what gains can be made if specific sources of uncertainty are
	reduced through improved research? DIALOGUE, an integrated assessment
	model, has been used to answer these questions. Central in the approach
	of DIALOGUE is the concept of parallel modeling, i.e., for each step
	in the chain from emissions to climate change a number of equivalent
	models are implemented. The following conclusions are drawn:},
  doi = {10.1023/A:1005516020996},
  keywords = {ANTHROPOGENIC SULFATE AEROSOLS, GLOBAL CARBON-CYCLE, SIMPLE-MODEL,
	SEA-LEVEL, SENSITIVITY, SIMULATIONS, TEMPERATURE, LIFETIME, DIOXIDE,
	OH},
  tags = {Uncertainty}
}

@ARTICLE{vogelfennessey1995,
  author = {Vogel, R. and Fennessey, N.},
  title = {Flow duration curves {II}: {A} review of applications in water resources
	planning},
  journal = {Water Resources Bulletin},
  year = {1995},
  volume = {31},
  pages = {1029--1039},
  number = {6},
  abstract = {A streamflow duration curve illustrates the relationship between the
	frequency and magnitude of streamflow. Flow duration curves have
	a long history in the field of water-resource engineering and have
	been used to solve problems in water-quality management, hydropower,
	instream flow methodologies, water-use planning, flood control, and
	river and reservoir sedimentation, and for scientific comparisons
	of streamflow characteristics across watersheds. This paper reviews
	traditional applications and provides extensions to some new applications,
	including water allocation, wasteload allocation, river and wetland
	inundation mapping, and the economic selection of a water-resource
	project},
  doi = {10.1111/j.1752-1688.1995.tb03419.x},
  keywords = {FDC, flow duration curve},
  tags = {FDC, General Hydrology}
}

@ARTICLE{vogelfennessey1994,
  author = {Vogel, R. and Fennessey, N.},
  title = {Flow duration curves {I}: {A} new interpretation and confidence intervals},
  journal = {Journal of Water Resources Planning and Management},
  year = {1994},
  volume = {120},
  pages = {485--504},
  number = {4},
  abstract = {A flow-duration curve (FDC) is simply the complement of the cumulative
	distribution function of daily, weekly, monthly (or some other time
	interval of) streamflow. Applications of FDCs include, but are not
	limited to, hydropower planning, water-quality management, river
	and reservoir sedimentation studies, habitat suitability, and low-flow
	augmentation. Although FDCs have a long and rich history in the field
	of hydrology, they are sometimes criticized because, traditionally,
	their interpretation depends on the particular period of record on
	which they are based. If one considers n individual FDCs, each corresponding
	to one of the individual n years of record, then one may treat those
	n annual FDCs in much the same way one treats a sequence of annual
	maximum or annual minimum streamflows. This new annual-based interpretation
	enables confidence intervals and recurrence intervals to be associated
	with FDCs in a nonparametric framework.},
  doi = {10.1061/(ASCE)0733-9496(1994)120:4(485)},
  tags = {FDC, General Hydrology}
}

@ARTICLE{vogelsankarasubramanian2003,
  author = {Vogel, R. and Sankarasubramanian, A.},
  title = {Validation of a watershed model without calibration},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1292},
  number = {10},
  abstract = {Traditional approaches for the validation of watershed models focus
	on the {``}goodness of fit{''} between model predictions and observations.
	It is possible for a watershed model to exhibit a {``}good{''} fit,
	yet not accurately represent hydrologic processes; hence {``}goodness
	of fit{''} can be misleading. Instead, we introduce an approach which
	evaluates the ability of a model to represent the observed covariance
	structure of the input (climate) and output (streamflow) without
	ever calibrating the model. An advantage of this approach is that
	it is not confounded by model error introduced during the calibration
	process. We illustrate that once a watershed model is calibrated,
	the unavoidable model error can cloud our ability to validate (or
	invalidate) the model. We emphasize that model hypothesis testing
	(validation) should be performed prior to, and independent of, parameter
	estimation (calibration), contrary to traditional practice in which
	watershed models are usually validated after calibrating the model.
	Our approach is tested using two different watershed models at a
	number of different watersheds in the United States},
  doi = {10.1029/2002WR001940},
  keywords = {rainfall-runoff, catchment model, verification, hypothesis testing,
	moment diagram},
  tags = {Calibration}
}

@ARTICLE{voss+al2002,
  author = {Voss, R. and May, W. and Roeckner, E.},
  title = {Enhanced resolution modelling study on anthropogenic climate change:
	changes in extremes of the hydrological cycle},
  journal = {International Journal of Climatology},
  year = {2002},
  volume = {22},
  pages = {755--777},
  number = {7},
  abstract = {Changes in variability and extremes of the hydrological cycle are
	studied in two 30 year simulations using a general circulation model
	at high horizontal resolution. The simulations represent the present-day
	climate and a period in which the radiative forcing corresponds to
	a doubling of the present-day concentrations of atmospheric greenhouse
	gases. In most regions and seasons the probability density function
	of daily precipitation experiences a stretching associated with a
	higher probability of heavy precipitation events in the warmer climate.
	Whereas extremely long wet spells show only moderate changes, the
	extremely long dry spells are extended at middle latitudes over most
	land areas. At high latitudes the changes in annual maximum river
	runoff are mainly controlled by changes in snow budget. Eight out
	of 14 selected major rivers show a statistically significant change
	in 10 year return values of the annual maximum discharge. In two
	cases a significant decrease is found and in six cases there is a
	significant increase.},
  doi = {10.1002/joc.757},
  owner = {rojasro},
  timestamp = {2010.08.04}
}

@ARTICLE{vrugt+al2008a,
  author = {Vrugt, J. and {ter Braak}, C. and Clark, M. and Hyman, J. and Robinson,
	B.},
  title = {Treatment of input uncertainty in hydrologic modeling: Doing hydrology
	backward with {M}arkov {C}hain {M}onte {C}arlo simulation},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W00B09},
  abstract = {There is increasing consensus in the hydrologic literature that an
	appropriate framework for streamflow forecasting and simulation should
	include explicit recognition of forcing and parameter and model structural
	error. This paper presents a novel Markov chain Monte Carlo (MCMC)
	sampler, entitled differential evolution adaptive Metropolis (DREAM),
	that is especially designed to efficiently estimate the posterior
	probability density function of hydrologic model parameters in complex,
	high-dimensional sampling problems. This MCMC scheme adaptively updates
	the scale and orientation of the proposal distribution during sampling
	and maintains detailed balance and ergodicity. It is then demonstrated
	how DREAM can be used to analyze forcing data error during watershed
	model calibration using a five-parameter rainfall-runoff model with
	streamflow data from two different catchments. Explicit treatment
	of precipitation error during hydrologic model calibration not only
	results in prediction uncertainty bounds that are more appropriate
	but also significantly alters the posterior distribution of the watershed
	model parameters. This has significant implications for regionalization
	studies. The approach also provides important new ways to estimate
	areal average watershed precipitation, information that is of utmost
	importance for testing hydrologic theory, diagnosing structural errors
	in models, and appropriately benchmarking rainfall measurement devices.},
  doi = {10.1029/2007WR006720},
  keywords = {uncertainty assessment, Markov chain Monte Carlo simulation, surface
	hydrology, streamflow forecasting, rainfall uncertainty, posterior
	exploration.},
  tags = {Uncertainty}
}

@ARTICLE{vrugt+al2008b,
  author = {Vrugt, J. and {ter Braak}, C. and Gupta, H. and Robinson, B.},
  title = {Equifinality of formal ({DREAM}) and informal ({GLUE}) {B}ayesian
	approaches in hydrologic modelling?},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2009},
  volume = {In press},
  pages = {1011--1026},
  number = {7},
  abstract = {In recent years, a strong debate has emerged in the hydrologic literature
	regarding what constitutes an appropriate framework for uncertainty
	estimation. Particularly, there is strong disagreement whether an
	uncertainty framework should have its roots within a proper statistical
	(Bayesian) context, or whether such a framework should be based on
	a different philosophy and implement informal measures and weaker
	inference to summarize parameter and predictive distributions. In
	this paper, we compare a formal Bayesian approach using Markov Chain
	Monte Carlo (MCMC) with generalized likelihood uncertainty estimation
	(GLUE) for assessing uncertainty in conceptual watershed modeling.
	Our formal Bayesian approach is implemented using the recently developed
	differential evolution adaptive metropolis (DREAM) MCMC scheme with
	a likelihood function that explicitly considers model structural,
	input and parameter uncertainty. Our results demonstrate that DREAM
	and GLUE can generate very similar estimates of total streamflow
	uncertainty. This suggests that formal and informal Bayesian approaches
	have more common ground than the hydrologic literature and ongoing
	debate might suggest. The main advantage of formal approaches is,
	however, that they attempt to disentangle the effect of forcing,
	parameter and model structural error on total predictive uncertainty.
	This is key to improving hydrologic theory and to better understand
	and predict the flow of water through catchments.},
  doi = {10.1007/s00477-008-0274-y},
  owner = {rojasro},
  timestamp = {2009.09.14}
}

@ARTICLE{vrugt+al2009b,
  author = {Vrugt, J. and {ter Braak}, C. and Gupta, H. and Robinson, B.},
  title = {Response to comment by {K}eith {B}even on "{E}quifinality of formal
	({DREAM}) and informal ({GLUE}) {B}ayesian approaches in hydrologic
	modeling?"},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2009},
  volume = {23},
  pages = {1061--1062},
  number = {7},
  doi = {10.1007/s00477-008-0284-9},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{vrugt+al2006c,
  author = {Vrugt, J. and Clarck, M. and Diks, C. and Duan, Q. and Robinson,
	B.},
  title = {Multi--objective calibration of forecast ensembles using {B}ayesian
	model averaging},
  journal = {Geophysical Research Letters},
  year = {2006},
  volume = {33},
  pages = {1--6},
  number = {L19817},
  abstract = {Bayesian Model Averaging (BMA) has recently been proposed as a method
	for statistical postprocessing of forecast ensembles from numerical
	weather prediction models. The BMA predictive probability density
	function (PDF) of any weather quantity of interest is a weighted
	average of PDFs centered on the bias-corrected forecasts from a set
	of different models. However, current applications of BMA calibrate
	the forecast specific PDFs by optimizing a single measure of predictive
	skill. Here we propose a multi-criteria formulation for postprocessing
	of forecast ensembles. Our multi-criteria framework implements different
	diagnostic measures to reflect different but complementary metrics
	of forecast skill, and uses a numerical algorithm to solve for the
	Pareto set of parameters that have consistently good performance
	across multiple performance metrics. Two illustrative case studies
	using 48-hour ensemble data of surface temperature and sea level
	pressure, and multi-model seasonal forecasts of temperature, show
	that a multi-criteria formulation provides a more appealing basis
	for selecting the appropriate BMA model.},
  doi = {10.1029/2006GL027126},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{vrugt+al2003b,
  author = {Vrugt, J. and Gupta, H. and Bastidas, L. and Bouten, W. and Sorooshian,
	S.},
  title = {Effective and efficient algorithm for multiobjective optimization
	of hydrologic models},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = { 1214},
  abstract = {Practical experience with the calibration of hydrologic models suggests
	that any single-objective function, no matter how carefully chosen,
	is often inadequate to properly measure all of the characteristics
	of the observed data deemed to be important. One strategy to circumvent
	this problem is to define several optimization criteria (objective
	functions) that measure different (complementary) aspects of the
	system behavior and to use multicriteria optimization to identify
	the set of nondominated, efficient, or Pareto optimal solutions.
	In this paper, we present an efficient and effective Markov Chain
	Monte Carlo sampler, entitled the Multiobjective Shuffled Complex
	Evolution Metropolis (MOSCEM) algorithm, which is capable of solving
	the multiobjective optimization problem for hydrologic models. MOSCEM
	is an improvement over the Shuffled Complex Evolution Metropolis
	(SCEM-UA) global optimization algorithm, using the concept of Pareto
	dominance (rather than direct single-objective function evaluation)
	to evolve the initial population of points toward a set of solutions
	stemming from a stable distribution (Pareto set). The efficacy of
	the MOSCEM-UA algorithm is compared with the original MOCOM-UA algorithm
	for three hydrologic modeling case studies of increasing complexity.},
  doi = {10.1029/2002WR001746},
  keywords = {parameter optimization, Markov chain Monte Carlo, multicriteria calibration,
	population diversity, Pareto ranking, hydrologic models, RAINFALL-RUNOFF
	MODELS, LAND-SURFACE MODEL, GLOBAL OPTIMIZATION, MULTICRITERIA METHODS,
	AUTOMATIC CALIBRATION, PARAMETER UNCERTAINTY, CATCHMENT MODELS, MULTIPLE,
	COMPLEXITY},
  tags = {Calibration}
}

@ARTICLE{vrugt+al2003a,
  author = {Vrugt, J. and Gupta, H. and Bouten, W. and Sorooshian, S.},
  title = {A {S}huffled {C}omplex {E}volution {M}etropolis algorithm for optimization
	and uncertainty assessment of hydrological model parameters},
  journal = {Water Resources Research},
  year = {2003},
  volume = {39},
  pages = {1201},
  number = {8},
  abstract = {Markov Chain Monte Carlo (MCMC) methods have become increasingly popular
	for estimating the posterior probability distribution of parameters
	in hydrologic models. However, MCMC methods require the a priori
	definition of a proposal or sampling distribution, which determines
	the explorative capabilities and efficiency of the sampler and therefore
	the statistical properties of the Markov Chain and its rate of convergence.
	In this presentation, we present an MCMC sampler entitled the Shuffled
	Complex Evolution Metropolis algorithm (SCEM-UA), which is well suited
	to infer the posterior distribution of hydrologic model parameters.
	The SCEM-UA algorithm is a modified version of the original SCE-UA
	global optimization algorithm developed by Duan et al. [1992]. Two
	case studies demonstrate that the adaptive capability of the SCEM
	algorithm significantly reduces the number of model simulations needed
	to infer the posterior distribution of the parameters when compared
	with the traditional Metropolis-Hastings samplers. },
  doi = {10.1029/2002WR001642},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{vrugt+al2006a,
  author = {Vrugt, J. and Gupta, H. and Dekker, S. and Sorooshian, S. and Wagener,
	T. and Bouten, W.},
  title = {Application of stochastic parameter optimization to the {S}acramento
	{S}oil {M}oisture {A}ccounting Model},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {325},
  pages = {288--307},
  abstract = {Hydrological models generally contain parameters that cannot be measured
	directly, but can only be meaningfully inferred by calibration against
	a historical record of input-output data. While considerable progress
	has been made in the development and application of automatic procedures
	for model calibration, such methods have received criticism for their
	lack of rigor in treating uncertainty in the parameter estimates.
	In this paper, we apply the recently developed Shuffled Complex Evolution
	Metropolis algorithm (SCEM-UA) to stochastic calibration of the parameters
	in the Sacramento Soil Moisture Accounting model (SAC-SMA) model
	using historical data from the Leaf River in Mississippi. The SCEM-UA
	algorithm is a Markov Chain Monte Carlo sampler that provides an
	estimate of the most likely parameter set and underlying posterior
	distribution within a single optimization run. In particular, we
	explore the relationship between the length and variability of the
	streamflow data and the Bayesian uncertainty associated with the
	SAC-SMA model parameters and compare SCEM-UA derived parameter values
	with those obtained using deterministic SCE-UA calibrations. Most
	significantly, for the Leaf River catchments under study our results
	demonstrate that most of the 13 SAC-SMA parameters are well identified
	by calibration to daily streamflow data suggesting that this data
	contains more information than has previously been reported in the
	literature. (c) 2006 Elsevier B.V. All rights reserved.},
  doi = {10.1016/j.jhydrol.2005.10.041},
  keywords = {hydrologic model, Bayesian statistics, parameter uncertainty, Markov
	Chain Monte Carlo methods, rainfall-runoff, RAINFALL-RUNOFF MODELS,
	HYDROLOGIC-MODELS, AUTOMATIC CALIBRATION, GLOBAL OPTIMIZATION, UNCERTAINTY,
	CATCHMENTS, MULTIPLE},
  tags = {Calibration}
}

@ARTICLE{vrugt+al2006b,
  author = {Vrugt, J. and {O. Nuallain}, B. and Robinson, B. and Bouten, W. and
	Dekker, S. and Sloot, P.},
  title = {Application of parallel computing to stochastic parameter estimation
	in environmental models},
  journal = {Computers \& Geosciences},
  year = {2006},
  volume = {32},
  pages = {1139--1155},
  number = {8},
  abstract = {Parameter estimation or model calibration is a common problem in many
	areas of process modeling, both in on-line applications such as real-time
	flood forecasting, and in off-line applications such as the modeling
	of reaction kinetics and phase equilibrium. The goal is to determine
	values of model parameters that provide the best fit to measured
	data, generally based on some type of least-squares or maximum likelihood
	criterion. Usually, this requires the solution of a non-linear and
	frequently non-convex optimization problem. In this paper we describe
	a user-friendly, computationally efficient parallel implementation
	of the Shuffled Complex Evolution Metropolis (SCEM-UA) global optimization
	algorithm for stochastic estimation of parameters in environmental
	models. Our parallel implementation takes better advantage of the
	computational power of a distributed computer system. Three case
	studies of increasing complexity demonstrate that parallel parameter
	estimation results in a considerable time savings when compared with
	traditional sequential optimization runs. The proposed method therefore
	provides an ideal means to solve complex optimization problems. (c)
	2005 Elsevier Ltd. All rights reserved.},
  doi = {10.1016/j.cageo.2005.10.015},
  keywords = {optimization, model, hydrology, bird migration, octave, message passing
	interface, RAINFALL-RUNOFF MODELS, MULTIOBJECTIVE OPTIMIZATION, METROPOLIS
	ALGORITHM, GLOBAL OPTIMIZATION, GENETIC ALGORITHMS, HYDROLOGIC-MODELS,
	BIRD MIGRATION, CALIBRATION, MINIMIZATION, SIMULATION},
  tags = {Calibration}
}

@ARTICLE{vrugtrobinson2007a,
  author = {Vrugt, J. and Robinson, B.},
  title = {Improved evolutionary optimization from genetically adaptive multimethod
	search},
  journal = {Proceedings of The National Academy of Sciences of The United States
	of America},
  year = {2007},
  volume = {104},
  pages = {708--711},
  abstract = {In the last few decades, evolutionary algorithms have emerged as a
	revolutionary approach for solving search and optimization problems
	involving multiple conflicting objectives. Beyond their ability to
	search intractably large spaces for multiple solutions, these algorithms
	are able to maintain a diverse population of solutions and exploit
	similarities of solutions by recombination. However, existing theory
	and numerical experiments have demonstrated that it is impossible
	to develop a single algorithm for population evolution that is always
	efficient for a diverse set of optimization problems. Here we show
	that significant improvements in the efficiency of evolutionary search
	can be achieved by running multiple optimization algorithms simultaneously
	using new concepts of global information sharing and genetically
	adaptive offspring creation. We call this approach a multialgorithm,
	genetically adaptive multiobjective, or AMALGAM, method, to evoke
	the image of a procedure that merges the strengths of different optimization
	algorithms. Benchmark results using a set of well known multiobjective
	test problems show that AMALGAM approaches a factor of 10 improvement
	over current optimization algorithms for the more complex, higher
	dimensional problems. The AMALGAM method provides new opportunities
	for solving previously intractable optimization problems.},
  doi = {10.1073/pnas.0610471104},
  keywords = {evolutionary search, multiple objectives, optimization problems, Pareto
	front, GLOBAL OPTIMIZATION, ALGORITHM},
  tags = {Calibration}
}

@ARTICLE{vrugtrobinson2007b,
  author = {Vrugt, J. and Robinson, B.},
  title = {Treatment of uncertainty using ensemble methods: {C}omparison of
	sequential data assimilation and {B}ayesian model averaging},
  journal = {Water Resources Research},
  year = {2007},
  volume = {43},
  pages = {W01411},
  abstract = {Predictive uncertainty analysis in hydrologic modeling has become
	an active area of research, the goal being to generate meaningful
	error bounds on model predictions. State-space filtering methods,
	such as the ensemble Kalman filter (EnKF), have shown the most flexibility
	to integrate all sources of uncertainty. However, predictive uncertainty
	analyses are typically carried out using a single conceptual mathematical
	model of the hydrologic system, rejecting a priori valid alternative
	plausible models and possibly underestimating uncertainty in the
	model itself. Methods based on Bayesian model averaging (BMA) have
	also been proposed in the statistical and meteorological literature
	as a means to account explicitly for conceptual model uncertainty.
	The present study compares the performance and applicability of the
	EnKF and BMA for probabilistic ensemble streamflow forecasting, an
	application for which a robust comparison of the predictive skills
	of these approaches can be conducted. The results suggest that for
	the watershed under consideration, BMA cannot achieve a performance
	matching that of the EnKF method.},
  doi = {10.1029/2005WR004838},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{vrugt+al2009d,
  author = {Vrugt, J.A. and Robinson, B.A. and Hyman, J.M.},
  title = {Self-Adaptive Multimethod Search for Global Optimization in Real-Parameter
	Spaces},
  journal = {IEEE Transactions on Evolutionary Computation},
  year = {2009},
  volume = {13},
  pages = {243-259},
  number = {2},
  abstract = {Many different algorithms have been developed in the last few decades
	for solving complex real-world search and optimization problems.
	The main focus in this research has been on the development of a
	single universal genetic operator for population evolution that is
	always efficient for a diverse set of optimization problems. In this
	paper, we argue that significant advances to the field of evolutionary
	computation can be made if we embrace a concept of self-adaptive
	multimethod optimization in which multiple different search algorithms
	are run concurrently, and learn from each other through information
	exchange using a common population of points. We present an evolutionary
	algorithm, entitled A Multialgorithm Genetically Adaptive Method
	for Single Objective Optimization (AMALGAM-SO), that implements this
	concept of self adaptive multimethod search. This method simultaneously
	merges the strengths of the covariance matrix adaptation (CMA) evolution
	strategy, genetic algorithm (GA), and particle swarm optimizer (PSO)
	for population evolution and implements a self-adaptive learning
	strategy to automatically tune the number of offspring these three
	individual algorithms are allowed to contribute during each generation.
	Benchmark results in 10, 30, and 50 dimensions using synthetic functions
	from the special session on real-parameter optimization of CEC 2005
	show that AMALGAM-SO obtains similar efficiencies as existing algorithms
	on relatively simple unimodal problems, but is superior for more
	complex higher dimensional multimodal optimization problems. The
	new search method scales well with increasing number of dimensions,
	converges in the close proximity of the global minimum for functions
	with noise induced multimodality, and is designed to take full advantage
	of the power of distributed computer networks.},
  doi = {10.1109/TEVC.2008.924428},
  tags = {Calibration, PSO}
}

@ARTICLE{wohling2008,
  author = {W\"ohling, T. and Vrugt, J.},
  title = {Combining multiobjective optimization and {B}ayesian model averaging
	to calibrate forecast ensembles of soil hydraulic models},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W12432},
  abstract = {Most studies in vadose zone hydrology use a single conceptual model
	for predictive inference and analysis. Focusing on the outcome of
	a single model is prone to statistical bias and underestimation of
	uncertainty. In this study, we combine multiobjective optimization
	and Bayesian model averaging (BMA) to generate forecast ensembles
	of soil hydraulic models. To illustrate our method, we use observed
	tensiometric pressure head data at three different depths in a layered
	vadose zone of volcanic origin in New Zealand. A set of seven different
	soil hydraulic models is calibrated using a multiobjective formulation
	with three different objective functions that each measure the mismatch
	between observed and predicted soil water pressure head at one specific
	depth. The Pareto solution space corresponding to these three objectives
	is estimated with AMALGAM and used to generate four different model
	ensembles. These ensembles are postprocessed with BMA and used for
	predictive analysis and uncertainty estimation. Our most important
	conclusions for the vadose zone under consideration are (1) the mean
	BMA forecast exhibits similar predictive capabilities as the best
	individual performing soil hydraulic model, (2) the size of the BMA
	uncertainty ranges increase with increasing depth and dryness in
	the soil profile, (3) the best performing ensemble corresponds to
	the compromise (or balanced) solution of the three-objective Pareto
	surface, and (4) the combined multiobjective optimization and BMA
	framework proposed in this paper is very useful to generate forecast
	ensembles of soil hydraulic models.},
  doi = {10.1029/2008WR007154},
  owner = {rojasro},
  timestamp = {2010.02.22}
}

@ARTICLE{hypres1999,
  author = {W\"osten, J. and Lilly, A. and {Le Bas}, C.},
  title = {Development and use of a database of hydraulic properties of European
	soils},
  journal = {Geoderma},
  year = {1999},
  volume = {90},
  pages = {169--185},
  number = {3--4},
  abstract = {Many environmental studies on the protection of European soil and
	water resources make use of soil water simulation models. A major
	obstacle to the wider application of these models is the lack of
	easily accessible and representative soil hydraulic properties. In
	order to overcome this apparent lack of data, a project was initiated
	to bring together the available hydraulic data which resided within
	different institutions in Europe into one central database. This
	information was then used to derive a set of pedotransfer functions
	applicable to studies at a European scale. These pedotransfer functions
	predict the hydraulic properties from parameters collected during
	soil surveys and can be a good alternative for costly and time-consuming
	direct measurement of these properties. A total of 20 institutions
	from 12 European countries collaborated in establishing the database
	of Image draulic Image operties of Image uropean Image oils (HYPRES).
	This database has a flexible relational structure capable of holding
	a wide diversity of both soil pedological and hydraulic data. As
	these data were contributed by 20 different institutions it was necessary
	to standardise both the particle-size and the hydraulic data. A novel
	similarity interpolation procedure was successfully used to achieve
	standardization of particle-sizes according to the FAO clay, silt
	and sand particle-size ranges. Standardization of hydraulic data
	was achieved by fitting the Mualem-van Genuchten model parameters
	to the individual ?(h) and K(h) hydraulic properties stored in HYPRES.
	The HYPRES database contains information on a total of 5521 soil
	horizons (including replicates). Of these, 4030 horizons had sufficient
	data to be used in the derivation of pedotransfer functions. Information
	on both water retention and hydraulic conductivity was available
	for 1136 horizons whereas 2894 horizons had only information on water
	retention. Each soil horizon was allocated to one of 11 possible
	soil textural/pedological classes derived from the six FAO texture
	classes (five mineral and one organic) and the two pedological classes
	(topsoil and subsoil) recognised within the 1:1 000 000 scale Soil
	Geographical Data Base of Europe. Next, both class and continuous
	pedotransfer functions were developed. By using the class pedotransfer
	functions in combination with the 1:1 000 000 scale Soil Map of Europe,
	the spatial distribution of soil water availability within Europe
	was derived.},
  doi = {10.1016/s0016-7061(98)00132-3},
  owner = {rojasro},
  timestamp = {2011.05.12}
}

@ARTICLE{weglarczyk1998,
  author = {W\k{e}glarczyk, S.},
  title = {The interdependence and applicability of some statistical quality
	measures for hydrological models},
  journal = {Journal of Hydrology},
  year = {1998},
  volume = {206},
  pages = {98--103},
  number = {1-2},
  abstract = {There are only a few basic statistical measures of discrepancy between
	a model and the modelled quantity. All the other quality measures
	which can be called {`}derivative criteria{'} are mostly dimensionless
	functions which use a selected basic measure (most often it is the
	mean squared error of a model, MSE) calculated for the model assessed
	and another measure calculated for a reference model. For this reason
	these criteria are interrelated, which is not always sufficiently
	realized. It is shown that MSE depends on the correlation coefficient
	between observed and computed values R, another popular measure of
	model performance. Other often employed criteria: the integral squared
	error ISE, the special correlation coefficient Rs, the Nash-Sutcliffe
	efficiency E are also interdependent as they are all based on MSE.
	Thus, when employing two or more criteria at once, special care should
	be taken in order not to draw contradictory conclusions, examples
	of which are given in the paper.},
  doi = {10.1016/S0022-1694(98)00094-8},
  keywords = {Model performance, Accuracy criteria, Mean squared error, Correlation
	coefficient},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{wagener+al2001,
  author = {Wagener, T. and Boyle, D. and Lees, M. and Wheater, H. and Gupta,
	H. and Sorooshian, S.},
  title = {A framework for development and application of hydrological models},
  journal = {Hydrology and Earth System Sciences},
  year = {2001},
  volume = {5},
  pages = {13--26},
  number = {1},
  abstract = {Many existing hydrological modelling procedures do not make best use
	of available information, resulting in non-minimal uncertainties
	in model structure and parameters, and a lack of detailed information
	regarding model behaviour. A framework is required that balances
	the level of model complexity supported by the available data with
	the level of performance suitable for the desired application. Tools
	are needed that make optimal use of the information available in
	the data to identify model structure and parameters, and that allow
	a detailed analysis of model behaviour. This should result in appropriate
	levels of model complexity as a function of available data, hydrological
	system characteristics and modelling purpose. This paper introduces
	an analytical framework to achieve this, and tools to use within
	it, based on a multi-objective approach to model calibration and
	analysis. The utility of the framework is demonstrated with an example
	from the field of rainfall-runoff modelling.},
  doi = {10.5194/hess-5-13-2001},
  keywords = {hydrological modelling, multi-objective calibration, model complexity,
	parameter identifiability},
  tags = {conceptual model}
}

@ARTICLE{wagenergupta2005,
  author = {Wagener, T. and Gupta, H.},
  title = {Model identification for hydrological forecasting under uncertainty},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2005},
  volume = {19},
  pages = {378--387},
  number = {6},
  abstract = {Methods for the identification of models for hydrological forecasting
	have to consider the specific nature of these models and the uncertainties
	present in the modeling process. Current approaches fail to fully
	incorporate these two aspects. In this paper we review the nature
	of hydrological models and the consequences of this nature for the
	task of model identification. We then continue to discuss the history
	(“The need for more POWER‘’), the current state (“Learning from other
	fields”) and the future (“Towards a general framework”) of model
	identification. The discussion closes with a list of desirable features
	for an identification framework under uncertainty and open research
	questions in need of answers before such a framework can be implemented.},
  doi = {10.1007/s00477-005-0006-5},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{wagener+al2003,
  author = {Wagener, T. and McIntyre, N. and Lees, M. and Wheater, V and Gupta,
	H.},
  title = {Towards reduced uncertainty in conceptual rainfall-runoff modelling:
	dynamic identifiability analysis},
  journal = {Hydrological Processes},
  year = {2003},
  volume = {17},
  pages = {455--476},
  number = {2},
  abstract = {Conceptual modelling requires the identification of a suitable model
	structure and the estimation of parameter values through calibration
	against observed data. A lack of objective approaches to evaluate
	model structures and the inability of calibration procedures to distinguish
	between the suitability of different parameter sets are major sources
	of uncertainty in current modelling procedures. This paper presents
	an approach analysing the performance of the model in a dynamic fashion
	resulting in an improved use of available information. Model structures
	can be evaluated with respect to the failure of individual components,
	and periods of high information content for specific parameters can
	be identified. The procedure is termed dynamic identifiability analysis
	(DYNIA) and is applied to a model structure built from typical conceptual
	components. Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
  doi = {10.1002/hyp.1135},
  keywords = {conceptual rainfall-runoff models, model structural analysis, parameter
	identifiability, information content of data, HYDROLOGICAL MODEL,
	IMPROVED CALIBRATION, DISTRIBUTED MODELS, IDENTIFICATION, PREDICTION,
	VALIDATION, WATER, EUTROPHICATION, COMPLEXITY, CATCHMENTS},
  tags = {Uncertainty, conceptual model}
}

@INCOLLECTION{wagenerwheatergupta2003,
  author = {T. Wagener and H. S. Wheater and H. V. Gupta},
  title = {Calibration of watershed models},
  booktitle = {Identification and evaluation of watershed models},
  year = {2003},
  editor = {Q. Duan and H. V. Gupta and R. Turcotte and S. Sorooshian and A.
	N. Rousseau},
  optannote = {Wagener,T., Wheater,H.S., Gupta,H.V., Identification and evaluation
	of watershed models, In: Duan,Q., Gupta,H.V., Turcotte,R., Sorooshian,S.,
	Rousseau,A.N., editor, Calibration of watershed models, Washington,
	D.C., American Geophysical Union, 2003, ISBN: 0-8759-0355-X},
  tags = {Calibration}
}

@BOOK{wagener+al2004,
  title = {Rainfall--Runoff modelling in gauged and ungauged catchments},
  publisher = {Imperial College Press},
  year = {2004},
  author = {Wagener, T. and Wheather, H. and Gupta, H.},
  pages = {300},
  address = {{L}ondon {UK}},
  isbn = {1-86094-466-3},
  tags = {conceptual model}
}

@ARTICLE{wagner1995,
  author = {Wagner, B.},
  title = {Sampling design methods for groundwater modeling under uncertainty},
  journal = {Water Resources Research},
  year = {1995},
  volume = {31},
  pages = {2581--2591},
  number = {10},
  abstract = {A sampling network design model is presented that evaluates the trade-off
	between the varying costs of different types of data and the contribution
	of those data to improving model reliability. The methodology couples
	parameter-estimate and model-prediction uncertainty analyses with
	optimization to identify the mix of hydrogeologic information (e.g.,
	head, concentration, and/or hydraulic conductivity measurement locations)
	that will minimize model prediction uncertainty for a given data
	collection budget. Two alternative optimization algorithms are presented
	and compared: a branch-and-bound algorithm and a genetic algorithm.
	A series of synthetic examples are presented to demonstrate the adaptability
	of the methodology to different sampling scenarios. The examples
	reveal two important properties of this network design problem. First,
	model-parameter and model-prediction uncertainty analyses are important
	components of the network design methodology because they provide
	a natural framework for evaluating the cost/information trade-off
	for different types of data and different sampling network designs.
	Second, the genetic algorithm can identify near-optimal solutions
	for a small fraction of the computational effort needed to determine
	the globally optimal solutions of the branch-and-bound algorithm.},
  doi = {10.1029/95WR02107},
  owner = {rojasro},
  timestamp = {2010.02.17}
}

@ARTICLE{walker2003a,
  author = {Walker, W. and Harremo\"es, P. and Rotmans, J. and {Van der Sluijs},
	J. and {Van Asselt}, M. and Janssen, P. and {Krayer Von Krauss},
	M.},
  title = {Defining uncertainty: {A} conceptual basis for uncertainty management
	in model--based decision support},
  journal = {Integrated Assessment},
  year = {2003},
  volume = {4},
  pages = {5--17},
  number = {1},
  abstract = {The aim of this paper is to provide a conceptual basis for the systematic
	treatment of uncertainty in model-based decision support activities
	such as policy analysis, integrated assessment and risk assessment.
	It focuses on the uncertainty perceived from the point of view of
	those providing information to support policy decisions (i.e., the
	modellers’ view on uncertainty) – uncertainty regarding the analytical
	outcomes and conclusions of the decision support exercise. Within
	the regulatory and management sciences, there is neither commonly
	shared terminology nor full agreement on a typology of uncertainties.
	Our aim is to synthesise a wide variety of contributions on uncertainty
	in model-based decision support in order to provide an interdisciplinary
	theoretical framework for systematic uncertainty analysis. To that
	end we adopt a general definition of uncertainty as being any deviation
	from the unachievable ideal of completely deterministic knowledge
	of the relevant system. We further propose to discriminate among
	three dimensions of uncertainty: location, level and nature of uncertainty,
	and we harmonise existing typologies to further detail the concepts
	behind these three dimensions of uncertainty.We propose an uncertainty
	matrix as a heuristic tool to classify and report the various dimensions
	of uncertainty, thereby providing a conceptual framework for better
	communication among analysts as well as between them and policymakers
	and stakeholders. Understanding the various dimensions of uncertainty
	helps in identifying, articulating, and prioritising critical uncertainties,
	which is a crucial step to more adequate acknowledgement and treatment
	of uncertainty in decision support endeavours and more focused research
	on complex, inherently uncertain, policy issues.},
  doi = {10.1076/iaij.4.1.5.16466},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{walker2003b,
  author = {Walker, W. and Marchau, V.},
  title = {Dealing with uncertainty in policy analysis and policy making},
  journal = {Integrated Assessment},
  year = {2003},
  volume = {4},
  pages = {1--4},
  number = {1},
  doi = {10.1076/iaij.4.1.1.16462},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{wallace2012,
  author = {Wallace, J.},
  title = {Weather- and climate-related extreme events: Teachable moments},
  journal = {EOS, Transactions American Geophysical Union},
  year = {2012},
  volume = {93},
  pages = {0096--3941},
  number = {11},
  doi = {10.1029/2012EO110004},
  owner = {rojasro},
  timestamp = {2012.05.24}
}

@ARTICLE{wang2005,
  author = {Wang, Guiling},
  title = {Agricultural drought in a future climate: results from 15 global
	climate models participating in the IPCC 4th assessment},
  journal = {Climate Dynamics},
  year = {2005},
  volume = {25},
  pages = {739--753},
  abstract = {This study examines the impact of greenhouse gas warming on soil moisture
	based on predictions of 15 global climate models by comparing the
	after-stabilization climate in the SRESA1b experiment with the pre-industrial
	control climate. The models are consistent in predicting summer dryness
	and winter wetness in only part of the northern middle and high latitudes.
	Slightly over half of the models predict year-round wetness in central
	Eurasia and/or year-round dryness in Siberia and mid-latitude Northeast
	Asia. One explanation is offered that relates such lack of seasonality
	to the carryover effect of soil moisture storage from season to season.
	In the tropics and subtropics, a decrease of soil moisture is the
	dominant response. The models are especially consistent in predicting
	drier soil over the southwest North America, Central America, the
	Mediterranean, Australia, and the South Africa in all seasons, and
	over much of the Amazon and West Africa in the June–July–August (JJA)
	season and the Asian monsoon region in the December–January–February
	(DJF) season. Since the only major areas of future wetness predicted
	with a high level of model consistency are part of the northern middle
	and high latitudes during the non-growing season, it is suggested
	that greenhouse gas warming will cause a worldwide agricultural drought.
	Over regions where there is considerable consistency among the analyzed
	models in predicting the sign of soil moisture changes, there is
	a wide range of magnitudes of the soil moisture response, indicating
	a high degree of model dependency in terrestrial hydrological sensitivity.
	A major part of the inter-model differences in the sensitivity of
	soil moisture response are attributable to differences in land surface
	parameterization.},
  affiliation = {University of Connecticut Department of Civil and Environmental Engineering
	261 Glenbrook Road, U-2037 Storrs CT 06269 USA},
  doi = {10.1007/s00382-005-0057-9},
  issn = {0930-7575},
  issue = {7},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Berlin / Heidelberg}
}

@BOOK{wang1995,
  title = {Introduction to groundwater modelling: {F}inite difference and finite
	element methods},
  publisher = {Academic Press},
  year = {1995},
  author = {Wang, H. and Anderson, M.},
  pages = {237},
  address = {San Diego},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.03.25}
}

@ARTICLE{wang2006,
  author = {Wang, X. and Frankenberger, E. and Kladivko, E.},
  title = {Uncertainties in {DRAINMOD} predictions of subsurface drain flow
	for an Indiana silt loam using the {GLUE} methodology},
  journal = {Hydrological Processes},
  year = {2006},
  volume = {20},
  pages = {3069--3084},
  number = {14},
  abstract = {Good modelling practice requires the incorporation of uncertainty
	analysis into hydrologic/water quality models. The generalized likelihood
	uncertainty estimation procedure was used to evaluate the uncertainty
	in DRAINMOD predictions of daily, monthly, and yearly subsurface
	drain flow. A variance-based sensitivity analysis technique, the
	extended Fourier amplitude sensitivity test, was used to identify
	the main sources of prediction uncertainty. The analysis was conducted
	for the experimental drainage field at the Southeast Purdue Agricultural
	Center in Indiana. Six years of data were used and the uncertainties
	in eight model parameters were considered to analyse how uncertainties
	in input parameters propagate to model outputs. The width of 90%
	confidence interval bands of drain flow ranged from 0 to 0·6 cm day-1
	for daily predictions, from 0 to 3·1 cm month-1 for the monthly predictions,
	and from 7·6 to 12·4 cm year-1 for yearly predictions. Annual drain
	flow predicted by DRAINMOD fell well within the 90% confidence bounds.
	Model results were most sensitive to the vertical saturated hydraulic
	conductivity of the restrictive layer and the lateral hydraulic conductivity
	of the deepest soil layer, followed by the lateral hydraulic conductivity
	of the top soil layer and surface micro-storage. Parameter interactions
	also contributed to the prediction uncertainty.},
  doi = {10.1002/hyp.6080},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@ARTICLE{wang2007,
  author = {Wang, Y. and Dietrich, F. and Voss, F. and Pahlow, M.},
  title = {Identifying and reducing model structure uncertainty based on analysis
	of parameter interaction},
  journal = {Advances in Geosciences},
  year = {2007},
  volume = {11},
  pages = {117--122},
  number = {11},
  abstract = {Multi-objective optimization algorithms are widely used for the calibration
	of conceptual hydrological models. Such algorithms yield a set of
	Pareto-optimal solutions, reflecting the model structure uncertainty.
	In this study, a multi-objective optimization strategy is suggested,
	which aims at reducing the model structure uncertainty by considering
	parameter interaction within Pareto-optimal solutions. The approach
	has been used to develop a nested setup of a rainfall-runoff model,
	which is integrated in a probabilistic meso-/macroscale flood forecasting
	system. The optimization strategy aided in determining the best combination
	of a lumped (computationally efficient in operational real time forecasting)
	and a semi-distributed parameterization of the hydrological model.
	First results are shown for two subbasins of the Mulde catchment
	in Germany. The different phenomena of parameter interaction were
	analysed in this case study to reduce the model structure uncertainties.},
  doi = {10.5194/adgeo-11-117-2007},
  owner = {RRojas},
  timestamp = {2009.02.19}
}

@ARTICLE{wang+al2009a,
  author = {Wang, {Q-J} and Robertson, D. and Haines, C.},
  title = {A Bayesian network approach to knowledge integration and representation
	of farm irrigation: 1. Model development},
  journal = {Water Resources Research},
  year = {2009},
  volume = {45},
  pages = { W02409},
  abstract = {The funding for this work was provided by the Victorian Government
	Water for Growth Initiative through the Goulburn Broken Catchment
	Management Authority and the Department of Sustainability and Environment.
	The authors would like to gratefully acknowledge Ken Sampson, David
	Lawler, and other interviewees for their contribution to the development
	and validation of the farm irrigation systems model.},
  doi = {10.1029/2006WR005419},
  keywords = {WATER-USE EFFICIENCY, NORTHERN VICTORIA, PERENNIAL PASTURE, SALINE
	WATER, DAIRY FARMS, YIELD, MANAGEMENT, SYSTEMS, BALANCE, TREES},
  tags = {Irrigation, Disturbed Catchments}
}

@ARTICLE{ward+al2011,
  author = {Ward, P. J. and de Moel, H. and Aerts, J. C. J. H.},
  title = {How are flood risk estimates affected by the choice of return-periods?},
  journal = {Natural Hazards and Earth System Science},
  year = {2011},
  volume = {11},
  pages = {3181--3195},
  number = {12},
  doi = {10.5194/nhess-11-3181-2011}
}

@ARTICLE{wasserman2000,
  author = {Wasserman, L.},
  title = {Bayesian model selection and model averaging},
  journal = {Journal of Mathematical Psychology},
  year = {2000},
  volume = {44},
  pages = {92--107},
  number = {1},
  abstract = {This paper reviews the Bayesian approach to model selection and model
	averaging. In this review, I emphasize objective Bayesian methods
	based on noninformative priors. I will also discuss implementation
	details, approximations, and relationships to other methods.},
  doi = {10.1006/jmps.1999.1278},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{webster2003,
  author = {Webster, M.},
  title = {Communicating climate change uncertainty to policy--makers and the
	public},
  journal = {Climatic Change},
  year = {2003},
  volume = {61},
  pages = {1--8},
  number = {1--2},
  doi = {10.1023/A:1026351131038},
  tags = {Thesis, Uncertainty}
}

@ARTICLE{webster+al2002,
  author = {Webster, M. and Babiker, M. and Mayer, M. and Reilly, J. and Harnisch,
	J. and Hyman, R. and Sarofim, M. and Wang, C.},
  title = {Uncertainty in emissions projections for climate models},
  journal = {Atmospheric Environment},
  year = {2002},
  volume = {36},
  pages = {3659--3670},
  number = {22},
  abstract = {Future global climate projections are subject to large uncertainties.
	Major sources of this uncertainty are projections of anthropogenic
	emissions. We evaluate the uncertainty in future anthropogenic emissions
	using a computable general equilibrium model of the world economy.
	Results are simulated through 2100 for carbon dioxide (CO2), methane
	(CH4), nitrous oxide (N2O), hydrofluorocarbons (HFCs), perfluorocarbons
	(PFCs) and sulfur hexafluoride (SF6), sulfur dioxide (SO2), black
	carbon (BC) and organic carbon (OC), nitrogen oxides (NOx), carbon
	monoxide (CO), ammonia (NH3) and non-methane volatile organic compounds
	(NMVOCs). We construct mean and upper and lower 95% emissions scenarios
	(available from the authors at 1Â°Ã—1Â° latitude--longitude grid).
	Using the MIT Integrated Global System Model (IGSM), we find a temperature
	change range in 2100 of 0.9 to 4.0Â°C, compared with the Intergovernmental
	Panel on Climate Change emissions scenarios that result in a range
	of 1.3 to 3.6Â°C when simulated through MIT IGSM},
  doi = {10.1016/S1352-2310(02)00245-5},
  tags = {Uncertainty, Thesis}
}

@ARTICLE{webster+al2003,
  author = {Webster, M. and Forest, C. and Reilly, J. and Babiker, M. and Kicklighter,
	D. and Mayer, M. and Prinn, R. and Sarofim, M. and Sokolov, A. and
	Stone, P. and Wang, C.},
  title = {Uncertainty analysis of climate change and policy response},
  journal = {Climatic Change},
  year = {2003},
  volume = {61},
  pages = {295--320},
  number = {3},
  abstract = {To aid climate policy decisions, accurate quantitative descriptions
	of the uncertainty in climate outcomes under various possible policies
	are needed. Here, we apply an earth systems model to describe the
	uncertainty in climate projections under two different policy scenarios.
	This study illustrates an internally consistent uncertainty analysis
	of one climate assessment modeling framework, propagating uncertainties
	in both economic and climate components, and constraining climate
	parameter uncertainties based on observation. We find that in the
	absence of greenhouse gas emissions restrictions, there is a one
	in forty chance that global mean surface temperature change will
	exceed 4.9 Â°C by the year 2100. A policy case with aggressive emissions
	reductions over time lowers the temperature change to a one in forty
	chance of exceeding 3.2 Â°C, thus reducing but not eliminating the
	chance of substantial warming},
  doi = {10.1023/B:CLIM.0000004564.09961.9f},
  tags = {Uncertainty, Thesis}
}

@BOOK{webster2007,
  title = {Geostatistics for environment scientists},
  publisher = {John WIley \& Sons},
  year = {2007},
  author = {Webster, R. and Oliver, M.},
  address = {Chichester},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.03.27}
}

@TECHREPORT{WATCH-22-2010,
  author = {Weedon, G. and Gomes, S. and Viterbo, P. and Osterle, H. and Adam,
	J. and Bellouin, N. and Boucher, O. and Best},
  title = {{The WATCH forcing data 1958–2001: a meteorological forcing dataset
	land-surface and hydrological-models}},
  institution = {{WATCH project, Technical Report Nr. 22}},
  year = {2010},
  owner = {rojasro},
  timestamp = {2011.03.29}
}

@ARTICLE{weigel+al2009,
  author = {Weigel, A. and Liniger, M. and Appenzeller, C.},
  title = {Seasonal ensemble forecasts: {A}re recalibrated single models better
	than multimodels\?},
  journal = {Monthly Weather Review},
  year = {2009},
  volume = {137},
  pages = {1460--1479},
  number = {4},
  month = {April},
  abstract = {Multimodel ensemble combination (MMEC) has become an accepted technique
	to improve probabilistic forecasts from short- to long-range time
	scales. MMEC techniques typically widen ensemble spread, thus improving
	the dispersion characteristics and the reliability of the forecasts.
	This raises the question as to whether the same effect could be achieved
	in a potentially cheaper way by rescaling single model ensemble forecasts
	a posteriori such that they become reliable. In this study a climate
	conserving recalibration (CCR) technique is derived and compared
	with MMEC. With a simple stochastic toy model it is shown that both
	CCR and MMEC successfully improve forecast reliability. The difference
	between these two methods is that CCR conserves resolution but inevitably
	dilutes the potentially predictable signal while MMEC is in the ideal
	case able to fully retain the predictable signal and to improve resolution.
	Therefore, MMEC is conceptually to be preferred, particularly since
	the effect of CCR depends on the length of the data record and on
	distributional assumptions. In reality, however, multimodels consist
	only of a finite number of participating single models, and the model
	errors are often correlated. Under such conditions, and depending
	on the skill metric applied, CCR-corrected single models can on average
	have comparable skill as multimodel ensembles, particularly when
	the potential model predictability is low. Using seasonal near-surface
	temperature and precipitation forecasts of three models of the Development
	of a European Multimodel Ensemble System for Seasonal-to-Interannual
	Prediction (DEMETER) dataset, it is shown that the conclusions drawn
	from the toy-model experiments hold equally in a real multimodel
	ensemble prediction system. All in all, it is not possible to make
	a general statement on whether CCR or MMEC is the better method.
	Rather it seems that optimum forecasts can be obtained by a combination
	of both methods, but only if first MMEC and then CCR is applied.
	The opposite order—first CCR, then MMEC—is shown to be of only little
	effect, at least in the context of seasonal forecasts.},
  doi = {10.1175/2008MWR2773.1},
  owner = {rojasro},
  timestamp = {2010.07.30}
}

@ARTICLE{weigel2008,
  author = {Weigel, A. and Liniger, M. and Appenzeller, C.},
  title = {Can multi--model combination really enhance the prediction skill
	of probabilistic ensemble forecasts\?},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  year = {2008},
  volume = {134},
  pages = {241--260},
  number = {630},
  abstract = {The success of multi-model ensemble combination has been demonstrated
	in many studies. Given that a multi-model contains information from
	all participating models, including the less skilful ones, the question
	remains as to why, and under what conditions, a multi-model can outperform
	the best participating single model. It is the aim of this paper
	to resolve this apparent paradox. The study is based on a synthetic
	forecast generator, allowing the generation of perfectly-calibrated
	single-model ensembles of any size and skill. Additionally, the degree
	of ensemble under-dispersion (or overconfidence) can be prescribed.
	Multi-model ensembles are then constructed from both weighted and
	unweighted averages of these single-model ensembles. Applying this
	toy model, we carry out systematic model-combination experiments.
	We evaluate how multi-model performance depends on the skill and
	overconfidence of the participating single models. It turns out that
	multi-model ensembles can indeed locally outperform a best-model
	approach, but only if the single-model ensembles are overconfident.
	The reason is that multi-model combination reduces overconfidence,
	i.e. ensemble spread is widened while average ensemble-mean error
	is reduced. This implies a net gain in prediction skill, because
	probabilistic skill scores penalize overconfidence. Under these conditions,
	even the addition of an objectively-poor model can improve multi-model
	skill. It seems that simple ensemble inflation methods cannot yield
	the same skill improvement.},
  doi = {10.1002/qj.210},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{weiss+al2007,
  author = {Wei{\ss}, M. and Fl\"orke, M. and Menzel, L. and Alcamo, J.},
  title = {{Model-based scenarios of Mediterranean droughts}},
  journal = {Advances In Geosciences},
  year = {2007},
  volume = {12},
  pages = {145--151},
  abstract = {This study examines the change in current 100-year hydrological drought
	frequencies in the Mediterranean in comparison to the 2070s as simulated
	by the global model WaterGAP. The analysis considers socio-economic
	and climate changes as indicated by the IPCC scenarios A2 and B2
	and the global general circulation model ECHAM4. Under these conditions
	today's 100-year drought is estimated to occur 10 times more frequently
	in the future over a large part of the Northern Mediterranean while
	in North Africa, today's 100-year drought will occur less frequently.
	Water abstractions are shown to play a minor role in comparison to
	the impact of climate change, but can intensify the situation.},
  doi = {10.5194/adgeo-12-145-2007},
  owner = {rojasro},
  timestamp = {2012.07.05}
}

@ARTICLE{werritty2002,
  author = {Werritty, A.},
  title = {Living with uncertainty: {C}limate change, river flows and water
	resource management in {S}cotland},
  journal = {The Science of The Total Environment},
  year = {2002},
  volume = {294},
  pages = {29--40},
  number = {1--3},
  abstract = {The recent increased variability of Scotland's hydroclimate poses
	major problems for water resource managers charged with making informed
	investment decisions given the likely impact of future climate change.
	Two strategies are developed in this paper to assist managers faced
	with this environmental uncertainty. The first involves trend analysis
	of precipitation and runoff since the 1960s and 1970s viewed against
	longer-term variability reported from instrumental records. The second
	strategy is based upon current climate change scenarios coupled with
	GCMs, and downscaling of precipitation and temperature to provide
	inputs to rainfall-runoff models. The long-term records of precipitation
	(back to the 1860s) and runoff (back to the 1930s) reveal the late
	1980s and early 1990s as the wettest period on record for the west
	but not for the east. Over the period 1961--1996 the precipitation
	gradient has intensified across Scotland: wetter west; relatively
	dry east. Changes in runoff over the period 1970--1996 are also reported
	with increases in annual flows at 33 out of 38 stations (significantly
	at 12 stations) and decreases in low flows at 21 out of 38 stations
	(significantly at one station). The bulk of these flow increases
	occurred in the south and west especially in the autumn and spring.
	In terms of high flows over the period 1970--1996, four out of 44
	stations reported a change in magnitude and 15 reported an increase
	in the frequency of POT events. In terms of future climate change,
	Hulme and Jenkins (1998) predict annual precipitation increases of
	6--16% (Scotland) and 6--14% (Scottish Borders) from the 2020s to
	the 2080s based on the Hadley Centre model (HadCM2) medium--high
	scenario. Seasonal changes are concentrated in the autumn (SON) and
	winter (DJF) with increases as high as 24 and 29% for the autumn
	by the 2080s. ( Arnell NW, et al. Institute of Hydrology Report No.
	107, Wallingford, 1996), using an earlier transient Hadley experiment
	(IS92a), predict a 5--15% increase in annual runoff across Scotland
	by the 2050s, locally rising to 25%. Simulation flow duration curves
	for the 2050s generate Q95 values up by 5% or less (Rivers Don, Almond
	and Nith) and Q5 up by 10--24% (Rivers Don, Almond, Nith and Lyne
	Water). In terms of water resource planning, these predicted changes
	should be regarded as first order approximations, as they take no
	account of natural climatic variability, and could generate different
	absolute values if other scenarios were used. The predictions are,
	however, broadly consistent with trends in precipitation and runoff
	for Scotland since the 1970s. Major issues of concern to water resource
	managers are identified and commented upon in the light of these
	predictions},
  doi = {10.1016/S0048-9697(02)00050-5},
  keywords = {Climate change, River flows, Water resources, Scotland},
  tags = {Uncertainty}
}

@ARTICLE{westerberg+al2011,
  author = {Westerberg, I. and Guerrero, {J-L} and Younger, P. and Beven, K.
	and Seibert, J. and Halldin, S. and Freer, J. and Xu, {C-Y}},
  title = {Calibration of hydrological models using flow-duration curves},
  journal = {Hydrology and Earth System Sciences Discussions},
  year = {2010},
  volume = {7},
  pages = {9467--9522},
  number = {6},
  abstract = {The degree of belief we have in predictions from hydrologic models
	depends on how well they can reproduce observations. Calibrations
	with traditional performance measures such as the Nash-Sutcliffe
	model efficiency are challenged by problems including: (1) uncertain
	discharge data, (2) variable importance of the performance with flow
	magnitudes, (3) influence of unknown input/output errors and (4)
	inability to evaluate model performance when observation time periods
	for discharge and model input data do not overlap. A new calibration
	method using flow-duration curves (FDCs) was developed which addresses
	these problems. The method focuses on reproducing the observed discharge
	frequency distribution rather than the exact hydrograph. It consists
	of applying limits of acceptability for selected evaluation points
	(EPs) of the observed uncertain FDC in the extended GLUE approach.
	Two ways of selecting the EPs were tested {--} based on equal intervals
	of discharge and of volume of water. The method was tested and compared
	to a calibration using the traditional model efficiency for the daily
	four-parameter WASMOD model in the Paso La Ceiba catchment in Honduras
	and for Dynamic TOPMODEL evaluated at an hourly time scale for the
	Brue catchment in Great Britain. The volume method of selecting EPs
	gave the best results in both catchments with better calibrated slow
	flow, recession and evaporation than the other criteria. Observed
	and simulated time series of uncertain discharges agreed better for
	this method both in calibration and prediction in both catchments
	without resulting in overpredicted simulated uncertainty. An advantage
	with the method is that the rejection criterion is based on an estimation
	of the uncertainty in discharge data and that the EPs of the FDC
	can be chosen to reflect the aims of the modelling application e.g.
	using more/less EPs at high/low flows. While the new method is less
	sensitive to epistemic input/output errors than the normal use of
	limits of acceptability applied directly to the time series of discharge,
	it still requires a reasonable representation of the distribution
	of inputs. Additional constraints might therefore be required in
	catchments subject to snow. The results suggest that the new calibration
	method can be useful when observation time periods for discharge
	and model input data do not overlap. The new method could also be
	suitable for calibration to regional FDCs while taking uncertainties
	in the hydrological model and data into account.},
  doi = {10.5194/hessd-7-9467-2010},
  tags = {Calibration, FDC}
}

@INCOLLECTION{wheater2008,
  author = {Wheater, H.},
  title = {Modelling hydrological process in arid and semi--arid areas: {A}n
	introduction to the workshop},
  booktitle = {Hydrological Modelling in Arid and Semi-Arid Areas},
  publisher = {Cambridge University Press.},
  year = {2008},
  editor = {Howard Wheater and Soroosh Sorooshian and K. D. Sharma},
  tags = {conceptual model}
}

@INCOLLECTION{wheater+al2008,
  author = {Wheater, H. and {McIntire}, N. and Wagener, T.},
  title = {Calibration, Uncertainty, and regional analysis of conceptual rainfall-runoff
	models},
  booktitle = {Hydrological Modelling in Arid and Semi-Arid Areas},
  publisher = {Cambridge University Press.},
  year = {2008},
  editor = {Howard Wheater and Soroosh Sorooshian and K. D. Sharma},
  tags = {Calibration, Uncertainty}
}

@ARTICLE{whitechaubey2005,
  author = {White, K. and Chaubey, I.},
  title = {Sensitivity Analysis, Calibrations, and Validations for a Multisite
	and Multivariable {SWAT} Model},
  journal = {Journal of the American Water Resources Association},
  year = {2005},
  volume = {41},
  pages = {1077--1089},
  number = {5},
  abstract = {The ability of a watershed model to mimic specified watershed processes
	is assessed through the calibration and validation process. The Soil
	and Water Assessment Tool (SWAT) watershed model was implemented
	in the Beaver Reservoir Watershed of Northwest Arkansas. The objectives
	were to: (1) provide detailed information on calibrating and applying
	a multisite and multivariable SWAT model; (2) conduct sensitivity
	analysis; and (3) perform calibration and validation at three different
	sites for flow, sediment, total phosphorus (TP), and nitrate-nitrogen
	(NO3-N) plus nitrite-nitrogen (NO2-N). Relative sensitivity analysis
	was conducted to identify parameters that most influenced predicted
	flow, sediment, and nutrient model outputs. A multi objective function
	was defined that consisted of optimizing three statistics: percent
	relative error (RE), Nash-Sutcliffe Coefficient (RNS2), and coefficient
	of determination (R2). This function was used to successfully calibrate
	and validate a SWAT model of Beaver Reservoir Watershed at multi-sites
	while considering multivariables. Calibration and validation of the
	model is a key factor in reducing uncertainty and increasing user
	confidence in its predictive abilities, which makes the application
	of the model effective. Information on calibration and validation
	of multisite, multivariable SWAT models has been provided to assist
	watershed modelers in developing their models to achieve watershed
	management goals.},
  bibkey = {modeling; water quality; nonpoint source pollution; nutrients; SWAT
	model; sensitivity analysis; agriculture},
  doi = {10.1111/j.1752-1688.2005.tb03786.x},
  tags = {Sensitivity Analysis, SWAT}
}

@ARTICLE{whittemore2002,
  author = {Whittemore, R.},
  title = {Discussion - "Validation of the SWAT model on a large river basin
	with point and nonpoint sources," by C. Santhi, J.G. Arnold, J.R.
	Williams, W. A. Dugas, R. Srinivasan, and L.M. Hauck},
  journal = {Journal of the American Water Resources Association},
  year = {2002},
  volume = {38},
  pages = {1767--1768},
  abstract = {The State of Texas has initiated the development of a Total Maximum
	Daily Load program in the Bosque River Watershed, where point and
	nonpoint sources of pollution are a concern. Soil Water Assessment
	Tool (SWAT) was validated for flow, sediment, and nutrients in the
	watershed to evaluate alternative management scenarios and estimate
	their effects in controlling pollution. This paper discusses the
	calibration and validation at two locations, Hico and Valley Mills,
	along the North Bosque River. Calibration for flow was performed
	from 1960 through 1998. Sediment and nutrient calibration was done
	from 1993 through 1997 at Hico and from 1996 through 1997 at Valley
	Mills. Model validation was performed for 1998. Time series plots
	and statistical measures were used to verify model predictions. Predicted
	values generally matched well with the observed values during calibration
	and validation (R-2 greater than or equal to 0.6 and Nash-Suttcliffe
	Efficiency greater than or equal to 0.5, in most instances) except
	for some underprediction of nitrogen during calibration at both locations
	and sediment and organic nutrients during validation at Valley Mills.
	This study showed that SWAT was able to predict flow, sediment, and
	nutrients successfully and can be used to study the effects of alternative
	management scenarios.},
  doi = {10.1111/j.1752-1688.2002.tb01003.x},
  keywords = {watershed management, total maximum daily load, erosion, sedimentation,
	phosphorus loading, dairy manure management, BALANCE, QUALITY, FLOW},
  tags = {SWAT}
}

@ARTICLE{wigleyRaper2001,
  author = {Wigley, T. and Raper, S.},
  title = {Interpretation of high projections for global--mean warming},
  journal = {Science},
  year = {2001},
  volume = {293},
  pages = {451--454},
  number = {5529},
  abstract = {The Intergovernmental Panel on Climate Change (IPCC) has recently
	released its Third Assessment Report (TAR), in which new projections
	are given for global-mean warming in the absence of policies to limit
	climate change. The full warming range over 1990 to 2100, 1.4Â° to
	5.8Â°C, is substantially higher than the range given previously in
	the IPCC Second Assessment Report. Here we interpret the new warming
	range in probabilistic terms, accounting for uncertainties in emissions,
	the climate sensitivity, the carbon cycle, ocean mixing, and aerosol
	forcing. We show that the probabilities of warming values at both
	the high and low ends of the TAR range are very low. In the absence
	of climate-mitigation policies, the 90% probability interval for
	1990 to 2100 warming is 1.7Â° to 4.9Â°C},
  doi = {10.1126/science.1061604},
  pmid = {11463906},
  tags = {Climate Models}
}

@ARTICLE{wilby2005,
  author = {Wilby, R.},
  title = {Uncertainty in water resource model parameters used for climate change
	impact assessment},
  journal = {Hydrological Processes},
  year = {2005},
  volume = {19},
  pages = {3201--3219},
  number = {16},
  abstract = {Despite their acknowledged limitations, lumped conceptual models continue
	to be used widely for climate-change impact assessments. Therefore,
	it is important to understand the relative magnitude of uncertainties
	in water resource projections arising from the choice of model calibration
	period, model structure, and non-uniqueness of model parameter sets.
	In addition, external sources of uncertainty linked to choice of
	emission scenario, climate model ensemble member, downscaling technique(s),
	and so on, should be acknowledged. To this end, the CATCHMOD conceptual
	water balance model was used to project changes in daily flows for
	the River Thames at Kingston using parameter sets derived from different
	subsets of training data, including the full record. Monte Carlo
	sampling was also used to explore parameter stability and identifiability
	in the context of historic climate variability. Parameters reflecting
	rainfall acceptance at the soil surface in simpler model structures
	were found to be highly sensitive to the training period, implying
	that climatic variability does lead to variability in the hydrologic
	behaviour of the Thames basin. Non-uniqueness of parameters for more
	complex model structures results in relatively small variations in
	projected annual mean flow quantiles for different training periods
	compared with the choice of emission scenario. However, this was
	not the case for subannual flow statistics, where uncertainty in
	flow changes due to equifinality was higher in winter than summer,
	and comparable in magnitude to the uncertainty of the emission scenario.
	Therefore, it is recommended that climate-change impact assessments
	using conceptual water balance models should routinely undertake
	sensitivity analyses to quantify uncertainties due to parameter instability,
	identifiability and non-uniqueness. Copyright (c) 2005 John Wiley
	\& Sons, Ltd.},
  doi = {10.1002/hyp.5819},
  keywords = {hydrologic model, parameter stability, climate change, uncertainty,
	River Thames, GENERAL-CIRCULATION MODEL, SENSITIVITY-ANALYSIS, HYDROLOGICAL
	MODELS, LAND-USE, VARIABILITY, RUNOFF, BRITAIN, FUTURE, FLOWS, IDENTIFICATION},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{wilbyHarris2006,
  author = {Wilby, R. and Harris, I.},
  title = {A framework for assessing uncertainties in climate change impacts:
	{L}ow--flow scenarios for the {R}iver {T}hames, {UK}},
  journal = {Water Resources Research},
  year = {2006},
  volume = {42},
  pages = {W02419},
  number = {2},
  abstract = {A probabilistic framework is presented for combining information from
	an ensemble of four general circulation models (GCMs), two greenhouse
	gas emission scenarios, two statistical downscaling techniques, two
	hydrological model structures, and two sets of hydrological model
	parameters. GCMs were weighted according to an index of reliability
	for downscaled effective rainfall, a key determinant of low flows
	in the River Thames. Hydrological model structures were weighted
	by performance at reproducing annual low-flow series. Weights were
	also assigned to sets of water resource model (CATCHMOD) parameters
	using the Nash-Sutcliffe efficiency criterion. Emission scenarios
	and downscaling methods were unweighted. A Monte Carlo approach was
	then used to explore components of uncertainty affecting projections
	for the River Thames by the 2080s. The resulting cumulative distribution
	functions (CDFs) of low flows were most sensitive to uncertainty
	in the climate change scenarios and downscaling of different GCMs.
	Uncertainties due to the hydrological model parameters and emission
	scenario increase with time but were less important. Abrupt changes
	in low-flow CDFs were attributed to uncertainties in statistically
	downscaled summer rainfall. This was linked to different behavior
	of atmospheric moisture among the chosen GCMs},
  doi = {10.1029/2005WR004065},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{wilbyetal2000,
  author = {Wilby, R. and Hay, L. and {Gutowski Jr.}, W. and Arritt, R. and Takle,
	E. and Pan, Z. and Leavesley, G. and Clark, M.},
  title = {Hydrological responses to dynamically and statistically downscaled
	climate model output},
  journal = {Geophysical Research Letters},
  year = {2000},
  volume = {27},
  pages = {1199--1202},
  number = {8},
  abstract = {Daily rainfall and surface temperature series were simulated for the
	Animas River basin, Colorado using dynamically and statistically
	downscaled output from the National Center for Environmental Prediction/
	National Center for Atmospheric Research (NCEP/NCAR) re-analysis.
	A distributed hydrological model was then applied to the downscaled
	data. Relative to raw NCEP output, downscaled climate variables provided
	more realistic simulations of basin scale hydrology. However, the
	results highlight the sensitivity of modeled processes to the choice
	of downscaling technique, and point to the need for caution when
	interpreting future hydrological scenarios.},
  doi = {10.1029/1999GL006078},
  tags = {Downscaling}
}

@ARTICLE{wilby+al1999,
  author = {Wilby, R. and Hay, L. and Leavesley, G.},
  title = {A comparison of downscaled and raw {GCM} output: implications for
	climate change scenarios in the {S}an {J}uan {R}iver basin, {C}olorado},
  journal = {Journal of Hydrology},
  year = {1999},
  volume = {225},
  pages = {67--91},
  number = {1--2},
  abstract = {The fundamental rationale for statistical downscaling is that the
	raw outputs of climate change experiments from General Circulation
	Models (GCMs) are an inadequate basis for assessing the effects of
	climate change on land-surface processes at regional scales. This
	is because the spatial resolution of GCMs is too coarse to resolve
	important sub-grid scale processes (most notably those pertaining
	to the hydrological cycle) and because GCM output is often unreliable
	at individual and sub-grid box scales. By establishing empirical
	relationships between grid-box scale circulation indices (such as
	atmospheric vorticity and divergence) and sub-grid scale surface
	predictands (such as precipitation), statistical downscaling has
	been proposed as a practical means of bridging this spatial difference.
	This study compared three sets of current and future rainfall-runoff
	scenarios. The scenarios were constructed using: (1) statistically
	downscaled GCM output; (2) raw GCM output; and (3) raw GCM output
	corrected for elevational biases. Atmospheric circulation indices
	and humidity variables were extracted from the output of the UK Meteorological
	Office coupled ocean-atmosphere GCM (HadCM2) in order to downscale
	daily precipitation and temperature series for the Animas River in
	the San Juan River basin, Colorado. Significant differences arose
	between the modelled snowpack and how regimes of the three future
	climate scenarios. Overall, the raw GCM output suggests larger reductions
	in winter/spring snowpack and summer runoff than the downscaling,
	relative to current conditions. Further research is required to determine
	the generality of the water resource implications for other regions,
	GCM outputs and downscaled scenarios. (C) 1999 Elsevier Science B.V.
	All rights reserved.},
  doi = {10.1016/S0022-1694(99)00136-5},
  keywords = {climate change, downscaling, runoff, snowpack, general circulation
	model, Colorado, GENERAL-CIRCULATION MODEL, LOCAL WEATHER, BRITISH-ISLES,
	NORTH-AMERICA, PRECIPITATION, VALIDATION, SIMULATIONS, RAINFALL,
	PATTERNS, INDEXES},
  tags = {Downscaling}
}

@ARTICLE{wilby+al2006,
  author = {Wilby, R. and Whitehead, P. and Wade, A. and Butterfield, D. and
	Davis, R. and Watts, G.},
  title = {Integrated modelling of climate change impacts on water resources
	and quality in a lowland catchment: {R}iver {K}ennet, {UK}},
  journal = {Journal of Hydrology},
  year = {2006},
  volume = {330},
  pages = {204--220},
  number = {1--2},
  abstract = {An integrated approach to climate change impact assessment is explored
	by linking established models of regional climate (SDSM), water resources
	(CATCHMOD) and water quality (INCA) within a single framework. A
	case study of the River Kennet illustrates how the system can be
	used to investigate aspects of climate change uncertainty, deployable
	water resources, and water quality dynamics in upper and lower reaches
	of the drainage network. The results confirm the large uncertainty
	in climate change scenarios and freshwater impacts due to the choice
	of general circulation model (GCM). This uncertainty is shown to
	be greatest during summer months as evidenced by large variations
	between GCM-derived projections of future low river flows, deployable
	yield from groundwater, severity of nutrient flushing episodes, and
	long-term trends in surface water quality. Other impacts arising
	from agricultural land-use reform or delivery of EU Water Framework
	Directive objectives under climate change could be evaluated using
	the same framework.},
  doi = {10.1016/j.jhydrol.2006.04.033},
  keywords = {Climate change, Uncertainty, Downscaling, Water resources, Water quality,
	River Kennet},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{wilbywigley1997,
  author = {Wilby, R. and Wigley, T.},
  title = {Downscaling general circulation model output: a review of methods
	and limitations},
  journal = {Progress in Physical Geography},
  year = {1997},
  volume = {21},
  pages = {530--548},
  number = {4},
  abstract = {General circulation models (GCMs) suggest that rising concentrations
	of greenhouse gases may have significant consequences for the global
	climate. What is less clear is the extent to which local (subgrid)
	scale meteorological processes will be affected. So-called 'downscaling'
	techniques have subsequently emerged as a means of bridging the gap
	between what climate modellers are currently able to provide and
	what impact assessors require. This article reviews the present generation
	of downscaling tools under four main headings: regression methods;
	weather pattern (circulation)-based approaches; stochastic weather
	generators; and limited-area climate models. The penultimate section
	summarizes the results of an international experiment to intercompare
	several precipitation models used for downscaling. It shows that
	circulation-based downscaling methods perform well in simulating
	present observed and model-generated daily precipitation characteristics,
	but are able to capture only part of the daily precipitation variability
	changes associated with model-derived changes in climate. The final
	section examines a number of ongoing challenges to the future development
	of climate downscaling},
  doi = {10.1177/030913339702100403},
  tags = {Downscaling}
}

@INCOLLECTION{willmott1984,
  author = {Willmott, C.},
  title = {On the evaluation of model performance in physical geography},
  booktitle = {Spatial Statistics and Models},
  publisher = {Dordrecht, Holland: D. Reidel},
  year = {1984},
  editor = {G. L. Gaile and C. J. Willmott},
  pages = {443--460},
  bibkey = {"index of agreement"},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{willmott1981,
  author = {Willmott, C.},
  title = {On the validation of models},
  journal = {Physical Geography},
  year = {1981},
  volume = {2},
  pages = {184--194},
  keywords = {Index of Agreement, Calibration, goodness-of-fit},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{willmott+al1985,
  author = {Willmott, C. and Ackleson, S. and Davis, R. and Feddema, J. and Klink,
	K. and Legates, D. and {O'Donnell}, J. and Rowe, C.},
  title = {{S}tatistics for the {E}valuation and {C}omparison of {M}odels},
  journal = {Journal of Geophysical Research},
  year = {1985},
  volume = {90},
  pages = {8995--9005},
  number = {C5},
  abstract = {Procedures that may be used to evaluate the operational performance
	of a wide spectrum of geophysical models are introduced. Primarily
	using a complementary set of difference measures, both model acccuracy
	and precision can be meaningfully estimated, regardless of whether
	the model predicitons are manifested as scalars, directions, or vectors.
	It is additionally suggested that the reliability of the accuracy
	and precision measures can be determined from bootstrap estimates
	of confidence and significance. Recommended procedures are illustrated
	with a comparative evaluation of two models that estimate wind velocity
	over the South Atlantic Bight. },
  doi = {10.1029/JC090iC05p08995},
  keywords = {Index of Agreement, Calibration, gooness-of-fit},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{wilson+al2010,
  author = {Donna Wilson and Hege Hisdal and Deborah Lawrence},
  title = {Has streamflow changed in the Nordic countries? – Recent trends and
	comparisons to hydrological projections},
  journal = {Journal of Hydrology},
  year = {2010},
  volume = {394},
  pages = {334--346},
  number = {3–-4},
  abstract = {A pan-Nordic dataset of 151 streamflow records was analysed to detect
	spatial and temporal changes in streamflow. Prior to undertaking
	analyses, all streamflow records with significant levels of autocorrelation
	were pre-whitened to remove the adverse effect of temporal autocorrelation
	on the test results. The Mann–Kendall trend test was applied to study
	changes in annual and seasonal streamflow as well as floods and droughts
	for three periods: 1920–2005, 1941–2005 and 1961–2000. Field significance
	was evaluated to determine the percentage of stations that are expected
	to show a trend due to the effect of cross-correlation. The period
	analysed and the selection of stations influenced the regional patterns
	found, but the overall picture was that trends of increased streamflow
	dominate annual values and the winter and spring seasons. Trends
	identified in summer flows differed between the three periods analysed,
	whereas no trend was found for the autumn season. In all three periods,
	a signal towards earlier snowmelt floods was clear, as was the tendency
	towards more severe summer droughts in southern and eastern Norway.
	These trends in streamflow result from changes in both temperature
	and precipitation, but the temperature induced signal is stronger
	than precipitation influences. This is evident because the observed
	trends in winter and spring, where snowmelt is the dominant process,
	are greater than the annual trends. A qualitative comparison of the
	findings with available streamflow projections for the region showed
	that the strongest trends found are generally consistent with future
	changes expected in the projection periods, for example increased
	winter discharge and earlier snowmelt floods. However, there are
	predicted changes that are not reflected in past trends, such as
	the expected increase in autumn discharge in Norway. Hence, the changes
	expected because of increased temperatures are reflected in the observed
	trends, whereas changes anticipated due to increases in precipitation
	are not.},
  doi = {10.1016/j.jhydrol.2010.09.010},
  issn = {0022-1694},
  keywords = {Streamflow trends}
}

@ARTICLE{winsemius+al2006,
  author = {Winsemius, W. and Savenije, H. and Gerrits, A. and Zapreeva, A. and
	Klees, R.},
  title = {Comparison of two model approaches in the Zambezi river basin with
	regard to model reliability and identifiability},
  journal = {Hydrology and Earth System Sciences},
  year = {2006},
  volume = {10},
  pages = {339--352},
  abstract = {Variations of water stocks in the upper Zambezi river basin have been
	determined by 2 different hydrological modelling approaches. The
	purpose was to provide preliminary terrestrial storage estimates
	in the upper Zambezi, which will be compared with estimates derived
	from the Gravity Recovery And Climate Experiment (GRACE) in a future
	study. The first modelling approach is GIS-based, distributed and
	conceptual (STREAM). The second approach uses Lumped Elementary Watersheds
	identified and modelled conceptually (LEW). The STREAM model structure
	has been assessed using GLUE (Generalized Likelihood Uncertainty
	Estimation) a posteriori to determine parameter identifiability.
	The LEW approach could, in addition, be tested for model structure,
	because computational efforts of LEW are low.},
  doi = {10.5194/hess-10-339-2006},
  keywords = {EQUIFINALITY, CALIBRATION, RAINFALL, SYSTEMS, CLIMATE, SCALE, AREAS,
	NILE, SET},
  tags = {conceptual model}
}

@ARTICLE{woldeamlak2007,
  author = {Woldeamlak, S. and Batelaan, O. and {De Smedt}, F.},
  title = {Effects of climate change on the groundwater system in the {Grote}--{Nete}
	catchment, {Belgium}},
  journal = {Hydrogeology Journal},
  year = {2007},
  volume = {15},
  pages = {891--901},
  number = {5},
  abstract = {The effects of climate change on the groundwater systems in the Grote-Nete
	catchment, Belgium, covering an area of 525 km2, is modeled using
	wet (greenhouse), cold or NATCC (North Atlantic Thermohaline Circulation
	Change) and dry climate scenarios. Low, central and high estimates
	of temperature changes are adopted for wet scenarios. Seasonal and
	annual water balance components including groundwater recharge are
	simulated using the WetSpass model, while mean annual groundwater
	elevations and discharge are simulated with a steady-state MODFLOW
	groundwater model. WetSpass results for the wet scenarios show that
	wet winters and drier summers are expected relative to the present
	situation. MODFLOW results for wet high scenario show groundwater
	levels increase by as much as 79 cm, which could affect the distribution
	and species richness of meadows. Results obtained for cold scenarios
	depict drier winters and wetter summers relative to the present.
	The dry scenarios predict dry conditions for the whole year. There
	is no recharge during the summer, which is mainly attributed to high
	evapotranspiration rates by forests and low precipitation. Average
	annual groundwater levels drop by 0.5 m, with maximum of 3.1 m on
	the eastern part of the Campine Plateau. This could endanger aquatic
	ecosystem, shrubs, and crop production.},
  doi = {10.1007/s10040-006-0145-x},
  owner = {rojasro},
  timestamp = {2010.06.21}
}

@ARTICLE{wood+al2004,
  author = {Wood, A. and Leung, L. and Sridhar, V. and Lettenmaier, D.},
  title = {Hydrologic implications of dynamical and statistical approaches to
	downscaling climate model outputs},
  journal = {Climatic Change},
  year = {2004},
  volume = {62},
  pages = {189--216},
  number = {1--3},
  abstract = {Six approaches for downscaling climate model outputs for use in hydrologic
	simulation were evaluated, with particular emphasis on each method's
	ability to produce precipitation and other variables used to drive
	a macroscale hydrology model applied at much higher spatial resolution
	than the climate model. Comparisons were made on the basis of a twenty-year
	retrospective (1975--1995) climate simulation produced by the NCAR-DOE
	Parallel ClimateModel (PCM), and the implications of the comparison
	for a future(2040--2060) PCM climate scenario were also explored.
	The six approaches were made up of three relatively simple statistical
	downscaling methods -- linear interpolation (LI), spatial disaggregation
	(SD), and bias-correction and spatial disaggregation (BCSD) -- each
	applied to both PCM output directly(at T42 spatial resolution), and
	after dynamical downscaling via a Regional Climate Model (RCM --
	at 1/2-degree spatial resolution), for downscaling the climate model
	outputs to the 1/8-degree spatial resolution of the hydrological
	model. For the retrospective climate simulation, results were compared
	to an observed gridded climatology of temperature and precipitation,
	and gridded hydrologic variables resulting from forcing the hydrologic
	model with observations. The most significant findings are that the
	BCSD method was successful in reproducing the main features of the
	observed hydrometeorology from the retrospective climate simulation,
	when applied to both PCM and RCM outputs. Linear interpolation produced
	better results using RCM output than PCM output, but both methods
	(PCM-LI and RCM-LI) lead to unacceptably biased hydrologic simulations.
	Spatial disaggregation of the PCM output produced results similar
	to those achieved with the RCM interpolated output; nonetheless,
	neither PCM nor RCM output was useful for hydrologic simulation purposes
	without a bias-correction step. For the future climate scenario,
	only the BCSD-method (using PCM or RCM) was able to produce hydrologically
	plausible results. With the BCSD method, the RCM-derived hydrology
	was more sensitive to climate change than the PCM-derived hydrology.},
  doi = {10.1023/B:CLIM.0000013685.99609.9e},
  tags = {Downscaling, Thesis}
}

@ARTICLE{wriedt2009a,
  author = {Wriedt, G. and {Van der Velde}, M. and Aloe, A. and Bouraoui, F.},
  title = {Estimating irrigation water requirements in {Europe}},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {373},
  pages = {527--544},
  number = {3--4},
  month = {July},
  abstract = {In Southern Europe, irrigated agriculture is by far the largest consumer
	of freshwater resources. However, consistent information on irrigation
	water use in the European Union is still lacking. We applied the
	crop growth model EPIC to calculate irrigation requirements in the
	EU and Switzerland, combining available regional statistics on crop
	distribution and crop specific irrigated area with spatial data sources
	on soils, land use and climate. The model was applied at a 10 × 10
	km grid using different irrigation strategies over a period of 8
	years. The irrigation requirements reflect the spatial distribution
	of irrigated areas, climatic conditions and crops. Simulated net
	irrigation requirements range from 53 mm/yr in Denmark to 1120 mm/yr
	in Spain, translating into estimated volumetric net irrigation requirements
	of 107 mio. m3 and 35,919 mio. m3, respectively. We estimate gross
	irrigation demands to be 1.3–2.5 times higher than field requirements,
	depending on the efficiency of transport and irrigation management.
	A comparison with national and regional data on water abstractions
	for irrigation illustrates the information deficit related to currently
	available reported data, as not only model limitations but also different
	national approaches, country-specific uncertainties (illegal or unrecorded
	abstractions), and restrictions of actual water use come into play.
	In support of European environmental and agricultural policies, this
	work provides a large-scale overview on irrigation water requirements
	in Europe applying a uniform approach with a sufficiently high spatial
	resolution to support identification of hot spots and regional comparisons.
	It will also provide a framework for national irrigation water use
	estimations and supports further analysis of agricultural pressures
	on water quantity in Europe.},
  doi = {10.1016/j.jhydrol.2009.05.018},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{wriedt2009b,
  author = {Wriedt, G. and {Van der Velde}, M. and Aloe, A. and Bouraoui, F.},
  title = {A {European irrigation map for spatially distributed agricultural
	modelling}},
  journal = {Agricultural Water Management},
  year = {2009},
  volume = {96},
  pages = {771--789},
  number = {5},
  month = {May},
  abstract = {We present a pan-European irrigation map based on regional European
	statistics, a European land use map and a global irrigation map.
	The map provides spatial information on the distribution of irrigated
	areas per crop type which allows determining irrigated areas at the
	level of spatial modelling units. The map is a requirement for a
	European scale assessment of the impacts of irrigated agriculture
	on water resources based on spatially distributed modelling of crop
	growth and water balance. The irrigation map was compiled in a two
	step procedure. First, irrigated areas were distributed to potentially
	irrigated crops at a regional level (European statistical regions
	NUTS3), combining Farm Structure Survey (FSS) data on irrigated area,
	crop-specific irrigated area for crops whenever available, and total
	crop area. Second, crop-specific irrigated area was distributed within
	each statistical region based on the crop distribution given in our
	land use map. A global map of irrigated areas with a 5? resolution
	was used to further constrain the distribution within each NUTS3
	based on the density of irrigated areas. The constrained distribution
	of irrigated areas as taken from statistics to a high resolution
	dataset enables us to estimate irrigated areas for various spatial
	entities, including administrative, natural and artificial units,
	providing a reasonable input scenario for large-scale distributed
	modelling applications. The dataset bridges a gap between global
	datasets and detailed regional data on the distribution of irrigated
	areas and provides information for various assessments and modelling
	applications.},
  doi = {10.1016/j.agwat.2008.10.012},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@ARTICLE{wujohnston2007,
  author = {Wu, K. and Johnston, C.},
  title = {Hydrologic response to climatic variability in a Great Lakes Watershed:
	A case study with the {SWAT} model},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {337},
  pages = {187},
  number = {1-2},
  abstract = {Process-based hydrologic models are usually calibrated prior to application
	to ensure that they closely match reality. However, different hydrologic
	response to varied climatic conditions might affect model calibration
	and validation. A case study was conducted for a 901 km2 watershed
	of northern Michigan to compare the effects of calibrating the Soil
	and Water Assessment Tool (SWAT) watershed model with different climatic
	datasets representing drought (1948{--}1949) versus average (1969{--}1970)
	conditions. The effects of the different climatic conditions on parameter
	response and sensitivity were evaluated, and performance of the two
	calibration versions was compared using a common validation period,
	1950{--}1965. For the drought- and average-calibration periods, models
	were well calibrated, as indicated by high Nash-Sutcliffe efficiency
	coefficients (E = 0.8 and 0.9), and low deviation of discharge values
	(D = 2.9\% and 3.4\%). Evapotranspiration parameters differed under
	the two sets of climatic conditions. The plant water uptake compensation
	factor (EPCO), appropriately reflected plant water uptake patterns
	in varied climatic conditions. Snow melting parameters differed between
	the two scenarios. A comparison of baseflow values simulated by SWAT
	versus those computed by a hydrograph separation method showed that
	the SWAT method treated most snowmelt as surface runoff, whereas
	the latter method treated much of it as baseflow. The drought-calibrated
	version of the model performed much better during the validation
	period (1950{--}1965) (E = 0.8, D = 2.6\%) than did the average-calibrated
	version (E = 0.4, D = 41.4\%)},
  doi = {10.1016/j.jhydrol.2007.01.030},
  keywords = {SWAT, Snow melting, Evapotranspiration, Baseflow, Dry and average
	climatic condition, Great Lakes watershed},
  tags = {Applications, SWAT}
}

@ARTICLE{wuliu2012,
  author = {Wu, Y. and Liu, S.},
  title = {Automating calibration, sensitivity and uncertainty analysis of complex
	models using the R package Flexible Modeling Environment (FME): SWAT
	as an example},
  journal = {Environmental Modelling \& Software},
  year = {2012},
  volume = {31},
  pages = {99--109},
  number = {0},
  abstract = {Parameter optimization and uncertainty issues are a great challenge
	for the application of large environmental models like the Soil and
	Water Assessment Tool (SWAT), which is a physically-based hydrological
	model for simulating water and nutrient cycles at the watershed scale.
	In this study, we present a comprehensive modeling environment for
	SWAT, including automated calibration, and sensitivity and uncertainty
	analysis capabilities through integration with the R package Flexible
	Modeling Environment (FME). To address challenges (e.g., calling
	the model in R and transferring variables between Fortran and R)
	in developing such a two-language coupling framework, 1) we converted
	the Fortran-based SWAT model to an R function (R-SWAT) using the
	RFortran platform, and alternatively 2) we compiled SWAT as a Dynamic
	Link Library (DLL). We then wrapped SWAT (via R-SWAT) with FME to
	perform complex applications including parameter identifiability,
	inverse modeling, and sensitivity and uncertainty analysis in the
	R environment. The final R-SWAT-FME framework has the following key
	functionalities: automatic initialization of R, running Fortran-based
	SWAT and R commands in parallel, transferring parameters and model
	output between SWAT and R, and inverse modeling with visualization.
	To examine this framework and demonstrate how it works, a case study
	simulating streamflow in the Cedar River Basin in Iowa in the United
	Sates was used, and we compared it with the built-in auto-calibration
	tool of SWAT in parameter optimization. Results indicate that both
	methods performed well and similarly in searching a set of optimal
	parameters. Nonetheless, the R-SWAT-FME is more attractive due to
	its instant visualization, and potential to take advantage of other
	R packages (e.g., inverse modeling and statistical graphics). The
	methods presented in the paper are readily adaptable to other model
	applications that require capability for automated calibration, and
	sensitivity and uncertainty analysis.},
  doi = {10.1016/j.envsoft.2011.11.013},
  issn = {1364-8152},
  keywords = {Calibration}
}

@ARTICLE{xia+al2004,
  author = {Xia, Y. and Yang, {Z-L} and Jackson, C. and Stoffa, P. and Sen, M.},
  title = {Impacts of data length on optimal parameter and uncertainty estimation
	of a land surface model},
  journal = {Journal of Geophysical Research},
  year = {2004},
  volume = {109},
  pages = {D07101},
  number = {D7},
  abstract = {The optimal parameters and uncertainty estimation of land surface
	models require that appropriate length of forcing and calibration
	data be selected for computing error functions. Most of the previous
	studies used less than two years of data to optimize land surface
	models. In this study, 18-year hydrometeorological data at Valdai,
	Russia, were used to run the Chameleon Surface Model (CHASM). The
	optimal parameters were obtained by employing a global optimization
	technique called very fast simulated annealing. The uncertainties
	of model parameters were estimated by the Bayesian stochastic inversion
	technique. Forty-four experiments were conducted by using different
	lengths of data from the 18-year record, and a total of about 3 million
	parameter sets were produced. This study found that different calibration
	variables require different lengths of data to obtain optimal parameters
	and uncertainty estimates which are insensitive to the period selected.
	In the case of optimal parameters, monthly root-zone soil moisture,
	runoff, and evapotranspiration require 8, 3, and 1 years of data,
	respectively. In the case of uncertainty estimates, monthly root-zone
	soil moisture, runoff, and evapotranspiration require 8, 8, and 3
	years of data, respectively. Spin-up has little impact on the selection
	of optimal parameters and uncertainty estimates when evapotranspiration
	and runoff were calibrated. However, spin-up affects the selection
	of optimal parameters when soil moisture was calibrated},
  doi = {10.1029/2003JD004419},
  tags = {Calibration}
}

@ARTICLE{xiong2008,
  author = {Xiong, L. and O'Connor, K.},
  title = {An empirical method to improve the prediction limits of the {GLUE}
	methodology in rainfall--runoff modeling},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {349},
  pages = {115--124},
  number = {1--2},
  abstract = {The generalized likelihood uncertainty estimation (GLUE) method is
	one of the most widely used methods for investigating the uncertainties
	in the water resources and environmental modeling. However, some
	researches have found that the percentage of the observations falling
	within the prediction limits provided by the GLUE is much lower than
	the given certainty level used to produce these prediction limits
	in many cases. One possible reason contributing to such a low enveloping
	efficiency is the fact that, as the GLUE method indiscriminatingly
	accepts the simulation output of the hydrological model as its input,
	the errors in the simulation output of the hydrological model will
	be directly reflected in the prediction limits provided by the GLUE
	method. So, it is suggested in this paper to modify the original
	GLUE methodology by applying a new procedure designed to at least
	partially correct the simulation/prediction of the hydrological model
	prior to the derivation of the prediction limits at each time step,
	in an attempt to improve the efficiency of the GLUE prediction limits
	in enveloping the real-world observations. To test this simple concept,
	both the original GLUE and the suggested modified GLUE methods have
	been employed to produce the prediction limits of runoff on two different
	catchments. In terms of the containing ratio, i.e. the percentage
	of the observed data bracketed by the respective runoff prediction
	limits, the modified GLUE method has shown substantial improvements
	over the original method, in both the calibration and the verification
	periods.},
  doi = {10.1016/j.jhydrol.2007.10.029},
  owner = {RRojas},
  timestamp = {2008.07.16}
}

@ARTICLE{xu1999a,
  author = {Xu, {C.-Y.}},
  title = {From {GCMs} to river flow: {A} review of downscaling methods and
	hydrologic modelling approaches},
  journal = {Progress in Physical Geography},
  year = {1999},
  volume = {23},
  pages = {229--249},
  number = {2},
  abstract = {The scientific literature of the past decade contains a large number
	of reports detailing the development of downscaling methods and the
	use of hydrologic models to assess the potential effects of climate
	change on a variety of water resource issues. This article reviews
	the current state of methodologies for simulating hydrological responses
	to global climate change. Emphasis is given to recent advances in
	climatic downscaling and the problems related to the practical application
	of appropriate models in impact studies. Following a discussion of
	the advantages and deficiencies of the various approaches, challenges
	for the future study of the hydrological impacts of climate change
	are identified.},
  doi = {10.1177/030913339902300204},
  tags = {Downscaling, Impacts}
}

@ARTICLE{xu1999b,
  author = {Xu, {C.-Y.}},
  title = {Climate change and hydrologic models: {A} review of existing gaps
	and recent research developments},
  journal = {Water Resources Management},
  year = {1999},
  volume = {13},
  pages = {369--382},
  number = {5},
  abstract = {Global atmospheric general circulation models (GCMs) have been developed
	to simulate the present climate and used to predict future climatic
	change. While GCMs demonstrate significant skill at the continental
	and hemispheric spatial scales and incorporate a large proportion
	of the complexity of the global system, they are inherently unable
	to represent local subgrid-scale features and dynamics. The existing
	gap and the methodologies for narrowing the gap between GCMs' ability
	and the need of hydrological modelers are reviewed in this paper.
	Following the discussion of the advantages and deficiencies of various
	methods, the challenges for future studies of the hydrological impacts
	of climate change are identified},
  doi = {10.1023/A:1008190900459},
  tags = {Impacts}
}

@ARTICLE{Xu1999c,
  author = {Xu, {C.-Y.}},
  title = {Operational testing of a water balance model for predicting climate
	change impacts},
  journal = {Agricultural And Forest Meteorology},
  year = {1999},
  volume = {98-99},
  pages = {295--304},
  abstract = {The ability of water balance models to incorporate month-month or
	seasonal variations in climate, snowfall and snowmelt, groundwater
	fluctuations, soil moisture characteristics, and natural climatic
	variability makes them especially attractive for water resources
	studies of climatic changes. The use of conceptual models to explore
	the impact of climate changes has increased in recent years. Because
	of the success claimed for these studies, it is likely that computer
	simulation of catchments will increasingly be used by and for water
	resource managers as an aid to decision-making. There is therefore
	a need for a generally accepted method for demonstrating a model's
	fitness for such use. The simple split-sample test method may be
	reasonable in the simplest case of the 'filling-in missing data'
	problem but certainly not if the express purpose of the model is
	to simulate records for conditions different from those corresponding
	to the calibration record, such as the problem with predicting the
	effects of climate changes where the data on the changed system are
	not (and cannot be) available for comparison with the model predictions.
	Thus, model validation must demonstrate 'fitness for the said purpose'.},
  doi = {10.1016/S0168-1923(99)00106-9},
  keywords = {water balance models, validation methods, climate change impacts,
	NOPEX, HYDROLOGIC-MODELS, RIVER RUNOFF, SNOW COVER, NOPEX AREA, CATCHMENTS,
	RAINFALL, FLOW},
  tags = {Goodness-of-Fit, Calibration}
}

@ARTICLE{xusingh2004,
  author = {Xu, {C.-Y.} and Singh, V.},
  title = {Review on regional water resources Assessment models under stationary
	and changing climate},
  journal = {Water Resources Management},
  year = {2004},
  volume = {18},
  pages = {591--612},
  number = {6},
  abstract = {A comprehensive assessment of the water resources available in a region
	or a river basin is essential for finding sustainable solutions for
	water-related problems concerning both the quantity and quality of
	the water resources. Research on the development and application
	of water balance models at different spatial and temporal scales
	has been carried out since later part of the 19th century. As a result,
	a great deal of experience on various models and methods has been
	gained. This paper reviews both traditional long-term water balance
	methods and the new generation distributed models for assessing available
	water resources under stationary and changing climatic conditions
	at different spatial and temporal scales. The applicability and limitations
	of the methods are addressed. Finally, current advances and challenges
	in regional- and large-scale assessment of water resources are presented},
  doi = {10.1007/s11269-004-9130-0},
  tags = {Impacts}
}

@ARTICLE{xu+al2010,
  author = {Xu, {Y.-P.} and Booij, M. and Tong, {Y.-B.}},
  title = {Uncertainty analysis in statistical modeling of extreme hydrological
	events},
  journal = {Stochastic Environmental Research and Risk Assessment},
  year = {2010},
  volume = {24},
  pages = {567--578},
  number = {5},
  doi = {10.1007/s00477-009-0337-8},
  owner = {rojasro},
  timestamp = {2011.07.12}
}

@ARTICLE{yang+al2008,
  author = {Yang, J. and Reichert, P. and Abbaspour, K. and Xia, J. and Yang,
	H.},
  title = {Comparing uncertainty analysis techniques for a {SWAT} application
	to the {C}haohe {B}asin in {C}hina},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {358},
  pages = {1--23},
  number = {1-2},
  abstract = {Distributed watershed models are increasingly being used to support
	decisions about alternative management strategies in the areas of
	land use change, climate change, water allocation, and pollution
	control. For this reason it is important that these models pass through
	a careful calibration and uncertainty analysis. To fulfil this demand,
	in recent years, scientists have come up with various uncertainty
	analysis techniques for watershed models. To determine the differences
	and similarities of these techniques we compared five uncertainty
	analysis procedures: Generalized Likelihood Uncertainty Estimation
	(GLUE), Parameter Solution (ParaSol), Sequential Uncertainty FItting
	algorithm (SUFI-2), and a Bayesian framework implemented using Markov
	chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques.
	As these techniques are different in their philosophies and leave
	the user some freedom in formulating the generalized likelihood measure,
	objective function, or likelihood function, a literal comparison
	between these techniques is not possible. As there is a small spectrum
	of different applications in hydrology for the first three techniques,
	we made this choice according to their typical use in hydrology.
	For Bayesian inference, we used a recently developed likelihood function
	that does not obviously violate the statistical assumptions, namely
	a continuous-time autoregressive error model. We implemented all
	these techniques for the soil and water assessment tool (SWAT) and
	applied them to the Chaohe Basin in China. We compared the results
	with respect to the posterior parameter distributions, performances
	of their best estimates, prediction uncertainty, conceptual bases,
	computational efficiency, and difficulty of implementation. The comparison
	results for these categories are listed and the advantages and disadvantages
	are analyzed. From the point of view of the authors, if computationally
	feasible, Bayesian-based approaches are most recommendable because
	of their solid conceptual basis, but construction and test of the
	likelihood function requires critical attention.},
  doi = {10.1016/j.jhydrol.2008.05.012},
  keywords = {uncertainty analysis, watershed modeling, Bayesian inference, SUFI-2,
	GLUE, ParaSol, RAINFALL-RUNOFF MODELS, FLOOD FREQUENCY ESTIMATION,
	NONPOINT-SOURCE POLLUTION, PARAMETER-ESTIMATION, BAYESIAN-INFERENCE,
	WATERSHED MODEL, CLIMATE-CHANGE, ENVIRONMENTAL SYSTEMS, CONTINUOUS
	SIMULATION, CATCHMENT MODELS},
  tags = {Uncertainty, SWAT}
}

@ARTICLE{yang+al2007,
  author = {Yang, J. and Reichert, P. and Abbaspour, K. and Yang, H.},
  title = {Hydrological modelling of the Chaohe Basin in China: Statistical
	model formulation and Bayesian inference},
  journal = {Journal of Hydrology},
  year = {2007},
  volume = {340},
  pages = {167},
  number = {3-4},
  abstract = {Calibration of hydrologic models is very difficult because of measurement
	errors in input and response, errors in model structure, and the
	large number of non-identifiable parameters of distributed models.
	The difficulties even increase in arid regions with high seasonal
	variation of precipitation, where the modelled residuals often exhibit
	high heteroscedasticity and autocorrelation. On the other hand, support
	of water management by hydrologic models is important in arid regions,
	particularly if there is increasing water demand due to urbanization.
	The use and assessment of model results for this purpose require
	a careful calibration and uncertainty analysis. Extending earlier
	work in this field, we developed a procedure to overcome (i) the
	problem of non-identifiability of distributed parameters by introducing
	aggregate parameters and using Bayesian inference, (ii) the problem
	of heteroscedasticity of errors by combining a Box{--}Cox transformation
	of results and data with seasonally dependent error variances, (iii)
	the problems of autocorrelated errors, missing data and outlier omission
	with a continuous-time autoregressive error model, and (iv) the problem
	of the seasonal variation of error correlations with seasonally dependent
	characteristic correlation times. The technique was tested with the
	calibration of the hydrologic sub-model of the Soil and Water Assessment
	Tool (SWAT) in the Chaohe Basin in North China. The results demonstrated
	the good performance of this approach to uncertainty analysis, particularly
	with respect to the fulfilment of statistical assumptions of the
	error model. A comparison with an independent error model and with
	error models that only considered a subset of the suggested techniques
	clearly showed the superiority of the approach based on all the features
	(i){--}(iv) mentioned above.},
  doi = {10.1016/j.jhydrol.2007.04.006},
  keywords = {Watershed model calibration, Uncertainty analysis, Bayesian inference,
	Continuous-time autoregressive error model, MCMC, SWAT, UNCSIM, Aggregate
	parameters},
  tags = {SWAT, Calibration}
}

@ARTICLE{yapo+al1998,
  author = {Yapo, P. and Gupta, H. and Sorooshian, S.},
  title = {Multi-objective global optimization for hydrologic models},
  journal = {Journal of Hydrology},
  year = {1998},
  volume = {204},
  pages = {83--97},
  number = {1-4},
  abstract = {The development of automated (computer-based) calibration methods
	has focused mainly on the selection of a single-objective measure
	of the distance between the model-simulated output and the data and
	the selection of an automatic optimization algorithm to search for
	the parameter values which minimize that distance. However, practical
	experience with model calibration suggests that no single-objective
	function is adequate to measure the ways in which the model fails
	to match the important characteristics of the observed data. Given
	that some of the latest hydrologic models simulate several of the
	watershed output fluxes (e.g. water, energy, chemical constituents,
	etc.), there is a need for effective and efficient multi-objective
	calibration procedures capable of exploiting all of the useful information
	about the physical system contained in the measurement data time
	series. The MOCOM-UA algorithm, an effective and efficient methodology
	for solving the multiple-objective global optimization problem, is
	presented in this paper. The method is an extension of the successful
	SCE-UA single-objective global optimization algorithm. The features
	and capabilities of MOCOM-UA are illustrated by means of a simple
	hydrologic model calibration study.},
  doi = {10.1016/S0022-1694(97)00107-8},
  issn = {0022-1694},
  keywords = {Surface water, Watershed models, Parameter estimation, Calibration,
	Multiple objectives, Global optimization, pareto optimality, multi-objective},
  tags = {Calibration}
}

@ARTICLE{yapo+al1996,
  author = {Yapo, P. and Gupta, H. and Sorooshian, S.},
  title = {Automatic calibration of conceptual rainfall-runoff models: Sensitivity
	to calibration data},
  journal = {Journal of Hydrology},
  year = {1996},
  volume = {181},
  pages = {23--48},
  number = {1-4},
  abstract = {The identification of hydrologic models requires that appropriate
	data be selected for model calibration. In the research presented
	here, the shuffled complex evolution (SCE-UA) global optimization
	method was used to calibrate the NWSRFS-SMA conceptual rainfall-runoff
	flood forecasting model of the US National Weather Service, using
	a 40-year record of historical data. Based on 344 calibration runs
	using different lengths of data from different sections of the historical
	record, we conclude that approximately 8 years of data are required
	to obtain calibrations that are relatively insensitive to the period
	selected. Further, the reduction in parameter uncertainty is maximal
	when the wettest data periods on record are used. A residual analysis
	is used to compare the performance of the daily root mean square
	(DRMS) and heteroscedastic maximum likelihood error (HMLE) objective
	functions. The results suggest that the factor currently limiting
	model performance is the unavailability of strategies that explicitly
	account for model error during calibration.},
  doi = {10.1016/0022-1694(95)02918-4},
  keywords = {PBIAS, GLOBAL OPTIMIZATION, CATCHMENT MODELS},
  optkey = {Automatic calibration; rainfall-runoff models},
  optnumber = {1-4},
  optpages = {23-48},
  optvolume = {181},
  tags = {Calibration, Goodness-of-Fit}
}

@ARTICLE{yates1997,
  author = {Yates, D.},
  title = {Approaches to continental scale runoff for integrated assessment
	models},
  journal = {Journal of Hydrology},
  year = {1997},
  volume = {201},
  pages = {289--310},
  number = {1--4},
  abstract = {The need to incorporate water resources into regional and global integrated
	assessment models for evaluating global change impacts has been identified
	as being important, but given the spatial and temporal variability
	of freshwater resources and limited data availability, the inclusion
	of water within these large scale models has proven to be difficult.
	A compromise between accurate representation of complex processes
	and simplistic parameterization is necessary for capturing regional
	and temporal variability and to minimize computational requirements.
	An analysis of Western Europe's and Africa's freshwater runoff, which
	spans a range of climate variability, was performed at varying levels
	of spatial aggregation and at both monthly and annual time steps.
	Model results showed that regional runoff characteristics were lost
	beyond a data aggregation of 1Â° Ã— 1Â° resolution. A monthly soil
	moisture model proved to be adequate for assessing annual water availability,
	which is the scale often used in integrated assessment models. Simpler
	methods, such as an empirical annual model and a simple monthly model
	did not consistently replicate annual historic runoff.},
  doi = {10.1016/S0022-1694(97)00044-9},
  keywords = {Continental scale runoff, Integrated assessment models, Climate variability,
	Spatial aggregation},
  tags = {Climate Change, Large Scale, conceptual model}
}

@ARTICLE{ye2010,
  author = {Ye, M.},
  title = {M{MA}: {A} computer code for multimodel analysis},
  journal = {Ground Water},
  year = {2010},
  volume = {48},
  pages = {9--12},
  number = {1},
  doi = {10.1111/j.1745-6584.2009.00647.x},
  owner = {rojasro},
  timestamp = {2010.01.05}
}

@ARTICLE{yeetal2010,
  author = {Ye, M. and Lu, D. and Neuman, S. and Meyer, P.},
  title = {Comment on ``Inverse groundwater modeling for hydraulic conductivity
	estimation using {B}ayesian model averaging and variance window"
	by {Frank T.-C. Tsai and Xiaobao Li}},
  journal = {Water Resources Research},
  year = {2010},
  volume = {46},
  pages = {W02801},
  doi = {10.1029/2009WR008501},
  owner = {rojasro},
  timestamp = {2010.01.21}
}

@ARTICLE{yeetal2008a,
  author = {Ye, M. and Meyer, P. and Neuman, S.},
  title = {On model selection criteria in multimodel analysis},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W03428},
  abstract = {Hydrologic systems are open and complex, rendering them prone to multiple
	conceptualizations and mathematical descriptions. There has been
	a growing tendency to postulate several alternative hydrologic models
	for a site and use model selection criteria to (1) rank these models,
	(2) eliminate some of them, and/or (3) weigh and average predictions
	and statistics generated by multiple models. This has led to some
	debate among hydrogeologists about the merits and demerits of common
	model selection (also known as model discrimination or information)
	criteria such as AIC, AICc, BIC, and KIC and some lack of clarity
	about the proper interpretation and mathematical representation of
	each criterion. We examine the model selection literature to find
	that (1) all published rigorous derivations of AIC and AICc require
	that the (true) model having generated the observational data be
	in the set of candidate models; (2) though BIC and KIC were originally
	derived by assuming that such a model is in the set, BIC has been
	rederived by Cavanaugh and Neath (1999) without the need for such
	an assumption; and (3) KIC reduces to BIC as the number of observations
	becomes large relative to the number of adjustable model parameters,
	implying that it likewise does not require the existence of a true
	model in the set of alternatives. We explain why KIC is the only
	criterion accounting validly for the likelihood of prior parameter
	estimates, elucidate the unique role that the Fisher information
	matrix plays in KIC, and demonstrate through an example that it imbues
	KIC with desirable model selection properties not shared by AIC,
	AICc, or BIC. Our example appears to provide the first comprehensive
	test of how AIC, AICc, BIC, and KIC weigh and rank alternative models
	in light of the models' predictive performance under cross validation
	with real hydrologic data.},
  doi = {10.1029/2008WR006803},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{yeetal2004,
  author = {Ye, M. and Neuman, S. and Meyer, P.},
  title = {Maximum likelihood {B}ayesian averaging of spatial variability models
	in unsaturated fractured tuff},
  journal = {Water Resources Research},
  year = {2004},
  volume = {40},
  pages = {W05113},
  abstract = {Hydrologic analyses typically rely on a single conceptual-mathematical
	model. Yet hydrologic environments are open and complex, rendering
	them prone to multiple interpretations and mathematical descriptions.
	Adopting only one of these may lead to statistical bias and underestimation
	of uncertainty. Bayesian model averaging (BMA) [ Hoeting et al.,
	1999 ] provides an optimal way to combine the predictions of several
	competing models and to assess their joint predictive uncertainty.
	However, it tends to be computationally demanding and relies heavily
	on prior information about model parameters. Neuman [2002 , 2003]
	proposed a maximum likelihood version (MLBMA) of BMA to render it
	computationally feasible and to allow dealing with cases where reliable
	prior information is lacking. We apply MLBMA to seven alternative
	variogram models of log air permeability data from single-hole pneumatic
	injection tests in six boreholes at the Apache Leap Research Site
	(ALRS) in central Arizona. Unbiased ML estimates of variogram and
	drift parameters are obtained using adjoint state maximum likelihood
	cross validation [ Samper and Neuman, 1989a ] in conjunction with
	universal kriging and generalized least squares. Standard information
	criteria provide an ambiguous ranking of the models, which does not
	justify selecting one of them and discarding all others as is commonly
	done in practice. Instead, we eliminate some of the models based
	on their negligibly small posterior probabilities and use the rest
	to project the measured log permeabilities by kriging onto a rock
	volume containing the six boreholes. We then average these four projections
	and associated kriging variances, using the posterior probability
	of each model as weight. Finally, we cross validate the results by
	eliminating from consideration all data from one borehole at a time,
	repeating the above process and comparing the predictive capability
	of MLBMA with that of each individual model. We find that MLBMA is
	superior to any individual geostatistical model of log permeability
	among those we consider at the ALRS.},
  doi = {10.1029/2003WR002557},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{yeetal2005,
  author = {Ye, M. and Neuman, S. and Meyer, P. and Pohlmann, K.},
  title = {Sensitivity analysis and assessment of prior model probabilities
	in {MLBMA} with application to unsaturated fractured tuff},
  journal = {Water Resources Research},
  year = {2005},
  volume = {41},
  pages = {W12429},
  abstract = {Previous application of maximum likelihood Bayesian model averaging
	(MLBMA, Neuman (2002, 2003)) to alternative variogram models of log
	air permeability data in fractured tuff has demonstrated its effectiveness
	in quantifying conceptual model uncertainty and enhancing predictive
	capability (Ye et al., 2004). A question remained how best to ascribe
	prior probabilities to competing models. In this paper we examine
	the extent to which lead statistics of posterior log permeability
	predictions are sensitive to prior probabilities of seven corresponding
	variogram models. We then explore the feasibility of quantifying
	prior model probabilities by (1) maximizing Shannon's entropy H (Shannon,
	1948) subject to constraints reflecting a single analyst's (or a
	group of analysts') prior perception about how plausible each alternative
	model (or a group of models) is relative to others, and (2) selecting
	a posteriori the most likely among such maxima corresponding to alternative
	prior perceptions of various analysts or groups of analysts. Another
	way to select among alternative prior model probability sets, which,
	however, is not guaranteed to yield optimum predictive performance
	(though it did so in our example) and would therefore not be our
	preferred option, is a minimum-maximum approach according to which
	one selects a priori the set corresponding to the smallest value
	of maximum entropy. Whereas maximizing H subject to the prior perception
	of a single analyst (or group) maximizes the potential for further
	information gain through conditioning, selecting the smallest among
	such maxima gives preference to the most informed prior perception
	among those of several analysts (or groups). We use the same variogram
	models and log permeability data as Ye et al. (2004) to demonstrate
	that our proposed approach yields the least amount of posterior entropy
	(residual uncertainty after conditioning) and enhances predictive
	model performance as compared to (1) the noninformative neutral case
	in which all prior model probabilities are set equal to each other
	and (2) an informed case that nevertheless violates the principle
	of parsimony.},
  doi = {10.1029/2005WR004260},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{yeetal2009,
  author = {Ye, M. and Pohlman, K. and Chapman, J. and Pohll, G. and Reeves,
	D.},
  title = {A model--averaging method for assessing groundwater conceptual model
	uncertainty},
  journal = {Ground Water},
  year = {2009},
  volume = {48},
  pages = {716--728},
  number = {5},
  abstract = {This study evaluates alternative groundwater models with different
	recharge and geologic components at the northern Yucca Flat area
	of the Death Valley Regional Flow System (DVRFS), USA. Recharge over
	the DVRFS has been estimated using five methods, and five geological
	interpretations are available at the northern Yucca Flat area. Combining
	the recharge and geological components together with additional modeling
	components that represent other hydrogeological conditions yields
	a total of 25 groundwater flow models. As all the models are plausible
	given available data and information, evaluating model uncertainty
	becomes inevitable. On the other hand, hydraulic parameters (e.g.,
	hydraulic conductivity) are uncertain in each model, giving rise
	to parametric uncertainty. Propagation of the uncertainty in the
	models and model parameters through groundwater modeling causes predictive
	uncertainty in model predictions (e.g., hydraulic head and flow).
	Parametric uncertainty within each model is assessed using Monte
	Carlo simulation, and model uncertainty is evaluated using the model
	averaging method. Two model-averaging techniques (on the basis of
	information criteria and GLUE) are discussed. This study shows that
	contribution of model uncertainty to predictive uncertainty is significantly
	larger than that of parametric uncertainty. For the recharge and
	geological components, uncertainty in the geological interpretations
	has more significant effect on model predictions than uncertainty
	in the recharge estimates. In addition, weighted residuals vary more
	for the different geological models than for different recharge models.
	Most of the calibrated observations are not important for discriminating
	between the alternative models, because their weighted residuals
	vary only slightly from one model to another.},
  doi = {10.1111/j.1745-6584.2009.00633.x},
  owner = {rojasro},
  timestamp = {2010.01.05}
}

@ARTICLE{yeetal2008b,
  author = {Ye, M. and Pohlmann, K. and Chapman, J.},
  title = {Expert elicitation of recharge model probabilities for the {D}eath
	{V}alley regional flow system},
  journal = {Journal of Hydrology},
  year = {2008},
  volume = {354},
  pages = {102--115},
  number = {1--4},
  abstract = {This study uses expert elicitation to evaluate and select five alternative
	recharge models developed for the Death Valley regional flow system
	(DVRFS), covering southeast Nevada and the Death Valley area of California,
	USA. The five models were developed based on three independent techniques:
	an empirical approach, an approach based on unsaturated-zone studies
	and an approach based on saturated-zone studies. It is uncertain
	which recharge model (or models) should be used as input for groundwater
	models simulating flow and contaminant transport within the DVRFS.
	An expert elicitation was used to evaluate and select the recharge
	models and to determine prior model probabilities used for assessing
	model uncertainty. The probabilities were aggregated using simple
	averaging and iterative methods, with the latter method also considering
	between-expert variability. The most favorable model, on average,
	is the most complicated model that comprehensively incorporates processes
	controlling net infiltration and potential recharge. The simplest
	model, and the most widely used, received the second highest prior
	probability. The aggregated prior probabilities are close to the
	neutral choice that treats the five models as equally likely. Thus,
	there is no support for selecting a single model and discarding others,
	based on prior information and expert judgment. This reflects the
	inherent uncertainty in the recharge models. If a set of prior probability
	from a single expert is of more interest, we suggest selecting the
	set of the minimum Shannon’s entropy. The minimum entropy implies
	the smallest amount of uncertainty and the largest amount of information
	used to evaluate the models. However, when enough data are available,
	we prefer to use a cross-validation method to select the best set
	of prior model probabilities that gives the best predictive performance.},
  doi = {10.1016/j.jhydrol.2008.03.001},
  owner = {RRojas},
  timestamp = {2008.07.14}
}

@CONFERENCE{yeetal2006,
  author = {Ye, M. and Pohlmann, K. and Chapman, J. and Shafer, D.},
  title = {On evaluation of recharge model uncertainty: {A} priori and a posteriori},
  booktitle = {2006 {I}nternational {H}igh {L}evel {R}adioactive {W}aste {M}anagement
	{C}onference},
  year = {2006},
  address = {Las Vegas Nevada US},
  publisher = {American Nuclear Society},
  owner = {RRojas},
  timestamp = {2008.04.23}
}

@ARTICLE{ye+al1997,
  author = {Ye, W. and Bates, B. and Viney, N. and Sivapalan, M. and Jakeman,
	A.},
  title = {Performance of conceptual rainfall-runoff models in low-yielding
	ephemeral catchments},
  journal = {Water Resources Research},
  year = {1997},
  volume = {33},
  pages = {153--166},
  number = {1},
  abstract = {Low-yielding catchments with ephemeral streams involve highly nonlinear
	relationships between rainfall and runoff, and there is much less
	documentation and appreciation of the ability to predict streamflow
	in these veiy difficult cases than in humid catchments. The predictions
	of three conceptual rainfall-runoff models are assessed in three
	low-yielding, emphemeral streams over a 10-year period. The models
	are a simple conceptual model, Generalized Surface inFiltration Baseflow
	(GSFB; eight parameters), a hybrid metric/conceptual model, Identification
	of Hydrographs and Components from Rainfall, Evaporation and Streamflow
	data (IHACRES; six parameters), and a complex conceptual model, the
	Large Scale Catchment Model (LASCAM; 22 parameters). The Salmon (0.82
	km2), Stones (15 km2), and Canning (517 km2) catchments in Western
	Australia were selected for their range of sizes and low runoff yields
	(1.6{--}12.2\% of rainfall). Their behavior is representative of
	a large part of Australia and semiarid regions, where antecedent
	conditions are critical determinants of streamflow response to rainfall.
	Such catchments provide a stern test of the capability of conceptual
	models. Five-year calibration and validation performances were assessed
	with a range of statistics. The models were run daily but performance
	was assessed on both a daily and monthly basis by aggregating daily
	model streamflows and observations up to monthly. The models performed
	well, particularly in the monthly case where often more than 90\%
	of the variance of observed streamflow was explained in simulation
	on independent periods. However, while the simple conceptual model
	is adequate for monthly time periods, the daily simulation results
	indicate that a slightly more complex model (the hybrid model or
	the complex conceptual model) is required for daily predictions in
	these diy catchments. The model simulation results extend the following
	notion of Jakeman and Hornberger [1993] from humid to semiarid ephemeral
	catchments: that a model of about six parameters, albeit in an appropriate
	model structure, is sufficient to characterize the information in
	rainfall-discharge time series over a wide range of catchment sizes.
	Models of such modest complexity also predict runoff with good accuracy
	outside calibration periods, even in ephemeral, low-yielding catchments.
	The simulation results highlight the critical importance of the deep
	groundwater and antecedent moisture conditions on stream yields in
	ephemeral catchments and point to the desirability of accounting
	for these factors in arid-zone modeling.},
  doi = {10.1029/96WR02840},
  tags = {Calibration, conceptual model, Low-Flows}
}

@ARTICLE{yeh1986,
  author = {Yeh, {W. W.-G.}},
  title = {Review of parameter identification procedures in groundwater hydrology:
	{T}he inverse problem},
  journal = {Water Resources Research},
  year = {1986},
  volume = {22},
  pages = {95--108},
  number = {2},
  abstract = {The purpose of this survey is to review parameter identification procedures
	in groundwater hydrology and to examine computational techniques
	which have been developed to solve the inverse problem. Parameter
	identification methods are classified under the error criterion used
	in the formulation of the inverse problem. The problem of ill-posedness
	in connection with the inverse problem is addressed. Typical inverse
	solution techniques are highlighted. The review also includes the
	evaluation of methods used for computing the sensitivity matrix.
	Statistics which can be used to estimate the parameter uncertainty
	are outlined. Attempts have been made to compare and contrast representative
	inverse procedures, and direction for future research is suggested.},
  doi = {10.1029/WR022i002p00095},
  owner = {RRojas},
  timestamp = {2009.03.25}
}

@ARTICLE{yilmaz+al2008,
  author = {Yilmaz, K. and Gupta, H. and Wagener, T.},
  title = {A process-based diagnostic approach to model evaluation: Application
	to the {NWS} distributed hydrologic model},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W09417},
  number = {9},
  abstract = {Distributed hydrological models have the potential to provide improved
	streamflow forecasts along the entire channel network, while also
	simulating the spatial dynamics of evapotranspiration, soil moisture
	content, water quality, soil erosion, and land use change impacts.
	However, they are perceived as being difficult to parameterize and
	evaluate, thus translating into significant predictive uncertainty
	in the model results. Although a priori parameter estimates derived
	from observable watershed characteristics can help to minimize obstacles
	to model implementation, there exists a need for powerful automated
	parameter estimation strategies that incorporate diagnostic information
	regarding the causes of poor model performance. This paper investigates
	a diagnostic approach to model evaluation that exploits hydrological
	context and theory to aid in the detection and resolution of watershed
	model inadequacies, through consideration of three of the four major
	behavioral functions of any watershed system; overall water balance,
	vertical redistribution, and temporal redistribution (spatial redistribution
	was not addressed). Instead of using classical statistical measures
	(such as mean squared error), we use multiple hydrologically relevant
	{``}signature measures{''} to quantify the performance of the model
	at the watershed outlet in ways that correspond to the functions
	mentioned above and therefore help to guide model improvements in
	a meaningful way. We apply the approach to the Hydrology Laboratory
	Distributed Hydrologic Model (HL-DHM) of the National Weather Service
	and show that diagnostic evaluation has the potential to provide
	a powerful and intuitive basis for deriving consistent estimates
	of the parameters of watershed models},
  doi = {10.1029/2007WR006716},
  keywords = {Diagnostic evaluation, distributed hydrologic modeling, parameter
	estimation, calibration, models, watersheds, processes, information},
  tags = {Calibration, Thesis, Signatures}
}

@INCOLLECTION{young1983,
  author = {Young, P.},
  title = {The validity and credibility of models for badly-defined systems},
  booktitle = {Uncertainty and Forecasting of Water Quality},
  publisher = {Springer-Verlag:Berlin},
  year = {1983},
  editor = {M. B. Beck and G. van Straten},
  pages = {69--98},
  tags = {Calibration}
}

@ARTICLE{yu+al2012,
  author = {Yu, J. and Qin, X. and Larsen, O.},
  title = {Joint Monte Carlo and possibilistic simulation for flood damage assessment},
  journal = {Stochastic Environmental Research and Risk Assessment},
  pages = {1--11},
  abstract = {A joint Monte Carlo and fuzzy possibilistic simulation (MC-FPS) approach
	was proposed for flood risk assessment. Monte Carlo simulation was
	used to evaluate parameter uncertainties associated with inundation
	modeling, and fuzzy vertex analysis was applied for promulgating
	human-induced uncertainty in flood damage estimation. A study case
	was selected to show how to apply the proposed method. The results
	indicate that the outputs from MC-FPS would present as fuzzy flood
	damage estimate and probabilistic-possibilistic damage contour maps.
	The stochastic uncertainty in the flood inundation model and fuzziness
	in the depth-damage functions derivation would cause similar levels
	of influence on the final flood damage estimate. Under the worst
	scenario (i.e. a combined probabilistic and possibilistic uncertainty),
	the estimated flood damage could be 2.4 times higher than that computed
	from conventional deterministic approach; considering only the pure
	stochastic effect, the flood loss would be 1.4 times higher. It was
	also indicated that uncertainty in the flood inundation modeling
	has a major influence on the standard deviation of the simulated
	damage, and that in the damage-depth function has more notable impact
	on the mean of the fitted distributions. Through applying MC-FPS,
	rich information could be derived under various α-cut levels and
	cumulative probabilities, and it forms an important basis for supporting
	rational decision making for flood risk management under complex
	uncertainties.},
  affiliation = {School of Civil & Environmental Engineering, Nanyang Technological
	University, 50 Nanyang Avenue, Singapore, 639798 Singapore},
  doi = {10.1007/s00477-012-0635-4},
  issn = {1436-3240},
  keyword = {Earth and Environmental Science},
  publisher = {Springer Berlin / Heidelberg}
}

@ARTICLE{yuyang2000,
  author = {Yu, P. and Yang, T.},
  title = {Fuzzy multi-objective function for rainfall-runoff model calibration},
  journal = {Journal of Hydrology},
  year = {2000},
  volume = {238},
  pages = {1--14},
  number = {1-2},
  abstract = {Continuous rainfall-runoff modeling is essential to simulate the rainfall-runoff
	relationship in water resource projects. The performance of a rainfall-runoff
	model heavily depends on suitable choice of model parameters, which
	are normally calibrated by using an objective function. This study
	presents a fuzzy multi-objective function (FMOF) to improve the performance
	of conventional objective functions, such as the root mean square
	error (RMSE) and the mean absolute percent error (MPE). Daily rainfall
	and flow discharge measurements, as well as monthly evaporation estimates
	are used to calibrate and verify a rainfall-runoff model, over a
	9-year and a 4-year period, respectively. The model calibrated with
	RMSE and MPE as objective functions tends to match high and low flow
	periods, respectively. The FMOF leads to improved simulation of a
	wide range of flow stages as it can combine various objective functions
	with different acceptable levels. The method suggested herein is
	shown to be more appropriate for basins with extremely heterogeneous
	temporal flow distribution},
  doi = {10.1016/S0022-1694(00)00317-6},
  keywords = {Rainfall-runoff model, Objective function, Model calibration, Fuzzy
	theory},
  tags = {Calibration}
}

@ARTICLE{zakermoshfegh+al2008,
  author = {Zakermoshfegh, M. and Neyshabouri, S. and Lucas, C.},
  title = {Automatic Calibration of Lumped Conceptual Rainfall-Runoff Model
	Using Particle Swarm Optimization},
  journal = {Journal of Applied Sciences},
  year = {2008},
  volume = {8},
  pages = {3703--3708},
  number = {20},
  abstract = {The main objective in Conceptual Rainfall-Runoff (CRR) model calibration
	is to find a set of optimal model parameter values that provides
	a best fit between observed and estimated flow hydrographs, where
	the traditional trial and error manual calibration is very tedious
	and time consuming. Recently in multi dimensional combinatorial optimization
	problems, meta-heuristic algorithms have shown an encouraging performance
	with a low computational cost. In this study as a new application
	of Particle Swarm Optimization (PSO) algorithm, it is applied to
	automatic calibration of HEC-1 lumped CRR model and the methodology
	is tested in two example applications: a synthetic hypothetical example
	and a real case study for the Gorganrood river basin in the north
	of Iran. The results show encouraging performance of the proposed
	automated methodology. },
  doi = {10.3923/jas.2008.3703.3708},
  tags = {Calibration, PSO}
}

@MANUAL{hydroGOF,
  title = {hydroGOF: Goodness-of-fit functions for comparison of simulated and
	observed hydrological time series},
  author = {{Zambrano-Bigiarini}, M.},
  year = {2012},
  note = {R package version 0.3-2-2},
  owner = {rojasro},
  timestamp = {2012.03.07},
  url = {http://www.rforge.net/hydroGOF/, http://cran.r-project.org/web/packages/hydroGOF/}
}

@MANUAL{hydroTSM,
  title = {hydroTSM: Time series management, analysis and interpolation for
	hydrological modelling},
  author = {{Zambrano-Bigiarini}, M.},
  year = {2012},
  note = {R package version 0.3-3},
  owner = {rojasro},
  timestamp = {2012.03.22},
  url = {http://www.rforge.net/hydroTSM/}
}

@ARTICLE{hydroPSO2012,
  author = {{Zambrano-Bigiarini}, M. and Rojas, R.},
  title = {{hydroPSO: A model-independent particle swarm optimization software
	for calibration of environmental models}},
  journal = {Environmental Modelling \& Software},
  year = {2012},
  volume = {submitted},
  owner = {rojasro},
  timestamp = {2011.12.02}
}

@ARTICLE{zhang+al2006,
  author = {Zhang, D. and Beven, K. and Mermoud, A.},
  title = {A comparison of non--linear least square and {GLUE} for model calibration
	and uncertainty estimation for pesticide transport in soils},
  journal = {Advances in Water Resources},
  year = {2006},
  volume = {29},
  pages = {1924--1933},
  number = {12},
  abstract = {The problems of calibrating soil hydraulic and transport parameters
	are well documented, particularly when data are limited. Programs
	such as CXTFIT, UUCODE and PEST, based on well established principles
	of statistical inference, will often provide good fits to limited
	observations giving the impression that a useful model of a particular
	soil system has been obtained. This may be the case, but such an
	approach may grossly underestimate the uncertainties associated with
	future predictions of the system and resulting dependent variables.
	In this paper, this is illustrated by an application of CXTFIT within
	the generalised likelihood uncertainty estimation (GLUE) approach
	to model calibration which is based on a quite different philosophy.
	CXTFIT gives very good fits to the observed breakthrough curves for
	several different model formulations, resulting in very small parameter
	uncertainty estimates. The application of GLUE, however, shows that
	much wider ranges of parameter values can provide acceptable fits
	to the data. The wider range of potential outcomes should be more
	robust in model prediction, especially when used to constrain field
	scale models.},
  doi = {10.1016/j.advwatres.2006.02.004},
  owner = {RRojas},
  timestamp = {2008.12.03}
}

@ARTICLE{zhang+al2009b,
  author = {Zhang, X. and Srinivasan, R. and Bosch, D.},
  title = {Calibration and uncertainty analysis of the {SWAT} model using {G}enetic
	{A}lgorithms and {B}ayesian {M}odel {A}veraging},
  journal = {Journal of Hydrology},
  year = {2009},
  volume = {374},
  pages = {307},
  number = {3-4},
  abstract = {In this paper, the Genetic Algorithms (GA) and Bayesian Model Averaging
	(BMA) were used to simultaneously conduct calibration and uncertainty
	analysis for the Soil and Water Assessment Tool (SWAT). In this combined
	method, several SWAT models with different structures are first selected;
	next GA is used to calibrate each model using observed streamflow
	data; finally, BMA is applied to combine the ensemble predictions
	and provide uncertainty interval estimation. This method was tested
	in two contrasting basins, the Little River Experimental Basin in
	Georgia, USA, and the Yellow River Headwater Basin in China. The
	results obtained in the two case studies show that this combined
	method can provide deterministic predictions better than or comparable
	to the best calibrated model using GA. The 66.7\% and 90\% uncertainty
	intervals estimated by this method were analyzed. The differences
	between the percentage of coverage of observations and the corresponding
	expected coverage percentage are within 10\% for both calibration
	and validation periods in these two test basins. This combined methodology
	provides a practical and flexible tool to attain reliable deterministic
	simulation and uncertainty analysis of SWAT},
  doi = {10.1016/j.jhydrol.2009.06.023},
  keywords = {Optimization, Modeling, Basin, Uncertainty, SWAT},
  tags = {SWAT, Calibration, Uncertainty}
}

@ARTICLE{zhang+al2008a,
  author = {Zhang, X. and Srinivasan, R. and Debele, B. and Hao, F.},
  title = {Runoff simulation of the headwaters of the Yellow River using the
	{SWAT} model with three snowmelt algorithms},
  journal = {Journal of the American Water Resources Association},
  year = {2008},
  volume = {44},
  pages = {48--61},
  abstract = {The Soil and Water Assessment Tool (SWAT) model combined with different
	snowmelt algorithms was evaluated for runoff simulation of an 114,345
	km(2) mountainous river basin (the headwaters of the Yellow River),
	where snowmelt is a significant process. The three snowmelt algorithms
	incorporated into SWAT were as follows: (1) the temperature-index,
	(2) the temperature-index plus elevation band, and (3) the energy
	budget based SNOW17. The SNOW17 is more complex than the temperature-based
	snowmelt algorithms, and requires more detailed meteorological and
	topographical inputs. In order to apply the SNOW17 in the SWAT framework,
	SWAT was modified to operate at the pixel scale rather than the normal
	Hydrologic Response Unit scale. The three snowmelt algorithms were
	evaluated under two parameter scenarios, the default and the calibrated
	parameters scenarios. Under the default parameters scenario, the
	parameter values were determined based on a review of the current
	literature. The purpose of this type of evaluation was to assess
	the applicability of SWAT in ungauged basins, where there is little
	observed data available for calibration. Under the calibrated parameters
	scenario, the parameters were calibrated using an automatic calibration
	program, the Shuffled Complex Evolution (SCE-UA). The purpose of
	this type of evaluation was to assess the applicability of SWAT in
	gauged basins. Two time periods (1975-1985 and 1986-1990) of monthly
	runoff data were used in this study to evaluate the performance of
	SWAT with different snowmelt algorithms. Under the default parameters
	scenario, the SWAT model with complex energy budget based SNOW17
	performed the best for both time periods. Under the calibrated parameters
	scenario, the parameters were calibrated using monthly runoff from
	1975-1985 and validated using monthly runoff from 1986-1990. After
	parameter calibration, the performance of SWAT with the three snowmelt
	algorithms was improved from the default parameters scenario. Further,
	the SWAT model with temperature-index plus elevation band performed
	as well as the SWAT model with SNOW17. The SWAT model with temperature-index
	algorithm performed the poorest for both time periods under both
	scenarios. Therefore, it is suggested that the SNOW17 model be used
	for modeling ungauged basins; however, for gauged basins, the SNOW17
	and simple temperature-index plus elevation band models could provide
	almost equally good runoff simulation results.},
  doi = {10.1111/j.1752-1688.2007.00137.x},
  keywords = {Yellow River, SWAT, runoff, snow hydrology, simulation, GLOBAL OPTIMIZATION,
	TEMPERATURE, INDEX, CALIBRATION, HYDROLOGY, RADIATION, TERRAIN, BASINS},
  tags = {Lapse Rate}
}

@ARTICLE{zhangsrinivasan+al2010,
  author = {Zhang, X. and Srinivasan, R. and {Van Liew}, M.},
  title = {On the use of multi-algorithm, genetically adaptive multi-objective
	method for multi-site calibration of the {SWAT} model},
  journal = {Hydrological Processes},
  year = {2010},
  volume = {24},
  pages = {955--969},
  number = {8},
  abstract = {With the availability of spatially distributed data, distributed hydrologic
	models are increasingly used for simulation of spatially varied hydrologic
	processes to understand and manage natural and human activities that
	affect watershed systems. Multi-objective optimization methods have
	been applied to calibrate distributed hydrologic models using observed
	data from multiple sites. As the time consumed by running these complex
	models is increasing substantially, selecting efficient and effective
	multi-objective optimization algorithms is becoming a nontrivial
	issue. In this study, we evaluated a multi-algorithm, genetically
	adaptive multi-objective method (AMALGAM) for multi-site calibration
	of a distributed hydrologic model - Soil and Water Assessment Tool
	(SWAT), and compared its performance with two widely used evolutionary
	multi-objective optimization (EMO) algorithms (i.e. Strength Pareto
	Evolutionary Algorithm 2 (SPEA2) and Non-dominated Sorted Genetic
	Algorithm II (NSGA-II)). In order to provide insights into each method's
	overall performance, these three methods were tested in four watersheds
	with various characteristics. The test results indicate that the
	AMALGAM can consistently provide competitive or superior results
	compared with the other two methods. The multi-method search framework
	of AMALGAM, which can flexibly and adaptively utilize multiple optimization
	algorithms, makes it a promising tool for multi-site calibration
	of the distributed SWAT. For practical use of AMALGAM, it is suggested
	to implement this method in multiple trials with relatively small
	number of model runs rather than run it once with long iterations.
	In addition, incorporating different multi-objective optimization
	algorithms and multi-mode search operators into AMALGAM deserves
	further research},
  doi = {10.1002/hyp.7528},
  keywords = {Keywords:distributed hydrologic model, multi-method search, multi-objective
	optimization, multi-site calibration, soil and water assessment tool},
  tags = {SWAT, Calibration, PSO}
}

@ARTICLE{zhang+al2008b,
  author = {Zhang, X. and Srinivasan, R. and {Van Liew}, M.},
  title = {Multi-Site Calibration of The {SWAT} Model For Hydrologic Modeling},
  journal = {Transactions of the ASABE},
  year = {2008},
  volume = {51},
  pages = {2039--2049},
  number = {6},
  abstract = {The growing popularity of applying complex, semi-physically based
	distributed hydrologic models to solve water resource problems poses
	important issues that must be addressed related to the use of spatial
	data to calibrate and validate such models. In this study, a single-objective
	optimization method (GA) and a multi-objective optimization algorithm
	(SPEA2) were applied to optimize the parameters of the Soil and Water
	Assessment Tool (SWAT) using observed streamflow data at three monitoring
	sites within the Reynolds Creek Experimental Watershed, Idaho. Results
	indicated that different optimization schemes can lead to substantially
	different objective function values, parameter solutions, and corresponding
	simulated hydrographs. Thus, the selection of an optimization scheme
	can potentially impact modeled streamflow. Parameters estimated by
	optimizing the objective function at three monitoring sites consistently
	produced better goodness-of-fit than those obtained by optimization
	at a single monitoring site. This stresses the importance of collecting
	detailed, spatially distributed data to conduct simultaneous multi-site
	calibrations. When applied with multi-site data, the single-objective
	(GA) method better identified parameter solutions in the calibration
	period, but the multi-objective (SPEA2) method performed better in
	the validation period. Overall, the application of different optimization
	schemes in the Reynolds Creek Experimental Watershed demonstrated
	that the single-objective (GA) and the multi-objective (SPEA2) optimization
	methods can provide promising results for multi-site calibration
	and validation of the SWAT model. These results are expected to help
	the users of SWAT and other distributed hydrologic models understand
	the sensitivity of distributed hydrologic simulation to different
	calibration methods and to demonstrate the advantages and disadvantages
	of single-objective and multi-objective parameter estimation methods.},
  doi = {10.1029/2004WR003695.},
  keywords = {Automatic calibration, Distributed hydrologic model, Multi-objective
	optimization, Multi-site calibration, SWAT, MULTIOBJECTIVE EVOLUTIONARY
	ALGORITHMS, SENSITIVITY-ANALYSIS, GLOBAL OPTIMIZATION, RIVER-BASIN,
	SIMULATION, VALIDATION, WATERSHEDS, CATCHMENT, EFFICIENT, POINT},
  tags = {SWAT, Calibration}
}

@ARTICLE{zhang+al2009a,
  author = {Zhang, X. and Srinivasan, R. and Zhao, K. and {Van Liew}, M.},
  title = {Evaluation of global optimization algorithms for parameter calibration
	of a computationally intensive hydrologic model},
  journal = {Hydrological Processes},
  year = {2009},
  volume = {23},
  pages = {430--441},
  number = {3},
  abstract = {With the popularity of complex hydrologic models. the time taken to
	run these models is increasing substantially. Comparing and evaluating
	the efficacy of different optimization algorithms; for calibrating
	computationally intensive hydrologic models is becoming, a nontrivial
	issue. In this study, live global optimization algorithms (genetic
	algorithms. shuffled complex evolution, particle swarm optimization,
	differential evolution, and artificial immune system) were tested
	for automatic parameter calibration of a complex hydrologic model,
	Soil and Water Assessment Too] (SWAT), in four watersheds. The results
	show that genetic algorithms (GA) outperform the other four algorithms
	given model evaluation numbers larger than 2000. while particle swarm
	optimization (PSO) call obtain better parameter solutions than other
	algorithms given fewer number of model runs (less than 2000). Given
	limited computational time, the PSO algorithm is preferred, while
	GA should be chosen given plenty of computational resources. When
	applying GA and PSO for parameter optimization of SWAT, small population
	Size should be chosen. Copyright (C) 2008 John Wiley \& Sons, Ltd.},
  doi = {10.1002/hyp.7152},
  keywords = {global optimization algorithm, calibration, hydrologic model, SWAT,
	computational intensive, IMMUNE-BASED OPTIMIZATION, RAINFALL-RUNOFF
	MODELS, DIFFERENTIAL EVOLUTION, CATCHMENT MODELS, SWAT MODEL, RIVER,
	SIMULATION, HEADWATERS, MANAGEMENT, SYSTEM},
  tags = {SWAT, Calibration}
}

@ARTICLE{zhang+al2008,
  author = {Zhang, X. and Wagener, T. and Reed, P. and Bhushan, R.},
  title = {Reducing uncertainty in predictions in ungauged basins by combining
	hydrologic indices regionalization and multiobjective optimization},
  journal = {Water Resources Research},
  year = {2008},
  volume = {44},
  pages = {W00B04},
  abstract = {Approaches to predictions in ungauged basins have so far mainly focused
	on a priori parameter estimates from physical watershed characteristics
	or on the regionalization of model parameters. Recent studies suggest
	that the regionalization of hydrologic indices (e.g., streamflow
	characteristics) provides an additional way to extrapolate information
	about the expected watershed response to ungauged locations for use
	in continuous watershed modeling. This study contributes a novel
	multiobjective framework for identifying behavioral parameter ensembles
	for ungauged basins using suites of regionalized hydrologic indices.
	The new formulation enables the use of multiobjective optimization
	algorithms for the identification of model ensembles for predictions
	in ungauged basins for the first time. Application of the new formulation
	to 30 watersheds located in England and Wales and comparison of the
	results with a Monte Carlo approach demonstrate that the new formulation
	will significantly advance our ability to reduce the uncertainty
	of predictions in ungauged basins.},
  doi = {10.1029/2008WR006833},
  tags = {Regionalization, Calibration, Uncertainty}
}

@ARTICLE{zhangyuhu2005,
  author = {Zhang, {L-P} and Yu, {H-J} and Hu, {S-Xu}},
  title = {Optimal choice of parameters for particle swarm optimization},
  journal = {Journal of Zhejiang University SCIENCE},
  year = {2005},
  volume = {6A},
  pages = {528},
  number = {6},
  doi = {10.1631/jzus.2005.A0528},
  tags = {Calibration, PSO}
}

@ARTICLE{DEPSO2003,
  author = {Zhangn, {W-J} and Xie, {X-F}},
  title = {DEPSO: hybrid particle swarm with differential evolution operator},
  journal = {IEEE International Conference on Systems, Man and Cybernetics},
  year = {2003},
  volume = {4},
  pages = {3816--3821},
  number = {0},
  doi = {10.1109/ICSMC.2003.1244483},
  tags = {Calibration, PSO}
}

@INCOLLECTION{zhao2006,
  author = {Zhao, B.},
  title = {An Improved Particle Swarm Optimization Algorithm for Global Numerical
	Optimization},
  booktitle = {Computational {S}cience - {ICCS} 2006},
  publisher = {Springer-Verlag Berlin Heidelberg},
  year = {2006},
  editor = {Vassil Alexandrov and Geert van Albada and Peter Sloot and Jack Dongarra},
  volume = {3991},
  series = {Lecture Notes in Computer Science},
  pages = {657--664},
  abstract = {This paper presents an improved particle swarm optimization algorithm
	(IPSO) for global numerical optimization. The IPSO uses more particles{'}
	information to control the mutation operation. A new adaptive strategy
	for choosing parameters is also proposed to assure convergence of
	the IPSO. Meanwhile, we execute the IPSO to solve eight benchmark
	problems. The results show that the IPSO is superior to some existing
	methods for finding the best solution, in terms of both solution
	quality and algorithm robustness.},
  highlights = {IPSO, ipso},
  tags = {PSO, Calibration}
}

@BOOK{zheng2002,
  title = {Applied contaminant transport modelling: {T}heory and practice},
  publisher = {John Wiley \& Sons, Inc.},
  year = {2002},
  author = {Zheng, C. and Bennett, G.},
  pages = {621},
  address = {New York},
  edition = {Second},
  owner = {RRojas},
  timestamp = {2009.03.25}
}

@INPROCEEDINGS{zheng+al2003,
  author = {Zheng, {Y-L} and Ma, {L-H} and Zhang, {L-Y} and Qian, {J-X}},
  title = {On the convergence analysis and parameter selection in particle swarm
	optimization},
  booktitle = {Machine Learning and Cybernetics, 2003 International Conference on},
  year = {2003},
  pages = {1802--1807},
  abstract = {A PSO with increasing inertia weight, distinct from a widely used
	PSO with decreasing inertia weight, is proposed in this paper. Far
	from drawing conclusions from sole empirical study or rule of thumb,
	this algorithm is derived from particle trajectory study and convergence
	analysis. Four standard test functions are used to confirm its validity
	finally. From the experiments, it is clear that a PSO with increasing
	inertia weight outperforms the one with decreasing inertia weight,
	both in convergent speed and solution precision, with no additional
	computing load.},
  doi = {10.1109/ICMLC.2003.1259789},
  keywords = {convergence analysis, inertia weight, parameter selection, particle
	swarm optimization, particle trajectory study, thumb rule, convergence,
	optimisation, search problems},
  tags = {Calibration, PSO}
}

@ARTICLE{zhou+al2012,
  author = {Q. Zhou and P.S. Mikkelsen and K. Halsnæs and K. Arnbjerg-Nielsen},
  title = {Framework for economic pluvial flood risk assessment considering
	climate change effects and adaptation benefits},
  journal = {Journal of Hydrology},
  year = {2012},
  volume = {414–415},
  pages = {539--549},
  number = {0},
  abstract = {Summary Climate change is likely to affect the water cycle by influencing
	the precipitation patterns. It is important to integrate the anticipated
	changes into the design of urban drainage in response to the increased
	risk level in cities. This paper presents a pluvial flood risk assessment
	framework to identify and assess adaptation options in the urban
	context. An integrated approach is adopted by incorporating climate
	change impact assessment, flood inundation modeling, economic tool,
	and risk assessment, hereby developing a step-by-step process for
	cost-benefit assessment of climate change adaptation measures. A
	Danish case study indicates that the introduced framework presented
	in the paper can be considered as an important decision support tool
	that can supplement and further develop existing decision practices
	in relation to urban drainage.},
  doi = {10.1016/j.jhydrol.2011.11.031},
  issn = {0022-1694},
  keywords = {Flood risk assessment}
}

@ARTICLE{zierlbugmann2005,
  author = {Zierl, B. and Bugmann, H.},
  title = {Global change impacts on hydrological processes in {A}lpine catchments},
  journal = {Water Resources Research},
  year = {2005},
  volume = {41},
  pages = {W02028},
  abstract = {This analysis assesses the impacts of global change on hydrological
	processes in Alpine catchments. We applied the regional hydroecological
	simulation system RHESSys to five case studies from different climatic
	zones in the European Alps. Seven scenarios based on the so-called
	SRES emission scenarios of the Intergovernmental Panel on Climate
	Change were used to describe changes in climate and land use. Shifts
	in runoff regimes and annual and summer runoff were investigated.
	The focus was on analyzing differences in the hydrological responses
	of different Alpine climate zones, exploring differences between
	SRES scenarios (A1FI, A2, B1, and B2), and analyzing the importance
	of applying several global circulation models (HadCM3, CGCM2, CSIRO2,
	and PCM). Under all scenarios, rising temperatures were projected
	to strongly affect runoff regimes via the impact on snow cover. The
	overall effect is a substantial runoff regime shift across the Alps,
	with specific differences depending on the climatic zone considered.},
  doi = {10.1029/2004WR003447},
  tags = {Uncertainty, Impacts}
}

@ARTICLE{zimmerman2008,
  author = {Zimmerman, J. and Michelcic, J. and Smith, J.},
  title = {Global stressors on water quality and quantity},
  journal = {Environmental Science \& Technology},
  year = {2008},
  volume = {42},
  pages = {4247--4254},
  number = {12},
  month = {June},
  doi = {10.1021/es0871457},
  owner = {rojasro},
  timestamp = {2009.08.10}
}

@BOOK{bates+al2008,
  title = {Climate Change and Water},
  publisher = {{T}echnical {P}aper of the {I}ntergovernmental {P}anel on {C}limate
	{C}hange, {IPCC} {S}ecretariat},
  year = {2008},
  editor = {Bates, B. and Kundzewicz, Z. and Wu,S. and Palutikof, J.},
  pages = {210},
  address = {Geneva},
  tags = {IPCC}
}

